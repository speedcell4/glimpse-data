{
  "https://openreview.net/forum?id=Hkbd5xZRb": {
    "title": "Spherical CNNs",
    "volume": "oral",
    "abstract": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression",
    "checked": true,
    "id": "ce2845cadc5233ff0a647aa22ae3bbe646258890",
    "semantic_title": "spherical cnns",
    "citation_count": 907,
    "authors": []
  },
  "https://openreview.net/forum?id=rJTutzbA-": {
    "title": "On the insufficiency of existing momentum schemes for Stochastic Optimization",
    "volume": "oral",
    "abstract": "Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Rigorously speaking, fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there exist simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching. Furthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Acceleration. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD. The code for implementing the ASGD Algorithm can be found at https://github.com/rahulkidambi/AccSGD",
    "checked": true,
    "id": "a99f6f195d6cbc33a6259206c10d4ab0e167f969",
    "semantic_title": "on the insufficiency of existing momentum schemes for stochastic optimization",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk6kPgZA-": {
    "title": "Certifying Some Distributional Robustness with Principled Adversarial Training",
    "volume": "oral",
    "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches",
    "checked": true,
    "id": "818c52f4ba56cb8cf152ad614f2f4803057a5cfe",
    "semantic_title": "certifying some distributional robustness with principled adversarial training",
    "citation_count": 866,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwZSG-CZ": {
    "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
    "volume": "oral",
    "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity",
    "checked": true,
    "id": "ef9ddbc35676ce8ffc2a8067044473727839dbac",
    "semantic_title": "breaking the softmax bottleneck: a high-rank rnn language model",
    "citation_count": 373,
    "authors": []
  },
  "https://openreview.net/forum?id=S1JHhv6TW": {
    "title": "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions",
    "volume": "oral",
    "abstract": "The driving force behind deep networks is their ability to compactly represent rich classes of functions. The primary notion for formally reasoning about this phenomenon is expressive efficiency, which refers to a situation where one network must grow unfeasibly large in order to replicate functions of another. To date, expressive efficiency analyses focused on the architectural feature of depth, showing that deep networks are representationally superior to shallow ones. In this paper we study the expressive efficiency brought forth by connectivity, motivated by the observation that modern networks interconnect their layers in elaborate ways. We focus on dilated convolutional networks, a family of deep models delivering state of the art performance in sequence processing tasks. By introducing and analyzing the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that even a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not. Empirical evaluation demonstrates how the expressive efficiency of connectivity, similarly to that of depth, translates into gains in accuracy. This leads us to believe that expressive efficiency may serve a key role in developing new tools for deep network design",
    "checked": true,
    "id": "9593d6e790065c972668da9ee0ae6614bf63ca18",
    "semantic_title": "boosting dilated convolutional networks with mixed tensor decompositions",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy7fDog0b": {
    "title": "AmbientGAN: Generative models from lossy measurements",
    "volume": "oral",
    "abstract": "Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, current techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fully-observed samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the true underlying distribution can be provably recovered even in the presence of per-sample information loss for a class of measurement models. Based on this, we propose a new method of training Generative Adversarial Networks (GANs) which we call AmbientGAN. On three benchmark datasets, and for various measurement models, we demonstrate substantial qualitative and quantitative improvements. Generative models trained with our method can obtain $2$-$4$x higher inception scores than the baselines",
    "checked": true,
    "id": "b959d5655a3b2f92c2c1a8a7896fecafafea979d",
    "semantic_title": "ambientgan: generative models from lossy measurements",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=rkRwGg-0Z": {
    "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs",
    "volume": "oral",
    "abstract": "The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done",
    "checked": true,
    "id": "cb6866b5fa62ae3cbe21bafd772bcce7d9668dd6",
    "semantic_title": "beyond word importance: contextual decomposition to extract interactions from lstms",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=S1CChZ-CZ": {
    "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning",
    "volume": "oral",
    "abstract": "We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming",
    "checked": true,
    "id": "acbd6e09ad888e68472a5c1cb6ec83176c7969bd",
    "semantic_title": "ask the right questions: active question reformulation with reinforcement learning",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=H1tSsb-AW": {
    "title": "Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines",
    "volume": "oral",
    "abstract": "Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks",
    "checked": true,
    "id": "1bdcf6fe02ed2ff097e5f4ffdc5af159cd9a713a",
    "semantic_title": "variance reduction for policy gradient with action-dependent factorized baselines",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ8vJebC-": {
    "title": "Synthetic and Natural Noise Both Break Neural Machine Translation",
    "volume": "oral",
    "abstract": "Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise",
    "checked": true,
    "id": "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073",
    "semantic_title": "synthetic and natural noise both break neural machine translation",
    "citation_count": 744,
    "authors": []
  },
  "https://openreview.net/forum?id=HkL7n1-0b": {
    "title": "Wasserstein Auto-Encoders",
    "volume": "oral",
    "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality",
    "checked": true,
    "id": "6745c95b88ff9b12401a9ba6f4007f036be591a0",
    "semantic_title": "wasserstein auto-encoders",
    "citation_count": 1058,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk99zCeAb": {
    "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
    "volume": "oral",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset",
    "checked": true,
    "id": "744fe47157477235032f7bb3777800f9f2f45e52",
    "semantic_title": "progressive growing of gans for improved quality, stability, and variation",
    "citation_count": 7393,
    "authors": []
  },
  "https://openreview.net/forum?id=ryQu7f-RZ": {
    "title": "On the Convergence of Adam and Beyond",
    "volume": "oral",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance",
    "checked": true,
    "id": "c983653841b6987d9959318f074a595783838576",
    "semantic_title": "on the convergence of adam and beyond",
    "citation_count": 2506,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGXzmspb": {
    "title": "Training and Inference with Integers in Deep Neural Networks",
    "volume": "oral",
    "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands",
    "checked": true,
    "id": "acdf151b8efc2c6b05662d69f27531afc557dc85",
    "semantic_title": "training and inference with integers in deep neural networks",
    "citation_count": 391,
    "authors": []
  },
  "https://openreview.net/forum?id=BkisuzWRW": {
    "title": "Zero-Shot Visual Imitation",
    "volume": "oral",
    "abstract": "The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/",
    "checked": true,
    "id": "9217c4aa5a95d3209d2071e6889c8dd4b7d9309e",
    "semantic_title": "zero-shot visual imitation",
    "citation_count": 301,
    "authors": []
  },
  "https://openreview.net/forum?id=BJOFETxR-": {
    "title": "Learning to Represent Programs with Graphs",
    "volume": "oral",
    "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects",
    "checked": true,
    "id": "5f1d429ba574581ac14effe3ebab654a57dc0e39",
    "semantic_title": "learning to represent programs with graphs",
    "citation_count": 809,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk2aImxAb": {
    "title": "Multi-Scale Dense Networks for Resource Efficient Image Classification",
    "volume": "oral",
    "abstract": "In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network's prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \"easier\" and \"harder\" inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings",
    "checked": true,
    "id": "125ccd810f43f1cba83c6681836d000f83d1886d",
    "semantic_title": "multi-scale dense networks for resource efficient image classification",
    "citation_count": 766,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfXMz-Ab": {
    "title": "Neural Sketch Learning for Conditional Program Generation",
    "volume": "oral",
    "abstract": "We study the problem of generating source code in a strongly typed, Java-like programming language, given a label (for example a set of API calls or types) carrying a small amount of information about the code that is desired. The generated programs are expected to respect a `\"realistic\" relationship between programs and labels, as exemplified by a corpus of labeled programs available during training. Two challenges in such *conditional program generation* are that the generated programs must satisfy a rich set of syntactic and semantic constraints, and that source code contains many low-level features that impede learning. We address these problems by training a neural generator not on code but on *program sketches*, or models of program syntax that abstract out names and operations that do not generalize across programs. During generation, we infer a posterior distribution over sketches, then concretize samples from this distribution into type-safe programs using combinatorial techniques. We implement our ideas in a system for generating API-heavy Java code, and show that it can often predict the entire body of a method given just a few API calls or data types that appear in the method",
    "checked": true,
    "id": "d11777ca327c6d91de34d1d2ac50316b905578b2",
    "semantic_title": "neural sketch learning for conditional program generation",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk2u1g-0-": {
    "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
    "volume": "oral",
    "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest",
    "checked": true,
    "id": "6ac5eb309dd937d801d180c830377e4d551699a2",
    "semantic_title": "continuous adaptation via meta-learning in nonstationary and competitive environments",
    "citation_count": 354,
    "authors": []
  },
  "https://openreview.net/forum?id=HktK4BeCZ": {
    "title": "Learning Deep Mean Field Games for Modeling Large Population Behavior",
    "volume": "oral",
    "abstract": "We consider the problem of representing collective behavior of large populations and predicting the evolution of a population distribution over a discrete state space. A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions. We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP. This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning. Our method learns both the reward function and forward dynamics of an MFG from real data, and we report the first empirical test of a mean field game model of a real-world social media population",
    "checked": true,
    "id": "6aad7ac79c221b0b865b694ef9d05d9c91db1650",
    "semantic_title": "learning deep mean field games for modeling large population behavior",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=B1QRgziT-": {
    "title": "Spectral Normalization for Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques",
    "checked": true,
    "id": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
    "semantic_title": "spectral normalization for generative adversarial networks",
    "citation_count": 4449,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gJ1L2aW": {
    "title": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality",
    "volume": "oral",
    "abstract": "Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called `adversarial subspaces') in which adversarial examples lie. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the distinction of adversarial examples generated using state-of-the-art attacks. As a proof-of-concept, we show that a potential application of LID is to distinguish adversarial examples, and the preliminary results show that it can outperform several state-of-the-art detection measures by large margins for five attack strategies considered in this paper across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs",
    "checked": true,
    "id": "a18ada04d93981178234d9c8907fb99ea92fddcb",
    "semantic_title": "characterizing adversarial subspaces using local intrinsic dimensionality",
    "citation_count": 742,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGv1Z-AW": {
    "title": "Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input",
    "volume": "oral",
    "abstract": "The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured",
    "checked": true,
    "id": "c0617490e0ea57b28b89d4ca92e069f538609603",
    "semantic_title": "emergence of linguistic communication from referential games with symbolic and pixel input",
    "citation_count": 217,
    "authors": []
  },
  "https://openreview.net/forum?id=B18WgG-CZ": {
    "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning",
    "volume": "poster",
    "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations",
    "checked": true,
    "id": "afc2850945a871e72c245818f9bc141bd659b453",
    "semantic_title": "learning general purpose distributed sentence representations via large scale multi-task learning",
    "citation_count": 330,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ_UL-k0b": {
    "title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes",
    "volume": "poster",
    "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation",
    "checked": true,
    "id": "46b072cf918ec6f50403568a73d4347ea86b7e66",
    "semantic_title": "recasting gradient-based meta-learning as hierarchical bayes",
    "citation_count": 510,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNpifWAb": {
    "title": "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches",
    "volume": "poster",
    "abstract": "Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services",
    "checked": true,
    "id": "32760a5d2a55a586b2a9aab7278db89de065eae3",
    "semantic_title": "flipout: efficient pseudo-independent weight perturbations on mini-batches",
    "citation_count": 314,
    "authors": []
  },
  "https://openreview.net/forum?id=ry1arUgCW": {
    "title": "DORA The Explorer: Directed Outreaching Reinforcement Action-Selection",
    "volume": "poster",
    "abstract": "Exploration is a fundamental aspect of Reinforcement Learning, typically implemented using stochastic action-selection. Exploration, however, can be more efficient if directed toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality. While there are a few model-based solutions to this shortcoming, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-values improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to efficiently learn continuous MDPs. We demonstrate this by showing that our approach surpasses state of the art performance in the Freeway Atari 2600 game",
    "checked": true,
    "id": "89e9c05ea3b45628beba8134fd5c873dd55003e8",
    "semantic_title": "dora the explorer: directed outreaching reinforcement action-selection",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=SyELrEeAb": {
    "title": "Implicit Causal Models for Genome-wide Association Studies",
    "volume": "poster",
    "abstract": "Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%",
    "checked": true,
    "id": "83cffda7d9b47d0927d03fdc574a019487a3d5d8",
    "semantic_title": "implicit causal models for genome-wide association studies",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HJtEm4p6Z": {
    "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
    "volume": "poster",
    "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training an order of magnitude faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server",
    "checked": true,
    "id": "feca3f41b5b4cc13368d53f3168cc55a2420ec16",
    "semantic_title": "deep voice 3: scaling text-to-speech with convolutional sequence learning",
    "citation_count": 459,
    "authors": []
  },
  "https://openreview.net/forum?id=ryBnUWb0b": {
    "title": "Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data",
    "volume": "poster",
    "abstract": "In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy",
    "checked": true,
    "id": "83f286c386cb5572e572a03bc3297b3cf2e52827",
    "semantic_title": "predicting floor-level for 911 calls with neural networks and smartphone sensor data",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=B1jscMbAW": {
    "title": "Divide and Conquer Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5dd3e28170e4e0694e51f24d3859ff97a2314f54",
    "semantic_title": "divide and conquer networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=Hko85plCW": {
    "title": "Monotonic Chunkwise Attention",
    "volume": "poster",
    "abstract": "Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model",
    "checked": true,
    "id": "f0afdccf2903039d202085a771953a171dfd57b1",
    "semantic_title": "monotonic chunkwise attention",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=HyH9lbZAW": {
    "title": "Variational Message Passing with Structured Inference Networks",
    "volume": "poster",
    "abstract": "Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized inference similar to VAE. Finally, we derive a variational message passing algorithm to perform efficient natural-gradient inference while retaining the efficiency of the amortized inference. By simultaneously enabling structured, amortized, and natural-gradient inference for deep structured models, our method simplifies and generalizes existing methods",
    "checked": true,
    "id": "4a97e7280ac46a28900993d2cdf3db65ecd258ba",
    "semantic_title": "variational message passing with structured inference networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=HkuGJ3kCb": {
    "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations",
    "volume": "poster",
    "abstract": "Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\\em very simple}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations {\\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and text classification) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones",
    "checked": true,
    "id": "a2d407962bb1f5fcd209114f5687d4c11bf9dfad",
    "semantic_title": "all-but-the-top: simple and effective postprocessing for word representations",
    "citation_count": 311,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ33wwxRb": {
    "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data",
    "volume": "poster",
    "abstract": "Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers",
    "checked": true,
    "id": "c923208fb1e917f55e38b7c9f94f219554f398fa",
    "semantic_title": "sgd learns over-parameterized networks that provably generalize on linearly separable data",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=B1mvVm-C-": {
    "title": "Universal Agent for Disentangling Environments and Tasks",
    "volume": "poster",
    "abstract": "Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task. Hence, both environment and task specific knowledge are entangled into one framework. However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes. Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units. The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task. The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods",
    "checked": true,
    "id": "5c6ad1419289850d315646f6599303549b99aac4",
    "semantic_title": "universal agent for disentangling environments and tasks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOnmlWC-": {
    "title": "Policy Optimization by Genetic Distillation",
    "volume": "poster",
    "abstract": "Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency",
    "checked": true,
    "id": "dc1d45b529e6149bd49547fbca976a8de9847612",
    "semantic_title": "policy optimization by genetic distillation",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzRZ-WCZ": {
    "title": "Latent Space Oddity: on the Curvature of Deep Generative Models",
    "volume": "poster",
    "abstract": "Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear \"generator\" function that maps latent points into the input space. The nonlinearity of the generator implies that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and we demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalizes to other deep generative models",
    "checked": true,
    "id": "9f696b7156716c978b62a92714e7038a99f7a53c",
    "semantic_title": "latent space oddity: on the curvature of deep generative models",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=S1jBcueAb": {
    "title": "Depthwise Separable Convolutions for Neural Machine Translation",
    "volume": "poster",
    "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models",
    "checked": true,
    "id": "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83",
    "semantic_title": "depthwise separable convolutions for neural machine translation",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=H1BLjgZCb": {
    "title": "Generating Natural Adversarial Examples",
    "volume": "poster",
    "abstract": "Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers",
    "checked": true,
    "id": "3502b5ef1afb16f76bcae33db17179195bbcdaae",
    "semantic_title": "generating natural adversarial examples",
    "citation_count": 601,
    "authors": []
  },
  "https://openreview.net/forum?id=ByrZyglCb": {
    "title": "Robustness of Classifiers to Universal Perturbations: A Geometric Perspective",
    "volume": "poster",
    "abstract": "Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties",
    "checked": true,
    "id": "03522e27e398f4f5fa5e86cb2ed7e8c86a7e9bf1",
    "semantic_title": "robustness of classifiers to universal perturbations: a geometric perspective",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=H1WgVz-AZ": {
    "title": "Learning Approximate Inference Networks for Structured Prediction",
    "volume": "poster",
    "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This \"inference network\" outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups of 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \"label language model\" that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings",
    "checked": true,
    "id": "b34e44d48c35750456d50d225c026536596b83fa",
    "semantic_title": "learning approximate inference networks for structured prediction",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl63fZRb": {
    "title": "Parametrized Hierarchical Procedures for Neural Programming",
    "volume": "poster",
    "abstract": "Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite the potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parametrized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, using a program counter along with the observation to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a set of supervisor demonstrations, only some of which are annotated with the internal call structure, and apply it to efficient level-wise training of multi-level PHPs. We show in two benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural programs more accurately from smaller amounts of both annotated and unannotated demonstrations",
    "checked": true,
    "id": "f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f",
    "semantic_title": "parametrized hierarchical procedures for neural programming",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy-dQG-Rb": {
    "title": "Neural Speed Reading via Skim-RNN",
    "volume": "poster",
    "abstract": "Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives a significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models. In our experiments, we show that Skim-RNN can achieve significantly reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also shows that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs",
    "checked": true,
    "id": "1e4cfedd79a108d0d04cc498bb146e4dcd4b5f0a",
    "semantic_title": "neural speed reading via skim-rnn",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=ry8dvM-R-": {
    "title": "Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning",
    "volume": "poster",
    "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network – for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time",
    "checked": true,
    "id": "fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2",
    "semantic_title": "routing networks: adaptive selection of non-linear functions for multi-task learning",
    "citation_count": 249,
    "authors": []
  },
  "https://openreview.net/forum?id=SyzKd1bCW": {
    "title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation",
    "volume": "poster",
    "abstract": "Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function. These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models",
    "checked": true,
    "id": "682d194235ba3b573889836ba118502e8b525728",
    "semantic_title": "backpropagation through the void: optimizing control variates for black-box gradient estimation",
    "citation_count": 300,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ_wN01C-": {
    "title": "Deep Rewiring: Training very sparse deep networks",
    "volume": "poster",
    "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior",
    "checked": true,
    "id": "ccee800244908d2960830967e70ead7dd8266f7a",
    "semantic_title": "deep rewiring: training very sparse deep networks",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=H1-nGgWC-": {
    "title": "Gaussian Process Behaviour in Wide Deep Neural Networks",
    "volume": "poster",
    "abstract": "Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented",
    "checked": true,
    "id": "fb350d3b03e9308ccbd131d3d45dd44e383e6227",
    "semantic_title": "gaussian process behaviour in wide deep neural networks",
    "citation_count": 561,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e5ef-C-": {
    "title": "A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs",
    "volume": "poster",
    "abstract": "Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the \"meaning\" of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice",
    "checked": true,
    "id": "86fa6a6533e1b979eebd05bd6fb336ebf1eb99b9",
    "semantic_title": "a compressed sensing view of unsupervised text embeddings, bag-of-n-grams, and lstms",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=S1J2ZyZ0Z": {
    "title": "Interpretable Counting for Visual Question Answering",
    "volume": "poster",
    "abstract": "Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections. A distinction of our approach is its intuitive and interpretable output, as discrete counts are automatically grounded in the image. Furthermore, our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting",
    "checked": true,
    "id": "0605a012aeeee9bef773812a533c4f3cb7fa5a5f",
    "semantic_title": "interpretable counting for visual question answering",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=rylSzl-R-": {
    "title": "On Unifying Deep Generative Models",
    "volume": "poster",
    "abstract": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transfered techniques",
    "checked": true,
    "id": "c0c3ea2aa244920a206c196af3da1b81934a8c2e",
    "semantic_title": "on unifying deep generative models",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=B1al7jg0b": {
    "title": "Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation",
    "volume": "poster",
    "abstract": "Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed",
    "checked": true,
    "id": "0f57fdef846f5d033063c061c24191762e543f18",
    "semantic_title": "overcoming catastrophic interference using conceptor-aided backpropagation",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=HyWrIgW0W": {
    "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
    "volume": "poster",
    "abstract": "Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix",
    "checked": true,
    "id": "940912cfc9190f5cc79e3867060e543634b6b22e",
    "semantic_title": "stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
    "citation_count": 304,
    "authors": []
  },
  "https://openreview.net/forum?id=HyUNwulC-": {
    "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. We develop a parallel linear recurrence CUDA kernel and show that it can be applied to immediately speed up training and inference of several state of the art RNN architectures by up to 9x. We abstract recent work on linear RNNs into a new framework of linear surrogate RNNs and develop a linear surrogate model for the long short-term memory unit, the GILR-LSTM, that utilizes parallel linear recurrence. We extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training a GILR-LSTM on a synthetic sequence classification task with a one million timestep dependency",
    "checked": true,
    "id": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498",
    "semantic_title": "parallelizing linear recurrent neural nets over sequence length",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgNdt26Z": {
    "title": "Distributed Fine-tuning of Language Models on Private Data",
    "volume": "poster",
    "abstract": "One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm. In language modeling, users' language (e.g. in private messaging) could change in a year and be completely different from what we observe in publicly available data. At the same time, public data can be used for obtaining general knowledge (i.e. general model of English). We study approaches to distributed fine-tuning of a general model on user private data with the additional requirements of maintaining the quality on the general data and minimization of communication costs. We propose a novel technique that significantly improves prediction quality on users' language compared to a general model and outperforms gradient compression methods in terms of communication efficiency. The proposed procedure is fast and leads to an almost 70% perplexity reduction and 8.7 percentage point improvement in keystroke saving rate on informal English texts. Finally, we propose an experimental framework for evaluating differential privacy of distributed training of language models and show that our approach has good privacy guarantees",
    "checked": true,
    "id": "947b4752338a4fc44003acce97bccf99be49549a",
    "semantic_title": "distributed fine-tuning of language models on private data",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=rk49Mg-CW": {
    "title": "Stochastic Variational Video Prediction",
    "volume": "poster",
    "abstract": "Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication",
    "checked": true,
    "id": "59d86da5c5936e7a236678bf5eaaa7753c226fb1",
    "semantic_title": "stochastic variational video prediction",
    "citation_count": 543,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk9yuql0Z": {
    "title": "Mitigating Adversarial Effects Through Randomization",
    "volume": "poster",
    "abstract": "Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017_adv_challenge_defense",
    "checked": true,
    "id": "9a089c56eec68df722b2a5a52727143aacdc2532",
    "semantic_title": "mitigating adversarial effects through randomization",
    "citation_count": 1067,
    "authors": []
  },
  "https://openreview.net/forum?id=B12Js_yRb": {
    "title": "Learning to Count Objects in Natural Images for Visual Question Answering",
    "volume": "poster",
    "abstract": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%",
    "checked": true,
    "id": "30a3eee5e9302108416f6234d739373dde68d373",
    "semantic_title": "learning to count objects in natural images for visual question answering",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=SyyGPP0TZ": {
    "title": "Regularizing and Optimizing LSTM Language Models",
    "volume": "poster",
    "abstract": "In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at https://github.com/salesforce/awd-lstm-lm",
    "checked": true,
    "id": "58c6f890a1ae372958b7decf56132fe258152722",
    "semantic_title": "regularizing and optimizing lstm language models",
    "citation_count": 1098,
    "authors": []
  },
  "https://openreview.net/forum?id=B1zlp1bRW": {
    "title": "Large Scale Optimal Transport and Mapping Estimation",
    "volume": "poster",
    "abstract": "This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. This parameterization allows generalization of the mapping outside the support of the input measure. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling",
    "checked": true,
    "id": "2140bc28c3bb4982edef1af0753174c7c7aa4b79",
    "semantic_title": "large scale optimal transport and mapping estimation",
    "citation_count": 249,
    "authors": []
  },
  "https://openreview.net/forum?id=S1nQvfgA-": {
    "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs",
    "checked": true,
    "id": "d35534f3f59631951011539da2fe83f2844ca245",
    "semantic_title": "semantically decomposing the latent spaces of generative adversarial networks",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy6GHpkCW": {
    "title": "A Neural Representation of Sketch Drawings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "37fa040ec0c4bc1b85f3ca2929445f3229ed7f72",
    "semantic_title": "a neural representation of sketch drawings",
    "citation_count": 870,
    "authors": []
  },
  "https://openreview.net/forum?id=S1p31z-Ab": {
    "title": "Deep contextualized word representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3febb2bed8865945e7fddc99efd791887bb7e14f",
    "semantic_title": "deep contextualized word representations",
    "citation_count": 11569,
    "authors": []
  },
  "https://openreview.net/forum?id=SyOK1Sg0W": {
    "title": "Adaptive Quantization of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "accfaca7b884964582843a4cc9bb9dc0f3131528",
    "semantic_title": "adaptive quantization of neural networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gs9JgRZ": {
    "title": "Mixed Precision Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
    "semantic_title": "mixed precision training",
    "citation_count": 1811,
    "authors": []
  },
  "https://openreview.net/forum?id=B1EA-M-0Z": {
    "title": "Deep Neural Networks as Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "075556dd42900a6bc4552a2f2531ba21b9b7b4c0",
    "semantic_title": "deep neural networks as gaussian processes",
    "citation_count": 1100,
    "authors": []
  },
  "https://openreview.net/forum?id=S19dR9x0b": {
    "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "be83160b94893f2ef5edd5e68a3f3f8b089a87fd",
    "semantic_title": "alternating multi-bit quantization for recurrent neural networks",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=BJk7Gf-CZ": {
    "title": "Global Optimality Conditions for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "414915126855cab6a30fdcec635d90e793137c3b",
    "semantic_title": "global optimality conditions for deep neural networks",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOfBggRZ": {
    "title": "Detecting Statistical Interactions from Neural Network Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f85a8eaa7a1a1686f5a2bf721c63e337f03d8eb",
    "semantic_title": "detecting statistical interactions from neural network weights",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy21R9JAW": {
    "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "de01f74a899ecc7e3228cddbc743aaf6faf5e55f",
    "semantic_title": "towards better understanding of gradient-based attribution methods for deep neural networks",
    "citation_count": 891,
    "authors": []
  },
  "https://openreview.net/forum?id=BJNRFNlRW": {
    "title": "TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c837f6bb76af151145df50207992ed3848489bdb",
    "semantic_title": "training generative adversarial networks via primal-dual subgradient methods: a lagrangian perspective on gan",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=rkfOvGbCW": {
    "title": "Memory-based Parameter Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0be49527df4869a0132f5cbc8d4cfa3304ab5843",
    "semantic_title": "memory-based parameter adaptation",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzbhfWRW": {
    "title": "Learn to Pay Attention",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c70218603f0af1be5d063056cbe629e042141a86",
    "semantic_title": "learn to pay attention",
    "citation_count": 439,
    "authors": []
  },
  "https://openreview.net/forum?id=SJi9WOeRb": {
    "title": "Gradient Estimators for Implicit Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0007812ab0a7472f0d74802f00692a64117117f9",
    "semantic_title": "gradient estimators for implicit models",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=BkwHObbRZ": {
    "title": "Learning One-hidden-layer Neural Networks with Landscape Design",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "136d8b08d38a5c4829449b87bad6bad675ca8c71",
    "semantic_title": "learning one-hidden-layer neural networks with landscape design",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=ry6-G_66b": {
    "title": "Active Neural Localization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "80f9f597992dc4f77911fa6a94a4611f5daafa5d",
    "semantic_title": "active neural localization",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=HkZy-bW0-": {
    "title": "Temporally Efficient Deep Learning with Spikes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c0ef9d2fe95232a15cc459cad5caf0c7e68d1d55",
    "semantic_title": "temporally efficient deep learning with spikes",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=S1HlA-ZAZ": {
    "title": "The Kanerva Machine: A Generative Distributed Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5b2c83f41eacdf95e8b38300d2926ac37ea4709e",
    "semantic_title": "the kanerva machine: a generative distributed memory",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=SkZxCk-0Z": {
    "title": "Can Neural Networks Understand Logical Entailment?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bdea8b6ceabaeb86bd23c2d2585da1ff3858d968",
    "semantic_title": "can neural networks understand logical entailment?",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=SJLlmG-AZ": {
    "title": "Understanding image motion with group representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "869ef3835e5da24b4597a757948f1c0e19fa546d",
    "semantic_title": "understanding image motion with group representations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=S1D8MPxA-": {
    "title": "Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad3e968a38a5b2d1e72997f978c556a06f625e48",
    "semantic_title": "viterbi-based pruning for sparse matrix with fixed and high index compression ratio",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=SkFAWax0-": {
    "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e8f697f19c5d7d95f859e1ad6bcc219a902f52b",
    "semantic_title": "voiceloop: voice fitting and synthesis via a phonological loop",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=BJj6qGbRW": {
    "title": "Few-Shot Learning with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "572a1f77306e160c3893299c18f3ed862fb5f6d9",
    "semantic_title": "few-shot learning with graph neural networks",
    "citation_count": 1241,
    "authors": []
  },
  "https://openreview.net/forum?id=BkXmYfbAZ": {
    "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4f3375dab1886f37f542d998e61d8c30a927682",
    "semantic_title": "beyond shared hierarchies: deep multitask learning through soft layer ordering",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=BJE-4xW0W": {
    "title": "CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "329f14d68f8e4cbc3ee5109e70c76c6f3b63a19a",
    "semantic_title": "causalgan: learning causal implicit generative models with adversarial training",
    "citation_count": 257,
    "authors": []
  },
  "https://openreview.net/forum?id=ryUlhzWCZ": {
    "title": "TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d07fe76aa2ca2337c8aa6001c8cfe296fa824109",
    "semantic_title": "truncated horizon policy search: combining reinforcement learning & imitation learning",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=HkXWCMbRW": {
    "title": "Towards Image Understanding from Deep Compression Without Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e5604c3f61eb7e8b80bf423f7828d8c1fa0f1d32",
    "semantic_title": "towards image understanding from deep compression without decoding",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Xw62kRZ": {
    "title": "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dea6aeb514b1969ab879c793d46a0d2eceaa2cbf",
    "semantic_title": "leveraging grammar and reinforcement learning for neural program synthesis",
    "citation_count": 220,
    "authors": []
  },
  "https://openreview.net/forum?id=By4HsfWAZ": {
    "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2581b3e44592b3b3741474c8d6f483a90c29f139",
    "semantic_title": "deep learning for physical processes: incorporating prior scientific knowledge",
    "citation_count": 320,
    "authors": []
  },
  "https://openreview.net/forum?id=B17JTOe0-": {
    "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a538579ac50d659ac0bca9824d6446e741c586b3",
    "semantic_title": "emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=Byt3oJ-0W": {
    "title": "Learning Latent Permutations with Gumbel-Sinkhorn Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23deae1e548bc055c8920fc8529e59bba0816685",
    "semantic_title": "learning latent permutations with gumbel-sinkhorn networks",
    "citation_count": 272,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl3yM-Ab": {
    "title": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8e46f7fbb96549cd1b3b0bd226f06a611126b889",
    "semantic_title": "evidence aggregation for answer re-ranking in open-domain question answering",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ1Xmf-Rb": {
    "title": "FearNet: Brain-Inspired Model for Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e77d37e7fbfe90f58e4e96e8198903f364d6402",
    "semantic_title": "fearnet: brain-inspired model for incremental learning",
    "citation_count": 482,
    "authors": []
  },
  "https://openreview.net/forum?id=H15odZ-C-": {
    "title": "Semantic Interpolation in Implicit Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9301b04a76a03344b5e2ccb2ccfae6aa2d99e487",
    "semantic_title": "semantic interpolation in implicit models",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=rkHVZWZAZ": {
    "title": "The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee1ac4a86cafa34e4905327f3436fea2d5994fc8",
    "semantic_title": "the reactor: a fast and sample-efficient actor-critic agent for reinforcement learning",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=H135uzZ0-": {
    "title": "Mixed Precision Training of Convolutional Neural Networks using Integer Operations",
    "volume": "poster",
    "abstract": "The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017). On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware. In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations. The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator. We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput. To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision",
    "checked": true,
    "id": "eae2eba75cc90990ff31d94d1942ffdea7c452b8",
    "semantic_title": "mixed precision training of convolutional neural networks using integer operations",
    "citation_count": 154,
    "authors": []
  },
  "https://openreview.net/forum?id=rJvJXZb0W": {
    "title": "An efficient framework for learning sentence representations",
    "volume": "poster",
    "abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time",
    "checked": true,
    "id": "ad31866da7f14ae21bd38df0a3b1ffd1a1438122",
    "semantic_title": "an efficient framework for learning sentence representations",
    "citation_count": 544,
    "authors": []
  },
  "https://openreview.net/forum?id=ry80wMW0W": {
    "title": "Hierarchical Subtask Discovery with Non-Negative Matrix Factorization",
    "volume": "poster",
    "abstract": "Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks. In this setting, the subtask discovery problem can naturally be posed as finding an optimal low-rank approximation of the set of tasks the agent will face in a domain. We use non-negative matrix factorization to discover this minimal basis set of tasks, and show that the technique learns intuitive decompositions in a variety of domains. Our method has several qualitatively desirable features: it is not limited to learning subtasks with single goal states, instead learning distributed patterns of preferred states; it learns qualitatively different hierarchical decompositions in the same domain depending on the ensemble of tasks the agent will face; and it may be straightforwardly iterated to obtain deeper hierarchical decompositions",
    "checked": true,
    "id": "0df639c7ecf49371e5786ec2d628e63b845116a2",
    "semantic_title": "hierarchical subtask discovery with non-negative matrix factorization",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ_aoCyRZ": {
    "title": "SpectralNet: Spectral Clustering using Deep Neural Networks",
    "volume": "poster",
    "abstract": "Spectral clustering is a leading and popular technique in unsupervised data analysis. Two of its major limitations are scalability and generalization of the spectral embedding (i.e., out-of-sample-extension). In this paper we introduce a deep learning approach to spectral clustering that overcomes the above shortcomings. Our network, which we call SpectralNet, learns a map that embeds input data points into the eigenspace of their associated graph Laplacian matrix and subsequently clusters them. We train SpectralNet using a procedure that involves constrained stochastic optimization. Stochastic optimization allows it to scale to large datasets, while the constraints, which are implemented using a special purpose output layer, allow us to keep the network output orthogonal. Moreover, the map learned by SpectralNet naturally generalizes the spectral embedding to unseen data points. To further improve the quality of the clustering, we replace the standard pairwise Gaussian affinities with affinities leaned from unlabeled data using a Siamese network. Additional improvement can be achieved by applying the network to code representations produced, e.g., by standard autoencoders. Our end-to-end learning procedure is fully unsupervised. In addition, we apply VC dimension theory to derive a lower bound on the size of SpectralNet. State-of-the-art clustering results are reported for both the MNIST and Reuters datasets",
    "checked": true,
    "id": "40cec58d760940f62975e0d5e99632e28df0b5d0",
    "semantic_title": "spectralnet: spectral clustering using deep neural networks",
    "citation_count": 287,
    "authors": []
  },
  "https://openreview.net/forum?id=H1sUHgb0Z": {
    "title": "Learning From Noisy Singly-labeled Data",
    "volume": "poster",
    "abstract": "Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply) when worker quality exceeds a threshold. Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits",
    "checked": true,
    "id": "a4a8e91995ae8c8b203dd857bdc0915facddeebe",
    "semantic_title": "learning from noisy singly-labeled data",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=HJvvRoe0W": {
    "title": "An image representation based convolutional network for DNA classification",
    "volume": "poster",
    "abstract": "The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that takes an image-representation of primary DNA sequence as its input, and predicts key determinants of chromatin structure. The method is developed such that it is capable of detecting interactions between distal elements in the DNA sequence, which are known to be highly relevant. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time",
    "checked": true,
    "id": "bc0ac9cff977ec202a09d49eeca8ee6a59bf5553",
    "semantic_title": "an image representation based convolutional network for dna classification",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=S1uxsye0Z": {
    "title": "Adaptive Dropout with Rademacher Complexity Regularization",
    "volume": "poster",
    "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms",
    "checked": true,
    "id": "871d5b3cffda3e7a8d4493a0fa722297e74ec346",
    "semantic_title": "adaptive dropout with rademacher complexity regularization",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=SJQHjzZ0-": {
    "title": "Quantitatively Evaluating GANs With Divergences Proposed for Training",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) have been extremely effective in approximating complex distributions of high-dimensional, input data samples, and substantial progress has been made in understanding and improving GAN performance in terms of both theory and application. However, we currently lack quantitative methods for model assessment. Because of this, while many GAN variants being proposed, we have relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics and, interestingly, the test-time metrics do not favour networks that use the same training-time criterion. We also compare the proposed metrics to human perceptual scores",
    "checked": true,
    "id": "c9a201ffabe48f35226468617f783b3d7a8f0086",
    "semantic_title": "quantitatively evaluating gans with divergences proposed for training",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=ryup8-WCW": {
    "title": "Measuring the Intrinsic Dimension of Objective Landscapes",
    "volume": "poster",
    "abstract": "Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times",
    "checked": true,
    "id": "d55d1d035e91220335edff0fe8f5d249d8c4a00b",
    "semantic_title": "measuring the intrinsic dimension of objective landscapes",
    "citation_count": 417,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk7KsfW0-": {
    "title": "Lifelong Learning with Dynamically Expandable Networks",
    "volume": "poster",
    "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters",
    "checked": true,
    "id": "df1769afbbf3904877629dd7e785f195361ec531",
    "semantic_title": "lifelong learning with dynamically expandable networks",
    "citation_count": 1233,
    "authors": []
  },
  "https://openreview.net/forum?id=S1vuO-bCW": {
    "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning",
    "volume": "poster",
    "abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states",
    "checked": true,
    "id": "ec4df801c640169e18d8da1bdc65a0b6fc2d3d94",
    "semantic_title": "leave no trace: learning to reset for safe and autonomous reinforcement learning",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy8XvGb0-": {
    "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models",
    "volume": "poster",
    "abstract": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal \"realism\" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function",
    "checked": true,
    "id": "76ec5c774bb3fd04f9e68864a411286536a544c5",
    "semantic_title": "latent constraints: learning to generate conditionally from unconditional generative models",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=H1mCp-ZRZ": {
    "title": "Action-dependent Control Variates for Policy Optimization via Stein Identity",
    "volume": "poster",
    "abstract": "Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches",
    "checked": true,
    "id": "ad9f32abf3894332c9dccca93e4ac64fbb81c41f",
    "semantic_title": "action-dependent control variates for policy optimization via stein identity",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MczcgR-": {
    "title": "Understanding Short-Horizon Bias in Stochastic Meta-Optimization",
    "volume": "poster",
    "abstract": "Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes",
    "checked": true,
    "id": "9cb2a7e69daaefbe84e98cab721aa7db23ead5d6",
    "semantic_title": "understanding short-horizon bias in stochastic meta-optimization",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=BkLhaGZRW": {
    "title": "Improving GAN Training via Binarized Representation Entropy (BRE) Regularization",
    "volume": "poster",
    "abstract": "We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse, which helps G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D . Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning",
    "checked": true,
    "id": "9f074217d51ffb0da3b9716af4adae56215de488",
    "semantic_title": "improving gan training via binarized representation entropy (bre) regularization",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rkZvSe-RZ": {
    "title": "Ensemble Adversarial Training: Attacks and Defenses",
    "volume": "poster",
    "abstract": "Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks",
    "checked": true,
    "id": "136dee73f203df2f4831994bf4f0c0a4ad2e764e",
    "semantic_title": "ensemble adversarial training: attacks and defenses",
    "citation_count": 2738,
    "authors": []
  },
  "https://openreview.net/forum?id=Syhr6pxCW": {
    "title": "PixelNN: Example-based Image Synthesis",
    "volume": "poster",
    "abstract": "We present a simple nearest-neighbor (NN) approach that synthesizes high-frequency photorealistic images from an ``incomplete'' signal such as a low-resolution image, a surface normal map, or edges. Current state-of-the-art deep generative models designed for such conditional image synthesis lack two important things: (1) they are unable to generate a large set of diverse outputs, due to the mode collapse problem. (2) they are not interpretable, making it difficult to control the synthesized output. We demonstrate that NN approaches potentially address such limitations, but suffer in accuracy on small datasets. We design a simple pipeline that combines the best of both worlds: the first stage uses a convolutional neural network (CNN) to map the input to a (overly-smoothed) image, and the second stage uses a pixel-wise nearest neighbor method to map the smoothed output to multiple high-quality, high-frequency outputs in a controllable manner. Importantly, pixel-wise matching allows our method to compose novel high-frequency content by cutting-and-pasting pixels from different training exemplars. We demonstrate our approach for various input modalities, and for various domains ranging from human faces, pets, shoes, and handbags",
    "checked": true,
    "id": "731f0d42b1679aa6c89693ff62d41f74f2519124",
    "semantic_title": "pixelnn: example-based image synthesis",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ0hF1Z0b": {
    "title": "Learning Differentially Private Recurrent Language Models",
    "volume": "poster",
    "abstract": "We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset",
    "checked": true,
    "id": "ed46493d568030b42f0154d9e5bf39bbd07962b3",
    "semantic_title": "learning differentially private recurrent language models",
    "citation_count": 1289,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Lc-Gb0Z": {
    "title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem",
    "volume": "poster",
    "abstract": "As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator",
    "checked": true,
    "id": "af43a2517dd0c75d20b4bff5c4989bd946b68d84",
    "semantic_title": "deep learning as a mixed convex-combinatorial optimization problem",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkn7CBaTW": {
    "title": "Learning how to explain neural networks: PatternNet and PatternAttribution",
    "volume": "poster",
    "abstract": "DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks",
    "checked": true,
    "id": "ca2ed3e2e0514feb6f613b9875f3d98ade1b1dc1",
    "semantic_title": "learning how to explain neural networks: patternnet and patternattribution",
    "citation_count": 340,
    "authors": []
  },
  "https://openreview.net/forum?id=rkQkBnJAb": {
    "title": "Improving GANs Using Optimal Transport",
    "volume": "poster",
    "abstract": "We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation",
    "checked": true,
    "id": "69902406e7d08f8865f02185699978db499d25e7",
    "semantic_title": "improving gans using optimal transport",
    "citation_count": 324,
    "authors": []
  },
  "https://openreview.net/forum?id=SyZI0GWCZ": {
    "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",
    "volume": "poster",
    "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox)",
    "checked": true,
    "id": "1b225474e7a5794f98cdfbde8b12ccbc56799409",
    "semantic_title": "decision-based adversarial attacks: reliable attacks against black-box machine learning models",
    "citation_count": 1352,
    "authors": []
  },
  "https://openreview.net/forum?id=HJhIM0xAW": {
    "title": "Learning a neural response metric for retinal prosthesis",
    "volume": "poster",
    "abstract": "Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons, causing them to send artificial visual signals to the brain. However, electrical stimulation generally cannot precisely reproduce normal patterns of neural activity in the retina. Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response. This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed. Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina. The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input. Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis",
    "checked": true,
    "id": "440b22ccdc7990c65d948856f9fbf8ca762745ab",
    "semantic_title": "learning a neural response metric for retinal prosthesis",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SkT5Yg-RZ": {
    "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
    "volume": "poster",
    "abstract": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward",
    "checked": true,
    "id": "8499a250422a3c66357367c8d5fa504de5424c59",
    "semantic_title": "intrinsic motivation and automatic curricula via asymmetric self-play",
    "citation_count": 338,
    "authors": []
  },
  "https://openreview.net/forum?id=rydeCEhs-": {
    "title": "SMASH: One-Shot Model Architecture Search through HyperNetworks",
    "volume": "poster",
    "abstract": "Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks",
    "checked": true,
    "id": "e56b10f7cd4bf037beac84da5925dc4544fab974",
    "semantic_title": "smash: one-shot model architecture search through hypernetworks",
    "citation_count": 765,
    "authors": []
  },
  "https://openreview.net/forum?id=HJWLfGWRb": {
    "title": "Matrix capsules with EM routing",
    "volume": "poster",
    "abstract": "A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network",
    "checked": true,
    "id": "603caed9430283db6c7f43169555c8d18e97a281",
    "semantic_title": "matrix capsules with em routing",
    "citation_count": 1054,
    "authors": []
  },
  "https://openreview.net/forum?id=HyRVBzap-": {
    "title": "Cascade Adversarial Machine Learning Regularized with a Unified Embedding",
    "volume": "poster",
    "abstract": "Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario",
    "checked": true,
    "id": "45a710be199c8eb43f465c88fc4b343267c35d38",
    "semantic_title": "cascade adversarial machine learning regularized with a unified embedding",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=SJiHXGWAZ": {
    "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
    "volume": "poster",
    "abstract": "Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines",
    "checked": true,
    "id": "9ba0186ed40656329c421f55ada7313293e13f17",
    "semantic_title": "diffusion convolutional recurrent neural network: data-driven traffic forecasting",
    "citation_count": 3105,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgOLb-0W": {
    "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
    "volume": "poster",
    "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks",
    "checked": true,
    "id": "f1cbf097ce436f7304a1984f4a29ab41f75ebfe3",
    "semantic_title": "neural language modeling by jointly learning syntax and lexicon",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ZvaaeAZ": {
    "title": "WRPN: Wide Reduced-Precision Networks",
    "volume": "poster",
    "abstract": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory band- width and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN -- wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks",
    "checked": true,
    "id": "b18b2f8f6e55789b03f983f02f33283c39e6e36a",
    "semantic_title": "wrpn: wide reduced-precision networks",
    "citation_count": 267,
    "authors": []
  },
  "https://openreview.net/forum?id=HkMvEOlAb": {
    "title": "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f4d2e8dd636d04de745d0cfc459b5d50a0df53ae",
    "semantic_title": "learning latent representations in neural networks for clustering through pseudo supervision and graph-based activity regularization",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=SJyEH91A-": {
    "title": "Learning Wasserstein Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d7629df50f640e385dcbafb1735df4d0370fd06b",
    "semantic_title": "learning wasserstein embeddings",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=ryTp3f-0-": {
    "title": "Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1",
    "semantic_title": "reinforcement learning on web interfaces using workflow-guided exploration",
    "citation_count": 223,
    "authors": []
  },
  "https://openreview.net/forum?id=HkAClQgA-": {
    "title": "A Deep Reinforced Model for Abstractive Summarization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
    "semantic_title": "a deep reinforced model for abstractive summarization",
    "citation_count": 1560,
    "authors": []
  },
  "https://openreview.net/forum?id=By-7dz-AZ": {
    "title": "A Framework for the Quantitative Evaluation of Disentangled Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "adf2ac6b99b7d48b6a9c908532ca249de2cec3ae",
    "semantic_title": "a framework for the quantitative evaluation of disentangled representations",
    "citation_count": 470,
    "authors": []
  },
  "https://openreview.net/forum?id=ByKWUeWA-": {
    "title": "GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d5f51bd63d6efe03ff95f01ca517bc5d29bfbbf0",
    "semantic_title": "ganite: estimation of individualized treatment effects using generative adversarial nets",
    "citation_count": 399,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ1nzBeA-": {
    "title": "Multi-Task Learning for Document Ranking and Query Suggestion",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15e12b3b6719e8e4516b7516fae212ea61a94fce",
    "semantic_title": "multi-task learning for document ranking and query suggestion",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=H1aIuk-RW": {
    "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c342c71cb23199f112d0bc644fcce56a7306bf94",
    "semantic_title": "active learning for convolutional neural networks: a core-set approach",
    "citation_count": 1993,
    "authors": []
  },
  "https://openreview.net/forum?id=S1v4N2l0-": {
    "title": "Unsupervised Representation Learning by Predicting Image Rotations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aab368284210c1bb917ec2d31b84588e3d2d7eb4",
    "semantic_title": "unsupervised representation learning by predicting image rotations",
    "citation_count": 3304,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkc-TeZ0W": {
    "title": "A Hierarchical Model for Device Placement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "22ba3276c8797fddeb5d9db083ed1010da182549",
    "semantic_title": "a hierarchical model for device placement",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=SyJS-OgR-": {
    "title": "Multi-level Residual Networks from Dynamical Systems View",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cd958525291ee1ab856d23aa93cb95c86d87ccbe",
    "semantic_title": "multi-level residual networks from dynamical systems view",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=r1ZdKJ-0W": {
    "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b76b6e766547b3c6dbc2785a084ec3b72cb760d",
    "semantic_title": "deep gaussian embedding of graphs: unsupervised inductive learning via ranking",
    "citation_count": 648,
    "authors": []
  },
  "https://openreview.net/forum?id=r1dHXnH6-": {
    "title": "Natural Language Inference over Interaction Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1778e32c18bd611169e64c1805a51abff341ca53",
    "semantic_title": "natural language inference over interaction space",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=HyydRMZC-": {
    "title": "Spatially Transformed Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d3c071dbbb4520ed5875f7e064a9da87240534db",
    "semantic_title": "spatially transformed adversarial examples",
    "citation_count": 524,
    "authors": []
  },
  "https://openreview.net/forum?id=S18Su--CW": {
    "title": "Thermometer Encoding: One Hot Way To Resist Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b9127bee0f7d109da2672ba06d0f39a5a60335a",
    "semantic_title": "thermometer encoding: one hot way to resist adversarial examples",
    "citation_count": 614,
    "authors": []
  },
  "https://openreview.net/forum?id=SkhQHMW0W": {
    "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "92495abbac86394cb759bec15a763dbf49a8e590",
    "semantic_title": "deep gradient compression: reducing the communication bandwidth for distributed training",
    "citation_count": 1413,
    "authors": []
  },
  "https://openreview.net/forum?id=BkN_r2lR-": {
    "title": "Identifying Analogies Across Domains",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "411500f7aaa0f3c00d492b78b24b33da0fd0d58d",
    "semantic_title": "identifying analogies across domains",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJQVZW0b": {
    "title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ebc3bd9cd67193b3a8f84d43d5b4377107c680dc",
    "semantic_title": "hierarchical and interpretable skill acquisition in multi-task reinforcement learning",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=rJm7VfZA-": {
    "title": "Learning Parametric Closed-Loop Policies for Markov Potential Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f56eadb6312770fbd190ac7b0f86b70bd928ba7",
    "semantic_title": "learning parametric closed-loop policies for markov potential games",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=rkpoTaxA-": {
    "title": "Self-ensembling for visual domain adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c012e5b3ddb8a60420e8f92162d32ad135f9ba1",
    "semantic_title": "self-ensembling for visual domain adaptation",
    "citation_count": 521,
    "authors": []
  },
  "https://openreview.net/forum?id=rywHCPkAW": {
    "title": "Noisy Networks For Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
    "semantic_title": "noisy networks for exploration",
    "citation_count": 897,
    "authors": []
  },
  "https://openreview.net/forum?id=rkmu5b0a-": {
    "title": "MGAN: Training Generative Adversarial Nets with Multiple Generators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e152b726390ebbedf94e74dcc4dc943d121f1c3e",
    "semantic_title": "mgan: training generative adversarial nets with multiple generators",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=B13njo1R-": {
    "title": "Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "714bb2c21605cde034334aa9e345a9b19bc44cdf",
    "semantic_title": "progressive reinforcement learning with distillation for multi-skilled motion control",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=S1XolQbRW": {
    "title": "Model compression via distillation and quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6a4bf043af1a9ec7f104a7b7ab56806b241ceda",
    "semantic_title": "model compression via distillation and quantization",
    "citation_count": 734,
    "authors": []
  },
  "https://openreview.net/forum?id=B1IDRdeCW": {
    "title": "The High-Dimensional Geometry of Binary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "241ceceaff111fcdd91fdbd0538e7ab9b055aee0",
    "semantic_title": "the high-dimensional geometry of binary neural networks",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=SJaP_-xAb": {
    "title": "Deep Learning with Logged Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "52782e1865611a374f8938fc83e735525e2253da",
    "semantic_title": "deep learning with logged bandit feedback",
    "citation_count": 144,
    "authors": []
  },
  "https://openreview.net/forum?id=ryiAv2xAZ": {
    "title": "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "36653f8705b56e39642bcd123494eb680cd1636b",
    "semantic_title": "training confidence-calibrated classifiers for detecting out-of-distribution samples",
    "citation_count": 885,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeqO7x0-": {
    "title": "Unsupervised Cipher Cracking Using Discrete GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b589b9f10258efb5404f06693c659d68e67929d3",
    "semantic_title": "unsupervised cipher cracking using discrete gans",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=HktRlUlAZ": {
    "title": "Polar Transformer Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "54181b2d8daf5cc3649ad4b499c464dd48a0c63b",
    "semantic_title": "polar transformer networks",
    "citation_count": 187,
    "authors": []
  },
  "https://openreview.net/forum?id=B1X0mzZCW": {
    "title": "Fidelity-Weighted Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "19ff02bf17285335894b0a3af137c4c2fc6fbd4d",
    "semantic_title": "fidelity-weighted learning",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=S1cZsf-RW": {
    "title": "WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "41e6727bd2163b2d094873701f919db315f45001",
    "semantic_title": "whai: weibull hybrid autoencoding inference for deep topic modeling",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ae1lZRb": {
    "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b8d67593c4ab1b1e3eccc158daee76703b328aa",
    "semantic_title": "apprentice: using knowledge distillation techniques to improve low-precision network accuracy",
    "citation_count": 331,
    "authors": []
  },
  "https://openreview.net/forum?id=B1J_rgWRW": {
    "title": "Understanding Deep Neural Networks with Rectified Linear Units",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9375729d21a344a5ccccd5f53556ddf90b957cd9",
    "semantic_title": "understanding deep neural networks with rectified linear units",
    "citation_count": 644,
    "authors": []
  },
  "https://openreview.net/forum?id=r1vuQG-CW": {
    "title": "HexaConv",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d5cae4b8c3ee0abf350302c36648b353f167871",
    "semantic_title": "hexaconv",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=HyrCWeWCb": {
    "title": "Trust-PCL: An Off-Policy Trust Region Method for Continuous Control",
    "volume": "poster",
    "abstract": "Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO",
    "checked": null,
    "id": "bc8eb0c66f8977d3b23dedb247607a8af2360859",
    "semantic_title": "trust-pcl: an off-policy trust region method for continuous control",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=r1NYjfbR-": {
    "title": "Generative networks as inverse problems with Scattering transforms",
    "volume": "poster",
    "abstract": "Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood. We compute deep convolutional network generators by inverting a fixed embedding operator. Therefore, they do not require to be optimized with a discriminator or an encoder. The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images. This embedding is computed with a wavelet Scattering transform. Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder",
    "checked": true,
    "id": "2b91f9dff3d5b21411f3f88a59f5e55450d15312",
    "semantic_title": "generative networks as inverse problems with scattering transforms",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HJsjkMb0Z": {
    "title": "i-RevNet: Deep Invertible Networks",
    "volume": "poster",
    "abstract": "It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the $i$-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNet's learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the $i$-RevNet we reconstruct linear interpolations between natural image representations",
    "checked": null,
    "id": "11e7c4182a7813d5acf1be198c8c96d164fb95a2",
    "semantic_title": "i-revnet: deep invertible networks",
    "citation_count": 333,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJySbbAZ": {
    "title": "Training GANs with Optimism",
    "volume": "poster",
    "abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam",
    "checked": true,
    "id": "63f2a1289ff16ca84cca383c7687452bf61c82c6",
    "semantic_title": "training gans with optimism",
    "citation_count": 519,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Yy1BxCZ": {
    "title": "Don't Decay the Learning Rate, Increase the Batch Size",
    "volume": "poster",
    "abstract": "It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $\\epsilon$ and scaling the batch size $B \\propto \\epsilon$. Finally, one can increase the momentum coefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to 76.1% validation accuracy in under 30 minutes",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryH20GbRW": {
    "title": "Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions",
    "volume": "poster",
    "abstract": "Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely unsupervised fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1DmUzWAW": {
    "title": "A Simple Neural Attentive Meta-Learner",
    "volume": "poster",
    "abstract": "Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins",
    "checked": null,
    "id": "7e9c1e0d247b20a0683f4797d9ea248c3b53d424",
    "semantic_title": "a simple neural attentive meta-learner",
    "citation_count": 1276,
    "authors": []
  },
  "https://openreview.net/forum?id=BJGWO9k0Z": {
    "title": "Critical Percolation as a Framework to Analyze the Training of Deep Networks",
    "volume": "poster",
    "abstract": "In this paper we approach two relevant deep learning topics: i) tackling of graph structured input data and ii) a better understanding and analysis of deep networks and related learning algorithms. With this in mind we focus on the topological classification of reachability in a particular subset of planar graphs (Mazes). Doing so, we are able to model the topology of data while staying in Euclidean space, thus allowing its processing with standard CNN architectures. We suggest a suitable architecture for this problem and show that it can express a perfect solution to the classification task. The shape of the cost function around this solution is also derived and, remarkably, does not depend on the size of the maze in the large maze limit. Responsible for this behavior are rare events in the dataset which strongly regulate the shape of the cost function near this global minimum. We further identify an obstacle to learning in the form of poorly performing local minima in which the network chooses to ignore some of the inputs. We further support our claims with training experiments and numerical analysis of the cost function on networks with up to $128$ layers",
    "checked": true,
    "id": "f05a0fe30d9f9ccf29f74854fdeacbc132dbf23e",
    "semantic_title": "critical percolation as a framework to analyze the training of deep networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HkTEFfZRb": {
    "title": "Attacking Binarized Neural Networks",
    "volume": "poster",
    "abstract": "Neural networks with low-precision weights and activations offer compelling efficiency advantages over their full-precision equivalents. The two most frequently discussed benefits of quantization are reduced memory consumption, and a faster forward pass when implemented with efficient bitwise operations. We propose a third benefit of very low-precision neural networks: improved robustness against some adversarial attacks, and in the worst case, performance that is on par with full-precision models. We focus on the very low-precision case where weights and activations are both quantized to $\\pm$1, and note that stochastically quantizing weights in just one layer can sharply reduce the impact of iterative attacks. We observe that non-scaled binary neural networks exhibit a similar effect to the original \\emph{defensive distillation} procedure that led to \\emph{gradient masking}, and a false notion of security. We address this by conducting both black-box and white-box experiments with binary models that do not artificially mask gradients",
    "checked": null,
    "id": "64520a1f652ebd04565354fbb8a6281606c9724d",
    "semantic_title": "attacking binarized neural networks",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=SkHDoG-Cb": {
    "title": "Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings",
    "volume": "poster",
    "abstract": "Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. Bousmalis et al. (2017) propose a similar framework that jointly trains a translation mapping and a learning model. While these algorithms are shown to achieve the state-of-the-art performances on various tasks, it may have a room for improvement, as they do not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. While these algorithms are shown to achieve the state-of-the-art performances on various tasks, it may have a room for improvement, as it does not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by this limitation, we propose a new S+U learning algorithm, which fully leverage the flexibility of data simulators and bidirectional mappings between synthetic data and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017)",
    "checked": null,
    "id": "6ade1e0d4744d2eb5bf7bab97289ffd7eeb5a661",
    "semantic_title": "simulated+unsupervised learning with adaptive data generation and bidirectional mappings",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk6fD5yCb": {
    "title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks",
    "volume": "poster",
    "abstract": "There are many applications scenarios for which the computational performance and memory footprint of the prediction phase of Deep Neural Networks (DNNs) need to be optimized. Binary Deep Neural Networks (BDNNs) have been shown to be an effective way of achieving this objective. In this paper, we show how Convolutional Neural Networks (CNNs) can be implemented using binary representations. Espresso is a compact, yet powerful library written in C/CUDA that features all the functionalities required for the forward propagation of CNNs, in a binary file less than 400KB, without any external dependencies. Although it is mainly designed to take advantage of massive GPU parallelism, Espresso also provides an equivalent CPU implementation for CNNs. Espresso provides special convolutional and dense layers for BCNNs, leveraging bit-packing and bit-wise computations for efficient execution. These techniques provide a speed-up of matrix-multiplication routines, and at the same time, reduce memory usage when storing parameters and activations. We experimentally show that Espresso is significantly faster than existing implementations of optimized binary neural networks (~ 2 orders of magnitude). Espresso is released under the Apache 2.0 license and is available at http://github.com/organization/project",
    "checked": true,
    "id": "3d520a8673493aaeebe6fb3008c27de917fd8d2b",
    "semantic_title": "espresso: efficient forward propagation for binary deep neural networks",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=S1WRibb0Z": {
    "title": "Expressive power of recurrent neural networks",
    "volume": "poster",
    "abstract": "Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks – namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition – has been proven to have exponentially higher expressive power than shallow networks. I.e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. In this paper, we prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent neural networks – ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer. Using theoretical results on the relation between the tensor decompositions we compare expressive powers of the HT- and TT-Networks. We also implement the recurrent TT-Networks and provide numerical evidence of their expressivity",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyfHgI6aW": {
    "title": "Memory Augmented Control Networks",
    "volume": "poster",
    "abstract": "Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set",
    "checked": null,
    "id": "d94d4da65ff55117345639aae57046b77c65b36e",
    "semantic_title": "memory augmented control networks",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Ddp1-Rb": {
    "title": "mixup: Beyond Empirical Risk Minimization",
    "volume": "poster",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks",
    "checked": true,
    "id": "4feef0fd284feb1233399b400eb897f59ec92755",
    "semantic_title": "mixup: beyond empirical risk minimization",
    "citation_count": 9815,
    "authors": []
  },
  "https://openreview.net/forum?id=rkPLzgZAZ": {
    "title": "Modular Continual Learning in a Unified Visual Environment",
    "volume": "poster",
    "abstract": "A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a dynamic neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency",
    "checked": true,
    "id": "cf7a7f757fb3c83fb3f6e3b1889f014825dd906c",
    "semantic_title": "modular continual learning in a unified visual environment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxF5RgC-": {
    "title": "Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "eedaa9eb9c6a8228c79caf560614a431736b62d5",
    "semantic_title": "sparse persistent rnns: squeezing large recurrent networks on-chip",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=rJXMpikCZ": {
    "title": "Graph Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
    "semantic_title": "graph attention networks",
    "citation_count": 20061,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hcZZ-AW": {
    "title": "N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "92d621a603cda8c32214d70953e180fe5a442f3e",
    "semantic_title": "n2n learning: network to network compression via policy gradient reinforcement learning",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=rkLyJl-0-": {
    "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks",
    "volume": "poster",
    "abstract": "Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByBAl2eAZ": {
    "title": "Parameter Space Noise for Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "142497432fe179ddb6ffe600c64a837ec6179550",
    "semantic_title": "parameter space noise for exploration",
    "citation_count": 595,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwBEMWCZ": {
    "title": "Skip Connections Eliminate Singularities",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "31ea21d2fdf9e243aac746b68ca0111952fc58f4",
    "semantic_title": "skip connections eliminate singularities",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=SkA-IE06W": {
    "title": "When is a Convolutional Filter Easy to Learn?",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "fbe1d29737b66840d2bb9b74cd093858ef1805dd",
    "semantic_title": "when is a convolutional filter easy to learn?",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ8c3f-0b": {
    "title": "Auto-Encoding Sequential Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "2d8ea7941e608dab54004eb13f494837dd96d83b",
    "semantic_title": "auto-encoding sequential monte carlo",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=SJA7xfb0b": {
    "title": "Sobolev GAN",
    "volume": "poster",
    "abstract": "We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJJLHbb0-": {
    "title": "Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection",
    "volume": "poster",
    "abstract": "Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJcKhk-Ab": {
    "title": "Can recurrent neural networks warp time?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e9822e6c78b752f13ccf2943a8e41f9997f4a22",
    "semantic_title": "can recurrent neural networks warp time?",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=rytstxWAW": {
    "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2503dff90685857ce7295e37d0045e2eef41c8b8",
    "semantic_title": "fastgcn: fast learning with graph convolutional networks via importance sampling",
    "citation_count": 1518,
    "authors": []
  },
  "https://openreview.net/forum?id=Skz_WfbCZ": {
    "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "4fc3ee440c2b0f66255a9e6966cee871ee0cc6da",
    "semantic_title": "a pac-bayesian approach to spectrally-normalized margin bounds for neural networks",
    "citation_count": 605,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk6WhagRW": {
    "title": "Emergent Communication through Negotiation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "92ec8259e58f5107057e4e99fdcd03df6a08f9e7",
    "semantic_title": "emergent communication through negotiation",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=BJehNfW0-": {
    "title": "Do GANs learn the distribution? Some Theory and Empirics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bdabdeab5f92029c136f1e36ab79b4f06bac0ef7",
    "semantic_title": "do gans learn the distribution? some theory and empirics",
    "citation_count": 163,
    "authors": []
  },
  "https://openreview.net/forum?id=r11Q2SlRW": {
    "title": "Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "06dd367528f259c14a923c4912c61d48bcd16afd",
    "semantic_title": "auto-conditioned recurrent networks for extended complex human motion synthesis",
    "citation_count": 233,
    "authors": []
  },
  "https://openreview.net/forum?id=BJk59JZ0b": {
    "title": "Guide Actor-Critic for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1l4eQW0Z": {
    "title": "Kernel Implicit Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "2f39fd505b04ca363b9efe23f1937fd414a93ce7",
    "semantic_title": "kernel implicit variational inference",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJinbWRZ": {
    "title": "Model-Ensemble Trust-Region Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
    "semantic_title": "model-ensemble trust-region policy optimization",
    "citation_count": 453,
    "authors": []
  },
  "https://openreview.net/forum?id=S1DWPP1A-": {
    "title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "330d56c3641cddd8c78440e768cd80795f23cab4",
    "semantic_title": "unsupervised learning of goal spaces for intrinsically motivated goal exploration",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=rJUYGxbCW": {
    "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e83291498a3bc6b0efe8f9571e9c9ca1811707bd",
    "semantic_title": "pixeldefend: leveraging generative models to understand and defend against adversarial examples",
    "citation_count": 790,
    "authors": []
  },
  "https://openreview.net/forum?id=rJYFzMZC-": {
    "title": "Simulating Action Dynamics with Neural Process Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ry_WPG-A-": {
    "title": "On the Information Bottleneck Theory of Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1SnX5xCb": {
    "title": "Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Gi6LeRZ": {
    "title": "Learning from Between-class Examples for Deep Sound Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJGZq6g0-": {
    "title": "Emergent Communication in a Multi-Modal, Multi-Step Referential Game",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b9ff5712adb340c70fe3e415d09861ba45609280",
    "semantic_title": "emergent communication in a multi-modal, multi-step referential game",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=HyZoi-WRb": {
    "title": "Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "664bfbe4a061dcd01aab2a7b5fc30358a8a10350",
    "semantic_title": "debiasing evidence approximations: on importance-weighted autoencoders and jackknife variational inference",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=BkpiPMbA-": {
    "title": "Decision Boundary Analysis of Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "08c588465b7d801ad912ef3e9107fa511ea0e403",
    "semantic_title": "decision boundary analysis of adversarial examples",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=rJwelMbR-": {
    "title": "Divide-and-Conquer Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "904307cb58795241b22cfaa34f560e610997f5c1",
    "semantic_title": "divide-and-conquer reinforcement learning",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=Skj8Kag0Z": {
    "title": "Stabilizing Adversarial Nets with Prediction Methods",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "ec25504486d8751e00e613ca6fa64b256e3581c8",
    "semantic_title": "stabilizing adversarial nets with prediction methods",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=BkUHlMZ0b": {
    "title": "Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "72de5578808c87c5bab305e46854fdf0b0b2ddab",
    "semantic_title": "evaluating the robustness of neural networks: an extreme value theory approach",
    "citation_count": 467,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk8XMWgRb": {
    "title": "Not-So-Random Features",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "40dc2de4f461b0a673d35df49cd4d54b4c9865a5",
    "semantic_title": "not-so-random features",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=rkO3uTkAZ": {
    "title": "Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e5000713fa1dec7ba73162f516048b65110d96c0",
    "semantic_title": "memorization precedes generation: learning unsupervised gans with memory networks",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=HJJ23bW0b": {
    "title": "Initialization matters: Orthogonal Predictive State Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model. PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule. Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well. Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train. Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation. Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting. In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs. In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed",
    "checked": null,
    "id": "597b1dedb58ddb5ee5c35938c1a2574c7a11e0f6",
    "semantic_title": "initialization matters: orthogonal predictive state recurrent neural networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=SyoDInJ0-": {
    "title": "Reinforcement Learning Algorithm Selection",
    "volume": "poster",
    "abstract": "This paper formalises the problem of online algorithm selection in the context of Reinforcement Learning (RL). The setup is as follows: given an episodic task and a finite number of off-policy RL algorithms, a meta-algorithm has to decide which RL algorithm is in control during the next episode so as to maximize the expected return. The article presents a novel meta-algorithm, called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is to freeze the policy updates at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm selection. Under some assumptions, a thorough theoretical analysis demonstrates its near-optimality considering the structural sampling budget limitations. ESBAS is first empirically evaluated on a dialogue task where it is shown to outperform each individual algorithm in most configurations. ESBAS is then adapted to a true online setting where algorithms update their policies after each transition, which we call SSBAS. SSBAS is evaluated on a fruit collection task where it is shown to adapt the stepsize parameter more efficiently than the classical hyperbolic decay, and on an Atari game, where it improves the performance by a wide margin",
    "checked": null,
    "id": "07affdd73f1691a31cb9c2bc4f9df956873e7ad6",
    "semantic_title": "reinforcement learning algorithm selection",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=H1dh6Ax0Z": {
    "title": "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11c421320be0e098ddb7e52257f3d132e99715d4",
    "semantic_title": "treeqn and atreec: differentiable tree-structured models for deep reinforcement learning",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=r1VVsebAZ": {
    "title": "Synthesizing realistic neural population activity patterns using Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "The ability to synthesize realistic patterns of neural activity is crucial for studying neural information processing. Here we used the Generative Adversarial Networks (GANs) framework to simulate the concerted activity of a population of neurons. We adapted the Wasserstein-GAN variant to facilitate the generation of unconstrained neural population activity patterns while still benefiting from parameter sharing in the temporal domain. We demonstrate that our proposed GAN, which we termed Spike-GAN, generates spike trains that match accurately the first- and second-order statistics of datasets of tens of neurons and also approximates well their higher-order statistics. We applied Spike-GAN to a real dataset recorded from salamander retina and showed that it performs as well as state-of-the-art approaches based on the maximum entropy and the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not require to specify a priori the statistics to be matched by the model, and so constitutes a more flexible method than these alternative approaches. Finally, we show how to exploit a trained Spike-GAN to construct 'importance maps' to detect the most relevant statistical structures present in a spike train. Spike-GAN provides a powerful, easy-to-use technique for generating realistic spiking neural activity and for describing the most relevant features of the large-scale neural population recordings studied in modern systems neuroscience",
    "checked": null,
    "id": "9fd2a948964d9f18a77434f5eb868ba720689caa",
    "semantic_title": "synthesizing realistic neural population activity patterns using generative adversarial networks",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=H1T2hmZAb": {
    "title": "Deep Complex Networks",
    "volume": "poster",
    "abstract": "At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech spectrum prediction using TIMIT. We achieve state-of-the-art performance on these audio-related tasks",
    "checked": null,
    "id": "a742162820cdabee2831235665517d0e98041502",
    "semantic_title": "deep complex networks",
    "citation_count": 831,
    "authors": []
  },
  "https://openreview.net/forum?id=BJRZzFlRb": {
    "title": "Compressing Word Embeddings via Deep Compositional Code Learning",
    "volume": "poster",
    "abstract": "Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture",
    "checked": null,
    "id": "4bf6ce4a9366cdba069a45651606538f2febd8e6",
    "semantic_title": "compressing word embeddings via deep compositional code learning",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=H1q-TM-AW": {
    "title": "A DIRT-T Approach to Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks",
    "checked": null,
    "id": "008c901b3fd9e46ee8d3bddb616121e2887b7e67",
    "semantic_title": "a dirt-t approach to unsupervised domain adaptation",
    "citation_count": 621,
    "authors": []
  },
  "https://openreview.net/forum?id=rywDjg-RW": {
    "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples",
    "volume": "poster",
    "abstract": "Synthesizing user-intended programs from a small number of input-output exam- ples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively hand- engineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12× speed-up compared to state-of-the-art systems",
    "checked": null,
    "id": "85bdb70962147cd3586d99f50328025c82c9ac8e",
    "semantic_title": "neural-guided deductive search for real-time program synthesis from examples",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy2ogebAW": {
    "title": "Unsupervised Neural Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2a7afbb5609a723f8eea91bfde4b02579b048d6",
    "semantic_title": "unsupervised neural machine translation",
    "citation_count": 774,
    "authors": []
  },
  "https://openreview.net/forum?id=BkQqq0gRb": {
    "title": "Variational Continual Learning",
    "volume": "poster",
    "abstract": "This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way",
    "checked": null,
    "id": "d475f695dedd94e96771fdaa1e5c075fd01d11cf",
    "semantic_title": "variational continual learning",
    "citation_count": 732,
    "authors": []
  },
  "https://openreview.net/forum?id=H1uR4GZRZ": {
    "title": "Stochastic Activation Pruning for Robust Adversarial Defense",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f201c77e7ccdf1f37115e16accac3486a65c03d",
    "semantic_title": "stochastic activation pruning for robust adversarial defense",
    "citation_count": 548,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ-C6JbRW": {
    "title": "Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent",
    "volume": "poster",
    "abstract": "Contrary to most natural language processing research, which makes use of static datasets, humans learn language interactively, grounded in an environment. In this work we propose an interactive learning procedure called Mechanical Turker Descent (MTD) that trains agents to execute natural language commands grounded in a fantasy text adventure game. In MTD, Turkers compete to train better agents in the short term, and collaborate by sharing their agents' skills in the long term. This results in a gamified, engaging experience for the Turkers and a better quality teaching signal for the agents compared to static datasets, as the Turkers naturally adapt the training data to the agent's abilities",
    "checked": null,
    "id": "9af92f8c0309695fdcae7193e1b0c203ab11136f",
    "semantic_title": "mastering the dungeon: grounded language learning by mechanical turker descent",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=SkVqXOxCb": {
    "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation",
    "checked": null,
    "id": "9f605f8d5d6b143b8c34ccd498986a460c32e641",
    "semantic_title": "coulomb gans: provably optimal nash equilibria via potential fields",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=HJNMYceCW": {
    "title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback",
    "volume": "poster",
    "abstract": "We consider reinforcement learning and bandit structured prediction problems with very sparse loss feedback: only at the end of an episode. We introduce a novel algorithm, RESIDUAL LOSS PREDICTION (RESLOPE), that solves such problems by automatically learning an internal representation of a denser reward function. RESLOPE operates as a reduction to contextual bandits, using its learned loss representation to solve the credit assignment problem, and a contextual bandit oracle to trade-off exploration and exploitation. RESLOPE enjoys a no-regret reduction-style theoretical guarantee and outperforms state of the art reinforcement learning algorithms in both MDP environments and bandit structured prediction settings",
    "checked": null,
    "id": "f21ecd1a0bb77ab13320a4e24668c3e937ecf24b",
    "semantic_title": "residual loss prediction: reinforcement learning with no incremental feedback",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VjBebR-": {
    "title": "The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4fb11a58d5a3ffc0bb6d4ade334a366b4a431b02",
    "semantic_title": "the role of minimal complexity functions in unsupervised learning of semantic mappings",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=rkTS8lZAb": {
    "title": "Boundary Seeking GANs",
    "volume": "poster",
    "abstract": "Generative adversarial networks are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training, and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet without conditioning",
    "checked": null,
    "id": "92ed1b3a074bcbacf7000975ac8a684f9061b257",
    "semantic_title": "boundary seeking gans",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=BydLzGb0Z": {
    "title": "Twin Networks: Matching the Future for Sequence Generation",
    "volume": "poster",
    "abstract": "We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task",
    "checked": null,
    "id": "6402703b62325865d00da1f58dbbcaf9a2bc417d",
    "semantic_title": "twin networks: matching the future for sequence generation",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=rk6cfpRjZ": {
    "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
    "volume": "poster",
    "abstract": "Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is available",
    "checked": null,
    "id": "ca1060c50642f9f05735d3007873439347b3bea5",
    "semantic_title": "learning intrinsic sparse structures within long short-term memory",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=HyiAuyb0b": {
    "title": "TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL",
    "checked": null,
    "id": "02f94fa49bfb2b6592cc9783589c6e56aaa3861a",
    "semantic_title": "td or not td: analyzing the role of temporal differencing in deep reinforcement learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx9GQb0-": {
    "title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "65912ba93e57b7c96b5abe48435f433bfec6cd7b",
    "semantic_title": "improving the improved training of wasserstein gans: a consistency term and its dual effect",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Dy---0Z": {
    "title": "Distributed Prioritized Experience Replay",
    "volume": "poster",
    "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time",
    "checked": null,
    "id": "601a2d349fc26d7b82f905e924e2f91b0ac4b310",
    "semantic_title": "distributed prioritized experience replay",
    "citation_count": 740,
    "authors": []
  },
  "https://openreview.net/forum?id=H1cWzoxA-": {
    "title": "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but does not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN",
    "checked": null,
    "id": "0ef460c47377c3b9482d8177cbcafad1730a91a5",
    "semantic_title": "bi-directional block self-attention for fast and memory-efficient sequence modeling",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=SysEexbRb": {
    "title": "Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties",
    "volume": "poster",
    "abstract": "Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for linear neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of linear neural networks and shallow ReLU networks. One particular conclusion is that: While the loss function of linear networks has no spurious local minimum, the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum",
    "checked": null,
    "id": "f5b4a1e060161cdb07352dc66bf35690e5a3cfcd",
    "semantic_title": "critical points of linear neural networks: analytical forms and landscape properties",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=r1HhRfWRZ": {
    "title": "Learning Awareness Models",
    "volume": "poster",
    "abstract": "We consider the setting of an agent with a fixed body interacting with an unknown and uncertain external world. We show that models trained to predict proprioceptive information about the agent's body come to represent objects in the external world. In spite of being trained with only internally available signals, these dynamic body models come to represent external objects through the necessity of predicting their effects on the agent's own body. That is, the model learns holistic persistent representations of objects in the world, even though the only training signals are body signals. Our dynamics model is able to successfully predict distributions over 132 sensor readings over 100 steps into the future and we demonstrate that even when the body is no longer in contact with an object, the latent variables of the dynamics model continue to represent its shape. We show that active data collection by maximizing the entropy of predictions about the body---touch sensors, proprioception and vestibular information---leads to learning of dynamic models that show superior performance when used for control. We also collect data from a real robotic hand and show that the same models can be used to answer questions about properties of objects in the real world. Videos with qualitative results of our models are available at https://goo.gl/mZuqAV",
    "checked": null,
    "id": "4b82cfd0229f257f44d84bedb4bead85054597cc",
    "semantic_title": "learning awareness models",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=H1kG7GZAW": {
    "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations",
    "volume": "poster",
    "abstract": "Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We also propose a new disentanglement metric which is better aligned with the qualitative disentanglement observed in the decoder's output. We empirically observe significant improvement over existing methods in terms of both disentanglement and data likelihood (reconstruction quality)",
    "checked": null,
    "id": "26690f2548c6dbf630de202b40dec417b20c9b6c",
    "semantic_title": "variational inference of disentangled latent concepts from unlabeled observations",
    "citation_count": 523,
    "authors": []
  },
  "https://openreview.net/forum?id=rkrC3GbRW": {
    "title": "Learning a Generative Model for Validity in Complex Discrete Structures",
    "volume": "poster",
    "abstract": "Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether a partial sequence can function as the beginning of a full, valid sequence. This validator provides insight as to how individual sequence elements influence the validity of the overall sequence, and can be used to constrain sequence based models to generate valid sequences — and thus faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle which can evaluate validity of complete sequences provides a sparse reward signal. We demonstrate its effectiveness as a generative model of Python 3 source code for mathematical expressions, and in improving the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures",
    "checked": null,
    "id": "8a50c3aff2bb2111bcc0f6ddf0726af252a56daa",
    "semantic_title": "learning a generative model for validity in complex discrete structures",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=H196sainb": {
    "title": "Word translation without parallel data",
    "volume": "poster",
    "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available",
    "checked": null,
    "id": "562c09c112df56c5696c010d90a815d6018a86c8",
    "semantic_title": "word translation without parallel data",
    "citation_count": 1655,
    "authors": []
  },
  "https://openreview.net/forum?id=BywyFQlAW": {
    "title": "Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VGkIxRZ": {
    "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
    "volume": "poster",
    "abstract": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%",
    "checked": null,
    "id": "547c854985629cfa9404a5ba8ca29367b5f8c25f",
    "semantic_title": "enhancing the reliability of out-of-distribution image detection in neural networks",
    "citation_count": 2061,
    "authors": []
  },
  "https://openreview.net/forum?id=HJCXZQbAZ": {
    "title": "Hierarchical Density Order Embeddings",
    "volume": "poster",
    "abstract": "By representing words with probability densities rather than point vectors, proba- bilistic word embeddings can capture rich and interpretable semantic information and uncertainty (Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017). The uncertainty information can be particularly meaningful in capturing entailment relationships – whereby general words such as \"entity\" correspond to broad distributions that encompass more specific words such as \"animal\" or \"instrument\". We introduce density order embeddings, which learn hierarchical representations through encapsulation of probability distributions. In particular, we propose simple yet effective loss functions and distance metrics, as well as graph-based schemes to select negative samples to better learn hierarchical probabilistic representations. Our approach provides state-of-the-art performance on the WordNet hypernym relationship prediction task and the challenging HyperLex lexical entailment dataset – while retaining a rich and interpretable probabilistic representation",
    "checked": null,
    "id": "a621b40c8d7a7314d2988e6ac1aa3b318f9c1acd",
    "semantic_title": "hierarchical density order embeddings",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNGsseC-": {
    "title": "On the Expressive Power of Overlapping Architectures of Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "59228743a2f01445a5ecdb616b3c1a550343cdc1",
    "semantic_title": "on the expressive power of overlapping architectures of deep learning",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=BkSDMA36Z": {
    "title": "A New Method of Region Embedding for Text Classification",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "119120c0c7bab456a187ebc7454f78d7f8b03a32",
    "semantic_title": "a new method of region embedding for text classification",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=SJa9iHgAZ": {
    "title": "Residual Connections Encourage Iterative Inference",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "098b96b713aa8a324090662c7f35213512f4525e",
    "semantic_title": "residual connections encourage iterative inference",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=BydjJte0-": {
    "title": "Towards Reverse-Engineering Black-Box Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "e8b9fa6f9e0b606ff335a0557a838dea2696b084",
    "semantic_title": "towards reverse-engineering black-box neural networks",
    "citation_count": 368,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Dh8Tg0-": {
    "title": "Fix your classifier: the marginal value of training the last weight layer",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "178339ef29211540683b36c0e1c6acffd998cddd",
    "semantic_title": "fix your classifier: the marginal value of training the last weight layer",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=B1nZ1weCZ": {
    "title": "Learning to Multi-Task by Active Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "2b64a0a4c4ef3cde80c6ad49428648ef6e47cbfa",
    "semantic_title": "learning to multi-task by active sampling",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=HyyP33gAZ": {
    "title": "Activation Maximization Generative Adversarial Nets",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "4d4e657bf884097a2b88d56aef1a8733379bfb63",
    "semantic_title": "activation maximization generative adversarial nets",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=SyqShMZRb": {
    "title": "Syntax-Directed Variational Autoencoder for Structured Data",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "7dd434b3799a6c8c346a1d7ee77d37980a4ef5b9",
    "semantic_title": "syntax-directed variational autoencoder for structured data",
    "citation_count": 327,
    "authors": []
  },
  "https://openreview.net/forum?id=ByQpn1ZA-": {
    "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "471908e99d6965f0f6d249c9cd013485dc2b21df",
    "semantic_title": "many paths to equilibrium: gans do not need to decrease a divergence at every step",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=B1QgVti6Z": {
    "title": "Empirical Risk Landscape Analysis for Understanding Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "c7f938eb608b96a03d54f05caf3f675afd952c08",
    "semantic_title": "empirical risk landscape analysis for understanding deep neural networks",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hYRMbCW": {
    "title": "On the regularization of Wasserstein GANs",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "f64233585c793b56d61f95c1cf25282ad28ceb4b",
    "semantic_title": "on the regularization of wasserstein gans",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ANxQW0b": {
    "title": "Maximum a Posteriori Policy Optimisation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a8ef08940341381390d9a5672546354d0ce51328",
    "semantic_title": "maximum a posteriori policy optimisation",
    "citation_count": 478,
    "authors": []
  },
  "https://openreview.net/forum?id=rJzIBfZAb": {
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
    "semantic_title": "towards deep learning models resistant to adversarial attacks",
    "citation_count": 12029,
    "authors": []
  },
  "https://openreview.net/forum?id=HJewuJWCZ": {
    "title": "Learning to Teach",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "31094226ed723f560e8ca63d40b88d93dead3de5",
    "semantic_title": "learning to teach",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=SyProzZAW": {
    "title": "The power of deeper networks for expressing natural functions",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "60a74f80f6e9b924ec92c2a31245560b12469481",
    "semantic_title": "the power of deeper networks for expressing natural functions",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=BkrsAzWAb": {
    "title": "Online Learning Rate Adaptation with Hypergradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "512ca06114f5292f7d0b536ce030e319863c781a",
    "semantic_title": "online learning rate adaptation with hypergradient descent",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=BkUp6GZRW": {
    "title": "Boosting the Actor with Dual Critic",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "3a5f60c2dea325de1dc92ae1eb7a513945bd6af1",
    "semantic_title": "boosting the actor with dual critic",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=rkN2Il-RZ": {
    "title": "SCAN: Learning Hierarchical Compositional Visual Concepts",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "b22b4817757778bdca5b792277128a7db8206d08",
    "semantic_title": "scan: learning hierarchical compositional visual concepts",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=Skw0n-W0Z": {
    "title": "Temporal Difference Models: Model-Free Deep RL for Model-Based Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "852c931b5d9f9d4256befd725ee4185945c4964c",
    "semantic_title": "temporal difference models: model-free deep rl for model-based control",
    "citation_count": 240,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMvJrdaW": {
    "title": "Decoupling the Layers in Residual Networks",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "27f7ab99b7646474bff0d5942876db7756728f8b",
    "semantic_title": "decoupling the layers in residual networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ByS1VpgRZ": {
    "title": "cGANs with Projection Discriminator",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlMAAeC-": {
    "title": "Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "fc05801280853ff6f6a15c55d9b76d8c58182f39",
    "semantic_title": "improving the universality and learnability of neural programmer-interpreters with combinator abstraction",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ryazCMbR-": {
    "title": "Communication Algorithms via Deep Learning",
    "volume": "poster",
    "abstract": "Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that cre- atively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We show strong gen- eralizations, i.e., we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, as well as robustness and adaptivity to deviations from the AWGN setting",
    "checked": null,
    "id": "9a5eb38bcf3097cbeaa9307627015528e88efda3",
    "semantic_title": "communication algorithms via deep learning",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=SyX0IeWAW": {
    "title": "META LEARNING SHARED HIERARCHIES",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d2c4cbb535801549371d9783a98d1e43bddf4e5",
    "semantic_title": "meta learning shared hierarchies",
    "citation_count": 355,
    "authors": []
  },
  "https://openreview.net/forum?id=SyYe6k-CW": {
    "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
    "volume": "poster",
    "abstract": "Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting",
    "checked": null,
    "id": "93a636f0b6d450217fda5aaa26c74bb6b232f498",
    "semantic_title": "deep bayesian bandits showdown: an empirical comparison of bayesian deep networks for thompson sampling",
    "citation_count": 365,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Euwz-Rb": {
    "title": "Compositional Attention Networks for Machine Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "289fb3709475f5c87df8d97f129af54029d27fee",
    "semantic_title": "compositional attention networks for machine reasoning",
    "citation_count": 578,
    "authors": []
  },
  "https://openreview.net/forum?id=BkJ3ibb0-": {
    "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f7bb1636ced9036b3d0edafc7d82ad43164d41a3",
    "semantic_title": "defense-gan: protecting classifiers against adversarial attacks using generative models",
    "citation_count": 1179,
    "authors": []
  },
  "https://openreview.net/forum?id=SywXXwJAb": {
    "title": "Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design",
    "volume": "poster",
    "abstract": "Formal understanding of the inductive bias behind deep convolutional networks, i.e. the relation between the network's architectural features and the functions it is able to model, is limited. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning, and use it for obtaining novel theoretical observations regarding the inductive bias of convolutional networks. Specifically, we show a structural equivalence between the function realized by a convolutional arithmetic circuit (ConvAC) and a quantum many-body wave function, which facilitates the use of quantum entanglement measures as quantifiers of a deep network's expressive ability to model correlations. Furthermore, the construction of a deep ConvAC in terms of a quantum Tensor Network is enabled. This allows us to perform a graph-theoretic analysis of a convolutional network, tying its expressiveness to a min-cut in its underlying graph. We demonstrate a practical outcome in the form of a direct control over the inductive bias via the number of channels (width) of each layer. We empirically validate our findings on standard convolutional networks which involve ReLU activations and max pooling. The description of a deep convolutional network in well-defined graph-theoretic tools and the structural connection to quantum entanglement, are two interdisciplinary bridges that are brought forth by this work",
    "checked": null,
    "id": "c9316491b8d991fabbf9b28c449d66df6a50f841",
    "semantic_title": "deep learning and quantum entanglement: fundamental connections with implications to network design",
    "citation_count": 126,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Y8hhg0b": {
    "title": "Learning Sparse Neural Networks through L_0 Regularization",
    "volume": "poster",
    "abstract": "We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters. We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer",
    "checked": null,
    "id": "2ec7156913117949ab933f27f492d0149bc0031f",
    "semantic_title": "learning sparse neural networks through l0 regularization",
    "citation_count": 1144,
    "authors": []
  },
  "https://openreview.net/forum?id=B1n8LexRZ": {
    "title": "Generalizing Hamiltonian Monte Carlo with Neural Networks",
    "volume": "poster",
    "abstract": "We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper",
    "checked": null,
    "id": "b7c7b4bbda0a90d3e22e8f1207e3d3226e711a84",
    "semantic_title": "generalizing hamiltonian monte carlo with neural networks",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=HyjC5yWCW": {
    "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm",
    "volume": "poster",
    "abstract": "Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models",
    "checked": null,
    "id": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
    "semantic_title": "meta-learning and universality: deep representations and gradient descent can approximate any learning algorithm",
    "citation_count": 223,
    "authors": []
  },
  "https://openreview.net/forum?id=rJQDjk-0b": {
    "title": "Unbiased Online Recurrent Optimization",
    "volume": "poster",
    "abstract": "The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}. UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence. On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients",
    "checked": null,
    "id": "db337a93cdb1a636c086c1e5c855b9be016bfb77",
    "semantic_title": "unbiased online recurrent optimization",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=BkabRiQpb": {
    "title": "Consequentialist conditional cooperation in social dilemmas with imperfect information",
    "volume": "poster",
    "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action",
    "checked": null,
    "id": "596ca16325dd72c0fcc186389fdd0280b49bbefb",
    "semantic_title": "consequentialist conditional cooperation in social dilemmas with imperfect information",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwVAXyCW": {
    "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/",
    "checked": null,
    "id": "ae8d5be3caea59a21221f02ef04d49a86cb80191",
    "semantic_title": "skip rnn: learning to skip state updates in recurrent neural networks",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg0vbWC-": {
    "title": "Generating Wikipedia by Summarizing Long Sequences",
    "volume": "poster",
    "abstract": "We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations",
    "checked": null,
    "id": "8691706ad0cf5e83969658b2e6bfffdc379440c9",
    "semantic_title": "generating wikipedia by summarizing long sequences",
    "citation_count": 797,
    "authors": []
  },
  "https://openreview.net/forum?id=SJyVzQ-C-": {
    "title": "Fraternal Dropout",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks",
    "checked": null,
    "id": "415f18130edbe06e3e4806dfb0a1edcab6c241eb",
    "semantic_title": "fraternal dropout",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BySRH6CpW": {
    "title": "Learning Discrete Weights Using the Local Reparameterization Trick",
    "volume": "poster",
    "abstract": "Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge. One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments",
    "checked": null,
    "id": "15698db8c6b08891c8ab8b75a2738cad04c7b25b",
    "semantic_title": "learning discrete weights using the local reparameterization trick",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=r1wEFyWCW": {
    "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions",
    "volume": "poster",
    "abstract": "Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as ImageNet. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to PixelCNN result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on ImageNet and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset",
    "checked": null,
    "id": "bbe13b72314fffcc2f35b0660195f2f6607c00a0",
    "semantic_title": "few-shot autoregressive density estimation: towards learning to learn distributions",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy0GnUxCb": {
    "title": "Emergent Complexity via Multi-Agent Competition",
    "volume": "poster",
    "abstract": "Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX",
    "checked": null,
    "id": "c335ff618991f0a4cdde09271284172a7e5f6b7f",
    "semantic_title": "emergent complexity via multi-agent competition",
    "citation_count": 388,
    "authors": []
  },
  "https://openreview.net/forum?id=SkFqf0lAZ": {
    "title": "Memory Architectures in Recurrent Neural Network Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27981998aaef92952eabef2c1490b926f9150c4f",
    "semantic_title": "memory architectures in recurrent neural network language models",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=HJIoJWZCZ": {
    "title": "Adversarial Dropout Regularization",
    "volume": "poster",
    "abstract": "We present a domain adaptation method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by ``fooling'' a special domain classifier network. However, a drawback of this approach is that the domain classifier simply labels the generated features as in-domain or not, without considering the boundaries between classes. This means that ambiguous target features can be generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), which encourages the generator to output more discriminative features for the target domain. Our key idea is to replace the traditional domain critic with a critic that detects non-discriminative features by using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvements over the state of the art",
    "checked": null,
    "id": "c20148703a706d5883d323c3386978714fe1508d",
    "semantic_title": "adversarial dropout regularization",
    "citation_count": 284,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk9zbyZCZ": {
    "title": "Neural Map: Structured Memory for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training",
    "checked": null,
    "id": "3ee01ec27e4e66e089b72a9989724be611c2ad90",
    "semantic_title": "neural map: structured memory for deep reinforcement learning",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOExmWAb": {
    "title": "MaskGAN: Better Text Generation via Filling in the _______",
    "volume": "poster",
    "abstract": "Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model",
    "checked": null,
    "id": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
    "semantic_title": "maskgan: better text generation via filling in the ______",
    "citation_count": 470,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk9Xc_lR-": {
    "title": "On the Discrimination-Generalization Tradeoff in GANs",
    "volume": "poster",
    "abstract": "Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically neural networks. The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable). In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions. This is a very mild condition satisfied even by neural networks with a single neuron. Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics. When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set. When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training. Our analysis sheds lights on understanding the practical performance of GANs",
    "checked": null,
    "id": "4f74782ac26a8912215dd11503762699d341ca5d",
    "semantic_title": "on the discrimination-generalization tradeoff in gans",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=H1UOm4gA-": {
    "title": "Interactive Grounded Language Acquisition and Generalization in a 2D World",
    "volume": "poster",
    "abstract": "We build a virtual agent for learning language in a 2D maze-like world. The agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher's language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the world, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably interpolates and extrapolates to interpret sentences that contain new word combinations or new words missing from training sentences. The new words are transferred from the answers of language prediction. Such a language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms five comparison methods for interpreting zero-shot sentences. In addition, we demonstrate human-interpretable intermediate outputs of the model in the appendix",
    "checked": null,
    "id": "a4a90a2db209db2d5c49adfd2091ede2d4130f60",
    "semantic_title": "interactive grounded language acquisition and generalization in a 2d world",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=Syg-YfWCW": {
    "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
    "volume": "poster",
    "abstract": "Knowledge bases (KB), both automatically and manually constructed, are often incomplete --- many valid facts can be inferred from the KB by synthesizing existing information. A popular approach to KB completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities. Given the enormous size of KBs and the exponential number of paths, previous path-based models have considered only the problem of predicting a missing relation given two entities, or evaluating the truth of a proposed triple. Additionally, these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them. We propose a new algorithm, MINERVA, which addresses the much more difficult and practical task of answering questions where the relation is known, but only one entity. Since random walks are impractical in a setting with unknown destination and combinatorially many paths from a start node, we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths. On a comprehensive evaluation on seven knowledge base datasets, we found MINERVA to be competitive with many current state-of-the-art methods",
    "checked": null,
    "id": "e9f32b34fde7d0034a147d5996470634c9e19002",
    "semantic_title": "go for a walk and arrive at the answer: reasoning over paths in knowledge bases using reinforcement learning",
    "citation_count": 509,
    "authors": []
  },
  "https://openreview.net/forum?id=Skp1ESxRZ": {
    "title": "Towards Synthesizing Complex Programs From Input-Output Examples",
    "volume": "poster",
    "abstract": "In recent years, deep learning techniques have been developed to improve the performance of program synthesis from input-output examples. Albeit its significant progress, the programs that can be synthesized by state-of-the-art approaches are still simple in terms of their complexity. In this work, we move a significant step forward along this direction by proposing a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees. We show that this class of tasks are much more challenging than previously studied tasks, and the test accuracy of existing approaches is almost 0%. We tackle the challenges by developing three novel techniques inspired by three novel observations, which reveal the key ingredients of using deep learning to synthesize a complex program. First, the use of a non-differentiable machine is the key to effectively restrict the search space. Thus our proposed approach learns a neural program operating a domain-specific non-differentiable machine. Second, recursion is the key to achieve generalizability. Thus, we bake-in the notion of recursion in the design of our non-differentiable machine. Third, reinforcement learning is the key to learn how to operate the non-differentiable machine, but it is also hard to train the model effectively with existing reinforcement learning algorithms from a cold boot. We develop a novel two-phase reinforcement learning-based search algorithm to overcome this issue. In our evaluation, we show that using our novel approach, neural parsing programs can be learned to achieve 100% test accuracy on test inputs that are 500x longer than the training samples",
    "checked": null,
    "id": "42dea2a24629bd4b1b56619536e263b078becddd",
    "semantic_title": "towards synthesizing complex programs from input-output examples",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=BkrSv0lA-": {
    "title": "Loss-aware Weight Quantization of Deep Networks",
    "volume": "poster",
    "abstract": "The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network",
    "checked": null,
    "id": "d18cc16563b9e2cef58bbc2a9d212b0ca72de36c",
    "semantic_title": "loss-aware weight quantization of deep networks",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=HJzgZ3JCW": {
    "title": "Efficient Sparse-Winograd Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd's minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning",
    "checked": null,
    "id": "317b886336e3f5b599ab21795f4a4fef56f727f4",
    "semantic_title": "efficient sparse-winograd convolutional neural networks",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lUOzWCW": {
    "title": "Demystifying MMD GANs",
    "volume": "poster",
    "abstract": "We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training",
    "checked": null,
    "id": "9723066a5587e6267d8abfd7feefd0637a5a211c",
    "semantic_title": "demystifying mmd gans",
    "citation_count": 1488,
    "authors": []
  },
  "https://openreview.net/forum?id=HJC2SzZCW": {
    "title": "Sensitivity and Generalization in Neural Networks: an Empirical Study",
    "volume": "poster",
    "abstract": "In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with different architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the input-output Jacobian of the network, and that this correlates well with generalization. We further establish that factors associated with poor generalization -- such as full-batch training or using random labels -- correspond to higher sensitivity, while factors associated with good generalization -- such as data augmentation and ReLU non-linearities -- give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points",
    "checked": null,
    "id": "e837dfa120e8ce3cd587bde7b0787ef43fa7832d",
    "semantic_title": "sensitivity and generalization in neural networks: an empirical study",
    "citation_count": 439,
    "authors": []
  },
  "https://openreview.net/forum?id=B14TlG-RW": {
    "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
    "volume": "poster",
    "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8",
    "checked": null,
    "id": "8c1b00128e74f1cd92aede3959690615695d5101",
    "semantic_title": "qanet: combining local convolution with global self-attention for reading comprehension",
    "citation_count": 1095,
    "authors": []
  },
  "https://openreview.net/forum?id=SyZipzbCb": {
    "title": "Distributed Distributional Deterministic Policy Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d355e339298fc2ab920688c1709d4ba6476a2bc6",
    "semantic_title": "distributed distributional deterministic policy gradients",
    "citation_count": 480,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk8ZcAxR-": {
    "title": "Eigenoption Discovery through the Deep Successor Representation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58fb60c5592224901a26dd84220a2f3332c1fcf5",
    "semantic_title": "eigenoption discovery through the deep successor representation",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=r1q7n9gAb": {
    "title": "The Implicit Bias of Gradient Descent on Separable Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11adc8bd35bd897502f9b5452ab7ac668ec9b0fb",
    "semantic_title": "the implicit bias of gradient descent on separable data",
    "citation_count": 924,
    "authors": []
  },
  "https://openreview.net/forum?id=SyJ7ClWCb": {
    "title": "Countering Adversarial Images using Input Transformations",
    "volume": "poster",
    "abstract": "This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods",
    "checked": null,
    "id": "e225dd59ef4954db21479cdcbee497624b2d6d0f",
    "semantic_title": "countering adversarial images using input transformations",
    "citation_count": 1400,
    "authors": []
  },
  "https://openreview.net/forum?id=ByRWCqvT-": {
    "title": "Learning to cluster in order to transfer across domains and tasks",
    "volume": "poster",
    "abstract": "This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets. Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement",
    "checked": null,
    "id": "495582032e8467dc31c59772d690b2a079fcbb4f",
    "semantic_title": "learning to cluster in order to transfer across domains and tasks",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=Bys4ob-Rb": {
    "title": "Certified Defenses against Adversarial Examples",
    "volume": "poster",
    "abstract": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error",
    "checked": null,
    "id": "966e3c7a65ec75a6359b55c0cecaf3896d318432",
    "semantic_title": "certified defenses against adversarial examples",
    "citation_count": 967,
    "authors": []
  },
  "https://openreview.net/forum?id=rkcQFMZRb": {
    "title": "Variational image compression with a scale hyperprior",
    "volume": "poster",
    "abstract": "We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate--distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics",
    "checked": null,
    "id": "678c5b1771e7c15e86b454662dec8cd45d3d30bf",
    "semantic_title": "variational image compression with a scale hyperprior",
    "citation_count": 1786,
    "authors": []
  },
  "https://openreview.net/forum?id=BJQRKzbA-": {
    "title": "Hierarchical Representations for Efficient Architecture Search",
    "volume": "poster",
    "abstract": "We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour",
    "checked": null,
    "id": "856451974cce2d353d5d8a5a72104984a252375c",
    "semantic_title": "hierarchical representations for efficient architecture search",
    "citation_count": 928,
    "authors": []
  },
  "https://openreview.net/forum?id=rytNfI1AZ": {
    "title": "Training wide residual networks for deployment using a single bit for each weight",
    "volume": "poster",
    "abstract": "For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that binarize weights by applying the sign function in training; we apply scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test results of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error rates halve previously reported values, and are within about 1% of our error-rates for the same network with full-precision weights. For networks that overfit, we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. This applies to both full precision and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule, we found that training for 1-bit-per-weight is just as fast as full-precision networks, with better accuracy than standard schedules, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. For full training code and trained models in MATLAB, Keras and PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/",
    "checked": null,
    "id": "d5a8c9bef7020e800d3b02573a983ff1eddf285e",
    "semantic_title": "training wide residual networks for deployment using a single bit for each weight",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk0wHx-RW": {
    "title": "Learning Sparse Latent Representations with the Deep Copula Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "19d4fe6264a3ab140f94d43d784835ec25cea55e",
    "semantic_title": "learning sparse latent representations with the deep copula information bottleneck",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=r1iuQjxCZ": {
    "title": "On the importance of single directions for generalization",
    "volume": "poster",
    "abstract": "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyper- parameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance",
    "checked": null,
    "id": "45164e8d9508f3d1f20500e67358565587626789",
    "semantic_title": "on the importance of single directions for generalization",
    "citation_count": 333,
    "authors": []
  },
  "https://openreview.net/forum?id=HJcSzz-CZ": {
    "title": "Meta-Learning for Semi-Supervised Few-Shot Classification",
    "volume": "poster",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would",
    "checked": null,
    "id": "df093d69cd98cf4b26542f53614a79754754eb78",
    "semantic_title": "meta-learning for semi-supervised few-shot classification",
    "citation_count": 1282,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zriGeCZ": {
    "title": "Hyperparameter optimization: a spectral approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cf0986f965b9314d1a86e328420494e45bb61683",
    "semantic_title": "hyperparameter optimization: a spectral approach",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Dx7fbCW": {
    "title": "Generalizing Across Domains via Cross-Gradient Training",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByJHuTgA-": {
    "title": "On the State of the Art of Evaluation in Neural Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "2397ce306e5d7f3d0492276e357fb1833536b5d8",
    "semantic_title": "on the state of the art of evaluation in neural language models",
    "citation_count": 534,
    "authors": []
  },
  "https://openreview.net/forum?id=rypT3fb0b": {
    "title": "LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "8de574fff9b65d2a3cd0a5348458ee1045df3238",
    "semantic_title": "learning to share: simultaneous parameter tying and sparsification in deep learning",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=H1vEXaxA-": {
    "title": "Emergent Translation in Multi-Agent Communication",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c669f052c509c1c80a8c413fcb96b2b324d2f3ca",
    "semantic_title": "emergent translation in multi-agent communication",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=ry018WZAZ": {
    "title": "Deep Active Learning for Named Entity Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "43ef6f1b1c0622a23bc62af5a05dd5c813eba00d",
    "semantic_title": "deep active learning for named entity recognition",
    "citation_count": 456,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l8BtlCb": {
    "title": "Non-Autoregressive Neural Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "15e81c8d1c21f9e928c72721ac46d458f3341454",
    "semantic_title": "non-autoregressive neural machine translation",
    "citation_count": 795,
    "authors": []
  },
  "https://openreview.net/forum?id=HktJec1RZ": {
    "title": "Towards Neural Phrase-based Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": null,
    "id": "5616064812996ab1fae525f9679f300c7c307895",
    "semantic_title": "towards neural phrase-based machine translation",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=BJIgi_eCZ": {
    "title": "FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1fe6bee85774244d8674cbb20a25e8d153cecb17",
    "semantic_title": "fusionnet: fusing via fully-aware attention with application to machine comprehension",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=rkYTTf-AZ": {
    "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e3d772986d176057aca2f5e3eb783da53b559134",
    "semantic_title": "unsupervised machine translation using monolingual corpora only",
    "citation_count": 1098,
    "authors": []
  },
  "https://openreview.net/forum?id=ByJIWUnpW": {
    "title": "Automatically Inferring Data Quality for Spatiotemporal Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fedd6331f2d6a1225b564d5cda3d3db15b25015e",
    "semantic_title": "automatically inferring data quality for spatiotemporal forecasting",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Hksj2WWAW": {
    "title": "Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3495c2bd6dc6956357b170b0c3a50db7ab8582d4",
    "semantic_title": "combining symbolic expressions and black-box function evaluations in neural programs",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeqORgAW": {
    "title": "Proximal Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c5afa0e78fadfe8cd3f0006988de3e230edc0075",
    "semantic_title": "proximal backpropagation",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=H1meywxRW": {
    "title": "DCN+: Mixed Objective And Deep Residual Coattention for Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a",
    "semantic_title": "dcn+: mixed objective and deep residual coattention for question answering",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk3ddfWRW": {
    "title": "Imitation Learning from Visual Data with Multiple Intentions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "023237d0ea810771e7e90770835fa89a2931d276",
    "semantic_title": "imitation learning from visual data with multiple intentions",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ94fqApW": {
    "title": "Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60464c4bd94a14b63898e322f9ea651830e54ae0",
    "semantic_title": "rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers",
    "citation_count": 408,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Yp-j1Cb": {
    "title": "An Online Learning Approach to Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "245f8b05bdd1ac65a09a476440dc4b05ac05d4a0",
    "semantic_title": "an online learning approach to generative adversarial networks",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=rkZB1XbRZ": {
    "title": "Scalable Private Learning with PATE",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "44058a625cb64c311043145655645d8206e272c2",
    "semantic_title": "scalable private learning with pate",
    "citation_count": 618,
    "authors": []
  },
  "https://openreview.net/forum?id=SygwwGbRW": {
    "title": "Semi-parametric topological memory for navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c0d96d1ea69855a5a3abf614f17095c29b3339a4",
    "semantic_title": "semi-parametric topological memory for navigation",
    "citation_count": 383,
    "authors": []
  },
  "https://openreview.net/forum?id=HyRnez-RW": {
    "title": "Multi-Mention Learning for Reading Comprehension with Neural Cascades",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8a004eaebe04facb26039fed4fddcf4c855c7d49",
    "semantic_title": "multi-mention learning for reading comprehension with neural cascades",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=BJuWrGW0Z": {
    "title": "Dynamic Neural Program Embeddings for Program Repair",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "bbe5b2fa5db0d37d772eca7db0e1b1ddfec0f6a5",
    "semantic_title": "dynamic neural program embedding for program repair",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=rJWechg0Z": {
    "title": "Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b0868e208ce9c4e99da57b6dce57dc7950b5a7df",
    "semantic_title": "minimal-entropy correlation alignment for unsupervised deep domain adaptation",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=rkr1UDeC-": {
    "title": "Large scale distributed neural network training through online distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cc59b4b1eb7d4629f753bc24f029c5cced301381",
    "semantic_title": "large scale distributed neural network training through online distillation",
    "citation_count": 408,
    "authors": []
  },
  "https://openreview.net/forum?id=ry-TW-WAb": {
    "title": "Variational Network Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f93ae1a0b9e40b138da8c25855b6c68af4ee201a",
    "semantic_title": "variational network quantization",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=rk07ZXZRb": {
    "title": "Learning an Embedding Space for Transferable Robot Skills",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "565af8f2ef461b1d7368f3e9899e0f576e4f0a24",
    "semantic_title": "learning an embedding space for transferable robot skills",
    "citation_count": 322,
    "authors": []
  },
  "https://openreview.net/forum?id=Skdvd2xAZ": {
    "title": "A Scalable Laplace Approximation for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad8631abf269d2b886614f69221d25a732a7f58d",
    "semantic_title": "a scalable laplace approximation for neural networks",
    "citation_count": 411,
    "authors": []
  },
  "https://openreview.net/forum?id=S1sqHMZCb": {
    "title": "NerveNet: Learning Structured Policy with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "249408527106d7595d45dd761dd53c83e5a02613",
    "semantic_title": "nervenet: learning structured policy with graph neural networks",
    "citation_count": 280,
    "authors": []
  },
  "https://openreview.net/forum?id=rkhlb8lCZ": {
    "title": "Wavelet Pooling for Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1fdfc0ba42d4d7e38fccf9b66c4cc086514253bd",
    "semantic_title": "wavelet pooling for convolutional neural networks",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=HkCsm6lRb": {
    "title": "Generative Models of Visually Grounded Imagination",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "226f08bb8fd049fdca2f6a8a41ffe485a0ee6ebc",
    "semantic_title": "generative models of visually grounded imagination",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=rknt2Be0-": {
    "title": "Compositional Obverter Communication Learning from Raw Visual Input",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4814c10f84863e016d75e6af42e790f60759b9f4",
    "semantic_title": "compositional obverter communication learning from raw visual input",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=HyMTkQZAb": {
    "title": "Kronecker-factored Curvature Approximations for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "42d74488ea64d5420257f547f6d0a7a2909d87c0",
    "semantic_title": "kronecker-factored curvature approximations for recurrent neural networks",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=HkUR_y-RZ": {
    "title": "SEARNN: Training RNNs with global-local losses",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "345bbd344177815dfb9214c61403cb7eac6de450",
    "semantic_title": "searnn: training rnns with global-local losses",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=ryRh0bb0Z": {
    "title": "Multi-View Data Generation Without View Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6cc53bd117613797d7713a2c2f92983cf8a28df0",
    "semantic_title": "multi-view data generation without view supervision",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk5elxbRW": {
    "title": "Smooth Loss Functions for Deep Top-k Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4ced7f12424c70f0edfbc2c12f39529f35977e6",
    "semantic_title": "smooth loss functions for deep top-k classification",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=rkHywl-A-": {
    "title": "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "941ba185f01b1a0a27453fd178aa5f010510ee8b",
    "semantic_title": "learning robust rewards with adverserial inverse reinforcement learning",
    "citation_count": 209,
    "authors": []
  },
  "https://openreview.net/forum?id=BJij4yg0Z": {
    "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ae4b0b63ff26e52792be7f60bda3ed5db83c1577",
    "semantic_title": "a bayesian perspective on generalization and stochastic gradient descent",
    "citation_count": 253,
    "authors": []
  }
}