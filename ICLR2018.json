{
  "https://openreview.net/forum?id=Hkbd5xZRb": {
    "title": "Spherical CNNs",
    "volume": "oral",
    "abstract": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression",
    "checked": true,
    "id": "ce2845cadc5233ff0a647aa22ae3bbe646258890",
    "semantic_title": "spherical cnns",
    "citation_count": 908,
    "authors": []
  },
  "https://openreview.net/forum?id=rJTutzbA-": {
    "title": "On the insufficiency of existing momentum schemes for Stochastic Optimization",
    "volume": "oral",
    "abstract": "Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Rigorously speaking, fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there exist simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching. Furthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Acceleration. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD. The code for implementing the ASGD Algorithm can be found at https://github.com/rahulkidambi/AccSGD",
    "checked": true,
    "id": "a99f6f195d6cbc33a6259206c10d4ab0e167f969",
    "semantic_title": "on the insufficiency of existing momentum schemes for stochastic optimization",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk6kPgZA-": {
    "title": "Certifying Some Distributional Robustness with Principled Adversarial Training",
    "volume": "oral",
    "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches",
    "checked": true,
    "id": "818c52f4ba56cb8cf152ad614f2f4803057a5cfe",
    "semantic_title": "certifying some distributional robustness with principled adversarial training",
    "citation_count": 866,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwZSG-CZ": {
    "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "ef9ddbc35676ce8ffc2a8067044473727839dbac",
    "semantic_title": "breaking the softmax bottleneck: a high-rank rnn language model",
    "citation_count": 373,
    "authors": []
  },
  "https://openreview.net/forum?id=S1JHhv6TW": {
    "title": "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "9593d6e790065c972668da9ee0ae6614bf63ca18",
    "semantic_title": "boosting dilated convolutional networks with mixed tensor decompositions",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy7fDog0b": {
    "title": "AmbientGAN: Generative models from lossy measurements",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "b959d5655a3b2f92c2c1a8a7896fecafafea979d",
    "semantic_title": "ambientgan: generative models from lossy measurements",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=rkRwGg-0Z": {
    "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "cb6866b5fa62ae3cbe21bafd772bcce7d9668dd6",
    "semantic_title": "beyond word importance: contextual decomposition to extract interactions from lstms",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=S1CChZ-CZ": {
    "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "acbd6e09ad888e68472a5c1cb6ec83176c7969bd",
    "semantic_title": "ask the right questions: active question reformulation with reinforcement learning",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=H1tSsb-AW": {
    "title": "Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "1bdcf6fe02ed2ff097e5f4ffdc5af159cd9a713a",
    "semantic_title": "variance reduction for policy gradient with action-dependent factorized baselines",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ8vJebC-": {
    "title": "Synthetic and Natural Noise Both Break Neural Machine Translation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073",
    "semantic_title": "synthetic and natural noise both break neural machine translation",
    "citation_count": 744,
    "authors": []
  },
  "https://openreview.net/forum?id=HkL7n1-0b": {
    "title": "Wasserstein Auto-Encoders",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "6745c95b88ff9b12401a9ba6f4007f036be591a0",
    "semantic_title": "wasserstein auto-encoders",
    "citation_count": 1058,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk99zCeAb": {
    "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "744fe47157477235032f7bb3777800f9f2f45e52",
    "semantic_title": "progressive growing of gans for improved quality, stability, and variation",
    "citation_count": 7394,
    "authors": []
  },
  "https://openreview.net/forum?id=ryQu7f-RZ": {
    "title": "On the Convergence of Adam and Beyond",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c983653841b6987d9959318f074a595783838576",
    "semantic_title": "on the convergence of adam and beyond",
    "citation_count": 2506,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGXzmspb": {
    "title": "Training and Inference with Integers in Deep Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "acdf151b8efc2c6b05662d69f27531afc557dc85",
    "semantic_title": "training and inference with integers in deep neural networks",
    "citation_count": 391,
    "authors": []
  },
  "https://openreview.net/forum?id=BkisuzWRW": {
    "title": "Zero-Shot Visual Imitation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "9217c4aa5a95d3209d2071e6889c8dd4b7d9309e",
    "semantic_title": "zero-shot visual imitation",
    "citation_count": 301,
    "authors": []
  },
  "https://openreview.net/forum?id=BJOFETxR-": {
    "title": "Learning to Represent Programs with Graphs",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "5f1d429ba574581ac14effe3ebab654a57dc0e39",
    "semantic_title": "learning to represent programs with graphs",
    "citation_count": 809,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk2aImxAb": {
    "title": "Multi-Scale Dense Networks for Resource Efficient Image Classification",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "125ccd810f43f1cba83c6681836d000f83d1886d",
    "semantic_title": "multi-scale dense networks for resource efficient image classification",
    "citation_count": 766,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfXMz-Ab": {
    "title": "Neural Sketch Learning for Conditional Program Generation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "d11777ca327c6d91de34d1d2ac50316b905578b2",
    "semantic_title": "neural sketch learning for conditional program generation",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk2u1g-0-": {
    "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "6ac5eb309dd937d801d180c830377e4d551699a2",
    "semantic_title": "continuous adaptation via meta-learning in nonstationary and competitive environments",
    "citation_count": 354,
    "authors": []
  },
  "https://openreview.net/forum?id=HktK4BeCZ": {
    "title": "Learning Deep Mean Field Games for Modeling Large Population Behavior",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "6aad7ac79c221b0b865b694ef9d05d9c91db1650",
    "semantic_title": "learning deep mean field games for modeling large population behavior",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=B1QRgziT-": {
    "title": "Spectral Normalization for Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
    "semantic_title": "spectral normalization for generative adversarial networks",
    "citation_count": 4449,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gJ1L2aW": {
    "title": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "a18ada04d93981178234d9c8907fb99ea92fddcb",
    "semantic_title": "characterizing adversarial subspaces using local intrinsic dimensionality",
    "citation_count": 742,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGv1Z-AW": {
    "title": "Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c0617490e0ea57b28b89d4ca92e069f538609603",
    "semantic_title": "emergence of linguistic communication from referential games with symbolic and pixel input",
    "citation_count": 217,
    "authors": []
  },
  "https://openreview.net/forum?id=B18WgG-CZ": {
    "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning",
    "volume": "poster",
    "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations",
    "checked": true,
    "id": "afc2850945a871e72c245818f9bc141bd659b453",
    "semantic_title": "learning general purpose distributed sentence representations via large scale multi-task learning",
    "citation_count": 330,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ_UL-k0b": {
    "title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes",
    "volume": "poster",
    "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation",
    "checked": true,
    "id": "46b072cf918ec6f50403568a73d4347ea86b7e66",
    "semantic_title": "recasting gradient-based meta-learning as hierarchical bayes",
    "citation_count": 510,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNpifWAb": {
    "title": "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches",
    "volume": "poster",
    "abstract": "Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services",
    "checked": true,
    "id": "32760a5d2a55a586b2a9aab7278db89de065eae3",
    "semantic_title": "flipout: efficient pseudo-independent weight perturbations on mini-batches",
    "citation_count": 315,
    "authors": []
  },
  "https://openreview.net/forum?id=ry1arUgCW": {
    "title": "DORA The Explorer: Directed Outreaching Reinforcement Action-Selection",
    "volume": "poster",
    "abstract": "Exploration is a fundamental aspect of Reinforcement Learning, typically implemented using stochastic action-selection. Exploration, however, can be more efficient if directed toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality. While there are a few model-based solutions to this shortcoming, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-values improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to efficiently learn continuous MDPs. We demonstrate this by showing that our approach surpasses state of the art performance in the Freeway Atari 2600 game",
    "checked": true,
    "id": "89e9c05ea3b45628beba8134fd5c873dd55003e8",
    "semantic_title": "dora the explorer: directed outreaching reinforcement action-selection",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=SyELrEeAb": {
    "title": "Implicit Causal Models for Genome-wide Association Studies",
    "volume": "poster",
    "abstract": "Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%",
    "checked": true,
    "id": "83cffda7d9b47d0927d03fdc574a019487a3d5d8",
    "semantic_title": "implicit causal models for genome-wide association studies",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HJtEm4p6Z": {
    "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
    "volume": "poster",
    "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training an order of magnitude faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server",
    "checked": true,
    "id": "feca3f41b5b4cc13368d53f3168cc55a2420ec16",
    "semantic_title": "deep voice 3: scaling text-to-speech with convolutional sequence learning",
    "citation_count": 459,
    "authors": []
  },
  "https://openreview.net/forum?id=ryBnUWb0b": {
    "title": "Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data",
    "volume": "poster",
    "abstract": "In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy",
    "checked": true,
    "id": "83f286c386cb5572e572a03bc3297b3cf2e52827",
    "semantic_title": "predicting floor-level for 911 calls with neural networks and smartphone sensor data",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=B1jscMbAW": {
    "title": "Divide and Conquer Networks",
    "volume": "poster",
    "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. Rather than studying this as a black-box discrete regression problem with no assumption whatsoever on the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. This principle creates a powerful inductive bias that we leverage with neural architectures that are defined recursively and dynamically, by learning two scale- invariant atomic operations: how to split a given input into smaller sets, and how to merge two partially solved tasks into a larger partial solution. Our model can be trained in weakly supervised environments, namely by just observing input-output pairs, and in even weaker environments, using a non-differentiable reward signal. Moreover, thanks to the dynamic aspect of our architecture, we can incorporate the computational complexity as a regularization term that can be optimized by backpropagation. We demonstrate the flexibility and efficiency of the Divide- and-Conquer Network on several combinatorial and geometric tasks: convex hull, clustering, knapsack and euclidean TSP. Thanks to the dynamic programming nature of our model, we show significant improvements in terms of generalization error and computational complexity",
    "checked": true,
    "id": "5dd3e28170e4e0694e51f24d3859ff97a2314f54",
    "semantic_title": "divide and conquer networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=Hko85plCW": {
    "title": "Monotonic Chunkwise Attention",
    "volume": "poster",
    "abstract": "Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model",
    "checked": true,
    "id": "f0afdccf2903039d202085a771953a171dfd57b1",
    "semantic_title": "monotonic chunkwise attention",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=HyH9lbZAW": {
    "title": "Variational Message Passing with Structured Inference Networks",
    "volume": "poster",
    "abstract": "Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized inference similar to VAE. Finally, we derive a variational message passing algorithm to perform efficient natural-gradient inference while retaining the efficiency of the amortized inference. By simultaneously enabling structured, amortized, and natural-gradient inference for deep structured models, our method simplifies and generalizes existing methods",
    "checked": true,
    "id": "4a97e7280ac46a28900993d2cdf3db65ecd258ba",
    "semantic_title": "variational message passing with structured inference networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=HkuGJ3kCb": {
    "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations",
    "volume": "poster",
    "abstract": "Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\\em very simple}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations {\\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and text classification) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones",
    "checked": true,
    "id": "a2d407962bb1f5fcd209114f5687d4c11bf9dfad",
    "semantic_title": "all-but-the-top: simple and effective postprocessing for word representations",
    "citation_count": 311,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ33wwxRb": {
    "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data",
    "volume": "poster",
    "abstract": "Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers",
    "checked": true,
    "id": "c923208fb1e917f55e38b7c9f94f219554f398fa",
    "semantic_title": "sgd learns over-parameterized networks that provably generalize on linearly separable data",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=B1mvVm-C-": {
    "title": "Universal Agent for Disentangling Environments and Tasks",
    "volume": "poster",
    "abstract": "Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task. Hence, both environment and task specific knowledge are entangled into one framework. However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes. Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units. The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task. The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods",
    "checked": true,
    "id": "5c6ad1419289850d315646f6599303549b99aac4",
    "semantic_title": "universal agent for disentangling environments and tasks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOnmlWC-": {
    "title": "Policy Optimization by Genetic Distillation",
    "volume": "poster",
    "abstract": "Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency",
    "checked": true,
    "id": "dc1d45b529e6149bd49547fbca976a8de9847612",
    "semantic_title": "policy optimization by genetic distillation",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzRZ-WCZ": {
    "title": "Latent Space Oddity: on the Curvature of Deep Generative Models",
    "volume": "poster",
    "abstract": "Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear \"generator\" function that maps latent points into the input space. The nonlinearity of the generator implies that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and we demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalizes to other deep generative models",
    "checked": true,
    "id": "9f696b7156716c978b62a92714e7038a99f7a53c",
    "semantic_title": "latent space oddity: on the curvature of deep generative models",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=S1jBcueAb": {
    "title": "Depthwise Separable Convolutions for Neural Machine Translation",
    "volume": "poster",
    "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models",
    "checked": true,
    "id": "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83",
    "semantic_title": "depthwise separable convolutions for neural machine translation",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=H1BLjgZCb": {
    "title": "Generating Natural Adversarial Examples",
    "volume": "poster",
    "abstract": "Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers",
    "checked": true,
    "id": "3502b5ef1afb16f76bcae33db17179195bbcdaae",
    "semantic_title": "generating natural adversarial examples",
    "citation_count": 601,
    "authors": []
  },
  "https://openreview.net/forum?id=ByrZyglCb": {
    "title": "Robustness of Classifiers to Universal Perturbations: A Geometric Perspective",
    "volume": "poster",
    "abstract": "Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties",
    "checked": true,
    "id": "03522e27e398f4f5fa5e86cb2ed7e8c86a7e9bf1",
    "semantic_title": "robustness of classifiers to universal perturbations: a geometric perspective",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=H1WgVz-AZ": {
    "title": "Learning Approximate Inference Networks for Structured Prediction",
    "volume": "poster",
    "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This \"inference network\" outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups of 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \"label language model\" that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings",
    "checked": true,
    "id": "b34e44d48c35750456d50d225c026536596b83fa",
    "semantic_title": "learning approximate inference networks for structured prediction",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl63fZRb": {
    "title": "Parametrized Hierarchical Procedures for Neural Programming",
    "volume": "poster",
    "abstract": "Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite the potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parametrized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, using a program counter along with the observation to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a set of supervisor demonstrations, only some of which are annotated with the internal call structure, and apply it to efficient level-wise training of multi-level PHPs. We show in two benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural programs more accurately from smaller amounts of both annotated and unannotated demonstrations",
    "checked": true,
    "id": "f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f",
    "semantic_title": "parametrized hierarchical procedures for neural programming",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy-dQG-Rb": {
    "title": "Neural Speed Reading via Skim-RNN",
    "volume": "poster",
    "abstract": "Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives a significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models. In our experiments, we show that Skim-RNN can achieve significantly reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also shows that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs",
    "checked": true,
    "id": "1e4cfedd79a108d0d04cc498bb146e4dcd4b5f0a",
    "semantic_title": "neural speed reading via skim-rnn",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=ry8dvM-R-": {
    "title": "Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning",
    "volume": "poster",
    "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network â€“ for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time",
    "checked": true,
    "id": "fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2",
    "semantic_title": "routing networks: adaptive selection of non-linear functions for multi-task learning",
    "citation_count": 249,
    "authors": []
  },
  "https://openreview.net/forum?id=SyzKd1bCW": {
    "title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation",
    "volume": "poster",
    "abstract": "Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function. These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models",
    "checked": true,
    "id": "682d194235ba3b573889836ba118502e8b525728",
    "semantic_title": "backpropagation through the void: optimizing control variates for black-box gradient estimation",
    "citation_count": 300,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ_wN01C-": {
    "title": "Deep Rewiring: Training very sparse deep networks",
    "volume": "poster",
    "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior",
    "checked": true,
    "id": "ccee800244908d2960830967e70ead7dd8266f7a",
    "semantic_title": "deep rewiring: training very sparse deep networks",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=H1-nGgWC-": {
    "title": "Gaussian Process Behaviour in Wide Deep Neural Networks",
    "volume": "poster",
    "abstract": "Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented",
    "checked": true,
    "id": "fb350d3b03e9308ccbd131d3d45dd44e383e6227",
    "semantic_title": "gaussian process behaviour in wide deep neural networks",
    "citation_count": 561,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e5ef-C-": {
    "title": "A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs",
    "volume": "poster",
    "abstract": "Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the \"meaning\" of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice",
    "checked": true,
    "id": "86fa6a6533e1b979eebd05bd6fb336ebf1eb99b9",
    "semantic_title": "a compressed sensing view of unsupervised text embeddings, bag-of-n-grams, and lstms",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=S1J2ZyZ0Z": {
    "title": "Interpretable Counting for Visual Question Answering",
    "volume": "poster",
    "abstract": "Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections. A distinction of our approach is its intuitive and interpretable output, as discrete counts are automatically grounded in the image. Furthermore, our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting",
    "checked": true,
    "id": "0605a012aeeee9bef773812a533c4f3cb7fa5a5f",
    "semantic_title": "interpretable counting for visual question answering",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=rylSzl-R-": {
    "title": "On Unifying Deep Generative Models",
    "volume": "poster",
    "abstract": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transfered techniques",
    "checked": true,
    "id": "c0c3ea2aa244920a206c196af3da1b81934a8c2e",
    "semantic_title": "on unifying deep generative models",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=B1al7jg0b": {
    "title": "Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation",
    "volume": "poster",
    "abstract": "Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed",
    "checked": true,
    "id": "0f57fdef846f5d033063c061c24191762e543f18",
    "semantic_title": "overcoming catastrophic interference using conceptor-aided backpropagation",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=HyWrIgW0W": {
    "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
    "volume": "poster",
    "abstract": "Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such out-of-equilibrium behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix",
    "checked": true,
    "id": "940912cfc9190f5cc79e3867060e543634b6b22e",
    "semantic_title": "stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
    "citation_count": 304,
    "authors": []
  },
  "https://openreview.net/forum?id=HyUNwulC-": {
    "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. We develop a parallel linear recurrence CUDA kernel and show that it can be applied to immediately speed up training and inference of several state of the art RNN architectures by up to 9x. We abstract recent work on linear RNNs into a new framework of linear surrogate RNNs and develop a linear surrogate model for the long short-term memory unit, the GILR-LSTM, that utilizes parallel linear recurrence. We extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training a GILR-LSTM on a synthetic sequence classification task with a one million timestep dependency",
    "checked": true,
    "id": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498",
    "semantic_title": "parallelizing linear recurrent neural nets over sequence length",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgNdt26Z": {
    "title": "Distributed Fine-tuning of Language Models on Private Data",
    "volume": "poster",
    "abstract": "One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm. In language modeling, users' language (e.g. in private messaging) could change in a year and be completely different from what we observe in publicly available data. At the same time, public data can be used for obtaining general knowledge (i.e. general model of English). We study approaches to distributed fine-tuning of a general model on user private data with the additional requirements of maintaining the quality on the general data and minimization of communication costs. We propose a novel technique that significantly improves prediction quality on users' language compared to a general model and outperforms gradient compression methods in terms of communication efficiency. The proposed procedure is fast and leads to an almost 70% perplexity reduction and 8.7 percentage point improvement in keystroke saving rate on informal English texts. Finally, we propose an experimental framework for evaluating differential privacy of distributed training of language models and show that our approach has good privacy guarantees",
    "checked": true,
    "id": "947b4752338a4fc44003acce97bccf99be49549a",
    "semantic_title": "distributed fine-tuning of language models on private data",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=rk49Mg-CW": {
    "title": "Stochastic Variational Video Prediction",
    "volume": "poster",
    "abstract": "Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future. This can lead to low-quality predictions in real-world settings with stochastic dynamics. In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. Our SV2P implementation will be open sourced upon publication",
    "checked": true,
    "id": "59d86da5c5936e7a236678bf5eaaa7753c226fb1",
    "semantic_title": "stochastic variational video prediction",
    "citation_count": 543,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk9yuql0Z": {
    "title": "Mitigating Adversarial Effects Through Randomization",
    "volume": "poster",
    "abstract": "Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017_adv_challenge_defense",
    "checked": true,
    "id": "9a089c56eec68df722b2a5a52727143aacdc2532",
    "semantic_title": "mitigating adversarial effects through randomization",
    "citation_count": 1068,
    "authors": []
  },
  "https://openreview.net/forum?id=B12Js_yRb": {
    "title": "Learning to Count Objects in Natural Images for Visual Question Answering",
    "volume": "poster",
    "abstract": "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. We identify a fundamental problem due to soft attention in these models as a cause. To circumvent this problem, we propose a neural network component that allows robust counting from object proposals. Experiments on a toy task show the effectiveness of this component and we obtain state-of-the-art accuracy on the number category of the VQA v2 dataset without negatively affecting other categories, even outperforming ensemble models with our single model. On a difficult balanced pair metric, the component gives a substantial improvement in counting over a strong baseline by 6.6%",
    "checked": true,
    "id": "30a3eee5e9302108416f6234d739373dde68d373",
    "semantic_title": "learning to count objects in natural images for visual question answering",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=SyyGPP0TZ": {
    "title": "Regularizing and Optimizing LSTM Language Models",
    "volume": "poster",
    "abstract": "In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at https://github.com/salesforce/awd-lstm-lm",
    "checked": true,
    "id": "58c6f890a1ae372958b7decf56132fe258152722",
    "semantic_title": "regularizing and optimizing lstm language models",
    "citation_count": 1098,
    "authors": []
  },
  "https://openreview.net/forum?id=B1zlp1bRW": {
    "title": "Large Scale Optimal Transport and Mapping Estimation",
    "volume": "poster",
    "abstract": "This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. This parameterization allows generalization of the mapping outside the support of the input measure. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling",
    "checked": true,
    "id": "2140bc28c3bb4982edef1af0753174c7c7aa4b79",
    "semantic_title": "large scale optimal transport and mapping estimation",
    "citation_count": 249,
    "authors": []
  },
  "https://openreview.net/forum?id=S1nQvfgA-": {
    "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs",
    "checked": true,
    "id": "d35534f3f59631951011539da2fe83f2844ca245",
    "semantic_title": "semantically decomposing the latent spaces of generative adversarial networks",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy6GHpkCW": {
    "title": "A Neural Representation of Sketch Drawings",
    "volume": "poster",
    "abstract": "We present sketch-rnn, a recurrent neural network able to construct stroke-based drawings of common objects. The model is trained on a dataset of human-drawn images representing many different classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format",
    "checked": true,
    "id": "37fa040ec0c4bc1b85f3ca2929445f3229ed7f72",
    "semantic_title": "a neural representation of sketch drawings",
    "citation_count": 870,
    "authors": []
  },
  "https://openreview.net/forum?id=S1p31z-Ab": {
    "title": "Deep contextualized word representations",
    "volume": "poster",
    "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals",
    "checked": true,
    "id": "3febb2bed8865945e7fddc99efd791887bb7e14f",
    "semantic_title": "deep contextualized word representations",
    "citation_count": 11577,
    "authors": []
  },
  "https://openreview.net/forum?id=SyOK1Sg0W": {
    "title": "Adaptive Quantization of Neural Networks",
    "volume": "poster",
    "abstract": "Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model parameters to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each network parameter such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each parameter and assigns it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rates. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy",
    "checked": true,
    "id": "accfaca7b884964582843a4cc9bb9dc0f3131528",
    "semantic_title": "adaptive quantization of neural networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gs9JgRZ": {
    "title": "Mixed Precision Training",
    "volume": "poster",
    "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets",
    "checked": true,
    "id": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
    "semantic_title": "mixed precision training",
    "citation_count": 1811,
    "authors": []
  },
  "https://openreview.net/forum?id=B1EA-M-0Z": {
    "title": "Deep Neural Networks as Gaussian Processes",
    "volume": "poster",
    "abstract": "It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide, deep, networks and GPs with a particular covariance function. We further develop a computationally efficient pipeline to compute this covariance function. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We observe that the trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and that the GP-based predictions typically outperform those of finite-width networks. Finally we connect the prior distribution over weights and variances in our GP formulation to the recent development of signal propagation in random neural networks",
    "checked": true,
    "id": "075556dd42900a6bc4552a2f2531ba21b9b7b4c0",
    "semantic_title": "deep neural networks as gaussian processes",
    "citation_count": 1100,
    "authors": []
  },
  "https://openreview.net/forum?id=S19dR9x0b": {
    "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied. We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration. Both results beat the exiting quantization works with large margins. We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves excellent performance",
    "checked": true,
    "id": "be83160b94893f2ef5edd5e68a3f3f8b089a87fd",
    "semantic_title": "alternating multi-bit quantization for recurrent neural networks",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=BJk7Gf-CZ": {
    "title": "Global Optimality Conditions for Deep Neural Networks",
    "volume": "poster",
    "abstract": "We study the error landscape of deep linear and nonlinear neural networks with the squared error loss. Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete. For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum. Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting",
    "checked": true,
    "id": "414915126855cab6a30fdcec635d90e793137c3b",
    "semantic_title": "global optimality conditions for deep neural networks",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOfBggRZ": {
    "title": "Detecting Statistical Interactions from Neural Network Weights",
    "volume": "poster",
    "abstract": "Interpreting neural networks is a crucial and challenging task in machine learning. In this paper, we develop a novel framework for detecting statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights. Depending on the desired interactions, our method can achieve significantly better or similar interaction detection performance compared to the state-of-the-art without searching an exponential solution space of possible interactions. We obtain this accuracy and efficiency by observing that interactions between input features are created by the non-additive effect of nonlinear activation functions, and that interacting paths are encoded in weight matrices. We demonstrate the performance of our method and the importance of discovered interactions via experimental results on both synthetic datasets and real-world application datasets",
    "checked": true,
    "id": "5f85a8eaa7a1a1686f5a2bf721c63e337f03d8eb",
    "semantic_title": "detecting statistical interactions from neural network weights",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy21R9JAW": {
    "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks",
    "volume": "poster",
    "abstract": "Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures",
    "checked": true,
    "id": "de01f74a899ecc7e3228cddbc743aaf6faf5e55f",
    "semantic_title": "towards better understanding of gradient-based attribution methods for deep neural networks",
    "citation_count": 891,
    "authors": []
  },
  "https://openreview.net/forum?id=BJNRFNlRW": {
    "title": "TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN",
    "volume": "poster",
    "abstract": "We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modified objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples",
    "checked": true,
    "id": "c837f6bb76af151145df50207992ed3848489bdb",
    "semantic_title": "training generative adversarial networks via primal-dual subgradient methods: a lagrangian perspective on gan",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=rkfOvGbCW": {
    "title": "Memory-based Parameter Adaptation",
    "volume": "poster",
    "abstract": "Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling",
    "checked": true,
    "id": "0be49527df4869a0132f5cbc8d4cfa3304ab5843",
    "semantic_title": "memory-based parameter adaptation",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzbhfWRW": {
    "title": "Learn to Pay Attention",
    "volume": "poster",
    "abstract": "We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parametrised by the score matrices, must alone be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack",
    "checked": true,
    "id": "c70218603f0af1be5d063056cbe629e042141a86",
    "semantic_title": "learn to pay attention",
    "citation_count": 442,
    "authors": []
  },
  "https://openreview.net/forum?id=SJi9WOeRb": {
    "title": "Gradient Estimators for Implicit Models",
    "volume": "poster",
    "abstract": "Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversity",
    "checked": true,
    "id": "0007812ab0a7472f0d74802f00692a64117117f9",
    "semantic_title": "gradient estimators for implicit models",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=BkwHObbRZ": {
    "title": "Learning One-hidden-layer Neural Networks with Landscape Design",
    "volume": "poster",
    "abstract": "We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. Inspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties: 1. All local minima of $G$ are also global minima. 2. All global minima of $G$ correspond to the ground truth parameters. 3. The value and gradient of $G$ can be estimated using samples. With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations",
    "checked": true,
    "id": "136d8b08d38a5c4829449b87bad6bad675ca8c71",
    "semantic_title": "learning one-hidden-layer neural networks with landscape design",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=ry6-G_66b": {
    "title": "Active Neural Localization",
    "volume": "poster",
    "abstract": "Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine",
    "checked": true,
    "id": "80f9f597992dc4f77911fa6a94a4611f5daafa5d",
    "semantic_title": "active neural localization",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=HkZy-bW0-": {
    "title": "Temporally Efficient Deep Learning with Spikes",
    "volume": "poster",
    "abstract": "The vast majority of natural sensory data is temporally redundant. For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values. Typically, deep learning algorithms take no advantage of this redundancy to reduce computations. This can be an obscene waste of energy. We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data. We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation. Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain. We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers",
    "checked": true,
    "id": "c0ef9d2fe95232a15cc459cad5caf0c7e68d1d55",
    "semantic_title": "temporally efficient deep learning with spikes",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=S1HlA-ZAZ": {
    "title": "The Kanerva Machine: A Generative Distributed Memory",
    "volume": "poster",
    "abstract": "We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train",
    "checked": true,
    "id": "5b2c83f41eacdf95e8b38300d2926ac37ea4709e",
    "semantic_title": "the kanerva machine: a generative distributed memory",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=SkZxCk-0Z": {
    "title": "Can Neural Networks Understand Logical Entailment?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bdea8b6ceabaeb86bd23c2d2585da1ff3858d968",
    "semantic_title": "can neural networks understand logical entailment?",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=SJLlmG-AZ": {
    "title": "Understanding image motion with group representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "869ef3835e5da24b4597a757948f1c0e19fa546d",
    "semantic_title": "understanding image motion with group representations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=S1D8MPxA-": {
    "title": "Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad3e968a38a5b2d1e72997f978c556a06f625e48",
    "semantic_title": "viterbi-based pruning for sparse matrix with fixed and high index compression ratio",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=SkFAWax0-": {
    "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e8f697f19c5d7d95f859e1ad6bcc219a902f52b",
    "semantic_title": "voiceloop: voice fitting and synthesis via a phonological loop",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=BJj6qGbRW": {
    "title": "Few-Shot Learning with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "572a1f77306e160c3893299c18f3ed862fb5f6d9",
    "semantic_title": "few-shot learning with graph neural networks",
    "citation_count": 1241,
    "authors": []
  },
  "https://openreview.net/forum?id=BkXmYfbAZ": {
    "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4f3375dab1886f37f542d998e61d8c30a927682",
    "semantic_title": "beyond shared hierarchies: deep multitask learning through soft layer ordering",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=BJE-4xW0W": {
    "title": "CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "329f14d68f8e4cbc3ee5109e70c76c6f3b63a19a",
    "semantic_title": "causalgan: learning causal implicit generative models with adversarial training",
    "citation_count": 257,
    "authors": []
  },
  "https://openreview.net/forum?id=ryUlhzWCZ": {
    "title": "TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d07fe76aa2ca2337c8aa6001c8cfe296fa824109",
    "semantic_title": "truncated horizon policy search: combining reinforcement learning & imitation learning",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=HkXWCMbRW": {
    "title": "Towards Image Understanding from Deep Compression Without Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e5604c3f61eb7e8b80bf423f7828d8c1fa0f1d32",
    "semantic_title": "towards image understanding from deep compression without decoding",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Xw62kRZ": {
    "title": "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dea6aeb514b1969ab879c793d46a0d2eceaa2cbf",
    "semantic_title": "leveraging grammar and reinforcement learning for neural program synthesis",
    "citation_count": 220,
    "authors": []
  },
  "https://openreview.net/forum?id=By4HsfWAZ": {
    "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2581b3e44592b3b3741474c8d6f483a90c29f139",
    "semantic_title": "deep learning for physical processes: incorporating prior scientific knowledge",
    "citation_count": 320,
    "authors": []
  },
  "https://openreview.net/forum?id=B17JTOe0-": {
    "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a538579ac50d659ac0bca9824d6446e741c586b3",
    "semantic_title": "emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=Byt3oJ-0W": {
    "title": "Learning Latent Permutations with Gumbel-Sinkhorn Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23deae1e548bc055c8920fc8529e59bba0816685",
    "semantic_title": "learning latent permutations with gumbel-sinkhorn networks",
    "citation_count": 272,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl3yM-Ab": {
    "title": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8e46f7fbb96549cd1b3b0bd226f06a611126b889",
    "semantic_title": "evidence aggregation for answer re-ranking in open-domain question answering",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ1Xmf-Rb": {
    "title": "FearNet: Brain-Inspired Model for Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e77d37e7fbfe90f58e4e96e8198903f364d6402",
    "semantic_title": "fearnet: brain-inspired model for incremental learning",
    "citation_count": 482,
    "authors": []
  },
  "https://openreview.net/forum?id=H15odZ-C-": {
    "title": "Semantic Interpolation in Implicit Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9301b04a76a03344b5e2ccb2ccfae6aa2d99e487",
    "semantic_title": "semantic interpolation in implicit models",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=rkHVZWZAZ": {
    "title": "The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee1ac4a86cafa34e4905327f3436fea2d5994fc8",
    "semantic_title": "the reactor: a fast and sample-efficient actor-critic agent for reinforcement learning",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=H135uzZ0-": {
    "title": "Mixed Precision Training of Convolutional Neural Networks using Integer Operations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eae2eba75cc90990ff31d94d1942ffdea7c452b8",
    "semantic_title": "mixed precision training of convolutional neural networks using integer operations",
    "citation_count": 154,
    "authors": []
  },
  "https://openreview.net/forum?id=rJvJXZb0W": {
    "title": "An efficient framework for learning sentence representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad31866da7f14ae21bd38df0a3b1ffd1a1438122",
    "semantic_title": "an efficient framework for learning sentence representations",
    "citation_count": 544,
    "authors": []
  },
  "https://openreview.net/forum?id=ry80wMW0W": {
    "title": "Hierarchical Subtask Discovery with Non-Negative Matrix Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0df639c7ecf49371e5786ec2d628e63b845116a2",
    "semantic_title": "hierarchical subtask discovery with non-negative matrix factorization",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ_aoCyRZ": {
    "title": "SpectralNet: Spectral Clustering using Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "40cec58d760940f62975e0d5e99632e28df0b5d0",
    "semantic_title": "spectralnet: spectral clustering using deep neural networks",
    "citation_count": 287,
    "authors": []
  },
  "https://openreview.net/forum?id=H1sUHgb0Z": {
    "title": "Learning From Noisy Singly-labeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a4a8e91995ae8c8b203dd857bdc0915facddeebe",
    "semantic_title": "learning from noisy singly-labeled data",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=HJvvRoe0W": {
    "title": "An image representation based convolutional network for DNA classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc0ac9cff977ec202a09d49eeca8ee6a59bf5553",
    "semantic_title": "an image representation based convolutional network for dna classification",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=S1uxsye0Z": {
    "title": "Adaptive Dropout with Rademacher Complexity Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "871d5b3cffda3e7a8d4493a0fa722297e74ec346",
    "semantic_title": "adaptive dropout with rademacher complexity regularization",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=SJQHjzZ0-": {
    "title": "Quantitatively Evaluating GANs With Divergences Proposed for Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c9a201ffabe48f35226468617f783b3d7a8f0086",
    "semantic_title": "quantitatively evaluating gans with divergences proposed for training",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=ryup8-WCW": {
    "title": "Measuring the Intrinsic Dimension of Objective Landscapes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d55d1d035e91220335edff0fe8f5d249d8c4a00b",
    "semantic_title": "measuring the intrinsic dimension of objective landscapes",
    "citation_count": 417,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk7KsfW0-": {
    "title": "Lifelong Learning with Dynamically Expandable Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "df1769afbbf3904877629dd7e785f195361ec531",
    "semantic_title": "lifelong learning with dynamically expandable networks",
    "citation_count": 1233,
    "authors": []
  },
  "https://openreview.net/forum?id=S1vuO-bCW": {
    "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ec4df801c640169e18d8da1bdc65a0b6fc2d3d94",
    "semantic_title": "leave no trace: learning to reset for safe and autonomous reinforcement learning",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy8XvGb0-": {
    "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "76ec5c774bb3fd04f9e68864a411286536a544c5",
    "semantic_title": "latent constraints: learning to generate conditionally from unconditional generative models",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=H1mCp-ZRZ": {
    "title": "Action-dependent Control Variates for Policy Optimization via Stein Identity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad9f32abf3894332c9dccca93e4ac64fbb81c41f",
    "semantic_title": "action-dependent control variates for policy optimization via stein identity",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MczcgR-": {
    "title": "Understanding Short-Horizon Bias in Stochastic Meta-Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9cb2a7e69daaefbe84e98cab721aa7db23ead5d6",
    "semantic_title": "understanding short-horizon bias in stochastic meta-optimization",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=BkLhaGZRW": {
    "title": "Improving GAN Training via Binarized Representation Entropy (BRE) Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f074217d51ffb0da3b9716af4adae56215de488",
    "semantic_title": "improving gan training via binarized representation entropy (bre) regularization",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rkZvSe-RZ": {
    "title": "Ensemble Adversarial Training: Attacks and Defenses",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "136dee73f203df2f4831994bf4f0c0a4ad2e764e",
    "semantic_title": "ensemble adversarial training: attacks and defenses",
    "citation_count": 2738,
    "authors": []
  },
  "https://openreview.net/forum?id=Syhr6pxCW": {
    "title": "PixelNN: Example-based Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "731f0d42b1679aa6c89693ff62d41f74f2519124",
    "semantic_title": "pixelnn: example-based image synthesis",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ0hF1Z0b": {
    "title": "Learning Differentially Private Recurrent Language Models",
    "volume": "poster",
    "abstract": "We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset",
    "checked": true,
    "id": "ed46493d568030b42f0154d9e5bf39bbd07962b3",
    "semantic_title": "learning differentially private recurrent language models",
    "citation_count": 1289,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Lc-Gb0Z": {
    "title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem",
    "volume": "poster",
    "abstract": "As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator",
    "checked": true,
    "id": "af43a2517dd0c75d20b4bff5c4989bd946b68d84",
    "semantic_title": "deep learning as a mixed convex-combinatorial optimization problem",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkn7CBaTW": {
    "title": "Learning how to explain neural networks: PatternNet and PatternAttribution",
    "volume": "poster",
    "abstract": "DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks",
    "checked": true,
    "id": "ca2ed3e2e0514feb6f613b9875f3d98ade1b1dc1",
    "semantic_title": "learning how to explain neural networks: patternnet and patternattribution",
    "citation_count": 340,
    "authors": []
  },
  "https://openreview.net/forum?id=rkQkBnJAb": {
    "title": "Improving GANs Using Optimal Transport",
    "volume": "poster",
    "abstract": "We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation",
    "checked": true,
    "id": "69902406e7d08f8865f02185699978db499d25e7",
    "semantic_title": "improving gans using optimal transport",
    "citation_count": 324,
    "authors": []
  },
  "https://openreview.net/forum?id=SyZI0GWCZ": {
    "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",
    "volume": "poster",
    "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox)",
    "checked": true,
    "id": "1b225474e7a5794f98cdfbde8b12ccbc56799409",
    "semantic_title": "decision-based adversarial attacks: reliable attacks against black-box machine learning models",
    "citation_count": 1352,
    "authors": []
  },
  "https://openreview.net/forum?id=HJhIM0xAW": {
    "title": "Learning a neural response metric for retinal prosthesis",
    "volume": "poster",
    "abstract": "Retinal prostheses for treating incurable blindness are designed to electrically stimulate surviving retinal neurons, causing them to send artificial visual signals to the brain. However, electrical stimulation generally cannot precisely reproduce normal patterns of neural activity in the retina. Therefore, an electrical stimulus must be selected that produces a neural response as close as possible to the desired response. This requires a technique for computing a distance between the desired response and the achievable response that is meaningful in terms of the visual signal being conveyed. Here we propose a method to learn such a metric on neural responses, directly from recorded light responses of a population of retinal ganglion cells (RGCs) in the primate retina. The learned metric produces a measure of similarity of RGC population responses that accurately reflects the similarity of the visual input. Using data from electrical stimulation experiments, we demonstrate that this metric may improve the performance of a prosthesis",
    "checked": true,
    "id": "440b22ccdc7990c65d948856f9fbf8ca762745ab",
    "semantic_title": "learning a neural response metric for retinal prosthesis",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SkT5Yg-RZ": {
    "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
    "volume": "poster",
    "abstract": "We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will \"propose\" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward",
    "checked": true,
    "id": "8499a250422a3c66357367c8d5fa504de5424c59",
    "semantic_title": "intrinsic motivation and automatic curricula via asymmetric self-play",
    "citation_count": 338,
    "authors": []
  },
  "https://openreview.net/forum?id=rydeCEhs-": {
    "title": "SMASH: One-Shot Model Architecture Search through HyperNetworks",
    "volume": "poster",
    "abstract": "Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks",
    "checked": true,
    "id": "e56b10f7cd4bf037beac84da5925dc4544fab974",
    "semantic_title": "smash: one-shot model architecture search through hypernetworks",
    "citation_count": 765,
    "authors": []
  },
  "https://openreview.net/forum?id=HJWLfGWRb": {
    "title": "Matrix capsules with EM routing",
    "volume": "poster",
    "abstract": "A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network",
    "checked": true,
    "id": "603caed9430283db6c7f43169555c8d18e97a281",
    "semantic_title": "matrix capsules with em routing",
    "citation_count": 1054,
    "authors": []
  },
  "https://openreview.net/forum?id=HyRVBzap-": {
    "title": "Cascade Adversarial Machine Learning Regularized with a Unified Embedding",
    "volume": "poster",
    "abstract": "Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario",
    "checked": true,
    "id": "45a710be199c8eb43f465c88fc4b343267c35d38",
    "semantic_title": "cascade adversarial machine learning regularized with a unified embedding",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=SJiHXGWAZ": {
    "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
    "volume": "poster",
    "abstract": "Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large-scale road network traffic datasets and observe consistent improvement of 12% - 15% over state-of-the-art baselines",
    "checked": true,
    "id": "9ba0186ed40656329c421f55ada7313293e13f17",
    "semantic_title": "diffusion convolutional recurrent neural network: data-driven traffic forecasting",
    "citation_count": 3106,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgOLb-0W": {
    "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
    "volume": "poster",
    "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks",
    "checked": true,
    "id": "f1cbf097ce436f7304a1984f4a29ab41f75ebfe3",
    "semantic_title": "neural language modeling by jointly learning syntax and lexicon",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ZvaaeAZ": {
    "title": "WRPN: Wide Reduced-Precision Networks",
    "volume": "poster",
    "abstract": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory band- width and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN -- wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks",
    "checked": true,
    "id": "b18b2f8f6e55789b03f983f02f33283c39e6e36a",
    "semantic_title": "wrpn: wide reduced-precision networks",
    "citation_count": 267,
    "authors": []
  },
  "https://openreview.net/forum?id=HkMvEOlAb": {
    "title": "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization",
    "volume": "poster",
    "abstract": "In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature",
    "checked": true,
    "id": "f4d2e8dd636d04de745d0cfc459b5d50a0df53ae",
    "semantic_title": "learning latent representations in neural networks for clustering through pseudo supervision and graph-based activity regularization",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=SJyEH91A-": {
    "title": "Learning Wasserstein Embeddings",
    "volume": "poster",
    "abstract": "The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method",
    "checked": true,
    "id": "d7629df50f640e385dcbafb1735df4d0370fd06b",
    "semantic_title": "learning wasserstein embeddings",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=ryTp3f-0-": {
    "title": "Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) agents improve through trial-and-error, but when reward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying to emails, where a single mistake can ruin the entire sequence of actions. A common remedy is to \"warm-start\" the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level \"workflows\" which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., \"Step 1: click on a textbox; Step 2: enter some text\"). Our exploration policy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent's ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x",
    "checked": true,
    "id": "7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1",
    "semantic_title": "reinforcement learning on web interfaces using workflow-guided exploration",
    "citation_count": 223,
    "authors": []
  },
  "https://openreview.net/forum?id=HkAClQgA-": {
    "title": "A Deep Reinforced Model for Abstractive Summarization",
    "volume": "poster",
    "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries",
    "checked": true,
    "id": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
    "semantic_title": "a deep reinforced model for abstractive summarization",
    "citation_count": 1560,
    "authors": []
  },
  "https://openreview.net/forum?id=By-7dz-AZ": {
    "title": "A Framework for the Quantitative Evaluation of Disentangled Representations",
    "volume": "poster",
    "abstract": "Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by recent state-of-the-art models",
    "checked": true,
    "id": "adf2ac6b99b7d48b6a9c908532ca249de2cec3ae",
    "semantic_title": "a framework for the quantitative evaluation of disentangled representations",
    "citation_count": 470,
    "authors": []
  },
  "https://openreview.net/forum?id=ByKWUeWA-": {
    "title": "GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets",
    "volume": "poster",
    "abstract": "Estimating individualized treatment effects (ITE) is a challenging task due to the need for an individual's potential outcomes to be learned from biased data and without having access to the counterfactuals. We propose a novel method for inferring ITE based on the Generative Adversarial Nets (GANs) framework. Our method, termed Generative Adversarial Nets for inference of Individualized Treatment Effects (GANITE), is motivated by the possibility that we can capture the uncertainty in the counterfactual distributions by attempting to learn them using a GAN. We generate proxies of the counterfactual outcomes using a counterfactual generator, G, and then pass these proxies to an ITE generator, I, in order to train it. By modeling both of these using the GAN framework, we are able to infer based on the factual data, while still accounting for the unseen counterfactuals. We test our method on three real-world datasets (with both binary and multiple treatments) and show that GANITE outperforms state-of-the-art methods",
    "checked": true,
    "id": "d5f51bd63d6efe03ff95f01ca517bc5d29bfbbf0",
    "semantic_title": "ganite: estimation of individualized treatment effects using generative adversarial nets",
    "citation_count": 399,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ1nzBeA-": {
    "title": "Multi-Task Learning for Document Ranking and Query Suggestion",
    "volume": "poster",
    "abstract": "We propose a multi-task learning framework to jointly learn document ranking and query suggestion for web search. It consists of two major components, a document ranker, and a query recommender. Document ranker combines current query and session information and compares the combined representation with document representation to rank the documents. Query recommender tracks users' query reformulation sequence considering all previous in-session queries using a sequence to sequence approach. As both tasks are driven by the users' underlying search intent, we perform joint learning of these two components through session recurrence, which encodes search context and intent. Extensive comparisons against state-of-the-art document ranking and query suggestion algorithms are performed on the public AOL search log, and the promising results endorse the effectiveness of the joint learning framework",
    "checked": true,
    "id": "15e12b3b6719e8e4516b7516fae212ea61a94fce",
    "semantic_title": "multi-task learning for document ranking and query suggestion",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=H1aIuk-RW": {
    "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
    "volume": "poster",
    "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin",
    "checked": true,
    "id": "c342c71cb23199f112d0bc644fcce56a7306bf94",
    "semantic_title": "active learning for convolutional neural networks: a core-set approach",
    "citation_count": 1994,
    "authors": []
  },
  "https://openreview.net/forum?id=S1v4N2l0-": {
    "title": "Unsupervised Representation Learning by Predicting Image Rotations",
    "volume": "poster",
    "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4%$that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet",
    "checked": true,
    "id": "aab368284210c1bb917ec2d31b84588e3d2d7eb4",
    "semantic_title": "unsupervised representation learning by predicting image rotations",
    "citation_count": 3304,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkc-TeZ0W": {
    "title": "A Hierarchical Model for Device Placement",
    "volume": "poster",
    "abstract": "We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. Our method learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed method is trained with policy gradient and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation",
    "checked": true,
    "id": "22ba3276c8797fddeb5d9db083ed1010da182549",
    "semantic_title": "a hierarchical model for device placement",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=SyJS-OgR-": {
    "title": "Multi-level Residual Networks from Dynamical Systems View",
    "volume": "poster",
    "abstract": "Deep residual networks (ResNets) and their variants are widely used in many computer vision applications and natural language processing tasks. However, the theoretical principles for designing and training ResNets are still not fully understood. Recently, several points of view have emerged to try to interpret ResNet theoretically, such as unraveled view, unrolled iterative estimation and dynamical systems view. In this paper, we adopt the dynamical systems point of view, and analyze the lesioning properties of ResNet both theoretically and experimentally. Based on these analyses, we additionally propose a novel method for accelerating ResNet training. We apply the proposed method to train ResNets and Wide ResNets for three image classification benchmarks, reducing training time by more than 40\\% with superior or on-par accuracy",
    "checked": true,
    "id": "cd958525291ee1ab856d23aa93cb95c86d87ccbe",
    "semantic_title": "multi-level residual networks from dynamical systems view",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=r1ZdKJ-0W": {
    "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking",
    "volume": "poster",
    "abstract": "Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph",
    "checked": true,
    "id": "2b76b6e766547b3c6dbc2785a084ec3b72cb760d",
    "semantic_title": "deep gaussian embedding of graphs: unsupervised inductive learning via ranking",
    "citation_count": 648,
    "authors": []
  },
  "https://openreview.net/forum?id=r1dHXnH6-": {
    "title": "Natural Language Inference over Interaction Space",
    "volume": "poster",
    "abstract": "Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system",
    "checked": true,
    "id": "1778e32c18bd611169e64c1805a51abff341ca53",
    "semantic_title": "natural language inference over interaction space",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=HyydRMZC-": {
    "title": "Spatially Transformed Adversarial Examples",
    "volume": "poster",
    "abstract": "Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the L_p distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of L_p distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large L_p distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted",
    "checked": true,
    "id": "d3c071dbbb4520ed5875f7e064a9da87240534db",
    "semantic_title": "spatially transformed adversarial examples",
    "citation_count": 524,
    "authors": []
  },
  "https://openreview.net/forum?id=S18Su--CW": {
    "title": "Thermometer Encoding: One Hot Way To Resist Adversarial Examples",
    "volume": "poster",
    "abstract": "It is well known that it is possible to construct \"adversarial examples\" for neural networks: inputs which are misclassified by the network yet indistinguishable from true data. We propose a simple modification to standard neural network architectures, thermometer encoding, which significantly increases the robustness of the network to adversarial examples. We demonstrate this robustness with experiments on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that models with thermometer-encoded inputs consistently have higher accuracy on adversarial examples, without decreasing generalization. State-of-the-art accuracy under the strongest known white-box attack was increased from 93.20% to 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10. We explore the properties of these networks, providing evidence that thermometer encodings help neural networks to find more-non-linear decision boundaries",
    "checked": true,
    "id": "8b9127bee0f7d109da2672ba06d0f39a5a60335a",
    "semantic_title": "thermometer encoding: one hot way to resist adversarial examples",
    "citation_count": 614,
    "authors": []
  },
  "https://openreview.net/forum?id=SkhQHMW0W": {
    "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
    "volume": "poster",
    "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile",
    "checked": true,
    "id": "92495abbac86394cb759bec15a763dbf49a8e590",
    "semantic_title": "deep gradient compression: reducing the communication bandwidth for distributed training",
    "citation_count": 1413,
    "authors": []
  },
  "https://openreview.net/forum?id=BkN_r2lR-": {
    "title": "Identifying Analogies Across Domains",
    "volume": "poster",
    "abstract": "Identifying analogies across domains without supervision is a key task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision",
    "checked": true,
    "id": "411500f7aaa0f3c00d492b78b24b33da0fd0d58d",
    "semantic_title": "identifying analogies across domains",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJQVZW0b": {
    "title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ebc3bd9cd67193b3a8f84d43d5b4377107c680dc",
    "semantic_title": "hierarchical and interpretable skill acquisition in multi-task reinforcement learning",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=rJm7VfZA-": {
    "title": "Learning Parametric Closed-Loop Policies for Markov Potential Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f56eadb6312770fbd190ac7b0f86b70bd928ba7",
    "semantic_title": "learning parametric closed-loop policies for markov potential games",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=rkpoTaxA-": {
    "title": "Self-ensembling for visual domain adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c012e5b3ddb8a60420e8f92162d32ad135f9ba1",
    "semantic_title": "self-ensembling for visual domain adaptation",
    "citation_count": 521,
    "authors": []
  },
  "https://openreview.net/forum?id=rywHCPkAW": {
    "title": "Noisy Networks For Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3fab4d28be5ff73f42c8b307117aa8d680345ad9",
    "semantic_title": "noisy networks for exploration",
    "citation_count": 897,
    "authors": []
  },
  "https://openreview.net/forum?id=rkmu5b0a-": {
    "title": "MGAN: Training Generative Adversarial Nets with Multiple Generators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e152b726390ebbedf94e74dcc4dc943d121f1c3e",
    "semantic_title": "mgan: training generative adversarial nets with multiple generators",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=B13njo1R-": {
    "title": "Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "714bb2c21605cde034334aa9e345a9b19bc44cdf",
    "semantic_title": "progressive reinforcement learning with distillation for multi-skilled motion control",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=S1XolQbRW": {
    "title": "Model compression via distillation and quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6a4bf043af1a9ec7f104a7b7ab56806b241ceda",
    "semantic_title": "model compression via distillation and quantization",
    "citation_count": 734,
    "authors": []
  },
  "https://openreview.net/forum?id=B1IDRdeCW": {
    "title": "The High-Dimensional Geometry of Binary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "241ceceaff111fcdd91fdbd0538e7ab9b055aee0",
    "semantic_title": "the high-dimensional geometry of binary neural networks",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=SJaP_-xAb": {
    "title": "Deep Learning with Logged Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "52782e1865611a374f8938fc83e735525e2253da",
    "semantic_title": "deep learning with logged bandit feedback",
    "citation_count": 144,
    "authors": []
  },
  "https://openreview.net/forum?id=ryiAv2xAZ": {
    "title": "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "36653f8705b56e39642bcd123494eb680cd1636b",
    "semantic_title": "training confidence-calibrated classifiers for detecting out-of-distribution samples",
    "citation_count": 885,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeqO7x0-": {
    "title": "Unsupervised Cipher Cracking Using Discrete GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b589b9f10258efb5404f06693c659d68e67929d3",
    "semantic_title": "unsupervised cipher cracking using discrete gans",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=HktRlUlAZ": {
    "title": "Polar Transformer Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "54181b2d8daf5cc3649ad4b499c464dd48a0c63b",
    "semantic_title": "polar transformer networks",
    "citation_count": 187,
    "authors": []
  },
  "https://openreview.net/forum?id=B1X0mzZCW": {
    "title": "Fidelity-Weighted Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "19ff02bf17285335894b0a3af137c4c2fc6fbd4d",
    "semantic_title": "fidelity-weighted learning",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=S1cZsf-RW": {
    "title": "WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "41e6727bd2163b2d094873701f919db315f45001",
    "semantic_title": "whai: weibull hybrid autoencoding inference for deep topic modeling",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ae1lZRb": {
    "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b8d67593c4ab1b1e3eccc158daee76703b328aa",
    "semantic_title": "apprentice: using knowledge distillation techniques to improve low-precision network accuracy",
    "citation_count": 331,
    "authors": []
  },
  "https://openreview.net/forum?id=B1J_rgWRW": {
    "title": "Understanding Deep Neural Networks with Rectified Linear Units",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9375729d21a344a5ccccd5f53556ddf90b957cd9",
    "semantic_title": "understanding deep neural networks with rectified linear units",
    "citation_count": 644,
    "authors": []
  },
  "https://openreview.net/forum?id=r1vuQG-CW": {
    "title": "HexaConv",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d5cae4b8c3ee0abf350302c36648b353f167871",
    "semantic_title": "hexaconv",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=HyrCWeWCb": {
    "title": "Trust-PCL: An Off-Policy Trust Region Method for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc8eb0c66f8977d3b23dedb247607a8af2360859",
    "semantic_title": "trust-pcl: an off-policy trust region method for continuous control",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=r1NYjfbR-": {
    "title": "Generative networks as inverse problems with Scattering transforms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2b91f9dff3d5b21411f3f88a59f5e55450d15312",
    "semantic_title": "generative networks as inverse problems with scattering transforms",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HJsjkMb0Z": {
    "title": "i-RevNet: Deep Invertible Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11e7c4182a7813d5acf1be198c8c96d164fb95a2",
    "semantic_title": "i-revnet: deep invertible networks",
    "citation_count": 334,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJySbbAZ": {
    "title": "Training GANs with Optimism",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "63f2a1289ff16ca84cca383c7687452bf61c82c6",
    "semantic_title": "training gans with optimism",
    "citation_count": 520,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Yy1BxCZ": {
    "title": "Don't Decay the Learning Rate, Increase the Batch Size",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3299aee7a354877e43339d06abb967af2be8b872",
    "semantic_title": "don't decay the learning rate, increase the batch size",
    "citation_count": 996,
    "authors": []
  },
  "https://openreview.net/forum?id=ryH20GbRW": {
    "title": "Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5c1a251a6609ddfb6d6d7425169c8170d95db638",
    "semantic_title": "relational neural expectation maximization: unsupervised discovery of objects and their interactions",
    "citation_count": 292,
    "authors": []
  },
  "https://openreview.net/forum?id=B1DmUzWAW": {
    "title": "A Simple Neural Attentive Meta-Learner",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e9c1e0d247b20a0683f4797d9ea248c3b53d424",
    "semantic_title": "a simple neural attentive meta-learner",
    "citation_count": 1283,
    "authors": []
  },
  "https://openreview.net/forum?id=BJGWO9k0Z": {
    "title": "Critical Percolation as a Framework to Analyze the Training of Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f05a0fe30d9f9ccf29f74854fdeacbc132dbf23e",
    "semantic_title": "critical percolation as a framework to analyze the training of deep networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HkTEFfZRb": {
    "title": "Attacking Binarized Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "64520a1f652ebd04565354fbb8a6281606c9724d",
    "semantic_title": "attacking binarized neural networks",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=SkHDoG-Cb": {
    "title": "Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ade1e0d4744d2eb5bf7bab97289ffd7eeb5a661",
    "semantic_title": "simulated+unsupervised learning with adaptive data generation and bidirectional mappings",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk6fD5yCb": {
    "title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3d520a8673493aaeebe6fb3008c27de917fd8d2b",
    "semantic_title": "espresso: efficient forward propagation for binary deep neural networks",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=S1WRibb0Z": {
    "title": "Expressive power of recurrent neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a73f6ecbf2c242fba2b2ff4e70efaf47df66049e",
    "semantic_title": "expressive power of recurrent neural networks",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=HyfHgI6aW": {
    "title": "Memory Augmented Control Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d94d4da65ff55117345639aae57046b77c65b36e",
    "semantic_title": "memory augmented control networks",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Ddp1-Rb": {
    "title": "mixup: Beyond Empirical Risk Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4feef0fd284feb1233399b400eb897f59ec92755",
    "semantic_title": "mixup: beyond empirical risk minimization",
    "citation_count": 9831,
    "authors": []
  },
  "https://openreview.net/forum?id=rkPLzgZAZ": {
    "title": "Modular Continual Learning in a Unified Visual Environment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cf7a7f757fb3c83fb3f6e3b1889f014825dd906c",
    "semantic_title": "modular continual learning in a unified visual environment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxF5RgC-": {
    "title": "Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eedaa9eb9c6a8228c79caf560614a431736b62d5",
    "semantic_title": "sparse persistent rnns: squeezing large recurrent networks on-chip",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=rJXMpikCZ": {
    "title": "Graph Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
    "semantic_title": "graph attention networks",
    "citation_count": 20357,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hcZZ-AW": {
    "title": "N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
    "volume": "poster",
    "abstract": "While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks",
    "checked": true,
    "id": "92d621a603cda8c32214d70953e180fe5a442f3e",
    "semantic_title": "n2n learning: network to network compression via policy gradient reinforcement learning",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=rkLyJl-0-": {
    "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks",
    "volume": "poster",
    "abstract": "Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer",
    "checked": true,
    "id": "3f7f6c2b7f2264b316d767623834379878212fc8",
    "semantic_title": "neumann optimizer: a practical optimization algorithm for deep neural networks",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=ByBAl2eAZ": {
    "title": "Parameter Space Noise for Exploration",
    "volume": "poster",
    "abstract": "Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks",
    "checked": true,
    "id": "142497432fe179ddb6ffe600c64a837ec6179550",
    "semantic_title": "parameter space noise for exploration",
    "citation_count": 597,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwBEMWCZ": {
    "title": "Skip Connections Eliminate Singularities",
    "volume": "poster",
    "abstract": "Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets",
    "checked": true,
    "id": "31ea21d2fdf9e243aac746b68ca0111952fc58f4",
    "semantic_title": "skip connections eliminate singularities",
    "citation_count": 231,
    "authors": []
  },
  "https://openreview.net/forum?id=SkA-IE06W": {
    "title": "When is a Convolutional Filter Easy to Learn?",
    "volume": "poster",
    "abstract": "We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions. Our theory also justifies the two-stage learning rate strategy in deep neural networks. While our focus is theoretical, we also present experiments that justify our theoretical findings",
    "checked": true,
    "id": "fbe1d29737b66840d2bb9b74cd093858ef1805dd",
    "semantic_title": "when is a convolutional filter easy to learn?",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ8c3f-0b": {
    "title": "Auto-Encoding Sequential Monte Carlo",
    "volume": "poster",
    "abstract": "We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models",
    "checked": true,
    "id": "2d8ea7941e608dab54004eb13f494837dd96d83b",
    "semantic_title": "auto-encoding sequential monte carlo",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=SJA7xfb0b": {
    "title": "Sobolev GAN",
    "volume": "poster",
    "abstract": "We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization",
    "checked": true,
    "id": "238f36eff8468bf6dd4483c4d49934c28da3f9a9",
    "semantic_title": "sobolev gan",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=BJJLHbb0-": {
    "title": "Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection",
    "volume": "poster",
    "abstract": "Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score",
    "checked": true,
    "id": "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
    "semantic_title": "deep autoencoding gaussian mixture model for unsupervised anomaly detection",
    "citation_count": 1665,
    "authors": []
  },
  "https://openreview.net/forum?id=SJcKhk-Ab": {
    "title": "Can recurrent neural networks warp time?",
    "volume": "poster",
    "abstract": "Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. We prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach. This result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort",
    "checked": true,
    "id": "0e9822e6c78b752f13ccf2943a8e41f9997f4a22",
    "semantic_title": "can recurrent neural networks warp time?",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=rytstxWAW": {
    "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
    "volume": "poster",
    "abstract": "The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate",
    "checked": true,
    "id": "2503dff90685857ce7295e37d0045e2eef41c8b8",
    "semantic_title": "fastgcn: fast learning with graph convolutional networks via importance sampling",
    "citation_count": 1521,
    "authors": []
  },
  "https://openreview.net/forum?id=Skz_WfbCZ": {
    "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
    "volume": "poster",
    "abstract": "We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis",
    "checked": true,
    "id": "4fc3ee440c2b0f66255a9e6966cee871ee0cc6da",
    "semantic_title": "a pac-bayesian approach to spectrally-normalized margin bounds for neural networks",
    "citation_count": 610,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk6WhagRW": {
    "title": "Emergent Communication through Negotiation",
    "volume": "poster",
    "abstract": "Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation",
    "checked": true,
    "id": "92ec8259e58f5107057e4e99fdcd03df6a08f9e7",
    "semantic_title": "emergent communication through negotiation",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=BJehNfW0-": {
    "title": "Do GANs learn the distribution? Some Theory and Empirics",
    "volume": "poster",
    "abstract": "Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support. In other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this evidence is presented that well-known GANs approaches do learn distributions of fairly low support. (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs, and consequently to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition",
    "checked": true,
    "id": "bdabdeab5f92029c136f1e36ab79b4f06bac0ef7",
    "semantic_title": "do gans learn the distribution? some theory and empirics",
    "citation_count": 163,
    "authors": []
  },
  "https://openreview.net/forum?id=r11Q2SlRW": {
    "title": "Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis",
    "volume": "poster",
    "abstract": "We present a real-time method for synthesizing highly complex human motions using a novel training regime we call the auto-conditioned Recurrent Neural Network (acRNN). Recently, researchers have attempted to synthesize new motion by using autoregressive techniques, but existing methods tend to freeze or diverge after a couple of seconds due to an accumulation of errors that are fed back into the network. Furthermore, such methods have only been shown to be reliable for relatively simple human motions, such as walking or running. In contrast, our approach can synthesize arbitrary motions with highly complex styles, including dances or martial arts in addition to locomotion. The acRNN is able to accomplish this by explicitly accommodating for autoregressive noise accumulation during training. Our work is the first to our knowledge that demonstrates the ability to generate over 18,000 continuous frames (300 seconds) of new complex human motion w.r.t. different styles",
    "checked": true,
    "id": "06dd367528f259c14a923c4912c61d48bcd16afd",
    "semantic_title": "auto-conditioned recurrent networks for extended complex human motion synthesis",
    "citation_count": 233,
    "authors": []
  },
  "https://openreview.net/forum?id=BJk59JZ0b": {
    "title": "Guide Actor-Critic for Continuous Control",
    "volume": "poster",
    "abstract": "Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised learning. Our main theoretical contributions are two folds. First, we show that GAC updates the guide actor by performing second-order optimization in the action space where the curvature matrix is based on the Hessians of the critic. Second, we show that the deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Through experiments, we show that our method is a promising reinforcement learning method for continuous controls",
    "checked": true,
    "id": "27bc1680936e27cfc808923443b39c00fd12959b",
    "semantic_title": "guide actor-critic for continuous control",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=r1l4eQW0Z": {
    "title": "Kernel Implicit Variational Inference",
    "volume": "poster",
    "abstract": "Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks",
    "checked": true,
    "id": "2f39fd505b04ca363b9efe23f1937fd414a93ce7",
    "semantic_title": "kernel implicit variational inference",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJinbWRZ": {
    "title": "Model-Ensemble Trust-Region Policy Optimization",
    "volume": "poster",
    "abstract": "Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks",
    "checked": true,
    "id": "27dfecb6bb0308c7484e13dcaefd5eeebba677d3",
    "semantic_title": "model-ensemble trust-region policy optimization",
    "citation_count": 453,
    "authors": []
  },
  "https://openreview.net/forum?id=S1DWPP1A-": {
    "title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration",
    "volume": "poster",
    "abstract": "Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered feature space, limiting their autonomy. In this work, we propose an approach using deep representation learning algorithms to learn an adequate goal space. This is a developmental 2-stage approach: first, in a perceptual learning stage, deep learning algorithms use passive raw sensor observations of world changes to learn a corresponding latent space; then goal exploration happens in a second stage by sampling goals in this latent space. We present experiments with a simulated robot arm interacting with an object, and we show that exploration algorithms using such learned representations can closely match, and even sometimes improve, the performance obtained using engineered representations",
    "checked": true,
    "id": "330d56c3641cddd8c78440e768cd80795f23cab4",
    "semantic_title": "unsupervised learning of goal spaces for intrinsically motivated goal exploration",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=rJUYGxbCW": {
    "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
    "volume": "poster",
    "abstract": "Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10",
    "checked": true,
    "id": "e83291498a3bc6b0efe8f9571e9c9ca1811707bd",
    "semantic_title": "pixeldefend: leveraging generative models to understand and defend against adversarial examples",
    "citation_count": 791,
    "authors": []
  },
  "https://openreview.net/forum?id=rJYFzMZC-": {
    "title": "Simulating Action Dynamics with Neural Process Networks",
    "volume": "poster",
    "abstract": "Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics. Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives",
    "checked": true,
    "id": "26953cc3d09920b54071b73866f85d6bb1a6184c",
    "semantic_title": "simulating action dynamics with neural process networks",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=ry_WPG-A-": {
    "title": "On the Information Bottleneck Theory of Deep Learning",
    "volume": "poster",
    "abstract": "The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period",
    "checked": true,
    "id": "0a255e716a89b787336ab956f0aa74424629c950",
    "semantic_title": "on the information bottleneck theory of deep learning",
    "citation_count": 580,
    "authors": []
  },
  "https://openreview.net/forum?id=r1SnX5xCb": {
    "title": "Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "For every prediction we might wish to make, we must decide what to observe (what source of information) and when to observe it. Because making observations is costly, this decision must trade off the value of information against the cost of observation. Making observations (sensing) should be an active choice. To solve the problem of active sensing we develop a novel deep learning architecture: Deep Sensing. At training time, Deep Sensing learns how to issue predictions at various cost-performance points. To do this, it creates multiple representations at various performance levels associated with different measurement rates (costs). This requires learning how to estimate the value of real measurements vs. inferred measurements, which in turn requires learning how to infer missing (unobserved) measurements. To infer missing measurements, we develop a Multi-directional Recurrent Neural Network (M-RNN). An M-RNN differs from a bi-directional RNN in that it sequentially operates across streams in addition to within streams, and because the timing of inputs into the hidden layers is both lagged and advanced. At runtime, the operator prescribes a performance level or a cost constraint, and Deep Sensing determines what measurements to take and what to infer from those measurements, and then issues predictions. To demonstrate the power of our method, we apply it to two real-world medical datasets with significantly improved performance",
    "checked": true,
    "id": "dc4c91fcf89d11f3210546c374ef7b17a64565f7",
    "semantic_title": "deep sensing: active sensing using multi-directional recurrent neural networks",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Gi6LeRZ": {
    "title": "Learning from Between-class Examples for Deep Sound Recognition",
    "volume": "poster",
    "abstract": "Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning (BC learning). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds. We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher's criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes. The experimental results show that BC learning improves the performance on various sound recognition networks, datasets, and data augmentation schemes, in which BC learning proves to be always beneficial. Furthermore, we construct a new deep sound recognition network (EnvNet-v2) and train it with BC learning. As a result, we achieved a performance surpasses the human level",
    "checked": true,
    "id": "2c5174a4f3f6978cc28e2d3c15feae8610372bcd",
    "semantic_title": "learning from between-class examples for deep sound recognition",
    "citation_count": 237,
    "authors": []
  },
  "https://openreview.net/forum?id=rJGZq6g0-": {
    "title": "Emergent Communication in a Multi-Modal, Multi-Step Referential Game",
    "volume": "poster",
    "abstract": "Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct modalities of an object, and their information exchange is bidirectional and of arbitrary duration. The multi-modal multi-step setting allows agents to develop an internal communication significantly closer to natural language, in that they share a single set of messages, and that the length of the conversation may vary according to the difficulty of the task. We examine these properties empirically using a dataset consisting of images and textual descriptions of mammals, where the agents are tasked with identifying the correct object. Our experiments indicate that a robust and efficient communication protocol emerges, where gradual information exchange informs better predictions and higher communication bandwidth improves generalization",
    "checked": true,
    "id": "b9ff5712adb340c70fe3e415d09861ba45609280",
    "semantic_title": "emergent communication in a multi-modal, multi-step referential game",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=HyZoi-WRb": {
    "title": "Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference",
    "volume": "poster",
    "abstract": "The importance-weighted autoencoder (IWAE) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the IWAE bounds as ordinary variational evidence lower bounds (ELBO) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the true marginal likelihood where for the bound defined on $K$ samples we show the bias to be of order O(1/K). In our theoretical analysis of the IWAE objective we derive asymptotic bias and variance expressions. Based on this analysis we develop jackknife variational inference (JVI), a family of bias-reduced estimators reducing the bias to $O(K^{-(m+1)})$ for any given m < K while retaining computational efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates in variational autoencoders. We also report first results on applying JVI to learning variational autoencoders. Our implementation is available at https://github.com/Microsoft/jackknife-variational-inference",
    "checked": true,
    "id": "664bfbe4a061dcd01aab2a7b5fc30358a8a10350",
    "semantic_title": "debiasing evidence approximations: on importance-weighted autoencoders and jackknife variational inference",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=BkpiPMbA-": {
    "title": "Decision Boundary Analysis of Adversarial Examples",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which are carefully crafted instances aiming to cause prediction errors for DNNs. Recent research on adversarial examples has examined local neighborhoods in the input space of DNN models. However, previous work has limited what regions to consider, focusing either on low-dimensional subspaces or small balls. In this paper, we argue that information from larger neighborhoods, such as from more directions and from greater distances, will better characterize the relationship between adversarial examples and the DNN models. First, we introduce an attack, OPTMARGIN, which generates adversarial examples robust to small perturbations. These examples successfully evade a defense that only considers a small ball around an input instance. Second, we analyze a larger neighborhood around input instances by looking at properties of surrounding decision boundaries, namely the distances to the boundaries and the adjacent classes. We find that the boundaries around these adversarial examples do not resemble the boundaries around benign examples. Finally, we show that, under scrutiny of the surrounding decision boundaries, our OPTMARGIN examples do not convincingly mimic benign examples. Although our experiments are limited to a few specific attacks, we hope these findings will motivate new, more evasive attacks and ultimately, effective defenses",
    "checked": true,
    "id": "08c588465b7d801ad912ef3e9107fa511ea0e403",
    "semantic_title": "decision boundary analysis of adversarial examples",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=rJwelMbR-": {
    "title": "Divide-and-Conquer Reinforcement Learning",
    "volume": "poster",
    "abstract": "Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead partitions the initial state space into \"slices\", and optimizes an ensemble of policies, each on a different slice. The ensemble is gradually unified into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed at https://sites.google.com/view/dnc-rl/",
    "checked": true,
    "id": "904307cb58795241b22cfaa34f560e610997f5c1",
    "semantic_title": "divide-and-conquer reinforcement learning",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=Skj8Kag0Z": {
    "title": "Stabilizing Adversarial Nets with Prediction Methods",
    "volume": "poster",
    "abstract": "Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates",
    "checked": true,
    "id": "ec25504486d8751e00e613ca6fa64b256e3581c8",
    "semantic_title": "stabilizing adversarial nets with prediction methods",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=BkUHlMZ0b": {
    "title": "Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach",
    "volume": "poster",
    "abstract": "The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and is computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed give better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifiers",
    "checked": true,
    "id": "72de5578808c87c5bab305e46854fdf0b0b2ddab",
    "semantic_title": "evaluating the robustness of neural networks: an extreme value theory approach",
    "citation_count": 469,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk8XMWgRb": {
    "title": "Not-So-Random Features",
    "volume": "poster",
    "abstract": "We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of translation-invariant or rotation-invariant kernels. Our method produces a sequence of feature maps, iteratively refining the SVM margin. We provide rigorous guarantees for optimality and generalization, interpreting our algorithm as online equilibrium-finding dynamics in a certain two-player min-max game. Evaluations on synthetic and real-world datasets demonstrate scalability and consistent improvements over related random features-based methods",
    "checked": true,
    "id": "40dc2de4f461b0a673d35df49cd4d54b4c9865a5",
    "semantic_title": "not-so-random features",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=rkO3uTkAZ": {
    "title": "Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e5000713fa1dec7ba73162f516048b65110d96c0",
    "semantic_title": "memorization precedes generation: learning unsupervised gans with memory networks",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=HJJ23bW0b": {
    "title": "Initialization matters: Orthogonal Predictive State Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "597b1dedb58ddb5ee5c35938c1a2574c7a11e0f6",
    "semantic_title": "initialization matters: orthogonal predictive state recurrent neural networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=SyoDInJ0-": {
    "title": "Reinforcement Learning Algorithm Selection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "07affdd73f1691a31cb9c2bc4f9df956873e7ad6",
    "semantic_title": "reinforcement learning algorithm selection",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=H1dh6Ax0Z": {
    "title": "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11c421320be0e098ddb7e52257f3d132e99715d4",
    "semantic_title": "treeqn and atreec: differentiable tree-structured models for deep reinforcement learning",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=r1VVsebAZ": {
    "title": "Synthesizing realistic neural population activity patterns using Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9fd2a948964d9f18a77434f5eb868ba720689caa",
    "semantic_title": "synthesizing realistic neural population activity patterns using generative adversarial networks",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=H1T2hmZAb": {
    "title": "Deep Complex Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a742162820cdabee2831235665517d0e98041502",
    "semantic_title": "deep complex networks",
    "citation_count": 837,
    "authors": []
  },
  "https://openreview.net/forum?id=BJRZzFlRb": {
    "title": "Compressing Word Embeddings via Deep Compositional Code Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4bf6ce4a9366cdba069a45651606538f2febd8e6",
    "semantic_title": "compressing word embeddings via deep compositional code learning",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=H1q-TM-AW": {
    "title": "A DIRT-T Approach to Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "008c901b3fd9e46ee8d3bddb616121e2887b7e67",
    "semantic_title": "a dirt-t approach to unsupervised domain adaptation",
    "citation_count": 628,
    "authors": []
  },
  "https://openreview.net/forum?id=rywDjg-RW": {
    "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "85bdb70962147cd3586d99f50328025c82c9ac8e",
    "semantic_title": "neural-guided deductive search for real-time program synthesis from examples",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy2ogebAW": {
    "title": "Unsupervised Neural Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2a7afbb5609a723f8eea91bfde4b02579b048d6",
    "semantic_title": "unsupervised neural machine translation",
    "citation_count": 774,
    "authors": []
  },
  "https://openreview.net/forum?id=BkQqq0gRb": {
    "title": "Variational Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d475f695dedd94e96771fdaa1e5c075fd01d11cf",
    "semantic_title": "variational continual learning",
    "citation_count": 736,
    "authors": []
  },
  "https://openreview.net/forum?id=H1uR4GZRZ": {
    "title": "Stochastic Activation Pruning for Robust Adversarial Defense",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f201c77e7ccdf1f37115e16accac3486a65c03d",
    "semantic_title": "stochastic activation pruning for robust adversarial defense",
    "citation_count": 548,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ-C6JbRW": {
    "title": "Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9af92f8c0309695fdcae7193e1b0c203ab11136f",
    "semantic_title": "mastering the dungeon: grounded language learning by mechanical turker descent",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=SkVqXOxCb": {
    "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f605f8d5d6b143b8c34ccd498986a460c32e641",
    "semantic_title": "coulomb gans: provably optimal nash equilibria via potential fields",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=HJNMYceCW": {
    "title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f21ecd1a0bb77ab13320a4e24668c3e937ecf24b",
    "semantic_title": "residual loss prediction: reinforcement learning with no incremental feedback",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VjBebR-": {
    "title": "The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4fb11a58d5a3ffc0bb6d4ade334a366b4a431b02",
    "semantic_title": "the role of minimal complexity functions in unsupervised learning of semantic mappings",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=rkTS8lZAb": {
    "title": "Boundary Seeking GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "92ed1b3a074bcbacf7000975ac8a684f9061b257",
    "semantic_title": "boundary seeking gans",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=BydLzGb0Z": {
    "title": "Twin Networks: Matching the Future for Sequence Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6402703b62325865d00da1f58dbbcaf9a2bc417d",
    "semantic_title": "twin networks: matching the future for sequence generation",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=rk6cfpRjZ": {
    "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ca1060c50642f9f05735d3007873439347b3bea5",
    "semantic_title": "learning intrinsic sparse structures within long short-term memory",
    "citation_count": 142,
    "authors": []
  },
  "https://openreview.net/forum?id=HyiAuyb0b": {
    "title": "TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL",
    "checked": true,
    "id": "02f94fa49bfb2b6592cc9783589c6e56aaa3861a",
    "semantic_title": "td or not td: analyzing the role of temporal differencing in deep reinforcement learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx9GQb0-": {
    "title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect",
    "volume": "poster",
    "abstract": "Despite being impactful on a variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As a result, it gives rise to not only better photo-realistic samples than the previous methods but also state-of-the-art semi-supervised learning results. In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images",
    "checked": true,
    "id": "65912ba93e57b7c96b5abe48435f433bfec6cd7b",
    "semantic_title": "improving the improved training of wasserstein gans: a consistency term and its dual effect",
    "citation_count": 262,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Dy---0Z": {
    "title": "Distributed Prioritized Experience Replay",
    "volume": "poster",
    "abstract": "We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time",
    "checked": true,
    "id": "601a2d349fc26d7b82f905e924e2f91b0ac4b310",
    "semantic_title": "distributed prioritized experience replay",
    "citation_count": 742,
    "authors": []
  },
  "https://openreview.net/forum?id=H1cWzoxA-": {
    "title": "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but does not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called \"bi-directional block self-attention network (Bi-BloSAN)\", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN",
    "checked": true,
    "id": "0ef460c47377c3b9482d8177cbcafad1730a91a5",
    "semantic_title": "bi-directional block self-attention for fast and memory-efficient sequence modeling",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=SysEexbRb": {
    "title": "Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties",
    "volume": "poster",
    "abstract": "Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for linear neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of linear neural networks and shallow ReLU networks. One particular conclusion is that: While the loss function of linear networks has no spurious local minimum, the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum",
    "checked": true,
    "id": "f5b4a1e060161cdb07352dc66bf35690e5a3cfcd",
    "semantic_title": "critical points of linear neural networks: analytical forms and landscape properties",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=r1HhRfWRZ": {
    "title": "Learning Awareness Models",
    "volume": "poster",
    "abstract": "We consider the setting of an agent with a fixed body interacting with an unknown and uncertain external world. We show that models trained to predict proprioceptive information about the agent's body come to represent objects in the external world. In spite of being trained with only internally available signals, these dynamic body models come to represent external objects through the necessity of predicting their effects on the agent's own body. That is, the model learns holistic persistent representations of objects in the world, even though the only training signals are body signals. Our dynamics model is able to successfully predict distributions over 132 sensor readings over 100 steps into the future and we demonstrate that even when the body is no longer in contact with an object, the latent variables of the dynamics model continue to represent its shape. We show that active data collection by maximizing the entropy of predictions about the body---touch sensors, proprioception and vestibular information---leads to learning of dynamic models that show superior performance when used for control. We also collect data from a real robotic hand and show that the same models can be used to answer questions about properties of objects in the real world. Videos with qualitative results of our models are available at https://goo.gl/mZuqAV",
    "checked": true,
    "id": "4b82cfd0229f257f44d84bedb4bead85054597cc",
    "semantic_title": "learning awareness models",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=H1kG7GZAW": {
    "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations",
    "volume": "poster",
    "abstract": "Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We also propose a new disentanglement metric which is better aligned with the qualitative disentanglement observed in the decoder's output. We empirically observe significant improvement over existing methods in terms of both disentanglement and data likelihood (reconstruction quality)",
    "checked": true,
    "id": "26690f2548c6dbf630de202b40dec417b20c9b6c",
    "semantic_title": "variational inference of disentangled latent concepts from unlabeled observations",
    "citation_count": 523,
    "authors": []
  },
  "https://openreview.net/forum?id=rkrC3GbRW": {
    "title": "Learning a Generative Model for Validity in Complex Discrete Structures",
    "volume": "poster",
    "abstract": "Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether a partial sequence can function as the beginning of a full, valid sequence. This validator provides insight as to how individual sequence elements influence the validity of the overall sequence, and can be used to constrain sequence based models to generate valid sequences â€” and thus faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle which can evaluate validity of complete sequences provides a sparse reward signal. We demonstrate its effectiveness as a generative model of Python 3 source code for mathematical expressions, and in improving the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures",
    "checked": true,
    "id": "8a50c3aff2bb2111bcc0f6ddf0726af252a56daa",
    "semantic_title": "learning a generative model for validity in complex discrete structures",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=H196sainb": {
    "title": "Word translation without parallel data",
    "volume": "poster",
    "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available",
    "checked": true,
    "id": "562c09c112df56c5696c010d90a815d6018a86c8",
    "semantic_title": "word translation without parallel data",
    "citation_count": 1663,
    "authors": []
  },
  "https://openreview.net/forum?id=BywyFQlAW": {
    "title": "Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity",
    "volume": "poster",
    "abstract": "We introduce and study minimax curriculum learning (MCL), a new method for adaptively selecting a sequence of training subsets for a succession of stages in machine learning. The subsets are encouraged to be small and diverse early on, and then larger, harder, and allowably more homogeneous in later stages. At each stage, model weights and training sets are chosen by solving a joint continuous-discrete minimax optimization, whose objective is composed of a continuous loss (reflecting training set hardness) and a discrete submodular promoter of diversity for the chosen subset. MCL repeatedly solves a sequence of such optimizations with a schedule of increasing training set size and decreasing pressure on diversity encouragement. We reduce MCL to the minimization of a surrogate function handled by submodular maximization and continuous gradient methods. We show that MCL achieves better performance and, with a clustering trick, uses fewer labeled samples for both shallow and deep models while achieving the same performance. Our method involves repeatedly solving constrained submodular maximization of an only slowly varying function on the same ground set. Therefore, we develop a heuristic method that utilizes the previous submodular maximization solution as a warm start for the current submodular maximization process to reduce computation while still yielding a guarantee",
    "checked": true,
    "id": "f81376925a626fc053883f440a1051b244d3e813",
    "semantic_title": "minimax curriculum learning: machine teaching with desirable difficulties and scheduled diversity",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VGkIxRZ": {
    "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
    "volume": "poster",
    "abstract": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%",
    "checked": true,
    "id": "547c854985629cfa9404a5ba8ca29367b5f8c25f",
    "semantic_title": "enhancing the reliability of out-of-distribution image detection in neural networks",
    "citation_count": 2085,
    "authors": []
  },
  "https://openreview.net/forum?id=HJCXZQbAZ": {
    "title": "Hierarchical Density Order Embeddings",
    "volume": "poster",
    "abstract": "By representing words with probability densities rather than point vectors, proba- bilistic word embeddings can capture rich and interpretable semantic information and uncertainty (Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017). The uncertainty information can be particularly meaningful in capturing entailment relationships â€“ whereby general words such as \"entity\" correspond to broad distributions that encompass more specific words such as \"animal\" or \"instrument\". We introduce density order embeddings, which learn hierarchical representations through encapsulation of probability distributions. In particular, we propose simple yet effective loss functions and distance metrics, as well as graph-based schemes to select negative samples to better learn hierarchical probabilistic representations. Our approach provides state-of-the-art performance on the WordNet hypernym relationship prediction task and the challenging HyperLex lexical entailment dataset â€“ while retaining a rich and interpretable probabilistic representation",
    "checked": true,
    "id": "a621b40c8d7a7314d2988e6ac1aa3b318f9c1acd",
    "semantic_title": "hierarchical density order embeddings",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNGsseC-": {
    "title": "On the Expressive Power of Overlapping Architectures of Deep Learning",
    "volume": "poster",
    "abstract": "Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field). To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well. Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers",
    "checked": true,
    "id": "59228743a2f01445a5ecdb616b3c1a550343cdc1",
    "semantic_title": "on the expressive power of overlapping architectures of deep learning",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=BkSDMA36Z": {
    "title": "A New Method of Region Embedding for Text Classification",
    "volume": "poster",
    "abstract": "To represent a text as a bag of properly identified \"phrases\" and use the representation for processing the text is proved to be useful. The key question here is how to identify the phrases and represent them. The traditional method of utilizing n-grams can be regarded as an approximation of the approach. Such a method can suffer from data sparsity, however, particularly when the length of n-gram is large. In this paper, we propose a new method of learning and utilizing task-specific distributed representations of n-grams, referred to as \"region embeddings\". Without loss of generality we address text classification. We specifically propose two models for region embeddings. In our models, the representation of a word has two parts, the embedding of the word itself, and a weighting matrix to interact with the local context, referred to as local context unit. The region embeddings are learned and used in the classification task, as parameters of the neural network classifier. Experimental results show that our proposed method outperforms existing methods in text classification on several benchmark datasets. The results also indicate that our method can indeed capture the salient phrasal expressions in the texts",
    "checked": true,
    "id": "119120c0c7bab456a187ebc7454f78d7f8b03a32",
    "semantic_title": "a new method of region embedding for text classification",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=SJa9iHgAZ": {
    "title": "Residual Connections Encourage Iterative Inference",
    "volume": "poster",
    "abstract": "Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual architectures naturally encourage features to move along the negative gradient of loss during the feedforward phase. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and hurts generalization performance, and show that simple existing strategies can help alleviating this problem",
    "checked": true,
    "id": "098b96b713aa8a324090662c7f35213512f4525e",
    "semantic_title": "residual connections encourage iterative inference",
    "citation_count": 156,
    "authors": []
  },
  "https://openreview.net/forum?id=BydjJte0-": {
    "title": "Towards Reverse-Engineering Black-Box Neural Networks",
    "volume": "poster",
    "abstract": "Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models",
    "checked": true,
    "id": "e8b9fa6f9e0b606ff335a0557a838dea2696b084",
    "semantic_title": "towards reverse-engineering black-box neural networks",
    "citation_count": 370,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Dh8Tg0-": {
    "title": "Fix your classifier: the marginal value of training the last weight layer",
    "volume": "poster",
    "abstract": "Neural networks are commonly used as models for classification for a wide variety of tasks. Typically, a learned affine transformation is placed at the end of such models, yielding a per-class value used for classification. This classifier can have a vast number of parameters, which grows linearly with the number of possible classes, thus requiring increasingly more resources. In this work we argue that this classifier can be fixed, up to a global scale constant, with little or no loss of accuracy for most tasks, allowing memory and computational benefits. Moreover, we show that by initializing the classifier with a Hadamard matrix we can speed up inference as well. We discuss the implications for current understanding of neural network models",
    "checked": true,
    "id": "178339ef29211540683b36c0e1c6acffd998cddd",
    "semantic_title": "fix your classifier: the marginal value of training the last weight layer",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=B1nZ1weCZ": {
    "title": "Learning to Multi-Task by Active Sampling",
    "volume": "poster",
    "abstract": "One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training. In this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance",
    "checked": true,
    "id": "2b64a0a4c4ef3cde80c6ad49428648ef6e47cbfa",
    "semantic_title": "learning to multi-task by active sampling",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=HyyP33gAZ": {
    "title": "Activation Maximization Generative Adversarial Nets",
    "volume": "poster",
    "abstract": "Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric",
    "checked": true,
    "id": "4d4e657bf884097a2b88d56aef1a8733379bfb63",
    "semantic_title": "activation maximization generative adversarial nets",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=SyqShMZRb": {
    "title": "Syntax-Directed Variational Autoencoder for Structured Data",
    "volume": "poster",
    "abstract": "Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, e.g., computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing stochastic lazy attributes. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only syntactically valid, but also semantically reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches",
    "checked": true,
    "id": "7dd434b3799a6c8c346a1d7ee77d37980a4ef5b9",
    "semantic_title": "syntax-directed variational autoencoder for structured data",
    "citation_count": 329,
    "authors": []
  },
  "https://openreview.net/forum?id=ByQpn1ZA-": {
    "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step",
    "checked": true,
    "id": "471908e99d6965f0f6d249c9cd013485dc2b21df",
    "semantic_title": "many paths to equilibrium: gans do not need to decrease a divergence at every step",
    "citation_count": 211,
    "authors": []
  },
  "https://openreview.net/forum?id=B1QgVti6Z": {
    "title": "Empirical Risk Landscape Analysis for Understanding Deep Neural Networks",
    "volume": "poster",
    "abstract": "This work aims to provide comprehensive landscape analysis of empirical risk in deep neural networks (DNNs), including the convergence behavior of its gradient, its stationary points and the empirical risk itself to their corresponding population counterparts, which reveals how various network parameters determine the convergence performance. In particular, for an $l$-layer linear neural network consisting of $\\dm_i$ neurons in the $i$-th layer, we prove the gradient of its empirical risk uniformly converges to the one of its population risk, at the rate of $\\mathcal{O}(r^{2l} \\sqrt{l\\sqrt{\\max_i \\dm_i} s\\log(d/l)/n})$. Here $d$ is the total weight dimension, $s$ is the number of nonzero entries of all the weights and the magnitude of weights per layer is upper bounded by $r$. Moreover, we prove the one-to-one correspondence of the non-degenerate stationary points between the empirical and population risks and provide convergence guarantee for each pair. We also establish the uniform convergence of the empirical risk to its population counterpart and further derive the stability and generalization bounds for the empirical risk. In addition, we analyze these properties for deep \\emph{nonlinear} neural networks with sigmoid activation functions. We prove similar results for convergence behavior of their empirical risk gradients, non-degenerate stationary points as well as the empirical risk itself. To our best knowledge, this work is the first one theoretically characterizing the uniform convergence of the gradient and stationary points of the empirical risk of DNN models, which benefits the theoretical understanding on how the neural network depth $l$, the layer width $\\dm_i$, the network size $d$, the sparsity in weight and the parameter magnitude $r$ determine the neural network landscape",
    "checked": true,
    "id": "c7f938eb608b96a03d54f05caf3f675afd952c08",
    "semantic_title": "empirical risk landscape analysis for understanding deep neural networks",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hYRMbCW": {
    "title": "On the regularization of Wasserstein GANs",
    "volume": "poster",
    "abstract": "Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets",
    "checked": true,
    "id": "f64233585c793b56d61f95c1cf25282ad28ceb4b",
    "semantic_title": "on the regularization of wasserstein gans",
    "citation_count": 213,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ANxQW0b": {
    "title": "Maximum a Posteriori Policy Optimisation",
    "volume": "poster",
    "abstract": "We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings",
    "checked": true,
    "id": "a8ef08940341381390d9a5672546354d0ce51328",
    "semantic_title": "maximum a posteriori policy optimisation",
    "citation_count": 478,
    "authors": []
  },
  "https://openreview.net/forum?id=rJzIBfZAb": {
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "volume": "poster",
    "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models",
    "checked": true,
    "id": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
    "semantic_title": "towards deep learning models resistant to adversarial attacks",
    "citation_count": 12170,
    "authors": []
  },
  "https://openreview.net/forum?id=HJewuJWCZ": {
    "title": "Learning to Teach",
    "volume": "poster",
    "abstract": "Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach ``learning to teach''. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding)",
    "checked": true,
    "id": "31094226ed723f560e8ca63d40b88d93dead3de5",
    "semantic_title": "learning to teach",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=SyProzZAW": {
    "title": "The power of deeper networks for expressing natural functions",
    "volume": "poster",
    "abstract": "It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n",
    "checked": true,
    "id": "60a74f80f6e9b924ec92c2a31245560b12469481",
    "semantic_title": "the power of deeper networks for expressing natural functions",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=BkrsAzWAb": {
    "title": "Online Learning Rate Adaptation with Hypergradient Descent",
    "volume": "poster",
    "abstract": "We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation",
    "checked": true,
    "id": "512ca06114f5292f7d0b536ce030e319863c781a",
    "semantic_title": "online learning rate adaptation with hypergradient descent",
    "citation_count": 250,
    "authors": []
  },
  "https://openreview.net/forum?id=BkUp6GZRW": {
    "title": "Boosting the Actor with Dual Critic",
    "volume": "poster",
    "abstract": "This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic. Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks",
    "checked": true,
    "id": "3a5f60c2dea325de1dc92ae1eb7a513945bd6af1",
    "semantic_title": "boosting the actor with dual critic",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=rkN2Il-RZ": {
    "title": "SCAN: Learning Hierarchical Compositional Visual Concepts",
    "volume": "poster",
    "abstract": "The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts",
    "checked": true,
    "id": "b22b4817757778bdca5b792277128a7db8206d08",
    "semantic_title": "scan: learning hierarchical compositional visual concepts",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=Skw0n-W0Z": {
    "title": "Temporal Difference Models: Model-Free Deep RL for Model-Based Control",
    "volume": "poster",
    "abstract": "Model-free reinforcement learning (RL) has been proven to be a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even for off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods",
    "checked": true,
    "id": "852c931b5d9f9d4256befd725ee4185945c4964c",
    "semantic_title": "temporal difference models: model-free deep rl for model-based control",
    "citation_count": 240,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMvJrdaW": {
    "title": "Decoupling the Layers in Residual Networks",
    "volume": "poster",
    "abstract": "We propose a Warped Residual Network (WarpNet) using a parallelizable warp operator for forward and backward propagation to distant layers that trains faster than the original residual neural network. We apply a perturbation theory on residual networks and decouple the interactions between residual units. The resulting warp operator is a first order approximation of the output over multiple layers. The first order perturbation theory exhibits properties such as binomial path lengths and exponential gradient scaling found experimentally by Veit et al (2016). We demonstrate through an extensive performance study that the proposed network achieves comparable predictive performance to the original residual network with the same number of parameters, while achieving a significant speed-up on the total training time. As WarpNet performs model parallelism in residual network training in which weights are distributed over different GPUs, it offers speed-up and capability to train larger networks compared to original residual networks",
    "checked": true,
    "id": "27f7ab99b7646474bff0d5942876db7756728f8b",
    "semantic_title": "decoupling the layers in residual networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ByS1VpgRZ": {
    "title": "cGANs with Projection Discriminator",
    "volume": "poster",
    "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator",
    "checked": true,
    "id": "ab559473a01836e72b9fb9393d6e07c5745528f3",
    "semantic_title": "cgans with projection discriminator",
    "citation_count": 772,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlMAAeC-": {
    "title": "Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction",
    "volume": "poster",
    "abstract": "To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction dramatically reduces the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the collaboration of the core with the other components. We propose a small set of four combinators to capture the most pervasive programming patterns. Due to the finiteness and simplicity of this combinator set and the offloading of some burden of interpretation from the core, we are able construct a CNPI that is universal with respect to the set of all combinatorizable programs, which is adequate for solving most algorithmic tasks. Moreover, besides supervised training on execution traces, CNPI can be trained by policy gradient reinforcement learning with appropriately designed curricula",
    "checked": true,
    "id": "fc05801280853ff6f6a15c55d9b76d8c58182f39",
    "semantic_title": "improving the universality and learnability of neural programmer-interpreters with combinator abstraction",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=ryazCMbR-": {
    "title": "Communication Algorithms via Deep Learning",
    "volume": "poster",
    "abstract": "Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that cre- atively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We show strong gen- eralizations, i.e., we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, as well as robustness and adaptivity to deviations from the AWGN setting",
    "checked": true,
    "id": "9a5eb38bcf3097cbeaa9307627015528e88efda3",
    "semantic_title": "communication algorithms via deep learning",
    "citation_count": 220,
    "authors": []
  },
  "https://openreview.net/forum?id=SyX0IeWAW": {
    "title": "META LEARNING SHARED HIERARCHIES",
    "volume": "poster",
    "abstract": "We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitivesâ€”policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy",
    "checked": true,
    "id": "4d2c4cbb535801549371d9783a98d1e43bddf4e5",
    "semantic_title": "meta learning shared hierarchies",
    "citation_count": 355,
    "authors": []
  },
  "https://openreview.net/forum?id=SyYe6k-CW": {
    "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
    "volume": "poster",
    "abstract": "Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting",
    "checked": true,
    "id": "93a636f0b6d450217fda5aaa26c74bb6b232f498",
    "semantic_title": "deep bayesian bandits showdown: an empirical comparison of bayesian deep networks for thompson sampling",
    "citation_count": 366,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Euwz-Rb": {
    "title": "Compositional Attention Networks for Machine Reasoning",
    "volume": "poster",
    "abstract": "We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results",
    "checked": true,
    "id": "289fb3709475f5c87df8d97f129af54029d27fee",
    "semantic_title": "compositional attention networks for machine reasoning",
    "citation_count": 578,
    "authors": []
  },
  "https://openreview.net/forum?id=BkJ3ibb0-": {
    "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
    "volume": "poster",
    "abstract": "In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies",
    "checked": true,
    "id": "f7bb1636ced9036b3d0edafc7d82ad43164d41a3",
    "semantic_title": "defense-gan: protecting classifiers against adversarial attacks using generative models",
    "citation_count": 1182,
    "authors": []
  },
  "https://openreview.net/forum?id=SywXXwJAb": {
    "title": "Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design",
    "volume": "poster",
    "abstract": "Formal understanding of the inductive bias behind deep convolutional networks, i.e. the relation between the network's architectural features and the functions it is able to model, is limited. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning, and use it for obtaining novel theoretical observations regarding the inductive bias of convolutional networks. Specifically, we show a structural equivalence between the function realized by a convolutional arithmetic circuit (ConvAC) and a quantum many-body wave function, which facilitates the use of quantum entanglement measures as quantifiers of a deep network's expressive ability to model correlations. Furthermore, the construction of a deep ConvAC in terms of a quantum Tensor Network is enabled. This allows us to perform a graph-theoretic analysis of a convolutional network, tying its expressiveness to a min-cut in its underlying graph. We demonstrate a practical outcome in the form of a direct control over the inductive bias via the number of channels (width) of each layer. We empirically validate our findings on standard convolutional networks which involve ReLU activations and max pooling. The description of a deep convolutional network in well-defined graph-theoretic tools and the structural connection to quantum entanglement, are two interdisciplinary bridges that are brought forth by this work",
    "checked": true,
    "id": "c9316491b8d991fabbf9b28c449d66df6a50f841",
    "semantic_title": "deep learning and quantum entanglement: fundamental connections with implications to network design",
    "citation_count": 126,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Y8hhg0b": {
    "title": "Learning Sparse Neural Networks through L_0 Regularization",
    "volume": "poster",
    "abstract": "We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ regularized objective is differentiable with respect to the distribution parameters. We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer",
    "checked": false,
    "id": "2ec7156913117949ab933f27f492d0149bc0031f",
    "semantic_title": "learning sparse neural networks through l0 regularization",
    "citation_count": 1150,
    "authors": []
  },
  "https://openreview.net/forum?id=B1n8LexRZ": {
    "title": "Generalizing Hamiltonian Monte Carlo with Neural Networks",
    "volume": "poster",
    "abstract": "We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper",
    "checked": true,
    "id": "b7c7b4bbda0a90d3e22e8f1207e3d3226e711a84",
    "semantic_title": "generalizing hamiltonian monte carlo with neural networks",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=HyjC5yWCW": {
    "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm",
    "volume": "poster",
    "abstract": "Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models",
    "checked": true,
    "id": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a",
    "semantic_title": "meta-learning and universality: deep representations and gradient descent can approximate any learning algorithm",
    "citation_count": 223,
    "authors": []
  },
  "https://openreview.net/forum?id=rJQDjk-0b": {
    "title": "Unbiased Online Recurrent Optimization",
    "volume": "poster",
    "abstract": "The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}. UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence. On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients",
    "checked": true,
    "id": "db337a93cdb1a636c086c1e5c855b9be016bfb77",
    "semantic_title": "unbiased online recurrent optimization",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=BkabRiQpb": {
    "title": "Consequentialist conditional cooperation in social dilemmas with imperfect information",
    "volume": "poster",
    "abstract": "Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action",
    "checked": true,
    "id": "596ca16325dd72c0fcc186389fdd0280b49bbefb",
    "semantic_title": "consequentialist conditional cooperation in social dilemmas with imperfect information",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwVAXyCW": {
    "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ae8d5be3caea59a21221f02ef04d49a86cb80191",
    "semantic_title": "skip rnn: learning to skip state updates in recurrent neural networks",
    "citation_count": 219,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg0vbWC-": {
    "title": "Generating Wikipedia by Summarizing Long Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8691706ad0cf5e83969658b2e6bfffdc379440c9",
    "semantic_title": "generating wikipedia by summarizing long sequences",
    "citation_count": 801,
    "authors": []
  },
  "https://openreview.net/forum?id=SJyVzQ-C-": {
    "title": "Fraternal Dropout",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "415f18130edbe06e3e4806dfb0a1edcab6c241eb",
    "semantic_title": "fraternal dropout",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=BySRH6CpW": {
    "title": "Learning Discrete Weights Using the Local Reparameterization Trick",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15698db8c6b08891c8ab8b75a2738cad04c7b25b",
    "semantic_title": "learning discrete weights using the local reparameterization trick",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=r1wEFyWCW": {
    "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bbe13b72314fffcc2f35b0660195f2f6607c00a0",
    "semantic_title": "few-shot autoregressive density estimation: towards learning to learn distributions",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy0GnUxCb": {
    "title": "Emergent Complexity via Multi-Agent Competition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c335ff618991f0a4cdde09271284172a7e5f6b7f",
    "semantic_title": "emergent complexity via multi-agent competition",
    "citation_count": 392,
    "authors": []
  },
  "https://openreview.net/forum?id=SkFqf0lAZ": {
    "title": "Memory Architectures in Recurrent Neural Network Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27981998aaef92952eabef2c1490b926f9150c4f",
    "semantic_title": "memory architectures in recurrent neural network language models",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=HJIoJWZCZ": {
    "title": "Adversarial Dropout Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c20148703a706d5883d323c3386978714fe1508d",
    "semantic_title": "adversarial dropout regularization",
    "citation_count": 286,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk9zbyZCZ": {
    "title": "Neural Map: Structured Memory for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ee01ec27e4e66e089b72a9989724be611c2ad90",
    "semantic_title": "neural map: structured memory for deep reinforcement learning",
    "citation_count": 261,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOExmWAb": {
    "title": "MaskGAN: Better Text Generation via Filling in the _______",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
    "semantic_title": "maskgan: better text generation via filling in the ______",
    "citation_count": 470,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk9Xc_lR-": {
    "title": "On the Discrimination-Generalization Tradeoff in GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f74782ac26a8912215dd11503762699d341ca5d",
    "semantic_title": "on the discrimination-generalization tradeoff in gans",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=H1UOm4gA-": {
    "title": "Interactive Grounded Language Acquisition and Generalization in a 2D World",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a4a90a2db209db2d5c49adfd2091ede2d4130f60",
    "semantic_title": "interactive grounded language acquisition and generalization in a 2d world",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=Syg-YfWCW": {
    "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e9f32b34fde7d0034a147d5996470634c9e19002",
    "semantic_title": "go for a walk and arrive at the answer: reasoning over paths in knowledge bases using reinforcement learning",
    "citation_count": 513,
    "authors": []
  },
  "https://openreview.net/forum?id=Skp1ESxRZ": {
    "title": "Towards Synthesizing Complex Programs From Input-Output Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "42dea2a24629bd4b1b56619536e263b078becddd",
    "semantic_title": "towards synthesizing complex programs from input-output examples",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=BkrSv0lA-": {
    "title": "Loss-aware Weight Quantization of Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d18cc16563b9e2cef58bbc2a9d212b0ca72de36c",
    "semantic_title": "loss-aware weight quantization of deep networks",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=HJzgZ3JCW": {
    "title": "Efficient Sparse-Winograd Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "317b886336e3f5b599ab21795f4a4fef56f727f4",
    "semantic_title": "efficient sparse-winograd convolutional neural networks",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lUOzWCW": {
    "title": "Demystifying MMD GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9723066a5587e6267d8abfd7feefd0637a5a211c",
    "semantic_title": "demystifying mmd gans",
    "citation_count": 1507,
    "authors": []
  },
  "https://openreview.net/forum?id=HJC2SzZCW": {
    "title": "Sensitivity and Generalization in Neural Networks: an Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e837dfa120e8ce3cd587bde7b0787ef43fa7832d",
    "semantic_title": "sensitivity and generalization in neural networks: an empirical study",
    "citation_count": 442,
    "authors": []
  },
  "https://openreview.net/forum?id=B14TlG-RW": {
    "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8c1b00128e74f1cd92aede3959690615695d5101",
    "semantic_title": "qanet: combining local convolution with global self-attention for reading comprehension",
    "citation_count": 1095,
    "authors": []
  },
  "https://openreview.net/forum?id=SyZipzbCb": {
    "title": "Distributed Distributional Deterministic Policy Gradients",
    "volume": "poster",
    "abstract": "This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance",
    "checked": true,
    "id": "d355e339298fc2ab920688c1709d4ba6476a2bc6",
    "semantic_title": "distributed distributional deterministic policy gradients",
    "citation_count": 480,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk8ZcAxR-": {
    "title": "Eigenoption Discovery through the Deep Successor Representation",
    "volume": "poster",
    "abstract": "Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential",
    "checked": true,
    "id": "58fb60c5592224901a26dd84220a2f3332c1fcf5",
    "semantic_title": "eigenoption discovery through the deep successor representation",
    "citation_count": 142,
    "authors": []
  },
  "https://openreview.net/forum?id=r1q7n9gAb": {
    "title": "The Implicit Bias of Gradient Descent on Separable Data",
    "volume": "poster",
    "abstract": "We show that gradient descent on an unregularized logistic regression problem, for almost all separable datasets, converges to the same direction as the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods",
    "checked": true,
    "id": "11adc8bd35bd897502f9b5452ab7ac668ec9b0fb",
    "semantic_title": "the implicit bias of gradient descent on separable data",
    "citation_count": 925,
    "authors": []
  },
  "https://openreview.net/forum?id=SyJ7ClWCb": {
    "title": "Countering Adversarial Images using Input Transformations",
    "volume": "poster",
    "abstract": "This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods",
    "checked": true,
    "id": "e225dd59ef4954db21479cdcbee497624b2d6d0f",
    "semantic_title": "countering adversarial images using input transformations",
    "citation_count": 1409,
    "authors": []
  },
  "https://openreview.net/forum?id=ByRWCqvT-": {
    "title": "Learning to cluster in order to transfer across domains and tasks",
    "volume": "poster",
    "abstract": "This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets. Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement",
    "checked": true,
    "id": "495582032e8467dc31c59772d690b2a079fcbb4f",
    "semantic_title": "learning to cluster in order to transfer across domains and tasks",
    "citation_count": 221,
    "authors": []
  },
  "https://openreview.net/forum?id=Bys4ob-Rb": {
    "title": "Certified Defenses against Adversarial Examples",
    "volume": "poster",
    "abstract": "While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error",
    "checked": true,
    "id": "966e3c7a65ec75a6359b55c0cecaf3896d318432",
    "semantic_title": "certified defenses against adversarial examples",
    "citation_count": 969,
    "authors": []
  },
  "https://openreview.net/forum?id=rkcQFMZRb": {
    "title": "Variational image compression with a scale hyperprior",
    "volume": "poster",
    "abstract": "We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate--distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics",
    "checked": true,
    "id": "678c5b1771e7c15e86b454662dec8cd45d3d30bf",
    "semantic_title": "variational image compression with a scale hyperprior",
    "citation_count": 1819,
    "authors": []
  },
  "https://openreview.net/forum?id=BJQRKzbA-": {
    "title": "Hierarchical Representations for Efficient Architecture Search",
    "volume": "poster",
    "abstract": "We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour",
    "checked": true,
    "id": "856451974cce2d353d5d8a5a72104984a252375c",
    "semantic_title": "hierarchical representations for efficient architecture search",
    "citation_count": 930,
    "authors": []
  },
  "https://openreview.net/forum?id=rytNfI1AZ": {
    "title": "Training wide residual networks for deployment using a single bit for each weight",
    "volume": "poster",
    "abstract": "For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that binarize weights by applying the sign function in training; we apply scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test results of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error rates halve previously reported values, and are within about 1% of our error-rates for the same network with full-precision weights. For networks that overfit, we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. This applies to both full precision and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule, we found that training for 1-bit-per-weight is just as fast as full-precision networks, with better accuracy than standard schedules, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. For full training code and trained models in MATLAB, Keras and PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/",
    "checked": true,
    "id": "d5a8c9bef7020e800d3b02573a983ff1eddf285e",
    "semantic_title": "training wide residual networks for deployment using a single bit for each weight",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk0wHx-RW": {
    "title": "Learning Sparse Latent Representations with the Deep Copula Information Bottleneck",
    "volume": "poster",
    "abstract": "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data",
    "checked": true,
    "id": "19d4fe6264a3ab140f94d43d784835ec25cea55e",
    "semantic_title": "learning sparse latent representations with the deep copula information bottleneck",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=r1iuQjxCZ": {
    "title": "On the importance of single directions for generalization",
    "volume": "poster",
    "abstract": "Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyper- parameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance",
    "checked": true,
    "id": "45164e8d9508f3d1f20500e67358565587626789",
    "semantic_title": "on the importance of single directions for generalization",
    "citation_count": 333,
    "authors": []
  },
  "https://openreview.net/forum?id=HJcSzz-CZ": {
    "title": "Meta-Learning for Semi-Supervised Few-Shot Classification",
    "volume": "poster",
    "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would",
    "checked": true,
    "id": "df093d69cd98cf4b26542f53614a79754754eb78",
    "semantic_title": "meta-learning for semi-supervised few-shot classification",
    "citation_count": 1287,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zriGeCZ": {
    "title": "Hyperparameter optimization: a spectral approach",
    "volume": "poster",
    "abstract": "We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable. Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning. In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization. We also outperform Random Search $8\\times$. Our method is inspired by provably-efficient algorithms for learning decision trees using the discrete Fourier transform. We obtain improved sample-complexty bounds for learning decision trees while matching state-of-the-art bounds on running time (polynomial and quasipolynomial, respectively)",
    "checked": true,
    "id": "cf0986f965b9314d1a86e328420494e45bb61683",
    "semantic_title": "hyperparameter optimization: a spectral approach",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Dx7fbCW": {
    "title": "Generalizing Across Domains via Cross-Gradient Training",
    "volume": "poster",
    "abstract": "We present CROSSGRAD , a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD jointly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and (2) data augmentation is a more stable and accurate method than domain adversarial training",
    "checked": true,
    "id": "cabc42832388b1995d1f815f9fc4253f3f593993",
    "semantic_title": "generalizing across domains via cross-gradient training",
    "citation_count": 519,
    "authors": []
  },
  "https://openreview.net/forum?id=ByJHuTgA-": {
    "title": "On the State of the Art of Evaluation in Neural Language Models",
    "volume": "poster",
    "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset",
    "checked": true,
    "id": "2397ce306e5d7f3d0492276e357fb1833536b5d8",
    "semantic_title": "on the state of the art of evaluation in neural language models",
    "citation_count": 535,
    "authors": []
  },
  "https://openreview.net/forum?id=rypT3fb0b": {
    "title": "LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. This has motivated a large body of work to reduce the complexity of the neural network by using sparsity-inducing regularizers. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., l1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all the corresponding weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization in the training process to simultaneously identify significant neurons and groups of parameter that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e., keeping only the significant neurons and enforcing the learned tying structure. We evaluate the proposed approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization performance",
    "checked": true,
    "id": "8de574fff9b65d2a3cd0a5348458ee1045df3238",
    "semantic_title": "learning to share: simultaneous parameter tying and sparsification in deep learning",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=H1vEXaxA-": {
    "title": "Emergent Translation in Multi-Agent Communication",
    "volume": "poster",
    "abstract": "While most machine translation systems to date are trained on large parallel corpora, humans learn language in a different way: by being grounded in an environment and interacting with other humans. In this work, we propose a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task. We find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals. The emergent translation is interactive and multimodal, and crucially does not require parallel corpora, but only monolingual, independent text and corresponding images. Our proposed translation model achieves this by grounding the source and target languages into a shared visual modality, and outperforms several baselines on both word-level and sentence-level translation tasks. Furthermore, we show that agents in a multilingual community learn to translate better and faster than in a bilingual communication setting",
    "checked": true,
    "id": "c669f052c509c1c80a8c413fcb96b2b324d2f3ca",
    "semantic_title": "emergent translation in multi-agent communication",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=ry018WZAZ": {
    "title": "Deep Active Learning for Named Entity Recognition",
    "volume": "poster",
    "abstract": "Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data",
    "checked": true,
    "id": "43ef6f1b1c0622a23bc62af5a05dd5c813eba00d",
    "semantic_title": "deep active learning for named entity recognition",
    "citation_count": 457,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l8BtlCb": {
    "title": "Non-Autoregressive Neural Machine Translation",
    "volume": "poster",
    "abstract": "Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 Englishâ€“German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 Englishâ€“Romanian",
    "checked": true,
    "id": "15e81c8d1c21f9e928c72721ac46d458f3341454",
    "semantic_title": "non-autoregressive neural machine translation",
    "citation_count": 798,
    "authors": []
  },
  "https://openreview.net/forum?id=HktJec1RZ": {
    "title": "Towards Neural Phrase-based Machine Translation",
    "volume": "poster",
    "abstract": "In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order and can decode in linear time. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in output languages",
    "checked": true,
    "id": "5616064812996ab1fae525f9679f300c7c307895",
    "semantic_title": "towards neural phrase-based machine translation",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=BJIgi_eCZ": {
    "title": "FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension",
    "volume": "poster",
    "abstract": "This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it identifies an attention scoring function that better utilizes the \"history of word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%",
    "checked": true,
    "id": "1fe6bee85774244d8674cbb20a25e8d153cecb17",
    "semantic_title": "fusionnet: fusing via fully-aware attention with application to machine comprehension",
    "citation_count": 183,
    "authors": []
  },
  "https://openreview.net/forum?id=rkYTTf-AZ": {
    "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
    "volume": "poster",
    "abstract": "Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time",
    "checked": true,
    "id": "e3d772986d176057aca2f5e3eb783da53b559134",
    "semantic_title": "unsupervised machine translation using monolingual corpora only",
    "citation_count": 1098,
    "authors": []
  },
  "https://openreview.net/forum?id=ByJIWUnpW": {
    "title": "Automatically Inferring Data Quality for Spatiotemporal Forecasting",
    "volume": "poster",
    "abstract": "Spatiotemporal forecasting has become an increasingly important prediction task in machine learning and statistics due to its vast applications, such as climate modeling, traffic prediction, video caching predictions, and so on. While numerous studies have been conducted, most existing works assume that the data from different sources or across different locations are equally reliable. Due to cost, accessibility, or other factors, it is inevitable that the data quality could vary, which introduces significant biases into the model and leads to unreliable prediction results. The problem could be exacerbated in black-box prediction models, such as deep neural networks. In this paper, we propose a novel solution that can automatically infer data quality levels of different sources through local variations of spatiotemporal signals without explicit labels. Furthermore, we integrate the estimate of data quality level with graph convolutional networks to exploit their efficient structures. We evaluate our proposed method on forecasting temperatures in Los Angeles",
    "checked": true,
    "id": "fedd6331f2d6a1225b564d5cda3d3db15b25015e",
    "semantic_title": "automatically inferring data quality for spatiotemporal forecasting",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Hksj2WWAW": {
    "title": "Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs",
    "volume": "poster",
    "abstract": "Neural programming involves training neural networks to learn programs, mathematics, or logic from data. Previous works have failed to achieve good generalization performance, especially on problems and programs with high complexity or on large domains. This is because they mostly rely either on black-box function evaluations that do not capture the structure of the program, or on detailed execution traces that are expensive to obtain, and hence the training data has poor coverage of the domain under consideration. We present a novel framework that utilizes black-box function evaluations, in conjunction with symbolic expressions that define relationships between the given functions. We employ tree LSTMs to incorporate the structure of the symbolic expression trees. We use tree encoding for numbers present in function evaluation data, based on their decimal representation. We present an evaluation benchmark for this task to demonstrate our proposed model combines symbolic reasoning and function evaluation in a fruitful manner, obtaining high accuracies in our experiments. Our framework generalizes significantly better to expressions of higher depth and is able to fill partial equations with valid completions",
    "checked": true,
    "id": "3495c2bd6dc6956357b170b0c3a50db7ab8582d4",
    "semantic_title": "combining symbolic expressions and black-box function evaluations in neural programs",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeqORgAW": {
    "title": "Proximal Backpropagation",
    "volume": "poster",
    "abstract": "We propose proximal backpropagation (ProxProp) as a novel algorithm that takes implicit instead of explicit gradient steps to update the network parameters during neural network training. Our algorithm is motivated by the step size limitation of explicit gradient descent, which poses an impediment for optimization. ProxProp is developed from a general point of view on the backpropagation algorithm, currently the most common technique to train neural networks via stochastic gradient descent and variants thereof. Specifically, we show that backpropagation of a prediction error is equivalent to sequential gradient descent steps on a quadratic penalty energy, which comprises the network activations as variables of the optimization. We further analyze theoretical properties of ProxProp and in particular prove that the algorithm yields a descent direction in parameter space and can therefore be combined with a wide variety of convergent algorithms. Finally, we devise an efficient numerical implementation that integrates well with popular deep learning frameworks. We conclude by demonstrating promising numerical results and show that ProxProp can be effectively combined with common first order optimizers such as Adam",
    "checked": true,
    "id": "c5afa0e78fadfe8cd3f0006988de3e230edc0075",
    "semantic_title": "proximal backpropagation",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=H1meywxRW": {
    "title": "DCN+: Mixed Objective And Deep Residual Coattention for Question Answering",
    "volume": "poster",
    "abstract": "Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1",
    "checked": true,
    "id": "0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a",
    "semantic_title": "dcn+: mixed objective and deep residual coattention for question answering",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk3ddfWRW": {
    "title": "Imitation Learning from Visual Data with Multiple Intentions",
    "volume": "poster",
    "abstract": "Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. LfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module. We demonstrate our method on real robot visual object reaching tasks, and show that it can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9",
    "checked": true,
    "id": "023237d0ea810771e7e90770835fa89a2931d276",
    "semantic_title": "imitation learning from visual data with multiple intentions",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ94fqApW": {
    "title": "Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers",
    "volume": "poster",
    "abstract": "Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions in resource-limited scenarios. A widely-used practice in relevant work assumes that a smaller-norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs) that does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computationally difficult and not-always-useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: first to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels to be constant, and then to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interest- ing aspects and competitive performance",
    "checked": true,
    "id": "60464c4bd94a14b63898e322f9ea651830e54ae0",
    "semantic_title": "rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers",
    "citation_count": 408,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Yp-j1Cb": {
    "title": "An Online Learning Approach to Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "245f8b05bdd1ac65a09a476440dc4b05ac05d4a0",
    "semantic_title": "an online learning approach to generative adversarial networks",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=rkZB1XbRZ": {
    "title": "Scalable Private Learning with PATE",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "44058a625cb64c311043145655645d8206e272c2",
    "semantic_title": "scalable private learning with pate",
    "citation_count": 618,
    "authors": []
  },
  "https://openreview.net/forum?id=SygwwGbRW": {
    "title": "Semi-parametric topological memory for navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c0d96d1ea69855a5a3abf614f17095c29b3339a4",
    "semantic_title": "semi-parametric topological memory for navigation",
    "citation_count": 383,
    "authors": []
  },
  "https://openreview.net/forum?id=HyRnez-RW": {
    "title": "Multi-Mention Learning for Reading Comprehension with Neural Cascades",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8a004eaebe04facb26039fed4fddcf4c855c7d49",
    "semantic_title": "multi-mention learning for reading comprehension with neural cascades",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=BJuWrGW0Z": {
    "title": "Dynamic Neural Program Embeddings for Program Repair",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "bbe5b2fa5db0d37d772eca7db0e1b1ddfec0f6a5",
    "semantic_title": "dynamic neural program embedding for program repair",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=rJWechg0Z": {
    "title": "Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b0868e208ce9c4e99da57b6dce57dc7950b5a7df",
    "semantic_title": "minimal-entropy correlation alignment for unsupervised deep domain adaptation",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=rkr1UDeC-": {
    "title": "Large scale distributed neural network training through online distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cc59b4b1eb7d4629f753bc24f029c5cced301381",
    "semantic_title": "large scale distributed neural network training through online distillation",
    "citation_count": 409,
    "authors": []
  },
  "https://openreview.net/forum?id=ry-TW-WAb": {
    "title": "Variational Network Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f93ae1a0b9e40b138da8c25855b6c68af4ee201a",
    "semantic_title": "variational network quantization",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=rk07ZXZRb": {
    "title": "Learning an Embedding Space for Transferable Robot Skills",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "565af8f2ef461b1d7368f3e9899e0f576e4f0a24",
    "semantic_title": "learning an embedding space for transferable robot skills",
    "citation_count": 322,
    "authors": []
  },
  "https://openreview.net/forum?id=Skdvd2xAZ": {
    "title": "A Scalable Laplace Approximation for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad8631abf269d2b886614f69221d25a732a7f58d",
    "semantic_title": "a scalable laplace approximation for neural networks",
    "citation_count": 411,
    "authors": []
  },
  "https://openreview.net/forum?id=S1sqHMZCb": {
    "title": "NerveNet: Learning Structured Policy with Graph Neural Networks",
    "volume": "poster",
    "abstract": "We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting",
    "checked": true,
    "id": "249408527106d7595d45dd761dd53c83e5a02613",
    "semantic_title": "nervenet: learning structured policy with graph neural networks",
    "citation_count": 280,
    "authors": []
  },
  "https://openreview.net/forum?id=rkhlb8lCZ": {
    "title": "Wavelet Pooling for Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "Convolutional Neural Networks continuously advance the progress of 2D and 3D image and object classification. The steadfast usage of this algorithm requires constant evaluation and upgrading of foundational concepts to maintain progress. Network regularization techniques typically focus on convolutional layer operations, while leaving pooling layer operations without suitable options. We introduce Wavelet Pooling as another alternative to traditional neighborhood pooling. This method decomposes features into a second level decomposition, and discards the first-level subbands to reduce feature dimensions. This method addresses the overfitting problem encountered by max pooling, while reducing features in a more structurally compact manner than pooling via neighborhood regions. Experimental results on four benchmark classification datasets demonstrate our proposed method outperforms or performs comparatively with methods like max, mean, mixed, and stochastic pooling",
    "checked": true,
    "id": "1fdfc0ba42d4d7e38fccf9b66c4cc086514253bd",
    "semantic_title": "wavelet pooling for convolutional neural networks",
    "citation_count": 185,
    "authors": []
  },
  "https://openreview.net/forum?id=HkCsm6lRb": {
    "title": "Generative Models of Visually Grounded Imagination",
    "volume": "poster",
    "abstract": "It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C's). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of Suzuki et al., 2017 and the BiVCCA method of Wang et al., 2016) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset (Liu et al., 2015)",
    "checked": true,
    "id": "226f08bb8fd049fdca2f6a8a41ffe485a0ee6ebc",
    "semantic_title": "generative models of visually grounded imagination",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=rknt2Be0-": {
    "title": "Compositional Obverter Communication Learning from Raw Visual Input",
    "volume": "poster",
    "abstract": "One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand- engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment",
    "checked": true,
    "id": "4814c10f84863e016d75e6af42e790f60759b9f4",
    "semantic_title": "compositional obverter communication learning from raw visual input",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=HyMTkQZAb": {
    "title": "Kronecker-factored Curvature Approximations for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017). It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized. The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well. In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs. This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form. We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks",
    "checked": true,
    "id": "42d74488ea64d5420257f547f6d0a7a2909d87c0",
    "semantic_title": "kronecker-factored curvature approximations for recurrent neural networks",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=HkUR_y-RZ": {
    "title": "SEARNN: Training RNNs with global-local losses",
    "volume": "poster",
    "abstract": "We propose SEARNN, a novel training algorithm for recurrent neural networks (RNNs) inspired by the \"learning to search\" (L2S) approach to structured prediction. RNNs have been widely successful in structured prediction applications such as machine translation or parsing, and are commonly trained using maximum likelihood estimation (MLE). Unfortunately, this training loss is not always an appropriate surrogate for the test error: by only maximizing the ground truth probability, it fails to exploit the wealth of information offered by structured losses. Further, it introduces discrepancies between training and predicting (such as exposure bias) that may hurt test performance. Instead, SEARNN leverages test-alike search space exploration to introduce global-local losses that are closer to the test error. We first demonstrate improved performance over MLE on two different tasks: OCR and spelling correction. Then, we propose a subsampling strategy to enable SEARNN to scale to large vocabulary sizes. This allows us to validate the benefits of our approach on a machine translation task",
    "checked": true,
    "id": "345bbd344177815dfb9214c61403cb7eac6de450",
    "semantic_title": "searnn: training rnns with global-local losses",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=ryRh0bb0Z": {
    "title": "Multi-View Data Generation Without View Supervision",
    "volume": "poster",
    "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize",
    "checked": true,
    "id": "6cc53bd117613797d7713a2c2f92983cf8a28df0",
    "semantic_title": "multi-view data generation without view supervision",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk5elxbRW": {
    "title": "Smooth Loss Functions for Deep Top-k Classification",
    "volume": "poster",
    "abstract": "The top-$k$ error is a common measure of performance in machine learning and computer vision. In practice, top-$k$ classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-$k$ classification can bring significant improvements. Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\\\"i}ve algorithm would require $\\mathcal{O}(\\binom{n}{k})$ operations, where $n$ is the number of classes. Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k n)$. Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of $k=5$. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy",
    "checked": true,
    "id": "b4ced7f12424c70f0edfbc2c12f39529f35977e6",
    "semantic_title": "smooth loss functions for deep top-k classification",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=rkHywl-A-": {
    "title": "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation that is competitive with direct imitation learning algorithms. Additionally, we show that AIRL is able to recover portable reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training",
    "checked": true,
    "id": "941ba185f01b1a0a27453fd178aa5f010510ee8b",
    "semantic_title": "learning robust rewards with adverserial inverse reinforcement learning",
    "citation_count": 209,
    "authors": []
  },
  "https://openreview.net/forum?id=BJij4yg0Z": {
    "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to \\citet{zhang2016understanding}, who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the ``noise scale\" $g = \\epsilon (\\frac{N}{B} - 1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ the training set size and $B$ the batch size. Consequently the optimum batch size is proportional to both the learning rate and the size of the training set, $B_{opt} \\propto \\epsilon N$. We verify these predictions empirically",
    "checked": true,
    "id": "ae4b0b63ff26e52792be7f60bda3ed5db83c1577",
    "semantic_title": "a bayesian perspective on generalization and stochastic gradient descent",
    "citation_count": 253,
    "authors": []
  }
}