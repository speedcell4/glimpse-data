{
  "https://openreview.net/forum?id=cXs5md5wAq": {
    "title": "Modelling Microbial Communities with Graph Neural Networks",
    "volume": "review",
    "abstract": "Understanding the interactions and interplay of microorganisms is a great challenge with many applications in medical and environmental settings. In this work, we model bacterial communities directly from their genomes using graph neural networks (GNNs). GNNs leverage the inductive bias induced by the set nature of bacteria, enforcing permutation invariance and granting combinatorial generalization. We propose to learn the dynamics implicitly by directly predicting community relative abundance profiles at steady state, thus escaping the need for growth curves. On two real-world datasets, we show for the first time generalization to unseen bacteria and different community structures. To investigate the prediction results more deeply, we created a simulation for flexible data generation and analyze effects of bacteria interaction strength, community size, and training data amount",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rhgIgTSSxW": {
    "title": "TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023",
    "volume": "review",
    "abstract": "Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers. However, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems. One of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models. For a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction. In this work, we present TabR -- essentially, a feed-forward network with a novel k-Nearest-Neighbors-like component in the middle. On a set of public benchmarks with datasets up to several million objects, TabR marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed \"GBDT-friendly\" benchmark (see Figure 1). Among the novel findings and technical details powering TabR, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them. In addition to the much higher performance, TabR is simple and significantly more efficient compared to prior retrieval-based tabular DL models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kKRbAY4CXv": {
    "title": "Neural Evolutionary Kernel Method: A Knowledge-Based Learning Architechture for Evolutionary PDEs",
    "volume": "review",
    "abstract": "Numerical solution of partial differential equations (PDEs) plays a vital role in various fields of science and engineering. In recent years, deep neural networks (DNNs) have emerged as a powerful tool for solving PDEs. DNN-based methods exploit the approximation capabilities of neural networks to obtain solutions to PDEs in general domains or high-dimensional spaces. However, many of these methods lack the use of mathematical prior knowledge, and DNN-based methods usually require a large number of sample points and parameters, making them computationally expensive and challenging to train. This paper aims to introduce a novel method named the Neural Evolutionary Kernel Method (NEKM) for solving a class of evolutionary PDEs through DNNs based kernels. By using operator splitting and boundary integral techniques, we propose particular neural network architectures which approximate evolutionary kernels of solutions and preserve structures of time-dependent PDEs. Mathematical prior knowledge are naturally built into these DNNs based kernels through convolutional representation with pre-trained Green functions, leading to serious reduction in the number of parameters in the NEKM and very efficient training processes. Experimental results demonstrate the efficiency and accuracy of the NEKM in solving heat equations and Allen-Cahn equations in complex domains and on manifolds, showcasing its promising potential for applications in data driven scientific computing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ApjY32f3Xr": {
    "title": "PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs",
    "volume": "review",
    "abstract": "While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research, particularly in areas such as domain decomposition methods and loss reweighting for handling multi-scale problems and complex geometry. While PINNacle does not guarantee success in all real-world scenarios, it represents a significant contribution to the field by offering a robust, diverse, and comprehensive benchmark suite that will undoubtedly foster further research and development in PINNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=eUgS9Ig8JG": {
    "title": "SaNN: Simple Yet Powerful Simplicial-aware Neural Networks",
    "volume": "review",
    "abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories, simplicial closures, and classifying graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qBL04XXex6": {
    "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
    "volume": "review",
    "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9DYMIpz9c": {
    "title": "Farzi Data: Autoregressive Data Distillation",
    "volume": "review",
    "abstract": "We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=rp5vfyp5Np": {
    "title": "BATTLE: Towards Behavior-oriented Adversarial Attacks against Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "Evaluating the performance of deep reinforcement learning (DRL) agents under adversarial attacks that aim to induce specific behaviors, i.e., behavior-oriented adversarial attacks, is crucial for understanding the robustness of DRL agents. Prior research primarily focuses on directing agents towards pre-determined states or policies, lacking generality and flexibility. Therefore, it is important to devise universal attacks that target inducing specific behaviors in a victim. In this work, we propose BATTLE, a universal behavior-oriented adversarial attack method. In BATTLE, an intention policy is trained to align with human preferences for flexible behavior orientation, while the adversary is trained to guide the victim policy to imitate the intention policy. To improve the attack performance, we introduce a weighting function that assigns importance weights over each state. Our empirical results over several manipulation tasks of Meta-world show the superiority of BATTLE in behavior-oriented adversarial attack settings, outperforming current adversarial attack algorithms. Furthermore, we also demonstrate that BATTLE can improve the robustness of agents under strong attacks by training with adversary. Lastly, we showcase the strong behavior-inducing capability of BATTLE by guiding Decision Transformer agents to act in line with human preferences across various MuJoCo tasks. Our videos are available in https://sites.google.com/view/jj9uxjgmba5lr3g",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=miGpIhquyB": {
    "title": "Understanding Large Language Models Through the Lens of Dataset Generation",
    "volume": "review",
    "abstract": "There has been increased interest in using Large Language Models (LLMs) for text dataset generation subject to a desired attribute, e.g., for use in downstream fine-tuning or training. These works generally focus on a single quality metric of the generated text, typically accuracy on a downstream task. However, this fails to consider whether the model even has the ability to faithfully model the data distribution of the desired real-world domain. In contrast, in this work, we additionally focus on important distributional metrics agnostic to the downstream task, such as data diversity and faithfulness. We show that even in simple domains, generated datasets reveal inherent trade-offs between these metrics across models and training regimes. Further, we find that our metrics not only describe the generated dataset, but also capture key aspects of the underlying model. This allows us to characterize the generated datasets, individual models and by comparison the properties of different model families and training paradigms. By focusing on sub-distributions well-represented in the training data of LLMs, we can, for example, show that popular instruction-tuning techniques strongly decrease the LLM's text generation abilities, with respect to distributional aspects like diversity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6AtXCnHCFy": {
    "title": "FSN: Feature Shift Network for Load-Domain Domain Generalization",
    "volume": "review",
    "abstract": "Conventional deep learning methods for fault detection often assume that the training and the testing sets share the same fault pattern spaces and domain spaces. However, some fault patterns are rare, and many real-world faults have not appeared in the training set. As a result, it's hard for the trained model to achieve desirable performance on the testing set. In this paper, we introduce a novel domain generalization, Load-Domain (LD) domain generalization, which is based on the analysis of the CWRU bearing dataset and its domain division method. For this scenario, we propose a feature shift model called FSN (Feature Shift Network). In the bearing dataset, domains are divided based on different operating conditions which have specific loads, so it's equivalent to load-based domain division. Moreover, the domain label corresponds to the actual load magnitude, making it unique as it contains physical information, which can boost detection accuracy on unknown domain beyond the training set. According to the knowledge above , FSN is trained for feature shift on adjacent source domains, and finally shifts target domain features into adjacent source domain feature space to achieve the purpose of domain generalization. Extensive experiments on CWRU demonstrate that FSN is better than the existed models in the LD domain generalization case. Furthermore, we have another test on MNIST, which also shows FSN can achieve the best performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.6,
    "authors": []
  },
  "https://openreview.net/forum?id=9ceadCJY4B": {
    "title": "Ask Again, Then Fail: Large Language Models' Vacillations in Judgement",
    "volume": "review",
    "abstract": "With the emergence of generative conversational large language models (LLMs) like ChatGPT, serving as virtual assistants in various fields, the stability and reliability of their responses have become crucial. However, during usage, it has been observed that these models tend to waver in their judgements when confronted with follow-up questions from users expressing skepticism or disagreement. In this work, we draw inspiration from questioning strategies in education and propose a \\textsc{Follow-up Questioning Mechanism} along with two evaluation metrics to assess the judgement consistency of LLMs before and after exposure to disturbances. We evaluate the judgement consistency of ChatGPT, PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning benchmarks. Empirical results show that even when the initial answers are correct, judgement consistency sharply decreases when LLMs face disturbances such as questioning, negation, or misleading. Additionally, we study these models' judgement consistency under various settings (sampling temperature and prompts) to validate this issue further, observing the impact of prompt tone and conducting an in-depth error analysis for deeper behavioral insights. Furthermore, we also explore several prompting methods to mitigate this issue and demonstrate their effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=gYcft1HIaU": {
    "title": "Do Current Large Language Models Master Adequate Clinical Knowledge?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) show promising potential in solving clinical problems. Current LLMs, including so-called medical LLMs, are reported to achieve excellent performance on certain medical evaluation benchmarks, such as medical question answering, medical exams, etc. However, such evaluations cannot assess whether LLMs have mastered sufficient, compressive, and necessary medical knowledge for solving real clinical problems, such as clinical diagnostic assistance. In this paper, we propose a framework to assess the mastery of LLMs in clinical knowledge. Firstly, we construct a large medical disease-based knowledge base, MedDisK, covering 10,632 common diseases across 18 clinical knowledge aspects, which are crucial for diagnosing and treating diseases. Built on that, we propose a MedDisK-based evaluation method MedDisKEval: We prompt LLMs to retrieve information related to these clinical knowledge aspects. Then, we evaluate an LLM's mastery of medical knowledge by measuring the similarity between the LLM-generated information and the content within our knowledge base. Our experimental findings reveal that over 50\\% of the clinical information generated by our evaluated LLMs is significantly inconsistent with the corresponding knowledge stored in our knowledge base. We further perform a significance analysis to compare the performance of medical LLMs with their backbone models, discovering that 5 out of 6 medical LLMs perform less effectively than their backbone models in over half of the clinical knowledge aspects. These observations demonstrate that existing LLMs have not mastered adequate knowledge for clinical practice. Our findings offer novel and constructive insights for the advancement of medical LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=10eQ4Cfh8p": {
    "title": "SIMULTANEOUS GENERATION AND IMPROVEMENT: A UNIFIED RL PARADIGM FOR FJSP OPTIMIZATION",
    "volume": "review",
    "abstract": "We present an end-to-end reinforcement learning framework designed to address the Flexible Job Shop Problem (FJSP). Our approach consists of two primary components: a generative model that produces problem solutions stepwise, and a secondary model that continually refines these (partial) solutions. Importantly, we train both models concurrently, enabling each to be cognizant of the other's policy and make informed decisions. Extensive experimentation demonstrates that our model delivers better performance in shorter time on several public datasets comparing to baseline algorithms. Furthermore, we highlight the superior generalizability of our approach, as it maintains strong performance on large-scale instances even when trained on small-scale instances. It is worth noting that this training paradigm can be readily adapted to other combinatorial optimization problems, such as the traveling salesman problemand beyond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BQvbL2sFQx": {
    "title": "Model-Agnostic Shift-Equivariant Downsampling",
    "volume": "review",
    "abstract": "The performance of convolutional neural networks (CNNs) are thought to be insensitive to image shifts. However, recent studies have revealed that downsampling layers in CNNs result in inconsistent outputs for shifted input images. In this study, we present an approach for performing downsampling that ensures absolute shift equivariance. By employing model-agnostic downsampling method that leverages origin selection functions obtained from coordinate-independent statistics of the feature map, we can achieve perfect shift equivariance, while still adhering to the conventional downsampling procedures. Our method allows CNNs to exhibit both improved accuracy and perfect shift invariance for image classification, while also achieving shift equivariance in semantic segmentation benchmarks. Furthermore, we introduce a methodology for achieving shift equivariance without the need for any additional training process. This is accomplished by transferring pretrained weights and replacing existing layers with shift-equivariant counterparts. Additionaly, we show that fine-tuning of the modified CNNs leads superior performance compared to previously proposed models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=eR4W9tnJoZ": {
    "title": "Visuo-emotional perception and Human Cognition to engineer content-generation using Generative AI",
    "volume": "review",
    "abstract": "Media platforms compete for users' attention. Their reach crucially depends on algorithmic real-time bidding and efficiency of hyper-personalized, rapidly generated, and user-optimized content. Attention is, although, a scare and fleeting quantity, often awarded less than 1 second per stimulus. Thus, the current strategy is to rely on the vast amount of user-generated data to mimic the content to the user. The underlying assumption is that this is sufficient incentive for attention. This strategy has evidently failed. As witnessed by the alarmingly low or short-lived successes of campaigns in recent times. This mismatch is exacerbated because most content consumed today is digital. Whereas strategies for digital content mimic our past understanding from mass-media. Hence, we formalize a new understanding of communication, specifically for the digital mediums. We prove that the digital medium needs a new understanding of communication protocols. To that end, we take a first principles approach to the new communication protocol: the neurological representations of communication, specifically, where the communication happens in less than 1 second per stimulus. First, we break down and elaborate on this neurological representation of decision-making. Next, we proffer use of our behavioural communication model for generation and optimization of content creatives. To that end, we elaborate methods for rapid, AI-generation content, increasing the efficiency of visual communication on digital media. Within this exploration we include themes of Hyperpersonalization and Search-engine optimization. Thus, we find that strategically produced content exhibits stronger associations to users' nonconscious needs, wants and goals, which elicits user attention and content-diversity significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=jx6njBKH8E": {
    "title": "Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships",
    "volume": "review",
    "abstract": "Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=rGvDRT4Z60": {
    "title": "FairPATE: Exposing the Pareto Frontier of Fairness, Privacy, Accuracy, and Coverage",
    "volume": "review",
    "abstract": "Deploying machine learning (ML) models often requires both fairness and privacy guarantees. In this work, we study the challenges of integrating group fairness interventions into the Private Aggregation of Teacher Ensemble (PATE) framework. We show that in the joint fairness-privacy setting, the placement of the fairness intervention before, or after PATE's noisy aggregation mechanism (which ensures its differential privacy guarantees) leads to excessive fairness violations, or inefficient privacy budgeting, respectively. With this in mind, we present FairPATE which adds a rejection mechanism due to fairness violations. Through careful adjustment of PATE's privacy accounting, we match the DP-SGD-based state-of-the-art privacy-fairness-accuracy trade-offs (Lowy et al., 2023) in demographic parity, and improve on them for equality of odds with 2% lower disparity at similar accuracy levels and privacy budgets. We also evaluate FairPATE in the setting where exact fairness guarantees need to be enforced by refusing to provide algorithmic decisions at inference-time (for instance, in a human-in-the-loop setting) thus trading off fairness with coverage. Based on our FairPATE, we provide, for the first time, empirical Pareto frontiers for fairness, privacy, accuracy, and coverage on a range of privacy and fairness benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=w73feIekdO": {
    "title": "Real-time computer vision on low-end boards via clustering motion vectors",
    "volume": "review",
    "abstract": "In this work, we suggest computer vision methods, specifically for video tracking and map creation from video. To this end, we utilize motion vectors and clusters, which are computed very efficiently in standard video encoders, usually via dedicated hardware. We suggest a provably good tracking algorithm for clustering these vectors, by considering them as segments. For this, we utilize a definition of a \\emph{coreset} which is essentially a weighted set of points that approximates the fitting loss for every model, up to a multiplicative factor of $1\\pm\\varepsilon$. Our method supports $M$-estimators that are robust to outliers, convex shapes, lines, and hyper-planes. We demonstrate the empirical contribution of our clustering method for video tracking and map creation from video, by running it on micro-computers (Le-Potato and Raspberry Pi) on synthetic and real-world videos with real-time running time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=kmn0BhQk7p": {
    "title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models",
    "volume": "review",
    "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95.8% top-3 accuracy at a fraction of the cost (100x) and time (240x) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for stronger and wider privacy protection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=cQgjz0mf0r": {
    "title": "Deep Network Partition Density Exhibits Double Descent",
    "volume": "review",
    "abstract": "The study of Deep Network (DN) training dynamics has largely focused on the dynamics of the loss function, evaluated on or around train and test set samples. In fact, many DN phenomenon were first introduced in literature with respect to the loss or accuracy dynamics during training, e.g., double descent, grokking. No other statistics about the DN has been found to be as informative as the loss function. In this study, we provide a novel statistic that measures the underlying DN's local complexity, exhibiting two key benefits: (i) it does not require any labels, and (ii) it is informative about the training loss and accuracy dynamics. Our proposed statistic is based on the concentration of partition regions around samples –which encompasses the local expressivity or complexity of a DN– and can be applied on arbitrary architectures, e.g. CNNs, VGGs and Resnets. We show that our statistic exhibits a double descent phenomenon during training, with the partition density first decreasing around training samples, then increasing (ascent), followed by an other descent during which neurons migrate towards the decision boundaries. We see this phenomenon happening for a number of different experimental setups, e.g., training with label noise, delayed generalization, i.e., grokking. Our observations provide a novel lens to study DN training dynamics from a spline theory perspective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=i8PjQT3Uig": {
    "title": "Locality Sensitive Sparse Encoding for Learning World Models Online",
    "volume": "review",
    "abstract": "Model-based reinforcement learning (MBRL) is known to have better sample efficiency. However, acquiring an accurate world model is challenging due to the non-stationarity of data generated from agent interaction, which typically causes catastrophic interference for neural networks (NN). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable: a model that is optimal for all previous experiences. Unfortunately, for NN-based models, FTL means re-training the NN on all accumulated data at every interaction step, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with efficient incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive encoding that is sparse in nature, which allows us to perform efficient online update even with very high dimensional nonlinear features. We present empirical results to validate the representation power of our encoding and verify that it is capable of learning incrementally under data covariate shift, a setting neural networks simply fail. Building on the demonstrated strength of our encoding, we further showcase its efficacy in MBRL settings, spanning both discrete and continuous control tasks. Our online world models, trained using a single pass of trajectory data, either surpass or match the capabilities of neural networks trained with replay and other continual learning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=JWrl5pJCnl": {
    "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Arm Actions with Large Language Model",
    "volume": "review",
    "abstract": "Foundation models have significantly advanced in various applications, including text-to-image generation, open-vocabulary segmentation, and natural language processing. This paper presents Instruct2Act, a framework that leverages Large Language Models (LLMs) to convert multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act uses LLMs to generate Python programs that form a comprehensive perception, planning, and action loop for robotic tasks. It uses pre-defined APIs to access multiple foundation models, with the Segment Anything Model (SAM) identifying potential objects and CLIP semantically classifying them. This approach combines the strengths of foundation models and robotic actions to transform complex high-level instructions into precise policy codes. Our approach is adaptable and versatile, capable of handling various instruction modalities and input types, and meeting specific task requirements. We validated the practicality and efficiency of our approach on robotic tasks including different tabletop and 6 Degree of Freedom(DoF) manipulation scenarios in both simulation and real-world environments. Furthermore, our zero-shot method surpasses many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://anonymous.4open.science/r/Instruct2Act, providing a solid benchmark for high-level robotic instruction tasks with diverse modality inputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=jUNSBetmAo": {
    "title": "Beyond Disentanglement: On the Orthogonality of Learned Representations",
    "volume": "review",
    "abstract": "Evaluating learned representations independently of designated downstream tasks is pivotal for crafting robust and adaptable algorithms across a diverse array of applications. Among such evaluations, the assessment of disentanglement in a learned representation has emerged as a significant technique. In a disentangled representation, independent data generating factors are encoded in mutually orthogonal subspaces, a characteristic enhancing numerous downstream applications, potentially bolstering interpretability, fairness, and robustness. However, a representation is often deemed well-disentangled if these orthogonal subspaces are one-dimensional and align with the canonical basis of the latent space – a powerful yet frequently challenging or unattainable condition in real-world scenarios – thus narrowing the applicability of disentanglement. Addressing this, we propose a novel evaluation scheme, Importance-Weighted Orthogonality (IWO), to gauge the mutual orthogonality between subspaces encoding the data generating factors, irrespective of their dimensionality or alignment with the canonical basis. For that matter, we introduce a new method, Latent Orthogonal Analysis (LOA), which identifies the subspace encoding each data generating factor and establishes an importance-ranked basis spanning it, thereby forming the foundational bedrock for IWO. Through extensive comparisons of learned representations from synthetic and real-world datasets, we demonstrate that, relative to existing disentanglement metrics, IWO offers a superior assessment of orthogonality and exhibits stronger correlation with downstream task performance across a spectrum of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=7QlKLvfVge": {
    "title": "Directional Rank Reduction for Backdoor Defense",
    "volume": "review",
    "abstract": "Recent studies have indicated the effectiveness of neuron pruning for backdoor defense. In this work, we explore the limitations of pruning-based defense through theoretical and empirical investigations. We argue that pruning-based defense necessitates the removal of neurons that affect normal performance when the effect of backdoor is entangled across normal neurons. To address this challenge, we propose an extended neuron pruning framework, named \\emph{Directional Rank Reduction (\\method)}. \\method consists of three procedures: orthogonal transformation, pruning, and inverse transformation. Through the transformation of the feature space prior to pruning, \\method is able to focus the trigger effects on a limited number of neurons for more efficient pruning with less damage, outperforming existing pruning-based defense strategies. We implement \\method using Sarle's Bimodality Coefficient (SBC) which is optimized as the criterion for the transformation matrix based on the separability assumption of benign and poisoned features. Extensive experimental results demonstrate the superiority of our method. On average, our approach substantially reduces the ASR by 4.5x and increases the ACC by 1.45\\% compared with the recently strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=yacRhge4zQ": {
    "title": "Regulation Games for Trustworthy Machine Learning",
    "volume": "review",
    "abstract": "Existing work on trustworthy machine learning (ML) often focuses on a single aspect of trust in ML (e.g., fairness, or privacy) and thus fails to obtain a holistic trust assessment. Furthermore, most techniques often fail to recognize that the parties who train models are not the same as the ones who assess their trustworthiness. We propose a framework that formulates trustworthy ML as a multi-objective multi-agent optimization problem to address these limitations. A holistic characterization of trust in ML naturally lends itself to a game theoretic formulation, which we call regulation games. We introduce and study a particular game instance, the SpecGame, which models the relationship between an ML model builder and regulators seeking to specify and enforce fairness and privacy regulations. Seeking socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of their objectives and avoids the inefficiencies of other equilibria. For instance, we show that for a gender classification application, the achieved privacy guarantee is 3.76× worse than the ordained privacy requirement if regulators do not take the initiative to specify their desired guarantees first. We hope that our framework can provide policy guidance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rEQ8OiBxbZ": {
    "title": "3D Molecular Pretraining via Localized Geometric Generation",
    "volume": "review",
    "abstract": "Self-supervised learning on 3D molecular structures has gained prominence in AI-driven drug discovery due to the high cost of annotating biochemical data. However, few have studied the selection of proper modeling semantic units within 3D molecular data, which is critical for an expressive pre-trained model as recognized in natural language processing and computer vision. In this study, we introduce \\textbf{L}ocalized G\\textbf{e}ometric \\textbf{G}enerati\\textbf{o}n (LEGO), a novel approach that treats tetrahedrons within 3D molecular structures as fundamental building blocks , leveraging their simplicity in three-dimension and their prevalence in molecular structural patterns such as carbon skeletons and functional groups. Inspired by masked language/image modeling, LEGO perturbs a portion of tetrahedrons and learns to reconstruct them in pretraining. The reconstruction of the noised local structures can be divided into a two-step process, namely spatial orientation prediction and internal arrangement generation. First, we predict the global orientation of the noised local structure within the whole molecule, equipping the model with positional information for these foundational components. Then, we geometrically reconstruct the internal arrangements of the noised local structures revealing their functional semantics. To address the atom-bond inconsistency problem in previous denoising methods and utilize the prior of chemical bonds, we propose to model the graph as a set of nodes and edges and explicitly generate the edges during pre-training. In this way, LEGO exploits the advantages of encoding structural geometry features as well as leveraging the expressiveness of self-supervised learning. Extensive experiments on molecular quantum and biochemical property prediction tasks demonstrate the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVSKuh9eK5": {
    "title": "CLIP Exhibits Improved Compositional Generalization Through Representation Disentanglement",
    "volume": "review",
    "abstract": "Vision-language models (VLMs), such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various flavors of distribution shifts. Recent studies attempted to investigate the leading cause of this property. In this work, we target the same goal, but focus on a certain type of distribution shift, in which test images contain unseen compositions of attribute-object pairs, but with the objects and attributes being individually seen during training. The models are expected to classify those images into the composition classes, i.e. attribute-object pairs, and also into object classes by ignoring attributes. We carefully designed an authentic image test dataset consisting of attributes for objects that are unlikely encountered in the CLIP training data. We found that the compositions diversity in the training data, as measured by normalized mutual information between objects and attributes, has a significant effect on the improvement of compositional generalization in the CLIP models. We found that image/text representation disentanglement with respect to the composition constituents also plays a key role in the improved generalization of these models. We notice that larger training datasets could potentially trigger emergence of such a disentanglement, as the compositions are typically more diverse in such datasets. We validate this hypothesis through different representation disentanglement metrics, including Z-Diff, and explicitness scores for various CLIPs. Our findings reveal a correlation between better OoD performance and higher scores in these disentanglement metrics, suggesting that improved disentanglement potentially contributes to enhanced compositional OoD generalization in VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=23OEmHVkpq": {
    "title": "Disentanglement Learning via Topology",
    "volume": "review",
    "abstract": "We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting to apply it for problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=E5CMyG6jl0": {
    "title": "Unified Language Model Alignment with Demonstration and Point-wise Human Preference",
    "volume": "review",
    "abstract": "Language model alignment is a cutting-edge technique in large language model training to align the model output to user's intent, e.g., being helpful and harmless. Recent alignment framework consists of two steps: supervised fine-tuning with demonstration data and preference learning with human preference data. Previous preference learning methods, such as RLHF and DPO, mainly focus on pair-wise preference data. However, in many real-world scenarios where human feedbacks are intrinsically point-wise, e.g., upvotes number or binary criterion, effective model alignment to user preference is under explored. In this paper, we fill this gap by developing a simplified tuning method for point-wise preference data. Further revelation on the connection between supervised fine-tuning and point-wise preference learning enables us to develop a unified framework for both human demonstration and point-wise preference data, which sheds new light on the construction of preference dataset. Extensive experiments demonstrate the superior performance and efficiency of our proposed methods. A new dataset with high-quality demonstration samples on harmlessness are constructed and made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOSsLRKQrX": {
    "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
    "volume": "review",
    "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don't? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination over a small set of learned concepts. We perform an iterative refinement over these slots to extract a disentangled representation, which is then fed to a trans- former architecture to predict the next set of latent object representations. Since our loss is unsupervised, we need to align the output object masks with those ex- tracted from the ground truth image, and we design a novel permutation module to achieve this alignment by learning a canonical ordering. We perform a series of experiments demonstrating that our learned representations help predict future dynamics in the standard setting, where we test on the same environment as train- ing, and in the setting of transfer, where certain object combinations are never seen before. Our method outperforms existing baselines in terms of pixel prediction and deciphering the dynamics, especially in the zero-shot transfer setting where existing approaches fail miserably. Further analysis reveals that our learned representations indeed help with significantly better disentanglement of objects compared to existing techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=1FWDEIGm33": {
    "title": "Large Language Models as superpositions of cultural perspectives",
    "volume": "review",
    "abstract": "Large language models (LLMs) are sometimes viewed as if they were individuals, with given values, personality, knowledge and abilities. We argue that this \"LLM as an individual\" metaphor misrepresents their nature. As opposed to humans, they exhibit highly context-dependent values and personality traits. We propose a new metaphor, \"LLM as a superposition of perspectives\" : LLMs simulate a multiplicity of behaviors, e.g. expressing values, which can be triggered by a given context. As a case study, we conduct experiments on how values vary as a function of context using psychology questionnaires. Crucially, we demonstrate that changes in the context that are unrelated to the topic of questionnaires - varying articles, simulated conversations on other topics, and textual formats - all result in significant unwanted, hard-to-predict changes in the expressed values. We refer to this as the unexpected perspective shift effect. We discuss how this questions the interpretations of studies using psychology questionnaires (and more generally benchmarks) to draw general conclusions about LLMs' values, knowledge and abilities. Indeed, expressing some values on a questionnaire says little about which values a model would express in other contexts. Instead, models should be studied in terms of how the expressed values change over contexts in both expected and unexpected ways. Following this insight, we introduce the concept of perspective controllability - a model's affordance to adopt various perspectives. We conduct a systematic comparison of the controllability of 16 different models over three questionnaires (PVQ, VSM, IPIP) and different methods for inducing perspectives. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=dYjuJGTEbc": {
    "title": "An Enhanced Gromov-Wasserstein Barycenter Method for Graph-based Clustering",
    "volume": "review",
    "abstract": "Optimal Transport (OT) recently has gained remarkable success in machine learning. These methods based on the Gromov-Wasserstein (GW) distance have proven highly effective in capturing complex data topologies and underlying structures. More specifically, Gromov-Wasserstein Learning (GWL) has recently introduced a framework for graph partitioning by minimizing the GW distance. Various improved versions stemming from this framework have showcased state-of-the-art performance on clustering tasks. Building upon GW barycenter, we introduce a novel approach that significantly enhances other GW-based models flexibility by relaxing the target distribution (cluster size) in GWL and using a wide class of positive semi-definite matrices. We then develop an efficient algorithm to solve the resulting non-convex problem by utilizing regularization and the successive upper-bound minimization techniques. The proposed method exhibits the capacity to identify improved partition results within an enriched searching space, as validated by our developed theoretical framework and numerical experiments. Furthermore, we bridge the proposed model with the well-known clustering methods including Non-negative Matrix Factorization, Min-Cut, Max-Dicut and other GW-based models. This connection provides a new solution to these traditional clustering problems from the perspective of OT. Real data experiments illustrate our method outperforms state-of-the-art graph partitioning methods on both directed and undirected graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=eepoE7iLpL": {
    "title": "Enhancing Neural Subset Selection: Integrating Background Information into Set Representations",
    "volume": "review",
    "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=jXR5pjs1rV": {
    "title": "Everyone Deserves A Reward: Learning Customized Human Preferences",
    "volume": "review",
    "abstract": "Reward models (RMs) are essential for aligning large language models (LLMs) with human preferences to improve interaction quality. However, the real world is pluralistic, which leads to diversified human preferences with respect to different religions, politics, cultures, etc. Moreover, each individual can have their unique preferences on various topics. Neglecting the diversity of preferences, current human feedback aligning methods only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. To explore customized preference learning, we collect a domain-specific preference (DSP) dataset, which consists of comprehensive user queries and corresponding responses preferred from four practical domains. Besides, from the perspective of data efficiency, we propose a three-stage customized RM learning scheme, then empirically verify its effectiveness on both general preference datasets and our DSP set. Furthermore, we test multiple training and data strategies on the three learning stages. We find several ways to better preserve the general preferring ability while training the customized RMs, especially general preference enrichment, and customized preference imitation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=lK2V2E2MNv": {
    "title": "Bridging Vision and Language Spaces with Assignment Prediction",
    "volume": "review",
    "abstract": "While pretrained large language models (LLMs) excel in understanding linguistic contexts, it is still an open question: Can LLMs extend their capabilities beyond linguistic contexts to non-linguistic information? This paper introduces VLAP, a novel approach that bridges vision encoders and language models through assignment prediction. Since the LLMs interpret and reason linguistic information from correlations between word embeddings, we harness the well-established word embeddings to map visual representations into language space. Specifically, we simultaneously assign the visual and text representations to a set of word embeddings within LLMs. We propose a new training objective, optimal transport-based assignment prediction, to enforce the consistency of word assignments for paired multimodal data. This allows frozen LLMs to ground their word embedding space in visual data and use their robust semantic taxonomy visually. Moreover, VLAP is memory- and parameter-efficient in that it trains only a single linear layer, and works without extra embedding space (e.g. learnable prototypes) for the assignment prediction. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based methods across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3wL1tj3kqE": {
    "title": "Fair Domain Generalization with Arbitrary Sensitive Attributes",
    "volume": "review",
    "abstract": "We consider the problem of fairness transfer in domain generalization. Traditional domain generalization methods are designed to generalize a model to unseen domains. Recent work has extended this capability to incorporate fairness as an additional requirement. However, it is only applicable to a single, unchanging sensitive attribute across all domains. As a naive approach to extend it to a multi-attribute context, we can train a model for each subset of the potential set of sensitive attributes. However, this results in $2^n$ models for $n$ attributes. We propose a novel approach that allows any combination of sensitive attributes in the target domain. We learn two representations, a domain invariant representation to generalize the model's performance, and a selective domain invariant representation to transfer the model's fairness to unseen domains. As each domain can have a different set of sensitive attributes, we transfer the fairness by learning a selective domain invariant representation which enforces similar representations among only those domains that have similar sensitive attributes. We demonstrate that our method decreases the current requirement of $2^n$ models to $1$ to accomplish this task. Moreover, our method outperforms the state-of-the-art on unseen target domains across multiple experimental settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=9L9j5bQPIY": {
    "title": "Metanetwork: A novel approach to interpreting ANNs",
    "volume": "review",
    "abstract": "Recent work on mechanistic interpretability, which attempts to demystify the black box of artificial neural network (ANN) models through analytical approaches, has made it possible to give a qualitative interpretation of how each component of the model works, even without using the dataset the model was trained on. However, it is also desirable from the viewpoint of interpretability to understand the ability of the entire model; and considering the previous studies on task embedding, the ability of the entire model should also be represented by a vector. In this study we propose a novel approach to quantitatively interpreting an unseen ANN's ability based on relationships with other ANNs through obtaining a low-dimensional representation of ANNs by training a \"metanetwork\" that autoencodes ANNs. As a first-ever attempt of such an approach, we train a \"metanetwork\" to autoencode ANNs consisting of one fully-connected layer. We demonstrate the validity of our proposed approach by showing that a simple k-Nearest Neighbor classifier can successfully predict properties of the training datasets of unseen models from their embedded representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=gtkFw6sZGS": {
    "title": "Generative Judge for Evaluating Alignment",
    "volume": "review",
    "abstract": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding *generality* (i.e., assessing performance across diverse scenarios), *flexibility* (i.e., examining under different protocols), and *interpretability* (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, **Auto-J**, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, **Auto-J** outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://anonymous.4open.science/r/Auto-J-ICLR-ver-0107",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7vVWiCrFnd": {
    "title": "Rethinking and Extending the Probabilistic Inference Capacity of GNNs",
    "volume": "review",
    "abstract": "Designing expressive Graph Neural Networks (GNNs) is an important topic in graph machine learning fields. Despite the existence of numerous approaches proposed to enhance GNNs based on Weisfeiler-Lehman (WL) tests, what GNNs \\emph{can and cannot} learn still lacks a deeper understanding. This paper adopts a fundamentally different approach to examine the expressive power of GNNs from a probabilistic perspective. By establishing connections between GNNs' predictions and the central inference problems of probabilistic graphical models (PGMs), we can analyze previous GNN variants with a novel hierarchical framework and gain new insights into their node-level and link-level behaviors. Additionally, we introduce novel methods that can provably enhance GNNs' ability to capture complex dependencies and make complex predictions. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.6,
    "authors": []
  },
  "https://openreview.net/forum?id=lNIj5FdXsC": {
    "title": "Recurrent Distance-Encoding Neural Networks for Graph Representation Learning",
    "volume": "review",
    "abstract": "Graph neural networks based on iterative one-hop message-passing have been shown to struggle in harnessing information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but suffer from high computational complexity and have to rely on ad-hoc positional encodings to bake in the graph inductive bias. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates nodes at different distances and uses a parallelizable linear recurrent network over the chain of distances to provide a natural encoding of its neighborhood structure. With no need for positional encoding, we empirically show that the performance of our model is competitive compared with that of state-of-the-art graph transformers on various benchmarks, at a drastically reduced computational complexity. In addition, we show that our model is theoretically more expressive than one-hop message-passing neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=bDWXhzZT40": {
    "title": "Learning model uncertainty as variance-minimizing instance weights",
    "volume": "review",
    "abstract": "Predictive uncertainty--a model's self-awareness regarding its accuracy on an input--is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditional reweighting approach that captures predictive uncertainty using an auxiliary network, and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing dropout variance, an approximation of Bayesian predictive uncertainty, We show in controlled experiments that we effectively capture diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings–selective classification, label noise, domain adaptation, calibration–and across datasets–Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs, Imagenet-C,-A,-R, Clothing-1.6M, etc. For Diabetic Retinopathy, we see upto 3.4\\%/3.3\\% accuracy & AUC gains over SOTA in selective classification. We also improve upon large-scale pretrained models such as PLEX",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=DwcV654WBP": {
    "title": "TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale",
    "volume": "review",
    "abstract": "The ultimate goal for foundation models is realizing task-agnostic, i.e., supporting out-of-the-box usage without task-specific fine-tuning. Although breakthroughs have been made in natural language processing and image representation learning, it is still challenging for video models to reach it due to the increasing uncertainty of spatiotemporal signals. To ease training, existing works leverage image foundation models' prior knowledge and equip them with efﬁcient temporal modules. Despite the satisfactory fine-tuning performance, we empirically find they fall short of out-of-the-box usage, given the even degraded performance in zero-shot/linear protocols compared to their baseline counterparts. In this work, we analyze the factor that leads to degradation from the perspective of language supervision distortion. We argue that tuning a text encoder end-to-end, as done in previous work, is suboptimal since it may overfit in terms of styles, thereby losing its original generalization ability to capture the semantics of various language registers. The overfitted text encoder, in turn, provides a harmful supervision signal, degrading the video representation. To tackle this issue, we propose a degradation-free pre-training strategy to retain the generalization ability of the text encoder via freezing shallow layers while enabling the task-related semantics capturing in tunable deep layers. As for the training objective, we adopted the transcript sorting task in TVTS incorporated with masking techniques to enable scalable training. As a result, we produce a series of models, dubbed TVTSv2, with up to one billion parameters. We achieve new state-of-the-arts on various video benchmarks with a frozen backbone, surpassing the recent ImageBind, InternVideo, etc. Code and models will be released publicly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SLw9fp4yI6": {
    "title": "Controlled Text Generation via Language Model Arithmetic",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style and character becomes more important. In this work we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at [ANONYMIZED]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=My7lkRNnL9": {
    "title": "Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization",
    "volume": "review",
    "abstract": "Forward-only\" algorithms, which train neural networks while avoiding a backward pass, have recently gained attention as a way of solving the biologically unrealistic aspects of backpropagation. Here, we first address compelling challenges related to the ``forward-only\" rules, which include reducing the performance gap with backpropagation and providing an analytical understanding of their dynamics. To this end, we show that the forward-only algorithm with top-down feedback is well-approximated by an \"adaptive-feedback-alignment\" algorithm and we analytically track its performance during learning in a prototype high-dimensional setting. Then, we compare different versions of forward-only algorithms, focusing on the Forward-Forward and PEPITA frameworks, and we show that they share the same principles. Overall, our work unveils the connections between three key neuro-inspired learning rules, providing a link between \"forward-only\" algorithms, i.e., Forward-Forward and PEPITA, and an approximation of backpropagation, i.e., Feedback Alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=xibcBSuuq0": {
    "title": "Do not Start with Trembling Hands: Improving Multi-agent Reinforcement Learning with Stable Prefix Policy",
    "volume": "review",
    "abstract": "In multi-agent reinforcement learning (MARL), the $\\epsilon$-greedy method plays an important role in balancing exploration and exploitation during the decision-making process in value-based algorithms. However, we find that $\\epsilon$-greedy can be deemed as the concept of \"trembling hands\" in game theory when the agents are more in need of exploitation, which may result in the Trembling Hands Nash Equilibrium solution, a suboptimal policy convergence. Besides, eliminating the $\\epsilon$-greedy algorithm leaves no exploration and may lead to unacceptable local optimal policies. To address this dilemma, we use the previously collected trajectories to plan an existing optimal template as candidate policy, which we call \\textbf{Stable Prefix Policy}, in contrast to trembling hands. When the policy is close to the optimal policy, the agents follow the planned template, and when the policy still needs exploration, the agents will adaptively dropout. We scale our approach to various value-based MARL methods and empirically verify our method in a cooperative MARL task, SMAC benchmarks. Experimental results demonstrate that our method achieves not only better performance but also faster convergence speed than baseline algorithms within 2M time steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0wJ5oCPdB": {
    "title": "Chain-of-Symbol Prompting for Spatial Relationships in Large Language Models",
    "volume": "review",
    "abstract": "While conventional Chain-of-Thought prompting shows promising performance on various language tasks for LLMs, the spatial scenarios are nearly unexplored. In this paper, we first investigate the performance of LLMs on complex spatial planning and understanding tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act or reason correspondingly in text. By evaluating on classic spatial planning scenarios through natural language descriptions, we found that current popular LLMs such as ChatGPT still lack abilities to handle spatial relationships in texts. This arises a question -- do the natural language is the best way to represent complex spatial environments for LLMs, or maybe other alternatives such as symbolic representations are both more efficient and effective for LLMs? To this end, we propose a novel method called **CoS** (**C**hain-**o**f-**S**ymbol Prompting) that represents the spatial relationships with condensed symbols during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. Extensive experiments indicate that CoS clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting described in natural langauge in all three spatial planning tasks and existing spatial QA benchmark, with even fewer tokens used in the inputs compared with CoT. The performance gain is strong, by up to 60.8\\% accuracy (from 31.8\\% to 92.6\\%) on Brick World scenarios for ChatGPT. CoS also reduces the number of tokens in the prompt obviously, by up to 65.8\\% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on the Brick World task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=IefMMX12yk": {
    "title": "Lightweight Graph Neural Network Search with Graph Sparsification",
    "volume": "review",
    "abstract": "Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constraint scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose Lightweight Graph Neural Architecture Search with Graph SparsIfication and Network Pruning (GASSIP). In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=pYmQId95iR": {
    "title": "RLP: A reinforcement learning benchmark for neural algorithmic reasoning",
    "volume": "review",
    "abstract": "Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Although Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments, its potential in learning generalizable and complex algorithms remains largely unexplored. To evaluate the current state of algorithmic reasoning in RL, we introduce an RL benchmark based on Simon Tatham's Portable Puzzle Collection. This benchmark contains 40 diverse logic puzzles of varying complexity levels, which serve as captivating challenges that test cognitive abilities, particularly in neural algorithmic reasoning. Our findings demonstrate that current RL approaches struggle with neural algorithmic reasoning, emphasizing the need for further research in this area. All of the software, including the environment, is available at https://github.com/rlppaper/rlp",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=AZGIwqCyYY": {
    "title": "Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning",
    "volume": "review",
    "abstract": "Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. While effective, these methods are confined to the system domain in which the type of system remains consistent and thus cannot ensure the adaptation to new, or unseen physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot guarantee the accurate prediction of the behavior of a two-body system or any other system with different physical laws. In this work, we take a significant leap forward by targeting cross domain generalization within the field of Hamiltonian dynamics. We model our system with a graph neural network and employ a meta learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a unified Hamiltonian representation that is generalizable across multiple system domains, thereby overcoming the limitations of system-specific models. We validate our approach on a dataset comprising various physical systems and evaluate its adaptability to a new type of dynamical system with previously unseen physics. Our results demonstrate that the meta trained model not only adapts effectively to new systems but also captures a generalized Hamiltonian representation that is consistent across different physical domains. Overall, through the use of meta learning, we offer a framework that achieves cross domain generalization, providing a step towards a unified model for understanding a wide array of dynamical systems via deep learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=b0IRscfEOb": {
    "title": "ReLiK: Retrieve, Read and LinK: Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
    "volume": "review",
    "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in various applications such as Information Retrieval, Question Answering, and Knowledge Graph Construction. However, existing approaches often suffer from either a lack of flexibility, low-performance issues, or computational inefficiency. In this paper, we propose ReLiK, a Retriever-Reader architecture, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass in contrast with previous Retriever-Reader-based methods, which necessitate a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed with respect to other competitors. Finally, we propose a model for closed Information Extraction (cIE), i.e. EL + RE, which sets a new state of the art by employing a shared Reader that simultaneously extracts entities and relations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=BTKAeLqLMw": {
    "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
    "volume": "review",
    "abstract": "Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present Deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA models using data samples automatically selected with our proposed approach. When assessed through both automatic metrics and human evaluation, Deita performs better or on par with the state-of-the-art open-source alignment models such as Vicuna and WizardLM with only 6K training data samples -- 10x less than the data used in the baselines. We anticipate this work to provide clear guidelines and tools on automatic data selection, aiding researchers and practitioners in achieving data-efficient alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=AJBkfwXh3u": {
    "title": "Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks",
    "volume": "review",
    "abstract": "Dynamic Graph Neural Networks (DyGNNs) have gained significant popularity in the research of dynamic graphs, but are limited by the low transparency, such that human-understandable insights can hardly be drawn from their predictions. Although a number of existing research have been devoted to investigating the interpretability of graph neural networks (GNNs), achieving the interpretability of DyGNNs is pivotally challenging due to the complex spatial-temporal correlations in dynamic graphs. To this end, we propose an innovative causality-inspired generative model based on structural causal model (SCM), which explores the underlying philosophies of DyGNN predictions by identifying the trivial, static, and dynamic causal relationships. To reach this goal, two critical tasks need to be accomplished including (1) disentangling the complex causal relationships, and (2) fitting the spatial-temporal explanations of DyGNNs in the SCM architecture. To tackle these challenges, the proposed method incorporates a contrastive learning module to disentangle trivial and causal relationships, and a dynamic correlating module to disentangle dynamic and static causal relationships, respectively. A dynamic VGAE-based framework is further developed, which generates causal-and-dynamic masks for spatial interpretability, and recognizes dynamic relationships along the time horizon through causal invention for temporal interpretability. Comprehensive experiments have been conducted on both synthetic and real-world datasets, where our approach yields substantial improvements, thereby demonstrating significant superiority",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmsqb6WpLz": {
    "title": "Dissecting learning and forgetting in language model finetuning",
    "volume": "review",
    "abstract": "Finetuning language models on domain-specific corpus is a common approach to enhance their domain knowledge and capability. While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities. In this study, we analyze the effects of finetuning on language models by dissecting its impacts on the modeling of topic, style, and factual knowledge in text. Our method uses instruction-following LLMs such as ChatGPT to auto-generate controlled-variable text examples which we use to probe the model. Our findings reveal that finetuning results in significant shifts in the language model's topic and style priors, while actual knowledge learning only contributes to a small fraction of the total probability change. Analysis shows that the adaptation of topic and style priors behave akin to learning simple features: they are learned rapidly and require little model capacity. They are also learned independently and primarily at the beginning of a text sequence. In contrast, factual knowledge is learned stably but slowly and requires significant model capacity to learn. The research offers insights and understanding into the finer dynamics of learning and forgetting in language models, and can potentially inform future research on improving domain adaptation and addressing the challenges of forgetting in continual learning of language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RpKA1wqgk0": {
    "title": "MetaFormer with Holistic Attention Modelling Improves Few-Shot Classification",
    "volume": "review",
    "abstract": "Pre-trained vision transformers have revolutionized few-shot image classification, and it has been recently demonstrated that the previous common practice of meta-learning in synergy with these pre-trained transformers still holds significance and contributes to further advancing their performance. Unfortunately, the majority of working insights such as task conditioning are specifically tailored for convolutional neural networks, thus failing to translate effectively to vision transformers. This work sets out to bridge this gap via a coherent and lightweight framework called MetaFormer, which maintains compatibility with off-the-shelf pre-trained vision transformers. The proposed MetaFormer consists of two attention modules, i.e., the Sample-level Attention Module (SAM) and the Task-level Attention Module (TAM). SAM works in conjunction with the patch-level attention in Transformers to enforce consistency in the attended features across samples within a task, while TAM regularizes learning of the current task with an attended task in the pool. Empirical results on four few-shot learning benchmarks, i.e., miniImageNet, tieredImageNet, CIFAR-FS, and FC100, showcase that our approach achieves the new state-of-the-art at a very modest increase in computational overhead. Furthermore, our approach excels in cross-domain task generalization scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xNdE7RiRyP": {
    "title": "TinyTrain: Deep Neural Network Training at the Extreme Edge",
    "volume": "review",
    "abstract": "On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\\geq$10\\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0\\% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098$\\times$ and 7.68$\\times$, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\\times$ faster and 3.5$\\times$ more energy-efficient training over status-quo approaches, and 2.23$\\times$ smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=TPZRq4FALB": {
    "title": "Test-time Adaption against Multi-modal Reliability Bias",
    "volume": "review",
    "abstract": "Test-time adaption (TTA) has emerged as a new paradigm for reconciling distribution shifts between domains without accessing source data. However, existing TTA methods mainly concentrate on uni-modal tasks, overlooking the complexity in multi-modal scenarios. In this paper, we delve into the multi-modal test-time adaption and reveal a new challenge named reliability bias. Different from the definition of traditional distribution shifts, reliability bias refers to the information discrepancies across different modalities derived from intra-modal distribution shifts. To solve the challenge, we propose a novel method, dubbed reliable fusion and robust adaption (RFRA). On the one hand, unlike the existing TTA paradigm that mainly repurposes the normalization layers, RFRA employs a new paradigm that modulates the attention between modalities in a self-adaptive way, supporting reliable fusion against reliability bias. On the other hand, RFRA adopts a novel objective function for robust multi-modal adaption, where the contributions of confident predictions could be amplified and the negative impacts of noisy predictions could be mitigated. Moreover, we introduce two new benchmarks to facilitate comprehensive evaluations of multi-modal TTA under reliability bias. Extensive experiments on the benchmarks not only verify the effectiveness of our method but also give some new observations to the community. The code and benchmarks will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPyLDIVBNP": {
    "title": "Fully Identical Initialization",
    "volume": "review",
    "abstract": "Deep neural networks (DNNs) have achieved numerous remarkable accomplishments in practice. The success of these networks hinges on effective initialization methods, which are vital for ensuring stable and rapid convergence during training. Recently, initialization methods that maintain identity transition within layers have shown good efficiency in network training. These techniques (e.g., Fixup) set specific weights to zero to achieve identity control. However, settings of remaining weight (e.g., Fixup uses random values to initialize non-zero weights) will affect inductive bias that is achieved only by a zero weight, which may be harmful to training. Addressing this concern, we introduce fully identical initialization (IDInit), an innovative method that preserves identity in both the main and sub-stem layers of residual networks. IDInit employs a padded identity-like matrix to overcome rank constraints in non-square weight matrices. Furthermore, we show a convergence problem of an identity matrix can be solved by adding a momentum term into the optimizer. Additionally, we explore enhancing the universality of IDInit by processing higher-order weights and addressing dead neuron problems. IDInit is a straightforward yet effective initialization method, promising improved convergence, stability, and performance across various settings, including large-scale datasets and deep models. It stands as a novel solution for initializing non-standard weight matrices, offering significant advantages in network training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=30L0rr9W8A": {
    "title": "LatentCBF: A Control Barrier Function in Latent Space for Safe Control",
    "volume": "review",
    "abstract": "Safe control is crucial for safety-critical autonomous systems that are deployed in dynamic and uncertain environments. Quadratic-programming-control-barrier-function (QP-CBF) is becoming a popular tool for safe controller synthesis. Traditional QP-CBF relies on explicit knowledge of the system dynamics and access to all states, which are not always available in practice. We propose LatentCBF (LCBF), a control barrier function defined in the latent space, which only needs an agent's observations, not full states. The transformation from observations to latent space is established by a Lipschitz network-based AutoEncoder. In addition, the system dynamics and control barrier functions are all learned in the latent space. We demonstrate the efficiency, safety, and robustness of LCBFs in simulation for quadrotors and cars",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=78iGZdqxYY": {
    "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification",
    "volume": "review",
    "abstract": "GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE's superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6Htk1rwkK": {
    "title": "Exploring mechanisms of Neural Robustness: probing the bridge between geometry and spectrum",
    "volume": "review",
    "abstract": "Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. In cancer detection, for example, slight image alterations can misclassify benign moles as malignant. Biological neural systems do not have such issues. Thus, understanding the biological mechanisms of robustness is an important step towards building trustworthy and safe systems. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=k65Nh7IV6X": {
    "title": "Two-shot learning of continuous interpolation using a conceptor-aided recurrent autoencoder",
    "volume": "review",
    "abstract": "Generalizing from only two time series towards unseen intermediate patterns poses a significant challenge in representation learning. In this paper, we introduce a novel representation learning algorithm, \"Conceptor-Aided Recurrent Autoencoder\" (CARAE), which leverages a conceptor-based regularization to learn to generate a continuous spectrum of intermediate temporal patterns while just being trained on two distinct examples. Here, conceptors, a linear subspace characterization of neuron activations, are employed to impose a low-dimensional geometrical bottleneck on the neural dynamics. During training, CARAE assembles a continuous and stable manifold between the two trained temporal patterns. Exploiting this manifold in the inference, CARAE facilitates continuous and phase-aligned interpolation between temporal patterns that are not linked within the training data. We demonstrate the effectiveness of the CARAE framework through comprehensive experiments on temporal pattern generation tasks and the generation of novel complex motion patterns based on the MoCap data set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9k0krNzvlV": {
    "title": "On the Learnability of Watermarks for Language Models",
    "volume": "review",
    "abstract": "Language model watermarking enables reliable detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can damage the reputation of a victim model by spoofing its watermark and generating harmful watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three distinct decoding-based watermarking strategies, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=n1LiKueC4F": {
    "title": "Personalized Language Generation via Bayesian Metric Augmented Retrieval",
    "volume": "review",
    "abstract": "Our paper presents a Bayesian adaptation of Retrieval Augmented Generation (RAG) designed to capture the characteristics of each user, encompassing factors such as their educational background and professions. We model each individual's characteristics using specific perturbations of the local metric of the embedding space. This perturbation introduces a crucial shift in the distance evaluation between the query's and the document's embedding, leading to different pertinent rankings of the retrieved documents. We propose a Bayesian learning procedure that assimilates user feedback and continuously enhances our estimation of the user-specific metric. In the beginning, when there is no information about the user, we use a diverse retrieval method for generation. After this burn-in phase, we learn a Bayesian posterior estimate of the metric, and inject this metric into the nearest neighbor search for document retrieval. This additional layer of metric information acquisition leads to empirical improvement in the retrieval quality and in the performance of the generated text on multiple concept explanation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=D0zeqL7Vnz": {
    "title": "Prompt Sketching for Large Language Models",
    "volume": "review",
    "abstract": "Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=elMKXvhhQ9": {
    "title": "Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision",
    "volume": "review",
    "abstract": "Graph Anomaly Detection (GAD) has surfaced as a significant field of research, predominantly due to its substantial influence in production environments. Although existing approaches for node anomaly detection have shown effectiveness, they have yet to fully address two major challenges: operating in settings with limited supervision and managing class imbalance effectively. In response to these challenges, we propose a novel model, ConsisGAD, which is tailored for GAD in scenarios characterized by limited supervision and is anchored in the principles of consistency training. Under limited supervision, ConsisGAD effectively leverages the abundance of unlabeled data for consistency training by incorporating a novel learnable data augmentation mechanism, thereby introducing controlled noise into the dataset. Moreover, ConsisGAD takes advantage of the variance in homophily distribution between normal and anomalous nodes to craft a simplified GNN backbone, enhancing its capability to distinguish effectively between these two classes. Comprehensive experiments on several benchmark datasets validate the superior performance of ConsisGAD in comparison to state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=O3BaKCxTAO": {
    "title": "OPTIMIZING STABILIZATION IN SINGULARLY PER- TURBED PROBLEMS WITH SUPG SCHEME",
    "volume": "review",
    "abstract": "This paper introduces ConvStabNet, a convolutional neural network that predicts optimal stabilization parameters for the Streamline Upwind Petrov Galerkin method (SUPG) stabilization scheme. To enhance the accuracy of SUPG in solving partial differential equations (PDE) with interior and bound- ary layers, ConvStabNet incorporates a loss function that combines a strong residual component and a cross-wind derivative term. ConvStabNet utilizes a shared parameter scheme, enabling the network to learn the correlations between cell properties and their respective stabilization parameters while effectively managing the parameter space. Comparative evaluations against state-of-the-art neural network solvers based on variational formulations demonstrate the superior performance of ConvStabNet. The results affirm ConvStabNet as a promising approach for accurately predicting stabilization parameters in SUPG, thereby establishing it as an improvement over neural network-based SUPG solvers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Iyve2ycvGZ": {
    "title": "Bellman Optimal Step-size Straightening of Flow-Matching Models",
    "volume": "review",
    "abstract": "Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the fine-tuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS) technique for distilling flow-matching generative models: it aims specifically for a few-step efficient image sampling while adhering to a computational budget constraint. First, this technique involves a dynamic programming algorithm that optimizes the step sizes of the pretrained network. Then, it refines the velocity network to match the optimal step sizes, aiming to straighten the generation paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and image quality. Our results reveal that BOSS achieves substantial gains in efficiency while maintaining competitive sample quality, effectively bridging the gap between low-resource constraints and the demanding requirements of flow-matching generative models. Our paper also fortifies the responsible development of artificial intelligence, offering a more sustainable generative model that reduces computational costs and environmental footprints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=j7S7o6ROn9": {
    "title": "Distributional Structured Pruning by Lower bounding the Total Variation Distance using Witness functions",
    "volume": "review",
    "abstract": "Recent literature introduced the notion of distributional structured pruning (DSP) in Deep Neural Networks by retaining discriminative filters that can effectively differentiate between classes. Crucial to DSP is the ability to estimate the discriminative ability of a filter, which is defined by the minimum pairwise Total Variation (TV) distance between the class-conditional feature distributions. Since the computation of TV distance is generally intractable, existing literature assumes the class-conditional feature distributions are Gaussian, thereby enabling the use of the tractable Hellinger lower bound to estimate discriminative ability. However, the Gaussian assumption is not only restrictive but also does not typically hold. In this work, we address this gap by deriving a lower bound on TV Distance which depends only on the moments of witness functions. Using linear witness functions, the bound establishes new relationships between the TV Distance and well-known discriminant-based classifiers, such as Fisher Discriminants and Minimax Probability machines. The lower bounds are used to produce a variety of pruning algorithms called WitnessPrune by varying the choice of witness function. We empirically show that we can achieve up to 7\\% greater accuracy for similar sparsity in hard-to-prune layers using a polynomial witness function as compared to the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Yg5eylBHe": {
    "title": "ZGS-Based Event-Driven Algorithms for Bayesian Optimization in Fully Distributed Multi-Agent Systems",
    "volume": "review",
    "abstract": "Bayesian optimization (BO) is a well-established framework for globally optimizing expensive-to-evaluate black-box functions with impressive efficiency. Although numerous BO algorithms have been developed for the centralized machine learning setting and some recent works have extended BO to the tree-structured federated learning, no previous studies have investigated BO within a fully distributed multi-agent system (MAS) in the field of distributed learning (DL). Addressing this gap, we introduce and investigate a novel paradigm, Distributed Bayesian Optimization (DBO), in which agents cooperatively optimize the same costly-to-evaluate black-box objectives. An innovative generalized algorithm, Zero-Gradient-Sum-Based Event-Driven Distributed Lower Confidence Bound (ZGS-ED-DLCB), is proposed to overcome the significant challenges of DBO and DL: We (a) adopt a surrogate model based on random Fourier features as an approximate alternative to a typical Gaussian process to enable the exchange of local knowledge between neighboring agents, and (b) employ the event-driven mechanism to enhance communication efficiency in MASs. Moreover, we propose a novel generalized fully distributed convergence theorem, which represents a substantial theoretical and practical breakthrough wrt the ZGS-based DL. The performance of our proposed algorithm has been rigorously evaluated through theoretical analysis and extensive experiments, demonstrating substantial advantages over the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=fGskrC9Wy1": {
    "title": "Boosted Long Short-Term Memory with Additional Inner Layers",
    "volume": "review",
    "abstract": "Long Short-Term Memory (LSTM) is widely known as a powerful type of Recurrent Neural Network, allowing it to achieve great results on many difficult sequential data tasks. Numerous experiments have shown that adding more complexity to neural network architectures may lead to a significant increase in performance that outweighs the incurred costs of an upgraded structure. In this paper, we propose a Boosted LSTM model created by adding layers inside the LSTM unit to optimize the model by enhancing its memory and reasoning capabilities. We evaluated the performance of different versions of Boosted LSTM architectures using three empirical tasks, studying the impact of different placements of additional layers, the activation functions used in the additional layers, and the model's hidden units. The experiments have shown that the Boosted LSTM unit, which uses Exponential Linear Unit as its boosted layers activation function, performs better than the similar models created from the simple LSTM units while often taking fewer epochs to achieve similar or better results, usually in a smaller number of training epochs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Xsrsj3cne4": {
    "title": "An Optimization-Based Framework for Adversarial Defence of Graph Neural Networks Via Adaptive Lipschitz Regularization",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) have exhibited exceptional performance across diverse application domains by harnessing the inherent interconnectedness of data. However, the emergence of adversarial attacks targeting GNNs poses a substantial and pervasive threat, compromising their overall performance and learning capabilities. While recent efforts have focused on enhancing GNN robustness from both data and architectural perspectives, more attention should be given to overall network stability in the face of input perturbations. Prior methods addressing network stability have routinely employed gradient normalization as a fundamental technique. This study introduces a unifying approach, termed as AdaLip, for adversarial training of GNNs through an optimization framework that leverages the explicit Lipschitz constant. By seamlessly integrating graph denoising and network regularization, AdaLip offers a comprehensive and versatile solution, extending its applicability and enabling robust regularization for diverse neural network architectures. Further, we develop a provably convergent iterative algorithm, leveraging block majorization-minimization, graph learning, and alternate minimization techniques to solve the proposed optimization problem. Simulation results on real datasets demonstrate the efficacy of AdaLip over state-of-the-art defence methods across diverse classes of poisoning attacks. On select datasets, AdaLip demonstrates GCN performance improvements of up to 20\\% against modification attacks and approximately 10\\% against injection attacks. Remarkably, AdaLip achieves a similar performance gain on heterophily graph datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=M11LONBkx1": {
    "title": "Diffusion with Synthetic Features: Feature Imputation for Graphs with Partially Observed Features",
    "volume": "review",
    "abstract": "In this paper, we tackle learning tasks on graphs with missing features, improving the applicability of graph neural networks to real-world graph-structured data. Previous diffusion-based imputation methods overlook the presence of channels with low-variance features, and these channels contribute very little to the performance in graph learning tasks. To overcome this issue, we propose a new diffusion-based imputation scheme using synthetic features in addition to observed features. The proposed scheme first identifies channels with low-variance features via pre-diffusion and generates a synthetic feature for a randomly chosen node in each low-variance channel. Then, our diffusion process spreads the synthetic features widely while considering observed features simultaneously. Extensive experiments on graphs with various rates of missing features demonstrate the effectiveness of our scheme, achieving state-of-the-art performance in both semi-supervised node classification and link prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2wwPG1wpsu": {
    "title": "LST-Bench:A Benchmark for long sequence time-series forecasting Task",
    "volume": "review",
    "abstract": "This paper introduces LST-Bench, a comprehensive benchmark designed for evaluating long sequence time-series forecasting(LSTF) models. This benchmark has been developed in response to recent advancements in deep learning methods in the field of LSTF tasks. LST-Bench includes Transformer-based, MLP-based, CNN-based, and RNN-based models, evaluating the performance of 11 major forecasting models on a set of commonly used 7 datasets and 7 new datasets that we have introduced. We conduct a thorough analysis of the experimental results, including the overall prediction performance of models and their generalization across different prediction lengths and datasets. Notably, we found that regardless of the model architecture, the phenomenon referred to as \"Degeneracy\" occurs when the model's predictions consistently maintain a low Mean Squared Error value but are characterized by repetitive and simplistic pattern generation, thus losing the meaningfulness of the predictions. Also, the model's optimal performance is very close to its performance after training for just one epoch. These two phenomenons emphasize the need for further investigation. Our LST-Bench will serve as a valuable resource for advancing research in the field of time series forecasting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6ARlSgun7J": {
    "title": "Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction",
    "volume": "review",
    "abstract": "Extreme Classification (XC) architectures, which utilize a massive one-vs-all classifier layer at the output, have demonstrated remarkable performance on problems with large label sets. Nonetheless, these have also been observed to falter on tail labels with few representative samples. This phenomenon has been attributed to factors such as classifier over-fitting and missing label bias, and solutions involving regularization and loss re-calibration have been developed. This paper explores the impact of label variance, a previously unexamined factor, on the tail performance in extreme classifiers. Label variance refers to the imprecision introduced in the ground truth when sampling it from a complex underlying distribution - a common phenomenon in most XC datasets. This compromises the quality of trained models, with a pronounced impact on the classifiers for infrequently sampled tail labels. This paper presents a method to systematically reduce label variance in XC by effectively utilizing the capabilities of an additional, tail-robust teacher model. It proposes a principled knowledge distillation framework, \\model, which enhances tail performance in extreme classifiers, with formal guarantees on generalization. Finally, we introduce an effective instantiation of this framework that employs a specialized Siamese teacher model. This model excels in tail accuracy and significantly enhances the quality of student one-vs-all classifiers. Comprehensive experiments are conducted on a diverse set of XC datasets which demonstrate that \\model can enhance tail performance by around 5\\% and 6\\% points in PSP and Coverage metrics respectively when integrated with leading extreme classifiers. Moreover, when added to the top-performing Renée classifier, it establishes a new state-of-the-art. Extensive ablations and analysis substantiate the efficacy of our design choices. Code and datasets will be released for research purposes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LegZeFYugN": {
    "title": "Time2Image: A Unified Image Representation Framework for Time Series Classification",
    "volume": "review",
    "abstract": "Time Series Classification (TSC) is a crucial and challenging task that holds significant importance across various domains, of which one of the kernel ingredients is to construct a suitable time series representation for better feature capture. However, extracting informative and robust time series representation with good generalization potential is still a challenging problem. To address this issue, we propose Time2Image, a novel image-based representation framework for TSC. At the heart of our framework is a proposed Adaptive Time Series Gaussian Mapping (ATSGM) module for robust time series encoding in 2D image structure, based on which we employ Vision Transformer (ViT) for subsequent classification tasks considering its prominent long-dependency modeling capability. Experiments were conducted on all 158 public time series datasets from UCR/UEA covering diverse domains, among which our method achieves top 1 performance in 86 datasets compared with existing State-Of-The-Art (SOTA) methods. In addition, our framework flexibly allows handling both univariate and multivariate time series with unequal length across different domains and takes inherent advantage of generalization ability due to our proposed ATSGM representation method. The source code will be publicly available soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=4IxtmklIym": {
    "title": "FruitBin: A tunable large-scale dataset for advancing 6D Pose estimation in fruit bin picking automation",
    "volume": "review",
    "abstract": "Bin picking is a ubiquitous application spanning across diverse industries, demanding automated solutions facilitated by robots. These automation systems hinge upon intricate components, including object instance-level segmentation and 6D pose estimation, which are pivotal for predicting future grasping and manipulation success. Contemporary computer vision approaches predominantly rely on deep learning methodologies and necessitate access to extensive instance-level datasets. However, prevailing datasets and benchmarks tend to be confined to oversimplified scenarios, such as those with singular objects on tables or low levels of object clustering. In this research, we introduce FruitBin. It emerges as an unparalleled resource, boasting an extensive collection of over a million images and 40 million instance-level 6D poses. Additionally FruitBin differs with other datasets whith its inclusive representation of a wide spectrum of challenges, encompassing symmetric and asymmetric fruits, objects with and without discernible texture, and diverse lighting conditions, all enriched with extended annotations and metadata. Leveraging the inherent challenges and the sheer scale of FruitBin, we highlight its potential as a versatile benchmarking tool that can be customized to suit various evaluation scenarios. As a demonstration of this adaptability, we have created two distinct types of benchmarks: one centered on novel scene generalization and another focusing on novel camera viewpoint generalization. Both benchmark types offer four levels of occlusion to facilitate the study of occlusion robustness. Notably, our study showcases the difficulty of FruitBin dataset, with two baseline 6D pose estimation models, one utilizing RGB images and the other RGB-D data, across these eight distinct benchmarks. FruitBin emerges as a pioneering dataset distinguishing itself by seamlessly integrating with robotic software. That enable direct testing of trained models in dynamic grasping tasks for the purpose of robot learning. Samples of the dataset with its associated code are provided in the supplementary materials. FruitBin promises to be a catalyst for advancing the field of robotics and automation, providing researchers and practitioners with a comprehensive resource to push the boundaries of 6D pose estimation in the context of fruit bin picking and beyond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=lmYGRGyL4i": {
    "title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential",
    "volume": "review",
    "abstract": "In the field of deep graph generative models, two families coexist: one-shot models, which fill the graph content in one go given a number of nodes, and sequential models, where new nodes and edges are inserted sequentially and autoregressively. Recently, one-shot models are seeing great popularity due to their rising sample quality and lower sampling time compared to the more costly autoregressive models. With this paper we unify the two worlds in a single framework, unlocking the whole spectrum of options where one-shot and sequential models are but the two extremes. We use the denoising diffusion models' theory to develop a node removal process, which destroys a given graph through many steps. An insertion model reverses this process by predicting how many nodes have been removed from the intermediate subgraphs. Then, generation happens by iteratively adding new blocks of nodes, with size sampled from the insertion model, and content generated using any one-shot model. By adjusting the knob on node removal, the framework allows for any degree of sequentiality, from one-shot to fully sequential, and any node ordering, e.g., random and BFS. Based on this, we conduct the first analysis of the sample quality-time trade-off across a range of molecular and generic graphs datasets. As a case study, we adapt DiGress, a diffusion-based one-shot model, to the whole spectrum of sequentiality, reaching new state of the art results, and motivating a renewed interest in developing autoregressive graph generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=P2gnDEHGu3": {
    "title": "Summing Up the Facts: Additive Mechanisms behind Factual Recall in LLMs",
    "volume": "review",
    "abstract": "How do large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form \\tokens{Fact: The Colosseum is in the country of}. We find that the mechanistic story behind factual recall is more complex than previously thought -- We show there exist four distinct and independent mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the \\textbf{additive motif}: models compute correct answers through adding together multiple independent contributions; the contributions from each mechanism are insufficient alone, but together they constructively interfere on the correct attribute when summed. In addition, we extend the method of direct logit attribution to attribute a head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0JTwZ30qPH": {
    "title": "Task-Oriented Multi-View Representation Learning",
    "volume": "review",
    "abstract": "Multi-view representation learning aims to learn a high-quality unified representation for an entity from its multiple observable views to facilitate the performance of downstream tasks. A typical multi-view representation learning framework consists of four main components: View-specific encoding, Single-view learning (SVL), Multi-view learning (MVL), and Fusion. Recent studies achieve promising performance by carefully designing SVL and MVL constraints, but almost all of them ignore the basic fact that \\textit{effective representations are different for different tasks, even for the same entity}. To bridge this gap, this work proposes a \\textbf{T}ask-\\textbf{O}riented \\textbf{M}ulti-\\textbf{V}iew \\textbf{R}epresentation \\textbf{L}earning (TOMRL) method, where the key idea is to modulate features in the View-specific encoding and Fusion modules according to the task guidance. To this end, we first design a gradient-based embedding strategy to flexibly represent multi-view tasks. After that, a meta-learner is trained to map the task embedding into a set of view-specific parameters and a view-shared parameter for modulation in the Encoding and Fusion modules, respectively. This whole process is formalized as a nested optimization problem and ultimately solved by a bi-level optimization scheme. Extensive experiments on four multi-view datasets validate that our TOMRL consistently improves the performance of most existing multi-view representation learning approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILStlRb1Sp": {
    "title": "Understanding the Mechanics and Dynamics of Memorisation in Large Language Models: A Case Study with Random Strings",
    "volume": "review",
    "abstract": "Understanding whether and to what extent large language models (LLMs) have memorised training data has important implications for the privacy of its training data and the reliability of its generated output. In this work, we focus on the more foundational question of how LLMs memorise training data. To this end, we systematically train LLMs of different sizes to memorise random token strings of different lengths and different entropies (i.e., sampled from different alphabet distributions) and study their ability to recall the strings. We observe many striking memorisation dynamics including (i) memorisation in phases with the alphabet distributions in the random strings being learnt before their relative positions in the string are memorised and (ii) memorisation in parts at the granularity of individual tokens, but not necessarily in the order in which they appear in the string. Next, we investigate memorisation mechanics by checking to what extent different parts of a token's prefix in the string are necessary and sufficient to recollect the token. We leverage our insights to explain the dynamics of memorising strings and we conclude by discussing the implications of our findings for quantifying memorisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hm6maU150b": {
    "title": "NeFL: Nested Federated Learning for Heterogeneous Clients",
    "volume": "review",
    "abstract": "Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies tackle the system heterogeneity by splitting a model into submodels, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting forward propagation of models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels of different architecture, we decouple a few parameters from parameters being trained for each submodel. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant performance gains, especially for the worst-case submodel. Furthermore, we demonstrate NeFL aligns with recent studies in FL, regarding pre-trained models of FL and the statistical heterogeneity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Ebt7JgMHv1": {
    "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
    "volume": "review",
    "abstract": "Mechanistic interpretability aims to attribute high-level model behaviors to specific, interpretable learned features. It is hypothesized that these features manifest as directions or low-dimensional subspaces within activation space. Accordingly, recent studies have explored the identification and manipulation of such subspaces to reverse-engineer computations, employing methods such as activation patching. In this work, we demonstrate that naïve approaches to subspace interventions can give rise to interpretability illusions. Specifically, even if patching along a subspace has the intended end-to-end causal effect on model behavior, this effect may be achieved by activating \\emph{a dormant parallel pathway} using a component that is \\textit{causally disconnected} from the model output. We demonstrate this in a mathematical example, realize the example empirically in two different settings (the Indirect Object Identification (IOI) task and factual recall), and argue that activating dormant pathways ought to be prevalent in practice. In the context of factual recall, we further show that the illusion is related to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localisation. However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (IOI) where prior manual circuit analysis allows an understanding of the location of the ground truth feature. We explore the additional evidence needed to argue that a patched subspace is faithful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=qgyLAr2cOs": {
    "title": "Fixed-Budget Best Arm Identification with Variance-Dependent Regret Bounds",
    "volume": "review",
    "abstract": "We investigate the problem of fixed-budget best arm identification (BAI) for minimizing expected simple regret. In an adaptive experiment, a decision maker draws one of multiple treatment arms based on past observations and observes the outcome of the drawn arm. After the experiment, the decision maker recommends the treatment arm with the highest expected outcome. We evaluate the decision based on the expected simple regret, which is the difference between the expected outcomes of the best arm and the recommended arm. Due to inherent uncertainty, we evaluate the regret using the minimax criterion. First, we derive asymptotic lower bounds for the worst-case expected simple regret, which are characterized by the variances of potential outcomes (leading factor). Based on the lower bounds, we propose the Adaptive-Sampling (AS)-Augmented Inverse Probability Weighting (AIPW) strategy, which utilizes the AIPW estimator in recommending the best arm. Our theoretical analysis shows that the AS-AIPW strategy is asymptotically minimax optimal, meaning that the leading factor of its worst-case expected simple regret matches our derived worst-case lower bound. Finally, we validate the proposed method's effectiveness through simulation studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=YItWKZci78": {
    "title": "Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems",
    "volume": "review",
    "abstract": "In this paper, we extend mean-field Langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. We propose \\emph{mean-field Langevin averaged gradient} (MFL-AG), a single-loop algorithm that implements gradient descent ascent in the distribution space with a novel weighted averaging, and establish average-iterate convergence to the mixed Nash equilibrium. We also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. Furthermore, we propose \\emph{mean-field Langevin anchored best response} (MFL-ABR), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. Finally, we study applications to zero-sum Markov games and conduct simulations to demonstrate the long-term optimality of MFL-AG and MFL-ABR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RvmrhrPy7j": {
    "title": "Causal Inference Using LLM-Guided Discovery",
    "volume": "review",
    "abstract": "At the core of causal inference lies the critical challenge of determining reliable causal graphs solely based on observational data. Since the well-known backdoor criterion depends on the graph, any errors in the graph can propagate downstream to effect inference. In this work, we initially show that complete graph information is not necessary for causal effect inference; the topological order over graph variables (causal order) alone suffices. Further, given a node pair, causal order is easier to elicit from domain experts compared to graph edges since determining the existence of an edge can depend extensively on other variables. Interestingly, we find that the same principle holds for Large Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated method to obtain causal order (and hence causal effect) with LLMs acting as virtual domain experts. To this end, we employ different prompting strategies and contextual cues to propose a robust technique of obtaining causal order from LLMs. Acknowledging LLMs' limitations, we also study possible techniques to integrate LLMs with established causal discovery algorithms, including constraint-based and score-based methods, to enhance their performance. Extensive experiments demonstrate that our approach significantly improves causal ordering accuracy as compared to established discovery algorithms, highlighting the potential of LLMs to enhance causal inference across diverse fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=030cjlZm4a": {
    "title": "Learning Predictive Checklists with Probabilistic Logic Programming",
    "volume": "review",
    "abstract": "Checklists have been widely recognized as effective tools for completing complex tasks in a systematic manner. Although originally intended for use in procedural tasks, their interpretability and ease of use have led to their adoption for predictive tasks as well, including in clinical settings. However, designing checklists can be challenging, often requiring expert knowledge and manual rule design based on available data. Recent work has attempted to address this issue by using machine learning to automatically generate predictive checklists from data, although these approaches have been limited to Boolean data. We propose a novel method for learning predictive checklists from diverse data modalities, such as images, time series, and text, by combining the power of dedicated deep learning architectures with the interpretability and conciseness of checklists. Our approach relies on probabilistic logic programming, a learning paradigm that enables matching the discrete nature of a checklist with continuous-valued data. We propose a regularization technique to tradeoff between the information captured in discrete concepts of continuous data and permit a tunable level of interpretability for the learned checklist concepts. We demonstrate that our method outperforms various explainable machine learning techniques on prediction tasks involving image sequences, clinical notes, and time series",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Rd4pGjTcTj": {
    "title": "Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions",
    "volume": "review",
    "abstract": "Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either singleturn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We then utilize Parrot-Ask to engage in multiturn conversations with ChatGPT across a diverse range of topics, resulting in a collection of 40K high-quality multi-turn dialogues (Parrot-40K). These data are subsequently employed to train a chat model that we have named Parrot-Chat. We demonstrate that the dialogues gathered from Parrot-Ask markedly outperform existing multi-turn instruction-following datasets in critical metrics, including topic diversity, number of turns, and resemblance to human conversation. With only 40K training examples, Parrot-Chat achieves strong performance against other 13B open-source models across a range of instruction-following benchmarks, and particularly excels in evaluations of multi-turn capabilities. All codes and datasets will be publicly available to facilitate future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bSlAUCyY4T": {
    "title": "Knowledge Graph Completion by Intermediate Variables Regularization",
    "volume": "review",
    "abstract": "Knowledge graph completion (KGC) can be framed as a 3-order binary tensor completion task. Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC. In this paper, we provide a summary of existing TDB models and derive a general form for them, serving as a foundation for further exploration of TDB models. Despite the expressiveness of TDB models, they are prone to overfitting. Existing regularization methods merely minimize the norms of embeddings to regularize the model, leading to suboptimal performance. Therefore, we propose a novel regularization method for TDB models that addresses this limitation. The regularization is applicable to most TDB models, incorporates existing regularization methods, and ensures tractable computation. Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor. To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting. Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ukmh3mWFf0": {
    "title": "Attributed Graph Clustering via Coarsening with Modularity",
    "volume": "review",
    "abstract": "Graph clustering is a widely used technique for partitioning graphs, community detection, and other tasks. Recent graph clustering algorithms depend on combinations of the features and adjacency matrix, or solely on the adjacency matrix. However, in order to achieve high-quality clustering, it is necessary to consider all these components. In this paper, we propose a novel unsupervised learning framework that incorporates modularity with graph coarsening techniques and important graph regularization terms that improve the clustering performance. Furthermore, we also take into account Dirichlet energies for smoothness of signals, spectral similarity, and coarsening reconstructional error. The proposed framework is solved efficiently by leveraging block majorization-minimization, $\\log\\det$ of the Laplacian, smoothness and modularity, and is readily integrable with deep learning architectures such as GCNs and VGAEs in the form of losses. Extensive theoretical analysis and experiments with benchmark datasets elucidate the proposed framework's efficacy in graph clustering over existing state-of-the-art methods on both attributed and non-attributed graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=l18hiEXRJS": {
    "title": "MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Deep Neural Networks",
    "volume": "review",
    "abstract": "Despite their successful application to a variety of tasks, neural networks remain limited, like other machine learning methods, by their sensitivity to shifts in the data: their performance can be severely impacted by differences in distribution between the data on which they were trained and that on which they are deployed. In this article, we propose a new family of representations, called MAGDiff, that we extract from any given neural network classifier and that allows for efficient covariate data shift detection without the need to train a new model dedicated to this task. These representations are computed by comparing the activation graphs of the neural network for samples belonging to the training distribution and to the target distribution, and yield powerful data- and task-adapted statistics for the two-sample tests commonly used for data set shift detection. We demonstrate this empirically by measuring the statistical powers of two-sample Kolmogorov-Smirnov (KS) tests on several different data sets and shift types, and showing that our novel representations induce significant improvements over a state-of-the-art baseline relying on the network output",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=lF2aip4Scn": {
    "title": "Demonstration-Regularized RL",
    "volume": "review",
    "abstract": "Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. Precisely, we study the demonstration-regularized reinforcement learning framework that leverages the expert demonstrations by $\\mathrm{KL}$-regularization for a policy learned by behavior cloning. Our findings reveal that utilizing $N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite and $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight convergence guarantees for the behaviour cloning procedure under general assumptions on the policy classes. Additionally, we establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF). In this respect, we provide theoretical evidence showing the benefits of KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, thus setting our approach apart from the prior works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=jFiFmHrIfD": {
    "title": "Explorative Latent Self-Supervised Active Search Algorithm (ELSA)",
    "volume": "review",
    "abstract": "In computer vision, attaining exceptional performance often necessitates access to large labeled datasets. The creation of extensive datasets through manual annotation is not only cost-prohibitive but also practically infeasible due to the scarcity of positive samples in imbalanced datasets where negative samples dominate. To tackle this intricate problem, we introduce Efficient Latent Space-based Self-Supervised Active Learning Search (ELSA), an active learning-based labeling assistant. ELSA distinguishes itself from existing interactive annotation methods by focusing exclusively on positive class labeling in massively imbalanced datasets replete with a substantial number of negative samples. Through the automatic exclusion of the majority of negative samples, ELSA achieves a remarkable level of precision and accuracy in its search. This novel framework comprises three fundamental components: a)an iterative Nearest Neighbor Search, b)a Sophisticated Random Sampler, c)a Linear Head powered by Active Learning. Our comprehensive study provides insights into the interplay of these components and their collective impact on search efficiency. Notably, we demonstrate that ELSA achieves orders of magnitude superior performance, in average starting with as little as 5 or less positive samples in ImageNet 1k we managed to detect as much as 80\\% of all the examples belonging to that class by only labeling as little as 0.67\\% of the entire dataset manually",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=TvkvWjxj3T": {
    "title": "Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models",
    "volume": "review",
    "abstract": "In image editing employing diffusion models, it is crucial to preserve the reconstruction quality of the original image while changing its style. Although existing methods ensure reconstruction quality through optimization, a drawback of these is the significant amount of time required for optimization. In this paper, we propose negative-prompt inversion, a method capable of achieving equivalent reconstruction solely through forward propagation without optimization, thereby enabling much faster editing processes. We experimentally demonstrate that the reconstruction quality of our method is comparable to that of existing methods, allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds, which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction quality with a moderate increase in computation time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xVBXz7wD2m": {
    "title": "GatedMTL: Learning to Share, Specialize, and Prune Representations for Multi-task Learning",
    "volume": "review",
    "abstract": "Jointly learning multiple tasks with a unified network can improve accuracy and data efficiency while simultaneously reducing computational and memory costs. However, in practice, Multi-task Learning (MTL) is challenging, as optimizing one task objective may inadvertently compromise the performance of another: This is known as task interference. A promising direction to mitigate such conflicts between tasks is to allocate task-specific parameters, free from interference, on top of shared features, allowing for positive information transfer across tasks, albeit at the cost of higher computational demands. In this work, we propose a novel MTL framework, GatedMTL, to address the fundamental challenges of task interference and computational constraints in MTL. GatedMTL learns the optimal balance between shared and specialized representations for a given computational budget. We leverage a learnable gating mechanism allowing each individual task to select and combine channels from its own task-specific features and a shared memory bank of features. Moreover, we regularize the gates to learn the optimal balance between allocating additional task-specific parameters and the model's computational costs. Through extensive empirical evaluations, we demonstrate SoTA results on three MTL benchmarks using convolutional as well as transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=3sOE3MFepx": {
    "title": "PDE-Diffusion: Physic guided diffusion model for solving partial derivative equations",
    "volume": "review",
    "abstract": "Solving partial differential equations (PDEs) is crucial in various disciplines, and their resolution often necessitates the use of computationally intensive numerical methods as well as specialized domain expertise. While data-driven approaches have emerged as promising alternatives, they encounter limitations in terms of generalizability, interpretability, and long-horizon predictive performance, as well as issues related to temporal incoherence. To address these challenges, we introduce the PDE-Diffusion, a two-stage model with three distinctive features: (i) the incorporation of physics-based priors to enhance model interpretability and generalization, (ii) a two-stage diffusion model that efficiently handles physical field forecasting without requiring multi-frame inputs, and (iii) the assimilation of PDE-informed constraints to ensure temporal coherence while producing high-quality predictive results. We conduct extensive experiments to evaluate PDE-Diffusion's capabilities using the PDEBench dataset and two of our newly proposed datasets. The results indicate that PDE-Diffusion delivers state-of-the-art performance in all cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2Pup7olzxj": {
    "title": "Differentiable Optimization in Plane-Wave Density Functional Theory for Solid States",
    "volume": "review",
    "abstract": "Plane-wave density functional theory is a computational quantum mechanical modeling method used to investigate the electronic structure of solids. It employs plane-waves as the basis set for representing electronic wave functions and leverages density functional theory to compute the electronic structure properties of many-body systems. Traditionally, the Self-Consistent Field (SCF) method is predominantly adopted for optimization in current DFT computations. However, this method encounters notable convergence and computational challenges, and its iterative nature obstructs the incorporation of emergent deep learning enhancements. To address these challenges, we introduce a fully differentiable optimization method tailored to resolve the intrinsic challenges associated with the optimization of plane-wave density functional methods. This methodology includes a direct total energy minimization approach for solving Kohn-Sham equations in periodic crystalline systems, which is coherent with deep learning infrastructures. The efficacy of our approach is illustrated through its two applications in solid-state physics: electron band structure prediction and geometry optimization. Our enhancements potentially pave the way for various gradient-based applications within deep learning paradigms in solid-state physics, extending the boundaries of material innovation and design. We illustrate the utility and diverse applications of our method on real crystal structures and compare its effectiveness with several established SCF-based packages, demonstrating its accuracy and robust convergence property",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vESNKdEMGp": {
    "title": "Multilingual Jailbreak Challenges in Large Language Models",
    "volume": "review",
    "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English data. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario entails malicious users combining jailbreak instructions with multilingual prompts to attack LLMs deliberately. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of jailbreak instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. Finally, we propose a novel \\textsc{Self-Defense} framework that addresses the multilingual jailbreak challenges via automatically generating multilingual safety training data for fine-tuning. Experiment results demonstrate its effectiveness with notable reduction in unsafe rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=DZ6B5u4vfe": {
    "title": "Instruction-tuned LLMs with World Knowledge are More Aligned to the Human Brain",
    "volume": "review",
    "abstract": "Instruction-tuning is a widely adopted method of finetuning that enables large language models (LLMs) to generate output that more closely resembles human responses to natural language queries, in many cases leading to human-level performance on diverse testbeds. However, it remains unclear whether instruction-tuning truly makes LLMs more similar to how humans process language. We investigate the effect of instruction-tuning on LLM-human similarity in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs across three datasets involving humans reading naturalistic stories and sentences, and discover that instruction-tuning generally enhances brain alignment by an average of 6%, but does not have a similar effect on behavioral alignment. To identify the factors underlying LLM-brain alignment, we compute the correlation between the brain alignment of LLMs and various model properties, such as model size, performance ability on problem-solving benchmarks, and ability on benchmarks requiring world knowledge spanning various domains. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results suggest that making world knowledge in LLMs more accessible via instruction-tuning also yields neural representations more similar to those of the human language system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RIaIpdUCPb": {
    "title": "Brain-inspired Geometry Constrain on Represention for Compositional Generalization",
    "volume": "review",
    "abstract": "Compositional Generalization (CG), referring as the generalization ability to new combinations of essential concepts, is thought to be one mechanism underlying human's remarkable capability of rapid generalization to new knowledge and tasks. Recent research on brain neural codes has found that the geometry structure of the neural representations is highly related to human compositional generalization ability. In this paper, we extend the above neural science observation into artificial neural networks (ANN) and find that the geometry structure of the representations in ANN impacts their compositional generalization. More importantly, we reveal that only good geometry structure is not sufficient for strong CG ability, a regularization is essential to ensure the classifier can fit the representation geometry structure. We propose a loss to optimize the representation extractor to form a well-organized representation space, and a regularization on the classifier to force it align with the geometry structure of representation space. With our proposed methods, the CG performance gains as large as 43\\% on the synthetic and 63\\% on real-world datasets, verifying the effectiveness of our brain-inspired ANN-enhancing approach towards human-like strong generalization ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=PuCno7nwgH": {
    "title": "Categorical Features of entities in Recommendation Systems Using Graph Neural Networks",
    "volume": "review",
    "abstract": "The paper tackles the challenge of capturing entity attribute-specific preferences in recommender systems, with a particular focus on the role of categorical features within GNN-based user-item recommender engines. Despite the significant influence of categorical features such as brand, category, and price bucket on the user decision-making process, there are not many studies dedicated to understanding the GNN's capability to extract and model such preferences effectively. The study extensively compares and tests various techniques for incorporating categorical features into the GNN framework to address this gap. These techniques include one-hot encoding-based node features, category-value nodes, and hyperedges. Three real-world datasets are used to answer what is the most optimal way to incorporate such information. In addition, the paper introduces a novel hyperedge-based method designed to leverage categorical features more effectively compared to existing approaches. The advantage of the hyperedge approach is demonstrated through extensive experiments in effectively modeling categorical features and extracting user attribute-specific preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=csukJcpYDe": {
    "title": "Generalized Policy Iteration using Tensor Approximation for Hybrid Control",
    "volume": "review",
    "abstract": "Control of dynamic systems involving hybrid actions is a challenging task in robotics. To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor approximation technique called Tensor Train (TT) to approximate the state-value and advantage function which enables us to efficiently handle hybrid systems. We demonstrate the superiority of our approach over previous baselines for some benchmark problems with hybrid action spaces. Additionally, the robustness and generalization of the policy for hybrid systems are showcased through a real-world robotics experiment involving a non-prehensile manipulation task which is considered to be a highly challenging control problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RzNlECeoOB": {
    "title": "$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence",
    "volume": "review",
    "abstract": "The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of a KL divergence between two statistical manifolds and replacing with $\\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-density regions when trained on heavy-tailed synthetic data. Furthermore, we show that our model excels at capturing rare features through real-data experiments on CelebA and imbalanced CIFAR datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=JBLHIR8kBZ": {
    "title": "Neuron to Graph: Interpreting Language Model Neurons at Scale",
    "volume": "review",
    "abstract": "Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N$2$G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N$2$G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGdoTL9g18": {
    "title": "Residual Factorized Fourier Neural Operator for simulation of three-dimensional turbulence",
    "volume": "review",
    "abstract": "Neural Operators, particularly Fourier Neural Operators (FNO), have proven highly effective in simulating partial differential equations (PDEs), such as the Navier-Stokes equations. We propose the Residual Factorized Fourier Neural Operator (Res-F-FNO) for simulating three-dimensional (3D) flows, specifically focusing on flow dynamics around a cube. We extend the Factorized Fourier Neural Operator (F-FNO) architecture by incorporating additional residual connections. This change effectively reintroduces small-scale dynamic flows that may be lost due to truncated Fourier modes, resulting in improved accuracy when modeling wind fields. Our proposed Res-F-FNO model surpasses the performance of the standard F-FNO, achieving an error reduction of over 30\\% in simulating 3D flows. Furthermore, we propose the concept of a skip-corrector, to address the problem of accumulated errors over multiple time steps. The skip-corrector was specifically trained to predict the behaviour of turbulences at a considerably extended time interval. Incorporating the skip-corrector into the prediction process reduces the average error in simulating 100 time steps by more than 50\\%. Additionally, we adopt a modified training approach in which random time steps are chosen as the initial condition for each sample in every epoch, as opposed to generating a dataset by propagating each sample across all time steps. This leads to a significant reduction in the the number of training iterations required for the models to achieve convergence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pp8Kb4hejU": {
    "title": "Adjustable Quantile-Guided Diffusion Policy for Diverse Behavior Generation in Offline RL",
    "volume": "review",
    "abstract": "Offline Reinforcement Learning (RL) addresses the challenge of learning optimal policies from pre-collected data, making it a promising approach for real-world applications where online interactions with an environment are costly or impractical. We propose an offline RL method named Quantile-Guided Diffusion Policy~(qGDP), which trains a quantile network to label the training dataset and uses these labeled samples to train the diffusion model and generate new samples with the trained model according to classifier-free guidance. qGDP can adjust the preference of sample generation between imitating and improving behavioral policies by adjusting the input condition and changing the guidance scale without re-training the model, which will significantly reduce the cost of tuning the algorithm. qGDP exhibits exceptional generalization capabilities and allows easy adjustment of action generation preferences without model retraining, reducing computational costs. Experimental results on the D4RL dataset demonstrate state-of-the-art performance and computational efficiency compared to other diffusion-based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FI0vOp2asx": {
    "title": "Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging",
    "volume": "review",
    "abstract": "Spectral snapshot compressive imaging (Spectral SCI) applies an optical encoder to compressively capture 2D measurements, followed by which the 3D hyperspectral data can be restored via training a deep reconstruction network. Existing reconstruction models are generally trained with a single well-calibrated hardware instance, making their performance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which, however, is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing spectral SCI systems by proposing a novel Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different coded apertures. Extensive experiments demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware configurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous setting. Moreover, a new Snapshot Spectral Heterogeneous Dataset (SSHD) has been built upon multiple practical spectral SCI systems. We will release the data and code to enrich further exploration of this practical computational imaging problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6a1pXXADC": {
    "title": "Prompt Optimization via Adversarial In-Context Learning",
    "volume": "review",
    "abstract": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=6yJuDK1DsK": {
    "title": "FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters",
    "volume": "review",
    "abstract": "Lifelong/continual test-time adaptation (TTA) refers to the problem where a pre-trained source domain model needs to be continually adapted at inference time to handle non-stationary test distributions. Continuously updating the source model over long horizons can result in significant drift in the source model, forgetting the source domain knowledge. Moreover, most of the existing approaches for lifelong TTA require adapting all the parameters, which can incur significant computational cost and memory consumption, limiting their applicability on edge devices for faster inference. We present FEATHER (liFelong tEst-time Adaptation wiTH lightwEight adapteRs), a novel lightweight approach that introduces only a small number of additional parameters to a pre-trained source model which can be unsupervisedly and efficiently adapted during test-time for the new test distribution(s), keeping the rest of the source model frozen. FEATHER disentangles the source domain knowledge from the target domain knowledge, making it robust against error accumulation over time. Another distinguishing aspect of FEATHER is that, unlike some recent approaches for lifelong TTA that require access to the source data for warm-starting the adaptation at test time, FEATHER does not have such a requirement. FEATHER is also orthogonal to the existing lifelong TTA approaches and can be augmented with these approaches, resulting in a significant reduction in the number of additional parameters needed to handle the lifelong TTA setting. Through extensive experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC Robustbench benchmark datasets, we demonstrate that, with substantially (85% to 94%) fewer trainable parameters, FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, resulting in faster adaptation and inference at test-time. The source code for FEATHER will be released upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=GrunXMbdXY": {
    "title": "FLAT-Chat: A Word Recovery Attack on Federated Language Model Training",
    "volume": "review",
    "abstract": "Gradient exchange is widely applied in collaborative training of machine learning models, including Federated Learning. Curious-but-honest participants could potentially infer the output labels in recently used training data by analyzing the latest gradient updates. Previous works mostly demonstrate the attack performance under constraint training settings, such as dozens of short sentences in a batch and a small output space for labels. In this work, we propose a novel gradient flattening attack on the last linear layer of a language model, which significantly improves the attacker's efficiency in inferring the words used in training. We validate the capability of the attack on two language generation tasks: machine translation and language modeling. The attack environment is scaled up to industrial settings of a large output vocabulary and realistic training batch sizes. To mitigate the negative impact of the new attack, we explore two defense methods and demonstrate that adding differential privacy with small noise could effectively defend against our new attack without degrading model utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=UTGv8CayNt": {
    "title": "Chain-of-Thought Predictive Control",
    "volume": "review",
    "abstract": "We study generalizable policy learning from demonstrations for complex low-level control tasks (e.g., contact-rich object manipulations). We propose a novel hierarchical imitation learning method that utilizes scalable, albeit sub-optimal, demonstrations. Firstly, we propose an observation space-agnostic approach that efficiently discovers the multi-step subgoal decomposition (sequences of key observations) of the demos in an unsupervised manner. By grouping temporarily close and functionally similar actions into subskill-level segments, the discovered breakpoints (the segment boundaries) constitute a chain of planning steps (i.e., the chain-of-thought) to complete the task. Next, we propose a Transformer-based design that effectively learns to predict the chain-of-thought (CoT) as the high-level guidance for low-level action. We couple action and CoT predictions via prompt tokens and a hybrid masking strategy, which enable dynamically updated CoT guidance at test time and improve feature representation of the trajectory for generalizable policy learning. Our method, named Chain-of-Thought Predictive Control (CoTPC), consistently surpasses existing strong baselines on a wide range of challenging low-level manipulation tasks with scalable yet sub-optimal demos",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=nTwb2vBLOV": {
    "title": "Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability",
    "volume": "review",
    "abstract": "The expressivity of Graph Neural Networks (GNNs) has been studied broadly in recent years to reveal the design principles for more powerful GNNs. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive GNNs. This paper proposes to maximize the expressivity of GNNs by graph canonization, then the power of such GNNs is studies from the perspective of model stability. A stable GNN will map similar graphs to close graph representations in the vectorial space, and the stability of GNNs is critical to generalize their performance to unseen graphs. We theoretically reveal the trade-off of expressivity and stability in graph-canonization-enhanced GNNs. Then we introduce a notion of universal graph canonization as the general solution to address the trade-off and characterize a widely applicable sufficient condition to solve the universal graph canonization. A comprehensive set of experiments demonstrates the effectiveness of the proposed method. In many popular graph benchmark datasets, graph canonization successfully enhances GNNs and provides highly competitive performance, indicating the capability and great potential of proposed method in general graph representation learning. In graph datasets where the sufficient condition holds, GNNs enhanced by universal graph canonization consistently outperform GNN baselines and successfully improve the SOTA performance up to $31$%, providing the optimal solution to numerous challenging real-world graph analytical tasks like gene network representation learning in bioinformatics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bUv5gJAAxH": {
    "title": "Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension",
    "volume": "review",
    "abstract": "Despite their impressive performance in classification, neural networks are known to be vulnerable to adversarial attacks. These attacks are small perturbations of the input data designed to fool the model. Naturally, a question arises regarding the potential connection between the architecture, settings, or properties of the model and the nature of the attack. In this work, we aim to shed light on this problem by focusing on the implicit bias of the neural network, which refers to its inherent inclination to favor specific patterns or outcomes. Specifically, we investigate one aspect of the implicit bias, which involves the essential Fourier frequencies required for accurate image classification. We conduct tests to assess the statistical relationship between these frequencies and those necessary for a successful attack. To delve into this relationship, we propose a new method that can uncover non-linear correlations between sets of coordinates, which, in our case, are the aforementioned frequencies. By exploiting the entanglement between intrinsic dimension and correlation, we provide empirical evidence that the network bias in Fourier space and the target frequencies of adversarial attacks are closely tied",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=QqqkskOFO9": {
    "title": "Rethinking Actor-Critic: Successive Actors for Critic Maximization",
    "volume": "review",
    "abstract": "Value-based actor-critic approaches have been widely employed for continuous and large discrete action space reinforcement learning tasks. Traditionally, an actor-network is trained to find the action that maximizes the critic (action-value function) with gradient ascent. We identify that often an actor fails to maximize the critic because (i) certain tasks have challenging action-value landscapes with several local optima, and (ii) the critic landscape varies non-stationarily over training. This inability to find the optimal action often leads to sample-inefficient training and suboptimal convergence. To address the challenge of better maximization of the critic's landscape, we present a novel reformulation of the actor by employing a sequence of sub-actors with increasingly tractable action-value landscapes. In large discrete and continuous action space tasks, we demonstrate that our approach finds actions that better maximize the action-value function than conventional actor-network approaches, enabling better performance. [https://sites.google.com/view/complexaction](https://sites.google.com/view/complexaction)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMMF1a9ifL": {
    "title": "Gradual Optimization Learning for Conformational Energy Minimization",
    "volume": "review",
    "abstract": "Molecular conformation optimization is crucial to computer-aided drug discovery and materials design. Traditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients. However, this is a computationally expensive approach that requires many interactions with a physical simulator. One way to accelerate this procedure is to replace the physical simulator with a neural network. Despite recent progress in neural networks for molecular conformation energy prediction, such models are prone to distribution shift, leading to inaccurate energy minimization. We find that the quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data. Still, it takes around $5 \\times 10^5$ additional conformations to match the physical simulator's optimization quality. In this work, we present the Gradual Optimization Learning Framework (GOLF) for energy minimization with neural networks that significantly reduces the required additional data. The framework consists of an efficient data-collecting scheme and an external optimizer. The external optimizer utilizes gradients from the energy prediction model to generate optimization trajectories, and the data-collecting scheme selects additional training data to be processed by the physical simulator. Our results demonstrate that the neural network trained with GOLF performs \\textit{on par} with the oracle on a benchmark of diverse drug-like molecules using $50$x less additional data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=buC4E91xZE": {
    "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection",
    "volume": "review",
    "abstract": "Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/ organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=6SNyuiph3F": {
    "title": "Chat Vector: A Simple Approach to Equip LLMs With New Language Chat Capabilities",
    "volume": "review",
    "abstract": "With the advancements in conversational AI, such as ChatGPT, this paper focuses on exploring developing Large Language Models (LLMs) for non-English languages, especially emphasizing alignment with human preferences. We introduce a computationally efficient method, leveraging \"chat vector,\" to synergize pre-existing knowledge and behaviors in LLMs, restructuring the conventional training paradigm from continual pretrain $\\rightarrow$ SFT $\\rightarrow$ RLHF to continual pretrain + chat. Our empirical studies, primarily focused on Traditional Chinese, employ LLaMA2 as the base model and acquire the chat vector by subtracting the pre-trained weights, LLaMA2, from the weights of LLaMA2-chat. Evaluating from three distinct facets, which are toxicity, ability of instruction following and multi-turn dialogue demonstrates the chat vector's superior efficacy in \"chatting\". To confirm the adaptability of our approach, we extend our experiments to include models pre-trained in both Korean and Simplified Chinese, illustrating the versatility of our methodology. Overall, we present a significant solution in aligning LLMs with human preferences efficiently across various languages, accomplished by the chat vector",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=nOf6sb63dT": {
    "title": "Generative Models are Self-Watermarked: Intellectual Property Declaration through Re-Generation",
    "volume": "review",
    "abstract": "Protecting intellectual property for generated data has emerged as a critical concern for AI corporations, as machine-generated content proliferates. Reusing generated data without permission poses a formidable barrier to safeguarding the intellectual property tied to these models. The verification of data ownership is further complicated by the use of Machine Learning as a Service (MLaaS), which often operates as a black-box system. Our work is dedicated to detecting data reuse from even an individual sample. In contrast to watermarking techniques that embed additional information as watermark triggers into models or generated content, our approach does not introduce artificial watermarks which may compromise the quality of model outputs. Our investigation reveals the existence of latent fingerprints inherently present within deep learning models. In response, we propose an explainable verification procedure to verify data ownership through re-generation. Furthermore, we introduce a novel methodology to amplify the model fingerprints through iterative data regeneration and a theoretical grounding on the proposed approach. We demonstrate the viability of our approach using recent advanced text and image generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxFme785fq": {
    "title": "Nonlinear Inference Learning for Differentially Private Massive Data",
    "volume": "review",
    "abstract": "The Bag of Little Bootstraps (BLB) method is widely utilized as a robust and computationally efficient approach in statistical inference studies involving large-scale data. However, this sampling technique overlooks the privacy protection of the original data. To address this limitation, we enhance the existing differential privacy algorithm and integrate it with the BLB method. This integration gives rise to a novel differential privacy mechanism, enabling a comprehensive statistical analysis of aggregated parameters while safeguarding the confidentiality of individual private data. Additionally, to address both the variability in noise variance under the differential privacy mechanism and the uncertainty surrounding estimate distributions, we employ the central limit theorem within the context of nonlinear expectation theory. This facilitates the derivation of the corresponding test statistic and the introduction of a hypothesis testing methodology. Furthermore, we validate the commendable performance of our proposed inference procedure through data simulation studies. The big data-oriented differential privacy-preserving mechanism proposed in this study effectively fulfills the requirements for privacy preservation without compromising subsequent statistical inference. This contribution holds significant reference value for the sharing of pertinent data and endeavors related to statistical analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uGtfk2OphU": {
    "title": "Boosting Selective Rationalization with Shortcuts Discovery",
    "volume": "review",
    "abstract": "The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on four real-world datasets clearly validate the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=tgjGR7eY5H": {
    "title": "RL4CO: a Unified Reinforcement Learning for Combinatorial Optimization Library",
    "volume": "review",
    "abstract": "Deep reinforcement learning offers notable benefits in addressing combinatorial problems over traditional solvers, reducing the reliance on domain-specific knowledge and expert solutions, and improving computational efficiency. Despite the recent surge in interest in neural combinatorial optimization, practitioners often do not have access to a standardized code base. Moreover, different algorithms are frequently based on fragmentized implementations that hinder reproducibility and fair comparison. To address these challenges, we introduce RL4CO, a unified Reinforcement Learning (RL) for Combinatorial Optimization (CO) library. We employ state-of-the-art software and best practices in implementation, such as modularity and configuration management, to be flexible, easily modifiable, and extensible by researchers. Thanks to our unified codebase, we benchmark baseline RL solvers with different evaluation schemes on zero-shot performance, generalization, and adaptability on diverse tasks. Notably, we find that some recent methods may fall behind their predecessors depending on the evaluation settings. We hope RL4CO will encourage the exploration of novel solutions to complex real-world tasks, allowing the community to compare with existing methods through a unified framework that decouples the science from software engineering. We open-source our library at https://anonymous.4open.science/r/rl4co-iclr",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SRn2o3ij25": {
    "title": "IKL: Boosting Long-Tail Recognition with Implicit Knowledge Learning",
    "volume": "review",
    "abstract": "In the field of visual long-tailed recognition, the long-tailed distribution of image representations often raises two key challenges: (1) the training process shows great uncertainty (e.g., uncertainty in the prediction of augmented views by the same expert for the same sample) and (2) a marked bias in the model's prediction towards the head class. To tackle the above issue, we propose a novel method termed Implicit Knowledge Learning (IKL) to extract the knowledge hidden in long-tail learning processes, aiming to significantly improve performance in long-tail recognition. Our IKL contains two core components: Implicit Uncertainty Regularization (IUR) and Implicit Correlation Labeling (ICL). The former method, IUR, exploits the uncertainty of the predictions over adjacent epochs. Then, it transfers the correct knowledge to reduce uncertainty and improve long-tail recognition accuracy. The latter approach, ICL, endeavors to reduce the bias introduced by one-hot labels by exploring the implicit knowledge in the model: inter-class similarity information. Our approach is lightweight enough to plug and play with existing long-tail learning methods, achieving state-of-the-art performance in popular long-tail benchmarks. The experimental results highlight the great potential of implicit knowledge learning in dealing with long-tail recognition. Our code will be open-sourced upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=LojXXo2xaf": {
    "title": "GPT Can Solve Mathematical Problems Without a Calculator",
    "volume": "review",
    "abstract": "Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Z6lN4GYrO": {
    "title": "S4G: Breaking the Bottleneck on Graphs with Structured State Spaces",
    "volume": "review",
    "abstract": "The majority of GNNs are based on message-passing mechanisms, however, message-passing neural networks (MPNN) have inherent limitations in capturing long-range interactions. The exponentially growing node information is compressed into fixed-size representations through multiple rounds of message passing, bringing the over-squashing problem, which severely hinders the flow of information on the graph and creates a bottleneck in graph learning. The natural idea of introducing global attention to point-to-point communication, as adopted in graph Transformers (GT), lacks inductive biases on graph structures and relies on complex positional encodings to enhance their performance in practical tasks. In this paper, we observe that the sensitivity between nodes in MPNN decreases exponentially with the shortest path distance. Contrarily, GT has a constant sensitivity, which leads to its loss of inductive bias. To address these issues, we introduce structured state spaces to capture the hierarchical structure of rooted-trees, achieving linear sensitivity with theoretical guarantees. We further propose a novel graph convolution based on the state-space model, resulting in a new paradigm that retains both the strong inductive biases from MPNN and the long-range modeling capabilities from GT. Extensive experimental results on long-range and general graph benchmarks demonstrate the superiority of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=KTtEICH4TO": {
    "title": "CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects",
    "volume": "review",
    "abstract": "Nonprehensile manipulation is essential for manipulating objects that are too thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty of contact modeling in conventional modeling-based approaches, reinforcement learning (RL) has recently emerged as a promising alternative. However, previous RL approaches either lack the ability to generalize over diverse object shapes, or use simple action primitives that limit the diversity of robot motions. Furthermore, using RL over diverse object geometry is challenging due to the high cost of training a policy that takes in high-dimensional sensory inputs. We propose a novel contact-based object representation and pretraining pipeline to tackle this. To enable massively parallel training, we leverage a lightweight patch-based transformer architecture for our encoder that processes point clouds, thus scaling our training across thousands of environments. Compared to learning from scratch, or other shape representation baselines, our representation facilitates both time- and data-efficient learning. We validate the efficacy of our overall system by zero-shot transferring the trained policy to novel real-world objects. We highly recommend the video attached in the supplementary material. Code and videos are available at \\url{https://sites.google.com/view/contact-non-prehensile}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5JWAOLBxwp": {
    "title": "An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks",
    "volume": "review",
    "abstract": "The usage of 3D vision algorithms, such as shape reconstruction, remains limited because they require inputs to be at a fixed canonical rotation. Recently, a simple equivariant network, Vector Neuron (VN) has been proposed that can be easily used with the state-of-the-art 3D neural network (NN) architectures. However, its performance is limited because it is designed to use only three-dimensional features, which is insufficient to capture the details present in 3D data. In this paper, we introduce an equivariant feature representation for mapping a 3D point to a high-dimensional feature space. Our feature can discern multiple frequencies present in 3D data, which, as shown by Tancik et al. (2020), is the key to designing an expressive feature for 3D vision tasks. Our representation can be used as an input to VNs, and the results demonstrate that with our feature representation, VN captures more details, overcoming the limitation raised in its original paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=GxrVyYoLSx": {
    "title": "Implicit Regularisation in Overparametrized Networks: A Multiscale Analysis of the Fokker-Planck equation",
    "volume": "review",
    "abstract": "In over-parametrised networks, a large continuous set of solutions (an invariant manifold) exists where the empirical loss is minimal. However, noise in the learning dynamics can introduce a drift along this manifold, biasing the dynamics towards solutions with higher ``smoothness'', therefore acting as a regularizer. In Li et al. (2022), a derivation of this drift was presented, borrowing the results from Katzenberger (1991), which shows that in the small learning-rate limit, $\\eta \\to 0$, the learning dynamics can be approximated by a stochastic differential equation (SDE), whose solution exhibit two distinct phases: a first phase, occurring over a number of steps $O(\\eta^{-1})$, where the parameters are deterministically driven towards the invariant manifold; and a second phase, over timescales $O(\\eta^{-2})$, in which noise induces a deterministic drift along the invariant manifold. This latter contribution to the drift, can be regarded as the result of averaging the dynamics over the $O(\\eta^{1/2})$ fluctuations orthogonal to the manifold, described by an Ornstein--Uhlenbeck process, as first suggested by Blanc et al. (2020). We offer a new derivation of the results by Li et al. (2022), that builds on the very intuitive arguments by Blanc et al. (2020), by implementing the averaging of the Fokker-Planck equation associated with the $\\eta \\to 0$ dynamics over such Ornstein--Uhlenbeck quasi-stationary state. Our contribution demonstrates the application of multiscale methods for elliptic partial differential equations (PDEs) (Pavliotis and Stuart (2008)) to optimization problems in machine learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=AqN23oqraW": {
    "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
    "volume": "review",
    "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models, and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge-related systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=hv3SklibkL": {
    "title": "Graph Parsing Networks",
    "volume": "review",
    "abstract": "Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering. Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph. In this work, inspired by bottom-up grammar induction, we propose an efficient graph parsing algorithm to infer the pooling structure, which then drives graph pooling. The resulting Graph Parsing Network (GPN) adaptively learns personalized pooling structure for each individual graph. GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node information intact. Experimental results on standard benchmarks demonstrate that GPN outperforms state-of-the-art graph pooling methods in graph classification tasks while being able to achieve competitive performance in node classification tasks. We also conduct a graph reconstruction task to show GPN's ability to preserve node information and measure both memory and time efficiency through relevant tests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0nTk5BSvO": {
    "title": "TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts",
    "volume": "review",
    "abstract": "Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph. By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a gating problem into a classification problem with pseudo labels. Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=idpV2AqusC": {
    "title": "Improving SAM Requires Rethinking its Optimization Formulation",
    "volume": "review",
    "abstract": "This paper rethinks Sharpness-Aware Minimization (SAM), which is originally formulated as a zero-sum game where the weights of a network and a bounded perturbation try to minimize/maximize, respectively, the same differentiable loss. We argue that SAM should instead be reformulated using the 0-1 loss, as this provides a tighter bound on its generalization gap. As a continuous relaxation, we follow the simple conventional approach where the minimizing (maximizing) player uses an upper bound (lower bound) surrogate to the 0-1 loss. This leads to a novel formulation of SAM as a bilevel optimization problem, dubbed as BiSAM. Through numerical evidence, we show that BiSAM consistently results in improved performance when compared to the original SAM and variants, while enjoying similar computational complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=72MSbSZtHv": {
    "title": "RedMotion: Motion Prediction via Redundancy Reduction",
    "volume": "review",
    "abstract": "Predicting the future motion of traffic agents is vital for self-driving vehicles to ensure their safe operation. We introduce RedMotion, a transformer model for motion prediction that incorporates two types of redundancy reduction. The first type of redundancy reduction is induced by an internal transformer decoder and reduces a variable-sized set of road environment tokens, such as road graphs with agent data, to a fixed-sized embedding. The second type of redundancy reduction is a self-supervised learning objective and applies the redundancy reduction principle to embeddings generated from augmented views of road environments. Our experiments reveal that our representation learning approach can outperform PreTraM, Traj-MAE, and GraphDINO in a semi-supervised setting. Our RedMotion model achieves results that are competitive with those of Scene Transformer or MTR++. We provide an anonymized open source implementation that is accessible via Colab: https://colab.research.google.com/drive/16pwsmOTYdPpbNWf2nm1olXcx1ZmsXHB8",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Dxl0EuFjlf": {
    "title": "TILDE-Q: A Transformation Invariant Loss Function for Time-Series Forecasting",
    "volume": "review",
    "abstract": "Time-series forecasting has gained increasing attention in the field of artificial intelligence due to its potential to address real-world problems across various domains, including energy, weather, traffic, and economy. While time-series forecasting is a well-researched field, predicting complex temporal patterns such as sudden changes in sequential data still poses a challenge with current models. This difficulty stems from minimizing $L_p$ norm distances as loss functions, such as mean absolute error (MAE) or mean square error (MSE), which are susceptible to both intricate temporal dynamics modeling and signal shape capturing. Furthermore, these functions often cause models to behave aberrantly and generate uncorrelated results with the original time-series. Consequently, the development of a shape-aware loss function that goes beyond mere point-wise comparison is essential. In this paper, we examine the definition of shape and distortions, which are crucial for shape-awareness in time-series forecasting, and provide a design rationale for the shape-aware loss function. Based on our design rationale, we propose a novel, compact loss function called TILDE-Q (Transformation Invariant Loss function with Distance EQuilibrium) that considers not only amplitude and phase distortions but also allows models to capture the shape of time-series sequences. Furthermore, TILDE-Q supports the simultaneous modeling of periodic and nonperiodic temporal dynamics. We evaluate the efficacy of TILDE-Q by conducting extensive experiments under both periodic and nonperiodic conditions with various models ranging from naive to state-of-the-art. The experimental results show that the models trained with TILDE-Q surpass those trained with other metrics, such as MSE and DILATE, in various real-world applications, including electricity, traffic, economics, weather, and electricity transformer temperature (ETT)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OsGUnYOzii": {
    "title": "Learning From Simplicial Data Based on Random Walks and 1D Convolutions",
    "volume": "review",
    "abstract": "Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically. To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships. Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks. We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=H0RztJssmQ": {
    "title": "Adaptive Environmental Modeling for Task-Oriented Language Agents",
    "volume": "review",
    "abstract": "Recent advancements in the realm of intelligent agents, particularly those employing large language models, have been notably significant. Notwithstanding these advancements, intelligent agents encounter substantial challenges, predominantly in interactive and dynamic scenarios such as online shopping, attributed to an absence of integrated environmental modeling. In this paper, we propose a task-oriented environmental adaptation approach, allowing language agents to autonomously model new environments. This approach comprises two pivotal phases: Pre-Task Environment Exploration and In-Task Environment Update. The Pre-Task Environment Exploration phase incorporates a greedy exploration strategy, leveraging an agent in the role of an Evaluator to optimally explore environmental information based on present observations and feasible actions. This strategy is implemented through a recursive algorithm, enabling agents to choose and execute the top-k scored actions, thereby efficiently forming an Action-Observation Tree as the initial environmental modeling. During the In-Task Environment Update phase, agents employ environmental information to enhance task performance. The information generated from task execution and interaction trajectories is used to refine environmental modeling. These processes are iteratively executed, achieving mutual enhancement. We conduct a systematic evaluation of the environmental modeling, assessing both its effectiveness and comprehensiveness. The results demonstrate that under our approach, agents can indeed construct accurate environmental modeling. Simultaneously, we observe a significant enhancement in agent performance on both the ALFWorld-Eco and the WebShop benchmark datasets due to the application of environmental modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=wkbeqr5XhC": {
    "title": "LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition",
    "volume": "review",
    "abstract": "Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is LUM-ViT, a Vision Transformer variant. Uniquely, LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a three-stage fine-tuning strategy. Our evaluations reveal that, by sampling a mere 10\\% of the original image pixels, LUM-ViT maintains the accuracy loss within 1.8\\% on the ImageNet classification task. The method sustains near-original accuracy when implemented on real-world optical hardware, demonstrating its practicality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXNLvfCxEr": {
    "title": "EvIL: Evolution Strategies for Generalisable Imitation Learning",
    "volume": "review",
    "abstract": "We present Evolutionary Imitation Learning (EvIL), a general approach to imitation learning (IL) able to predict agent behaviour across changing environment dynamics. In EvIL, we use Evolution Strategies to jointly meta-optimise the parameters (e.g. reward functions and dynamics) fed to an inner loop reinforcement learning procedure. In effect, this allows us to inherit some of the benefits of the inverse reinforcement learning approach to imitation learning while being significantly more flexible. Specifically, our algorithm can be applied with any policy optimisation method, without requiring the reward or training procedure to be differentiable. Our method succeeds at recovering a reward that induces expert-like behaviour across a variety of environments, even when the environment dynamics are not fully known. We test our method's effectiveness and generalisation capabilities in several tabular environments and continuous control settings and find that it outperforms both offline approaches, like behavioural cloning, and traditional inverse reinforcement learning techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MhzKwuvpm6": {
    "title": "RILe: Reinforced Imitation Learning",
    "volume": "review",
    "abstract": "Learning to imitate behaviors from a limited set of expert trajectories is a promising way to acquire a policy. In imitation learning (IL), an expert policy is trained directly from data in an efficient way, but requires vast amounts of data. On the other hand, inverse reinforcement learning (IRL) deduces a reward function from expert data and then learns a policy with reinforcement learning via this reward function. Although this mitigates the data requirement of imitation learning, IRL approaches suffer from efficiency issues because of sequential learning of the reward function and the policy. In this paper, we combine the strengths of imitation learning and inverse reinforcement learning and introduce RILe: Reinforced Imitation Learning. Our novel dual-agent framework enables joint training of a teacher agent and a student agent. The teacher agent learns the reward function from expert data. It observes the student agent's behavior and provides it with a reward signal. At the same time the student agent learns a policy by using reward signals given by the teacher. Training the student and the teacher jointly in a single learning process offers scalability and efficiency while learning the reward function helps to alleviate data-sensitivity. Experimental comparisons in reinforcement learning benchmarks against imitation learning baselines highlight the superior performance offered by RILe particularly when the number of expert trajectories is limited",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=EyDPfGy4Wh": {
    "title": "Few Heads are Enough",
    "volume": "review",
    "abstract": "The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. The recently proposed Flash-Attention reduces both compute and memory through a *hardware*-aware implementation. Can we achieve this also through *algorithmic* improvements? Here we present Expert Projection Attention (EPA) - a novel method that reduces both compute and memory requirements, while matching the language modeling performance of baseline Transformers using the same parameter budget. EPA uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient \"Fast Transformer\"",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=iKsu33WcmU": {
    "title": "ProtChatGPT: Towards Understanding Proteins with Large Language Models",
    "volume": "review",
    "abstract": "Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9TJDsOEaBC": {
    "title": "Bayesian Vector Optimization with Gaussian Processes",
    "volume": "review",
    "abstract": "Learning problems in which multiple conflicting objectives must be considered simultaneously often arise in various fields, including engineering, drug design, and environmental management. Traditional methods of multi-objective optimization, such as scalarization and identification of the Pareto set under componentwise order, have limitations in incorporating objective preferences and exploring the solution space accordingly. While vector optimization offers improved flexibility and adaptability via specifying partial orders based on ordering cones, current techniques designed for sequential experiments suffer from high sample complexity, which makes them unfit for large-scale learning problems. To address this issue, we propose VOGP, an ($\\epsilon,\\delta$)-PAC adaptive elimination algorithm that performs vector optimization using Gaussian processes. VOGP allows users to convey objective preferences through ordering cones while performing efficient sampling by exploiting the smoothness of the objective function, resulting in a more effective optimization process that requires fewer evaluations. We first establish provable theoretical guarantees for VOGP, and then derive information gain based and kernel specific sample complexity bounds. VOGP demonstrates strong empirical results on both real-world and synthetic datasets, outperforming previous work in sequential vector optimization and its special case multi-objective optimization. This work highlights the potential of VOGP as a powerful preference-driven method for addressing complex sequential vector optimization problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=I7kpf3mZ4n": {
    "title": "Meta- (out-of-context) learning in neural networks",
    "volume": "review",
    "abstract": "Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call **meta-out-of-context learning (meta-OCL)** via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily \"internalize\" the semantic content of text that is, *or appears to be*, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=lnffMykYSj": {
    "title": "On the Long Range Abilities of Transformers",
    "volume": "review",
    "abstract": "Despite their dominance in modern DL and, especially, NLP domains, transformer architectures exhibit sub-optimal performance on long-range tasks compared to recent layers that are specifically designed for this purpose. In this work, drawing inspiration from key attributes of long-range layers, such as state-space layers, linear RNN layers, and global convolution layers, we demonstrate that minimal mod- ifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers. We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality. As we show, integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters. Our experiments also shed light on the reasons for the inferior performance of transformers on long-range tasks and identify critical properties that are essential for successfully capturing long-range dependencies. Our code is attached as supplementary",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SQpnEfv9WH": {
    "title": "Social-Transmotion: Promptable Human Trajectory Prediction",
    "volume": "review",
    "abstract": "Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space. To address this, we introduce \\textit{Social-Transmotion}, a generic model that exploits the power of transformers to handle diverse and numerous visual cues, capturing the multi-modal nature of human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body poses. This, in turn, augments trajectory data, leading to enhanced human trajectory prediction. Our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between pedestrians based on the available visual cues, whether they are poses, bounding boxes, or a combination thereof. By the masking technique, we ensure our model's effectiveness even when certain visual cues are unavailable, although performance is further boosted with the presence of comprehensive visual data. We delve into the merits of using 2d versus 3d poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction. Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=JuyFppXzh2": {
    "title": "Gandalf: Learning label correlations in Extreme Multi-label Classification via Label Features",
    "volume": "review",
    "abstract": "Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches, amongst others. In this paper, we propose Gandalf, a novel approach which makes use of a label correlation graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. While most recent advances in XMC have been algorithmic, mainly aimed towards developing novel deep-learning frameworks, our data-centric augmentation approach is orthogonal to these methodologies, and can be applied in a plug-and-play manner to a variety of them. This generality and effectiveness of \\textit{Gandalf} is demonstrated by showing up to 30\\% relative improvements for 5 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3 million labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=wfgZc3IMqo": {
    "title": "Robust Classification via Regression-Based Loss Reweighting and Label Correction",
    "volume": "review",
    "abstract": "Deep neural networks and large-scale datasets have revolutionized the field of machine learning. However, these large networks are susceptible to overfitting to label noise, resulting in reduced generalization. To address this challenge, two promising approaches have emerged: i) loss reweighting, which reduces the influence of noisy examples on the training loss, and ii) label correction that replaces noisy labels with estimated true labels. These directions have been pursued separately or combined as independent methods, lacking a unified approach. In this work, we present a unified method that seamlessly combines loss reweighting and label correction to enhance robustness against label noise in classification tasks. Specifically, by leveraging ideas from compositional data analysis in statistics, we frame the problem as a regression task, where loss reweighting and label correction can naturally be achieved with a shifted Gaussian label noise model. Our unified approach achieves strong performance compared to recent baselines on several noisy labeled datasets. We believe this work is a promising step towards robust deep learning in the presence of label noise",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0xLWPdObG1": {
    "title": "Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features",
    "volume": "review",
    "abstract": "There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a sparsemax function, which allows feature selection in high-dimensional settings. To enhance practical performance and learning efficiency, we present an adjustment procedure for prediction of random parameters and a method-of-moments estimator for pretraining of variance component. Various experiential studies and real data analyses confirm the advantages of our proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0xT87opqKV": {
    "title": "ProteinAdapter: Adapting Pre-trained Large Protein Models for Efficient Protein Representation Learning",
    "volume": "review",
    "abstract": "The study of proteins is crucial in various scientific disciplines, but understanding their intricate multi-level relationships remains challenging. Recent advancements in Large Protein Models (LPMs) have demonstrated their ability in sequence and structure understanding, suggesting the potential of directly using them for efficient protein representation learning. In this work, we introduce ProteinAdapter, to efficiently transfer the general reference from the multiple Large Protein Models (LPMs), e.g., ESM-1b, to the task-specific knowledge. ProteinAdapter could largely save labor-intensive analysis on the 3D position and the amino acid order. We observe that such a simple yet effective approach works well on multiple downstream tasks. Specifically, (1) with limited extra parameters, ProteinAdapter enables multi-level protein representation learning by integrating both sequence and geometric structure embeddings from LPMs. (2) Based on the learned embedding, we further scale the proposed ProteinAdapter to multiple conventional protein tasks. Considering different task priors, we propose a unified multi-scale predictor to fully take advantage of the learned embeddings via task-specific focus. Extensive experiments on over 20 tasks show that ProteinAdapter outperforms state-of-the-art methods under both single-task and multi-task settings. We hope that the proposed method could accelerate the study of protein analysis in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3SJE1WLB4M": {
    "title": "Generalization error of spectral algorithms",
    "volume": "review",
    "abstract": "The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of \\emph{spectral algorithms} specified by profile $h(\\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, giving a new perspective on the KRR saturation phenomenon (iii) conjecture, and demonstrate for the considered data models, the universality of the loss w.r.t. non-spectral details of the problem, but only in case of noisy observation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ali45HfJqJ": {
    "title": "Observer Uncertainty of Learning in Games from a Covariance Perspective",
    "volume": "review",
    "abstract": "We investigate the accuracy of prediction in deterministic learning dynamics of zero-sum games with random initializations, specifically focusing on observer uncertainty and its relationship to the evolution of covariances. Zero-sum games are a prominent field of interest in machine learning due to their various applications, such as Generative Adversarial Networks. Concurrently, the accuracy of observation in dynamical systems from mechanics has long been a classic subject of investigation since the discovery of the Heisenberg Uncertainty Principle. This principle employs covariance and standard deviation of particle states to measure observation accuracy. In this study, we bring these two approaches together to analyze the follow-the-regularized-leader (FTRL) algorithm in two-player zero-sum games. We provide growth rates of covariance information for continuous-time FTRL, as well as its two canonical discretization methods (Euler and symplectic). Our analysis and experiments shows that employing symplectic discretization enhances the accuracy of prediction in learning dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=cbVnJa4l2o": {
    "title": "LLM+A: Grounding Large Language Models in Physical World with Affordance Prompting",
    "volume": "review",
    "abstract": "While large language models (LLMs) are successful in completing various language processing tasks, they easily fail to interact with the physical world properly such as generating control sequences. We find that the main reason is that LLMs are not grounded in the physical world. Existing LLM-based approaches circumvent this problem by relying on additional pre-defined skills or pre-trained sub-policies, making it hard to adapt to new tasks. In contrast, we aim to address this problem and explore the possibility to prompt pre-trained LLMs to accomplish a series of robotic manipulation tasks in a training-free paradigm. Accordingly, we propose a framework called LLM+A(ffordance), where the LLM serves as both the sub-task planner (that generates high-level plans) and the motion controller (that generates low-level control sequences). To ground these plans and control sequences on the physical world, we develop the \\textit{affordance prompting} technique that stimulates the LLM to 1) predict the consequences of generated plans and 2) generate affordance values for relevant objects. Empirically, we evaluate the effectiveness of LLM+A in various robotic manipulation tasks with natural language instructions and demonstrate that our approach substantially improves the performance by enhancing the feasibility of generated plans and control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WBCPdhQPuz": {
    "title": "DAS$^2$C: A Distributed Adaptive Minimax Method with Near-Optimal Convergence",
    "volume": "review",
    "abstract": "Applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose DAS$^2$C, a $\\underline{\\text{D}}$istributed $\\underline{\\text{A}}$daptive method with time-scale \\$\\underline{\\text{S}}$eparated $\\underline{\\text{S}}$tepsize $\\underline{\\text{C}}$ontrol for minimax optimization. The key strategy is to employ an adaptive stepsize control protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state errors due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For non-convex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation and quasi-independence of networks, leading to a near-optimal convergence rate of $\\tilde{\\mathcal{O}} \\left( \\epsilon ^{-\\left( 4+\\delta \\right)} \\right)$ for any small $\\delta > 0$, matching that of the centralized counterpart. To the best of our knowledge, DAS$^2$C is the $\\textit{first}$ distributed adaptive method guaranteeing exact convergence without requiring to know any problem-dependent parameters for nonconvex minimax problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=dCHbFDsCZz": {
    "title": "Learning to Reject with a Fixed Predictor: Application to Decontextualization",
    "volume": "review",
    "abstract": "We study the problem of classification with a reject option for a fixed predictor, crucial to natural language processing. We introduce a new problem formulation for this scenario, and an algorithm minimizing a new surrogate loss function. We provide a complete theoretical analysis of the surrogate loss function with a strong $H$-consistency guarantee. For evaluation, we choose the \\textit{decontextualization} task, and provide a manually-labelled dataset of $2\\mathord,000$ examples. Our algorithm significantly outperforms the baselines considered, with a $\\sim 25$% improvement in coverage when halving the error rate, which is only $\\sim 3$% away from the theoretical limit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JL42j1BL5h": {
    "title": "All Languages Matter: On the Multilingual Safety of Large Language Models",
    "volume": "review",
    "abstract": "Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages. In addition, we propose several simple and effective prompting methods to improve the multilingual safety of ChatGPT by evoking safety knowledge and improving cross-lingual generalization of safety alignment. Our prompting method can significantly reduce the ratio of unsafe responses from 19.1\\% to 9.7\\% for non-English queries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oDYXpvnv5f": {
    "title": "Deep Anti-Regularized Ensembles",
    "volume": "review",
    "abstract": "We consider the problem of uncertainty quantification in high dimensional regression and classification, for which deep ensembles have proven to be promising methods. Recent observations have shown that deep ensembles often return overconfident estimates outside the training domain, which is a major limitation because shifted distributions are often encountered in real-life scenarios. The principal challenge for this problem is to solve the trade-off between increasing the diversity of the ensemble outputs and making accurate in-distribution predictions. In this work, we show that an ensemble of networks with large weights fitting the training data are likely to meet these two objectives. We derive a simple and practical approach to produce such ensembles, based on an original anti-regularization term penalizing small weights and a control process of the weight increase which maintains the in-distribution loss under an acceptable threshold. The developed approach does not require any out-of-distribution training data neither any trade-off hyper-parameter calibration. We derive a theoretical framework for this approach and show that the proposed optimization can be seen as a \"water-filling\" problem. Several experiments in both regression and classification settings highlight that Deep Anti-Regularized Ensembles (DARE) significantly improve uncertainty quantification outside the training domain in comparison to recent deep ensembles and out-of-distribution detection methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RGE8Bs5Tra": {
    "title": "CLASS-INCREMENTAL LEARNING USING GENERATIVE EXPERIENCE REPLAY BASED ON TIME-AWARE REGULARIZATION",
    "volume": "review",
    "abstract": "Learning new tasks accumulatively without forgetting remains a critical challenge in continual learning. Generative experience replay addresses this challenge by synthesizing pseudo-data points for past learned tasks and later replaying them for concurrent training along with the new tasks' data. Generative replay is the best strategy for continual learning under a strict class-incremental setting when certain constraints need to be met: (i) constant model size, (ii) no pre-training dataset, and (iii) no memory buffer for storing past tasks data. Inspired by the biological nervous system mechanisms, we introduce a time-aware regularization method to dynamically fine-tune the three training objective terms used for generative replay: supervised learning, latent regularization, and data reconstruction. Experimental results on major benchmarks indicate that our method pushes the limit of a brain-inspired continual learner under such strict settings, improves memory retention, and increases the average performance over continually arriving tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=y3CsNQal2l": {
    "title": "Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging",
    "volume": "review",
    "abstract": "As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by aligning representations across languages or by explicitly translating target languages into source languages. However, these methods possess certain limitations and fail to fully exploit the potential of Large Language Models (LLMs). In this paper, we regard the ability of LLMs in a particular task and language as a combination of \"task ability\" and \"language ability\". In the context of parameter-efficient fine-tuning and cross-lingual transfer, task ability is represented by adapters fine-tuning on the target task in the source language, while language ability is the ability to solve problems using the specific target language. In this work, we propose a novel adaptive adapter merging method for cross-lingual transfer, termed as $\\texttt{AdaMergeX}$. As language ability is not tied to any specific task, we introduce another easily accessible reference task from which language ability is obtained by adapter merging. Then by further merging it with adapters tuned on the target task in the source language, we can achieve effective cross-lingual transfer. Furthermore, unlike existing model merging methods that employ arithmetic addition, we propose a new structured-adaptive merging method that adapts the merging process based on the structure of adapters. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=NF5uhYkI9C": {
    "title": "Thin-Thick Adapter: Segmenting Thin Scans Using Thick Annotations",
    "volume": "review",
    "abstract": "Medical imaging segmentation has been a prominent focus in the field of medical imaging analysis. Recent advances in radiological and storage technologies have led to an increased utilization of thin slice computed tomography (CT) acquisitions in clinical practice. These thin slices offer several advantages, including enhanced spatial resolution and sharper diagnostic information for clinicians. However, segmenting thin slices presents significant challenges. Annotations on thick is hard to adapt to the thin slices since there is a domain gap between thick and thin slices. Furthermore, there is no existing dataset which contains pixel-level thin annotations, and manually annotating thin slices is considerably more resource-intensive and time-consuming compared to annotating thick slices, making it impractical to obtain a sufficient quantity of high-quality thin annotations for training robust models in a supervised fashion. In response to these challenges, this paper introduces three key contributions. Firstly, we propose a research topic and setting focused on segmenting thin slice data exclusively, leveraging existing annotations from thick slices. Secondly, we present a newly created dataset called CQ500-Thin, which is a Non-Contrast CT scans featuring Intracranial Hemorrhage (ICH), including a subset of pixel-level thin annotations for evaluation purposes. This dataset serves as a benchmark for our proposed topic and methodology. Lastly, we introduce a robust pipeline named the Thin-Thick Adapter, which utilizes a simple-but-effective data alignment technique and a 3D-CPS for unsupervised domain adaptation. It is designed to address the thin slice segmentation problem and establish a foundational baseline for this emerging research area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ9lqUalq1": {
    "title": "$\\gamma$-Orthogonalized Tensor Deflation: Towards Robust \\& Interpretable Tensor Decomposition in the Presence of Correlated Components",
    "volume": "review",
    "abstract": "We tackle the problem of recovering a low-rank tensor signal with possibly correlated components from a random noisy tensor, or the so-called \\textit{spiked tensor model}. When the underlying components are orthogonal, they can be recovered efficiently using \\textit{tensor deflation}, while correlated components may alter the tensor deflation mechanism, thereby preventing efficient recovery. Relying on recently developed tools from random tensor theory, we deal precisely with the non-orthogonal case by deriving an asymptotic analysis of a \\textit{parameterized} deflation procedure, which we refer to as $\\gamma$-orthogonalized tensor deflation. Based on this analysis, an efficient tensor deflation algorithm is proposed by optimizing the parameter injected into the deflation mechanism, which in turn is proven to be optimal by construction for the studied tensor model. We perform a detailed theoretical and algorithmic analysis on the rank-2 order-3 model, and outline a general structure to tackle the problem in more generality for arbitrary ranks/orders, aiming to lead to a broader impact in machine learning and beyond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jZPqf2G9Sw": {
    "title": "Dynamics-Informed Protein Design with Structure Conditioning",
    "volume": "review",
    "abstract": "Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein's dynamical properties for its function, conditioning on dynamical properties remains elusive. We present a new approach to protein generative modeling by leveraging Normal Mode Analysis that enables us to capture dynamical properties too. We introduce a method for conditioning the diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to the classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding SDE theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of the dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the open-source unconditional protein diffusion model Genie into the conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible. Our work represents a first step towards incorporating dynamical behaviour in protein design and may open the door to designing more flexible and functional proteins in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6UQaXJm53B": {
    "title": "DfPO: Degeneration-free Policy Optimization via Action Masking in Natural Language Action Spaces",
    "volume": "review",
    "abstract": "As the pre-training objectives (e.g., next token prediction) of language models (LMs) are inherently not aligned with task scores, optimizing LMs to achieve higher downstream task scores is essential. One of the promising approaches is to fine-tune LMs by using reinforcement learning (RL). However, conventional RL methods based on PPO and a penalty of KL divergence are vulnerable to the text degeneration problem which LMs do not generate natural texts anymore after RL fine-tuning. To address this problem, we provide Degeneration-free Policy Optimization (DfPO) that can fine-tune LMs to generate texts that achieve improved downstream task scores, while preserving the naturalness of the generated texts. To achieve this, we introduce action-masked policy with which a behavior policy can avoid to select tokens that potentially make policy optimization unexpected. Then, we devise clipped advantage functions to separately perform likelihood maximization and minimization, conditioned on texts sampled from the action-masked policy. Our experiments on the GRUE benchmark demonstrate that DfPO successfully improves the downstream task scores, while preserving the naturalness of the generated texts. Moreover, even DfPO does not perform hyperparameter search, it outperforms PPO and NLPO which require additional hyperparameter search for the penalty ratio of KL divergence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZmbCZw81xf": {
    "title": "Syntactic Representations Enable Interpretable Hierarchical Word Vectors",
    "volume": "review",
    "abstract": "The distributed representations currently used are dense and uninterpretable, leading to interpretations that themselves are relative, overcomplete, and hard to interpret. We propose a method that transforms these word vectors into reduced syntactic representations. The resulting representations are interpretable in an absolute scale allowing better comparison and visualization of the word vectors and we successively demonstrate that the drawn interpretations are in line with human judgment. The syntactic representations are then used to create hierarchical word vectors using an incremental learning approach similar to the non-linear human learning approach. As these representations are drawn from pre-trained vectors, the generation process and learning approach are computationally efficient. Most importantly, we find out that the resulting hierarchical vectors outperform the original vectors in benchmark tests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=5451cIQdWp": {
    "title": "On Synthetic Data and Iterative Magnitude Pruning: a Linear Mode Connectivity Study",
    "volume": "review",
    "abstract": "Recent works have shown that distilled data representations can be leveraged for accelerating the training of DNNs. However, to date, very little is understood about the effect of these synthetic data representations in the area of architectural optimization, specifically with Iterative Magnitude Pruning (IMP) and pruning at initialization. We push the boundaries of pruning with distilled data, matching the performance of traditional IMP on ResNet-18 \\& CIFAR-10 while using 150x less training points to find a sparsity mask. We find that distilled data guides IMP to discard parameters contributing to the sharpness of the loss landscape, fostering smoother landscapes. These synthetic subnetworks are stable to SGD noise at initialization in settings when the dense model or subnetworks found with standard IMP are not, such as ResNet-10 on ImageNet-10. In other words, training from initialization across different shuffling of data will result in linear mode connectivity, a phenomenon which rarely happens without some pretraining. We visualize these loss landscapes and quantitatively measure sharpness through hessian approximations to understand these effects. This behavior is heavily linked to the compressed representation of the data, highlighting the importance of synthetic data in neural architectural validation. In order to find both a high performing and robust sparse architecture, a more optimal synthetic data representation is needed that can compress irrelevant noise like distilled data, yet better maintain task-specific information from the real data as dataset complexity increases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=e5hZmQXHHg": {
    "title": "VRAda: A Variance Reduced Adaptive Algorithm for Stochastic Parameter-Agnostic Minimax Optimizations",
    "volume": "review",
    "abstract": "Stochastic parameter-agnostic minimax optimization provides a novel avenue for adjusting learning rates without relying on problem-dependent parameters, bridging the gap between theoretical and empirical machine learning results. While previous studies have successfully decoupled the timescales of primal and dual variables and proposed unified parameter-agnostic algorithms for minimax optimizations, the problem of varying inherent variances within the stochastic setting persists. Such variance degradation affects the desired ratio of learning rates. Intuitively, variance-reduced techniques hold the potential to address this issue efficiently. However, they require manually tuning problem-dependent parameters to attain an optimal solution. In this paper, we introduce the Variance-Reduced Adaptive algorithm (VRAda), a solution addressing varying inherent variances and enabling the parameter-agnostic manner in stochastic minimax optimizations. Theoretical results show that VRAda achieves an optimal sample complexity of $O(1/\\epsilon^3)$ without large data batches, enabling it to find an $\\epsilon$-stationary point on non-convex-strongly-concave and non-convex-Polyak-\\L ojasiewicz objectives. To the best of our knowledge, VRAda is the first variance-reduced adaptive algorithm designed specifically for parameter-agnostic minimax optimization. Extensive experiments conducted across diverse applications validate the effectiveness of VRAda",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=774elYc5tw": {
    "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=tEgrUrUuwA": {
    "title": "Partitioning Message Passing for Graph Fraud Detection",
    "volume": "review",
    "abstract": "Label imbalance and homophily-heterophily mixture are the fundamental problems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud Detection (GFD) tasks. Existing GNN-based GFD models are designed to augment graph structure to accommodate the inductive bias of GNNs towards homophily, by excluding heterophilic neighbors during message passing. In our work, we argue that the key to applying GNNs for GFD is not to exclude but to {\\em distinguish} neighbors with different labels. Grounded in this perspective, we introduce Partitioning Message Passing (PMP), an intuitive yet effective message passing paradigm expressly crafted for GFD. Specifically, in the neighbor aggregation stage of PMP, neighbors with different classes are aggregated with distinct node-specific aggregation functions. By this means, the center node can adaptively adjust the information aggregated from its heterophilic and homophilic neighbors, thus avoiding the model gradient being dominated by benign nodes which occupy the majority of the population. We theoretically establish a connection between the spatial formulation of PMP and spectral analysis to characterize that PMP operates an adaptive node-specific spectral graph filter, which demonstrates the capability of PMP to handle heterophily-homophily mixed graphs. Extensive experimental results show that PMP can significantly boost the performance on GFD tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=EmQSOi1X2f": {
    "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
    "volume": "review",
    "abstract": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our analysis reveals the prevalence of self-contradictions when LMs generate text for open-domain topics, e.g., in 17.7% of all sentences produced by ChatGPT. Self-contradiction also complements retrieval-based methods, as a large portion of them (e.g., 35.8% for ChatGPT) cannot be verified using Wikipedia. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require external grounded knowledge. Our approach is practically effective and has been released as a push-button tool to benefit the public, with an anonymized version at https://iclr9113.com/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YkR9UFlQ1s": {
    "title": "Non-backtracking Graph Neural Networks",
    "volume": "review",
    "abstract": "The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-GNN on long-range graph benchmark and transductive node classification problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=otoggKnn0A": {
    "title": "FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes",
    "volume": "review",
    "abstract": "A typical task in the field of video understanding is hand action recognition, which has a wide range of applications. Existing works either mainly focus on full-body actions, or the defined action categories are relatively coarse-grained. In this paper, we propose FHA-Kitchens, a novel dataset of fine-grained hand actions in kitchen scenes. In particular, we focus on human hand interaction regions and perform deep excavation to further refine hand action information and interaction regions. Our FHA-Kitchens dataset consists of 2,377 video clips and 30,047 images collected from 8 different types of dishes, and all hand interaction regions in each image are labeled with high-quality fine-grained action classes and bounding boxes. We represent the action information in each hand interaction region as a triplet, resulting in a total of 878 action triplets. Based on the constructed dataset, we benchmark representative action recognition and detection models on the following three tracks: (1) supervised learning for hand interaction region and object detection, (2) supervised learning for fine-grained hand action recognition, and (3) intra- and inter-class domain generalization for hand interaction region detection. The experimental results offer compelling empirical evidence that highlights the challenges inherent in fine-grained hand action recognition, while also shedding light on potential avenues for future research, particularly in relation to pre-training strategy, model design, and domain generalization. The dataset will be released on the FHA-Kitchens project website",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4fVuBf5HE9": {
    "title": "Towards Analyzing Self-attention via Linear Neural Network",
    "volume": "review",
    "abstract": "Self-attention is a key component of the transformer architecture which has driven much of recent advances in AI. Theoretical analysis of self-attention has received significant attention and remains a work in progress. In this paper, we analyze gradient flow training of a simplified transformer model consisting of a single linear self-attention layer (thus it lacks softmax, MLP, and layer-normalization) with a single head on a histogram-like problem: the input is a sequence of characters from an alphabet and the output is the vector of counts of each letter in the input sequence. Our analysis goes via a reduction to 2-layer linear neural networks in which the input layer matrix is a diagonal matrix. We provide a complete analysis of gradient flow on these networks. Our reduction to linear neural networks involves one assumption which we empirically verify. Our analysis extends to various extensions of the histogram problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=lhZEodF8Dn": {
    "title": "Efficient Denoising Diffusion via Probabilistic Masking",
    "volume": "review",
    "abstract": "Diffusion models have exhibited remarkable advancements in generating high-quality data. However, a critical drawback of these models is their computationally intensive inference process, which requires a large number of timesteps to generate a single sample. Existing methods address this challenge by decoupling the forward and reverse processes, and they rely on handcrafted rules (e.g., uniform skipping) for sampling acceleration, leading to the risk of discarding important steps and deviating from the optimal trajectory. In this paper, we propose an Efficient Denoising Diffusion method via Probabilistic Masking (EDDPM) that can identify and skip the redundant steps during training. To determine whether a timestep should be skipped or not, we employ probabilistic reparameterization to continualize the binary determination mask. The mask distribution parameters are learned jointly with the diffusion model weights. By incorporating a real-time sparse constraint, our method can effectively identify and eliminate unnecessary steps during the training iterations, thereby improving inference efficiency. Notably, as the model becomes fully trained, the random masks converge to a sparse and deterministic one, retaining only a small number of essential steps. Empirical results demonstrate the superiority of our proposed EDDPM over the state-of-the-art sampling acceleration methods across various domains. EDDPM can generate high-quality samples with only 20\\% of the steps for time series imputation and achieve 4.89 FID with 5 steps for CIFAR-10. Moreover, when starting from a pretrained model, our method efficiently identifies the most informative timesteps within a single epoch, which demonstrates the potential of EDDPM to be a practical tool to explore large diffusion models with limited resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=dAqH7CfHjL": {
    "title": "Phase Transitions in Contrastive Learning",
    "volume": "review",
    "abstract": "How do self-supervised models actually train? We study the training dynamics of contrastive learning in three settings: a theoretical linear setting, on a low-dimensional physics-inspired dataset, and on full-fledged computer vision datasets including ImageNet. In all three settings, we show the existence of *phases*, i.e. locally stable or metastable representations, and of *phase transitions*, wherein a model rapidly and unexpectedly switches between different phases. Geometrically motivated metrics are developed to measure phase transitions. Finally, we show that phase transitions can be sped up with more robust augmentations. Code and visualizations will be made public upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=w8BL1NShjk": {
    "title": "There is More to Graphs than Meets the Eye: Learning Universal Features with Self-supervision",
    "volume": "review",
    "abstract": "We study the problem of learning universal features from multiple graphs through self-supervision. Graph self-supervised learning has been shown to facilitate representation learning, and produce competitive models compared to supervised baselines. However, existing methods of self-supervision learn features from one graph, and thus, produce models that are specialized to a particular graph. We hypothesize that leveraging multiple graphs of a family can improve the quality of learnt representations in the model by extracting features that are universal to the family of graphs. To achieve this, we propose a framework that can learn generalisable representations from disparate node features of different graphs. We first homogenise the disparate features with graph-specific modules, which feed into a universal representation learning module for generalisable feature learning. We show that leveraging multiple graphs of the same family improves the quality of representations and results in better performance on downstream node classification task compared to self-supervision with one graph. In this paper, we present a principled way to design foundation graph models that are capable of learning from a set of graphs in a holistic manner. This approach bridges the gap between self-supervised and supervised performance, while reducing the computational time for self-supervision and parameters of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ER1VDuwWvB": {
    "title": "CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity",
    "volume": "review",
    "abstract": "With distributed machine learning being a prominent technique for large-scale machine learning tasks, communication complexity has become a major bottleneck for speeding up training and scaling up machine numbers. In this paper, we propose a new technique named Common randOm REconstruction(CORE), which can be used to compress the information transmitted between machines in order to reduce communication complexity without other strict conditions. Especially, our technique CORE projects the vector-valued information to a low-dimensional one through common random vectors and reconstructs the information with the same random noises after communication. We apply CORE to two distributed tasks, respectively convex optimization on linear models and generic non-convex optimization, and design new distributed algorithms, which achieve provably lower communication complexities. For example, we show for linear models CORE-based algorithm can encode the gradient vector to $\\mathcal{O}(1)$-bits (against $\\mathcal{O}(d)$), with the convergence rate not worse, preceding the existing results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=d6oUP1tyNx": {
    "title": "The KNN Score for Evaluating Probabilistic Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "Time series forecasting is a critical task in various domains. With the aim of comprehending interconnections and dependencies among variables, as well as gaining insights into a range of potential future outcomes, probabilistic multivariate time series forecasting has emerged as a prominent approach. The evaluation of models employed in this task is crucial yet challenging. Comparing a set of predictions against a single observed future presents difficulties, and accurately measuring whether a model correctly predicts dependencies between different time steps and individual series further compounds the complexity. We observe that metrics which are currently employed fall short in providing a comprehensive assessment of model performance. To address this limitation, we propose a novel metric based on density estimation as an alternative. We showcase the advantages of our metric both qualitatively and quantitatively, underscoring its effectiveness in assessing forecast quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1zhM0XkQh0": {
    "title": "ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations",
    "volume": "review",
    "abstract": "Supervised adversarial training has been the most successful approach for improving the robustness of Deep Neural Networks against adversarial attacks. While several recent works have attempted to overcome the need for supervision or labeled training data by integrating adversarial training with contrastive Self-Supervised Learning (SSL) approaches such as SimCLR, their performance has been sub-optimal due to the increased training complexity. A recent approach mitigates this by utilizing supervision from a standard self-supervised trained model in a teacher-student setting that mimics supervised adversarial training. However, we find that there is still a large gap in performance when compared to supervised training, specifically on larger capacity models. We show that this is a result of mismatch in training objectives of the teacher and student, and propose Projected Feature Adversarial Training (ProFeAT) to bridge this gap by using a projection head in the adversarial training step. We further propose appropriate attack and defense losses at the feature and projector spaces, coupled with a combination of weak and strong augmentations for the teacher and student respectively, to improve generalization without increasing the training complexity. We demonstrate significant improvements in performance when compared to existing SSL methods, and performance on par with TRADES, a popular supervised adversarial training method, on several benchmark datasets and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=rmLTwKGiSP": {
    "title": "Semi-Anchored Gradient Methods for Nonconvex-Nonconcave Minimax Problems",
    "volume": "review",
    "abstract": "Nonconvex-nonconcave minimax problems are difficult to optimize by gradient methods. The extragradient method, proven to outperform the gradient descent ascent, has become standard but there is still room for improvement. On the other hand, under a bilinear setting, the primal-dual hybrid gradient (PDHG) method is one of the most popular methods. This was studied on a general convex-concave problem, but it has not been found useful in a more general nonconvex-nonconcave minimax problem. In this paper, we demonstrate its natural extension to a structured nonconvex-nonconcave minimax problem, whose saddle-subdifferential operator satisfies the weak Minty variational inequality condition, showing its potential. This new nonlinear variant of PDHG, named semi-anchored (SA) gradient method, is built upon the theory of Bregman proximal point method. This consequently provides a worst-case convergence rate, in terms of a new optimality measure for nonconvex-nonconcave minimax optimization, making it interesting on its own. We further illustrate the potential of the semi-anchoring by providing a numerical experiment on fair classification problem, in comparison with the extragradient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xw3fStKCwm": {
    "title": "Tensor-Train Point Cloud Compression and Efficient Approximate Nearest Neighbor Search",
    "volume": "review",
    "abstract": "Nearest-neighbor search in large vector databases is crucial for various machine learning applications. This paper introduces a novel method using **tensor-train** (TT) low-rank tensor decomposition to efficiently represent point clouds and enable fast approximate nearest-neighbor searches. We propose a probabilistic interpretation and utilize density estimation losses like Sliced Wasserstein to train TT decompositions, resulting in robust point cloud compression. We reveals an inherent hierarchical structure within TT point clouds, facilitating efficient approximate nearest-neighbor searches. In our paper, we provide detailed insights into the methodology and conduct comprehensive comparisons with existing methods. We demonstrate its effectiveness in various scenarios, including out-of-distribution (OOD) problems and approximate nearest-neighbor (ANN) search tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=5twh6pM4SR": {
    "title": "Automating Continual Learning",
    "volume": "review",
    "abstract": "General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from the so-called catastrophic forgetting (CF) problem---previously acquired skills are forgotten when a new task is learned. Developing continual learning algorithms to address CF remains an open research question. Instead of hand-crafting such algorithms, our new Automated Continual Learning (ACL) trains self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata---good performance on both old and new tasks---into its learning objectives. We demonstrate the effectiveness and promise of ACL on multiple few-shot and standard image classification tasks adopted for continual learning: Mini-ImageNet, Omniglot, FC100, MNIST-families, and CIFAR-10",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Cw6lk56w6z": {
    "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks",
    "volume": "review",
    "abstract": "In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on $18$ specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bXI0thP733": {
    "title": "Mitigating backdoor attacks with generative modelling and dataset relabelling",
    "volume": "review",
    "abstract": "Data-poisoning attacks change a small portion of the training dataset by introducing hand-crafted triggers and rewiring the corresponding labels towards a desired target class. Training on such data injects a backdoor into the model, that causes incorrect inference in selected test examples. Existing defenses mitigate the risks of such attacks through various modifications of the standard discriminative learning procedure. This paper explores a different approach that promises clean models by means of per-class generative modelling. We start by mapping the input data into a suitable latent space by leveraging a pre-trained self-supervised feature extractor. Interestingly, these representations get either preserved or heavily disturbed under recent backdoor attacks. In both cases, we find that per-class generative models give rise to probabilistic densities that allow both to detect the poisoned data and to find their original classes. This allows to patch the poisoned dataset by reverting the original labels and considering the triggers as a kind of augmentation. Our experiments show that training on patched datasets greatly reduces attack success rate and retains the clean accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=0GZ1Bq4Tfr": {
    "title": "Layer-wise Pre-weight Decay",
    "volume": "review",
    "abstract": "In deep learning, weight decay is a regularization mechanism been widely adopted to improve the generalization performance. Previously, a common understanding of the role of weight decay was that it contributes by pushing the model weights to approach 0 at each time step. However, our findings challenge this notion and argue the objective of weight decay is to make the weights approach the negative value of the update term instead of 0, thereby indicating a delay defect in certain steps that results in opposing penalties. In addition, we study the negative side effect of weight decay, revealing it will damage the inter-layer connectivity of the network while reducing weight magnitude. To address these issues, we first propose real-time weight decay to fix the delay defect by penalizing both the weights and the gradients at each time step. Then, we advance the decay step before the update function as pre-weight decay to mitigate the performance drop raised by the side effect. To further improve the general performance and enhance model robustness towards the decay rate, we finally introduce a layer-wise pre-weight decay to adjust the decay rate based on the layer index. Extensive analytical and comparative experiments demonstrate that the proposed $\\textit{layer-wise pre-weight decay}$ (LPWD) (i) exhibits remarkable robustness to the decay rate, and (ii) significantly improves the generalization performance across various conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=QHVTxso1Is": {
    "title": "Efficient Unsupervised Knowledge Distillation with Space Similarity",
    "volume": "review",
    "abstract": "In this paper, we aim to boost performance of knowledge distillation without the ground-truth labels. Hence, a student can only rely on the response generated by its teacher. Many existing approaches under this setting rely on some form of feature/embedding queue to capture neighbourhood information. These queues can be as large as over 100k samples. Also, some of these methods rely on multitude of operations which as a result increases the memory requirement for training many folds. In this work, we show that merely working with the input batch (often of size $256$) it is possible to not only incorporate neighbourhood information but also obtain state of the art unsupervised distillation performance. We achieve this by introducing a simple space similarity loss component which works alongside the well known normalized cosine similarity computed on the final features. In this loss, we motivate each dimension of a student's feature space to be similar to the corresponding dimension of its teacher. With this seemingly simple addition, we are able to compete against many contemporary methods which either rely on large number of queued features or heavy pre-processing. We perform extensive experiments comparing our proposed approach to other state of the art methods on various computer vision tasks for established architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0cJ8ERfnrM": {
    "title": "Antibody DomainBed: Out-of-Distribution Generalization in Therapeutic Protein Design",
    "volume": "review",
    "abstract": "Recently, there has been an increased interest in accelerating drug design with machine learning (ML). Active ML-guided design of biological sequences with favorable properties involves multiple design cycles in which (1) candidate sequences are proposed, (2) a subset of the candidates is selected using ML surrogate models trained to predict target properties of interest, and (3) sequences are experimentally validated. The returned experimental results from one cycle provide valuable feedback for the next one, but the modifications they inspire in the candidate proposals or experimental protocol can lead to distribution shifts that impair the performance of surrogate models in the upcoming cycle. For the surrogate models to achieve consistent performance across cycles, we must explicitly account for the distribution shifts in their training. We apply domain generalization (DG) methods to develop robust classifiers for predicting properties of therapeutic antibodies. We adapt a recent benchmark of DG algorithms, ``DomainBed,'' to deploy DG algorithms across 5 domains, or design cycles. Our results suggest that foundational models and ensembling (in both output and weight space) lead to better predictive performance on out-of-distribution domains. We publicly release our codebase and the associated dataset of antibody-antigen binding that emulates distribution shifts across design cycles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Ffjc8ApSbt": {
    "title": "Adaptive Causal Balancing for Collaborative Filtering",
    "volume": "review",
    "abstract": "Collaborative filtering builds personalized models from the collected user feedback. However, the collected data is observational rather than experimental, leading to various biases in the data, which can significantly affect the learned model. To address this issue, many studies have focused on propensity-based methods to combat the selection bias by reweighting the sample loss, and demonstrate that balancing is important for debiasing both theoretically and empirically. However, there are two questions that still need to be addressed: which function class should be balanced and how to effectively balance that function class? In this paper, we first perform theoretical analysis to show the effect of balancing finite-dimensional function classes on the bias of IPS and DR methods, and based on this, we propose a universal kernel-based balancing method to balance functions on the reproducing kernel Hilbert space. In addition, we propose a novel adaptive causal balancing method during the alternating update between unbiased evaluation and training of the prediction model. Specifically, the prediction loss of the model is projected in the kernel-based covariate function space, and the projection coefficients are used to determine which functions should be prioritized for balancing to reduce the estimation bias. We conduct extensive experiments on three real-world datasets to demonstrate the effectiveness of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=v5lmhckxlu": {
    "title": "Integrated Model Explanations by Independent and Collaborative Feature Influence via Linear-Nonlinear Perspectives",
    "volume": "review",
    "abstract": "In machine learning, model-agnostic explanation methods try to give explanation to model prediction by assessing the importance of input features. While linear simplification methods guarantee good properties, they have to include nonlinear feature interactions into linear coefficients. On the other hand, feature influence analysis methods examine feature relevance, but do not consistently preserve the desirable properties for robust explanations. Our approach seeks to inherit properties from linear simplification methods while systematically capturing feature interactions. To achieve this, we consider the explained model from two aspects: the linear aspect, which focuses on the independent influence of features to model predictions, and the nonlinear aspect, which concentrates on modeling feature interactions and their collaborative impact on model predictions. In practice, our method initially investigates both the linear and nonlinear aspects of the model being explained. It then extracts the independent and collaborative importance of features on model predictions and consistently combines them to ensure that the resulting feature importance preserves the desirable properties for robust explanations. Consequently, our Linear-Nonlinear Explanation (LNE) method provides a comprehensive understanding on how features influence model predictions. To validate its effectiveness, experiments demonstrate that linear, nonlinear, and the combined feature importance all offer valuable insights for explaining model predictions. We also compare the performance of LNE with other methods on explaining well-trained classifiers, and find our explanations align more closely with human intuitions. Additionally, user study shows our method can hint humans with potential biases in classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=eQcVfCK5cO": {
    "title": "Where is the Invisible: Spatial-Temporal Reasoning with Object Permanence",
    "volume": "review",
    "abstract": "Object permanence is a cognitive ability that enables humans to reason about the existence and location of objects that are not visible in the scene, such as those occluded or contained by other objects. This ability is crucial for visual object tracking, which aims to identify and localize the target object across video frames. However, most existing tracking methods rely on deep learning models that learn discriminative visual features from the visual context and fail to handle the cases where the object disappears from the image, e.g., occluded or contained by other objects. In this paper, we propose a novel framework for tracking invisible objects based on Qualitative-Quantitative Spatial-Temporal Reasoning (QQ-STR), inspired by the concept of object permanence. Our framework consists of three modules: a visual perception module, a qualitative spatial relation reasoner (SRR), and a quantitative relation-conditioned spatial-temporal relation analyst (SRA). The SRR module infers the qualitative relationship between each object and the target object based on the current and historical observations, while the SRA module predicts the quantitative location of the target object based on the inferred relationship and a diffusion model that captures the object's motion. We devise a self-supervised learning mechanism that does not require explicit relation annotations and leverages the predicted trajectories to locate the invisible object in videos. We evaluate our framework on a synthetic dataset (LA-CATER) and a new real-world RGB-D video dataset for invisible object tracking (iVOT) that contains challenging scenarios of human-object interactions with frequent occlusion and containment events. Our framework achieves comparable performance to state-of-the-art tracking methods that use additional relation annotations, demonstrating its generalization ability to novel scenes and viewpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ABIcBDLBVG": {
    "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
    "volume": "review",
    "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=AyXIDfvYg8": {
    "title": "Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching",
    "volume": "review",
    "abstract": "Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy loss for classification. While prior studies have demonstrated that both losses yield symmetric training representations under balanced data, this symmetry breaks under class imbalances. This paper presents an intriguing discovery: the introduction of a ReLU activation at the final layer effectively restores the symmetry in SCL-learned representations. We arrive at this finding analytically, by establishing that the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints form an orthogonal frame. Extensive experiments conducted across various datasets, architectures, and imbalance scenarios corroborate our finding. Importantly, our experiments reveal that the inclusion of the ReLU activation restores symmetry without compromising test accuracy. This constitutes the first geometry characterization of SCL under imbalances. Additionally, our analysis and experiments underscore the pivotal role of batch selection strategies in representation geometry. By proving necessary and sufficient conditions for mini-batch choices that ensure invariant symmetric representations, we introduce batch-binding as an efficient strategy that guarantees these conditions hold",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ADDCErFzev": {
    "title": "Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems",
    "volume": "review",
    "abstract": "According to the efficient coding hypothesis, neural populations encode information optimally when representations are high-dimensional and uncorrelated. However, such codes may carry a cost in terms of generalization and robustness. Past empirical studies of early visual cortex (V1) in rodents have suggested that this tradeoff indeed constrains sensory representations. However, it remains unclear whether these insights generalize across the hierarchy of the human visual system, and particularly to object representations in high-level occipitotemporal cortex (OTC). To gain new empirical clarity, here we develop a family of object recognition models with parametrically varying dropout proportion $p$, which induces systematically varying dimensionality of internal responses (while controlling all other inductive biases). We find that increasing dropout produces an increasingly smooth, low-dimensional representational space. Optimal robustness to lesioning is observed at around 70% dropout, after which both accuracy and robustness decline. Representational comparison to large-scale 7T fMRI data from occipitotemporal cortex in the Natural Scenes Dataset reveals that this optimal degree of dropout is also associated with maximal emergent neural predictivity. Finally, using new techniques for achieving denoised estimates of the eigenspectrum of human fMRI responses, we compare the rate of eigenspectrum decay between model and brain feature spaces. We observe that the match between model and brain representations is associated with a common balance between efficiency and robustness in the representational space. These results suggest that varying dropout may reveal an optimal point of balance between the efficiency of high-dimensional codes and the robustness of low dimensional codes in hierarchical vision systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tMKz4IgSZQ": {
    "title": "Controllable Text-to-Image Generation with Automatic Sketches",
    "volume": "review",
    "abstract": "Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-the-art on spatial arrangement and object positioning generation. It enhances users' control of object positions, sizes, etc., nearly doubling the accuracy of prior models. As a first attempt, our work shows the potential for employing LLMs to enhance performance in computer vision tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=p14iRzavpt": {
    "title": "Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher",
    "volume": "review",
    "abstract": "Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, knowledge distillation employs teacher-forcing learning, where the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this \"distribution closeness\" and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on five datasets demonstrate PTLoss can significantly improve the distillation effectiveness for teachers of various scales",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=hIpUwg8kAU": {
    "title": "Estimation error of gradient descent in deep regressions",
    "volume": "review",
    "abstract": "To achieve a theoretical understanding of deep learning, it is necessary to consider the approximation, generalization, and optimization errors. In recent years, there have been significant advancements in the literature regarding each or two of these errors. However, there have been few works that simultaneously analyze all three errors. This is due to the gap that exists between the optimization and generalization errors in over-parameterized regimes. In this work, we attempt to bridge this gap by establishing consistency between the outputs of gradient descent and the true regression function in the over-parameterized scenario. Our research offers a feasible perspective for a more comprehensive understanding of the theory behind deep learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=oVVLBxVmbZ": {
    "title": "Fast Conditional Intervention in Algorithmic Recourse with Reinforcement Learning",
    "volume": "review",
    "abstract": "Explaining the decisions made by machine learning classifiers aids individuals in identifying critical factors and charting future plans. Recent studies have shown that incorporating causal graphs of input features provides more realistic explanations; however, this also introduces new challenges such as handling noisy graphs and efficiently performing inference with black-box classifiers. In this work, we tackle these issues by presenting an efficient reinforcement learning (RL)-based approach with an idea of conditional intervention. Our intervention method is theoretically preferable and considers both feature dependencies and incompleteness of graphs. Simultaneously, the RL-based method offers the capacity to learn the intervention process while guarantees computational complexity at inference stage. In the experiments, we showcase the efficiency and superior performance of our solution when compared to baseline methods on both synthetic and real datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=327tbF3S65": {
    "title": "Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations",
    "volume": "review",
    "abstract": "Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel conditioning mechanism for evaluating INRs with the generated hierarchically decomposed basis fields to further enhance expressive power. Extensive experiments across four modalities, \\eg, 2D images, 3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets, demonstrate the versatility of DDMI and its superior performance compared to the existing INR generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9jmUwjZi7j": {
    "title": "DreamFuser: Value-guided Diffusion Policy for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "Recent advances in reinforcement learning have underscored the potential of diffusion models, particularly in the context of policy learning. While earlier applications were predominantly focused on single-timestep settings, trajectory-based diffusion policy learning promises significant superiority, especially for low-level control tasks. In this context, we introduce DreamFuser, a trajectory-based value optimization approach that seamlessly blends the merits of diffusion-based trajectory learning and efficient Q function learning over state and noisy action. To address the computational challenges associated with action sampling of diffusion policy during the training phase, we design the DreamFuser based on the Generalized Noisy Action Markov Decision Process (GNMDP), which views the diffusion denoising process as part of the MDP transition. Empirical tests reveal DreamFuser's advantages over existing diffusion policy algorithms, notably in low-level control tasks. When benchmarked against the standard benchmark of offline reinforcement learning D4RL, DreamFuser matches or even outperforms contemporary methods. This work also elucidates the parallels between the optimization process of DreamFuser over GNMDP and Diffusion Policy over MDP, demonstrating its computational and memory advantages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=H7R0z6V9fR": {
    "title": "TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems",
    "volume": "review",
    "abstract": "Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall re- main invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this pa- per, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and backward trajectories predicted by a contin- uous graph neural network-based ordinary differential equation (GraphODE). It effectively imposes time-reversal symmetry to enable more accurate model pre- dictions across a wider range of dynamical systems under classical mechanics. In addition, we further provide theoretical analysis to show that our regularization essentially minimizes higher-order Taylor expansion terms during the ODE inte- gration steps, which enables our model to be more noise-tolerant and even applica- ble to irreversible systems. Experimental results on a variety of physical systems demonstrate the effectiveness of our proposed method. Particularly, it achieves an MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0upMDCx8AA": {
    "title": "Post-Training Recovery from Injected Bias with Self-Influence",
    "volume": "review",
    "abstract": "Learning generalized models from biased data with strong spurious correlations to the class label is an important undertaking toward fairness in deep learning. In the absence of any prior knowledge or supervision of bias, recent studies tackle the problem by presuming the bias severity to be sufficiently high and employing a bias-amplified model trained by empirical risk minimization (ERM) to identify and utilize bias-conflicting samples that are free of spurious correlations. However, insufficient preciseness in detecting bias-conflicting samples results in injecting erroneous signals during training; conversely, it leads to learning malignant biases instead of excluding them. In practice, as the presumption about the magnitude of bias often does not hold, it is important for the model to demonstrate robust performance across a wide spectrum of biases. In this paper, we propose SePT (Self-influence-based Post-Training), a fine-tuning framework leveraging the self-influence score to filter bias-conflicting samples, which yields a pivotal subset with significantly diminished spurious correlations. Our method enables the quick recovery of a biased model from learned bias through fine-tuning with minimal friction. In addition, SePT also utilizes the remaining training dataset to adjust the model, thereby maintaining robust performance in situations with weak spurious correlation or even in the absence of it. Experiments on diverse benchmark datasets with a wide range of bias strengths show that SePT is capable of boosting the performance of both bias-injected and state-of-the-art debiased models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=djcciHhCrt": {
    "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always ($\\sim$98\\%) while maintaining high similarity to clean images ($\\sim$0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=bZh06ptG9r": {
    "title": "FedLoRA: When Personalized Federated Learning Meets Low-Rank Adaptation",
    "volume": "review",
    "abstract": "In this research paper, we introduce a novel approach to Personalized Federated Learning (PFL), which we call FedLoRA. This approach is inspired by recent advancements in fine-tuning Large Language Models (LLMs), particularly the Low-Rank Adaptation (LoRA) technique. The remarkable success of LoRA demonstrates that general linguistic knowledge is preserved in a pre-trained full-rank model, while domain-specific knowledge can be effectively retained within a low-rank parameter matrix. Building upon this insight, we present FedLoRA in the context of PFL, aiming to maintain shared general knowledge among all clients in a common full-rank matrix, while capturing client-specific knowledge within a personalized low-rank matrix. However, the integration of LoRA into PFL presents its own set of challenges. Unlike LoRA, which starts with pre-trained general knowledge, FedLoRA's full-rank matrix needs training from scratch. This phase can be notably influenced by data heterogeneity, potentially hindering its effective extraction of general knowledge. To address this challenge, we propose a new training strategy to mitigate the effects of data heterogeneity on the shared full-rank matrix. Our experimental results, obtained across multiple datasets exhibiting varying degrees of data heterogeneity, demonstrate that FedLoRA outperforms current state-of-the-art methods significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=MZs2dgOudB": {
    "title": "Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling",
    "volume": "review",
    "abstract": "Most meta-learning methods assume that the (very small) context set used to establish a new task at test time is passively provided. In some settings, however, it is feasible to actively select which points to label; the potential gain from a careful choice is substantial, but the setting requires major differences from typical active learning setups. We clarify the ways in which active meta-learning can be used to label a context set, depending on which parts of the meta-learning process use active learning. Within this framework, we propose a natural algorithm based on fitting Gaussian mixtures for selecting which points to label; though simple, the algorithm also has theoretical motivation. The proposed algorithm outperforms state-of-the-art active learning methods when used with various meta-learning algorithms across several benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1tDoI2WBGE": {
    "title": "A Neural Sandbox Framework for Discovering Spurious Concpets in LLM Decisions",
    "volume": "review",
    "abstract": "We introduce a neural sandbox framework for text classification via self-referencing defined label concepts from an Large Language Model(LLM). The framework draws inspiration from the define-optimize alignment problem, in which the motivations of a model are described initially and then the model is optimized to align with these predefined objectives. In our case, we design our framework to perform text classification. We take a frozen LLM as a vector embedding generator for text and provide our framework with defined concept words based on the labels along with the input text. We then optimize an operator to classify the input text based on the relevance scores to the concept operator words(cop-words). In our experiments with multiple text classification datasets and LLM models, we find, incorporating our sandbox network generally improves the accuracy by a range of 0.12\\% to 6.31\\% in accuracy and 0.3\\% to 8.82\\% in macro f1 when compared to a baseline. The framework, not only serves as a classification tool but also as a descriptive tool for the model's decision of its prediction, based on the provided cop-words. Through further evaluations involving the injection of \"foreign\" cop-words, we showcase the sandbox framework's capacity to exhibit a coherent understanding of learned concepts and construct methodologies to discover potential spurious behaviors and biases within it. Despite witnessing results confirming our network's ability to capture domain knowledge, we show evidence that the model's secondary incentives do not match human decisions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQZuCuFeAM": {
    "title": "From Random to Relevant: Harnessing Salient Masks in Non-IID Federated Learning",
    "volume": "review",
    "abstract": "Federated learning (FL) offers the ability to train models using decentralized data at client sites, ensuring data privacy by eliminating the need for data centralization. A predominant challenge with FL is the constrained computation and narrow communication bandwidth, particularly evident in resource-restricted edge client nodes. Various solutions, such as transmitting sparse models and iterative pruning have been suggested to tackle this. However, many existing methods necessitate the transmission of full model weights throughout the training, rely heavily on arbitrary or random pruning criteria or costly iterative pruning schedules. In this work, we propose SSFL, a streamlined approach for sparse decentralized FL training and communication. SSFL identifies a subnetwork prior to training, leveraging parameter saliency scores keeping in mind the distribution of local client data in non-IID scenarios. Distinctively, only the sparse model weights are communicated in each round between client models in a decentralized manner, sidestepping the conventional need of transferring the complete dense model at any phase of training. We validate SSFL's effectiveness using standard non-IID benchmarks, noting marked improvements in the sparsity-accuracy trade-offs. Finally, we deploy our method in a real-world federated learning framework and report improvement in communication time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ELlBpc0tfb": {
    "title": "MedJourney: Counterfactual Medical Image Generation by Instruction-Learning from Multimodal Patient Journeys",
    "volume": "review",
    "abstract": "Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such counterfactual generation methods can help differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual medical image generation is largely underexplored. In this paper, we present MedJourney, a novel method for counterfactual medical image generation by instruction-learning from multimodal patient journeys. Given a patient with two medical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion model for counterfactual medical image generation. Given the relative scarcity of image time series data, we introduce a two-stage curriculum that first pretrains the denoising network using the much more abundant single image-report pairs (with dummy prior image), and then continues training using the counterfactual triples. Experiments using the standard MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive battery of tests on counterfactual medical image generation, MedJourney substantially outperforms prior state-of-the-art methods in instruction image editing and medical image generation such as InstructPix2Pix and RoentGen. To facilitate future study in counterfactual medical generation, we plan to release our instruction-learning code and pretrained models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=uz7d2N2zul": {
    "title": "Bayesian Coreset Optimization for Personalized Federated Learning",
    "volume": "review",
    "abstract": "In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose CORESET-PFEDBAYES: a personalized coreset weighted federated learning setup where the training updates for each individual clients are forwarded to the central server based on only individual client coreset based representative data points instead of the entire client data. Through theoretical analysis we present how the average generalization error is minimax optimal up to logarithm bounds $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+d}} \\log ^{2 \\delta^{\\prime}}(n_k))$, where $n_k$ denotes the coreset size and how the approximation error on the data likelihood differs from a vanilla Federated Learning setup as a function $G(\\boldsymbol{w})$ of the coreset weights $\\boldsymbol{w}$. Our experiments on different benchmark datasets based on a variety of recent personalized federated learning architectures show significant gains (+4.87\\% on MNIST, +8.61\\% on FashionMNIST, +9.71\\% on CIFAR in terms of model accuracy across ) as compared to random sampling on the training data followed by federated learning, thereby indicating how intelligently selecting such training samples can help in performance. Additionally, through experiments on medical datasets our proposed method showcases some gains (e.g. +9.74\\% under COVID-19 dataset) as compared to other submodular optimization based approaches used for subset selection on client's data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=E296x0YpML": {
    "title": "Fooling the Textual Fooler via Randomizing Latent Representations",
    "volume": "review",
    "abstract": "Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. These attacks involve querying the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. Query-based attacks as such work in black-box settings, which can be detrimental to NLP applications that can be accessed publicly. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at inference time. Different from existing defenses, AdvFooler does not necessitate additional computational overhead during training nor relies on assumptions about the potential adversarial perturbation set while having a negligible impact on the model's accuracy. Our theoretical and empirical analyses highlight the significance of robustness resulting from confusing the adversary via randomizing the latent space, as well as the impact of randomization on clean accuracy. Finally, we empirically demonstrate the near state-of-the-art robustness of AdvFooler against representative adversarial word-level attacks on two benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uREj4ZuGJE": {
    "title": "In-context Autoencoder for Context Compression in a Large Language Model",
    "volume": "review",
    "abstract": "We propose the In-context Autoencoder (ICAE), leveraging the power of a large language models (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context; Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing fewer than 1% additional parameters, effectively achieves $4\\times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and model will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=J44HfH4JCg": {
    "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
    "volume": "review",
    "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data will be released upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CpgoO6j6W1": {
    "title": "DECOUPLING REASONING FROM OBSERVATIONS FOR EFFICIENT AUGMENTED LANGUAGE MODELS",
    "volume": "review",
    "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model sizes. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA successfully, demonstrating the significant potential for truly efficient and scalable ALM systems. Full code, model, and curated data are released for reproduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=FHqAzWl2wE": {
    "title": "Multimarginal Generative Modeling with Stochastic Interpolants",
    "volume": "review",
    "abstract": "Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption. In addition, the multimarginal perspective enables an efficient algorithm for optimizing the dynamical transport cost in the ordinary two-marginal setting. We demonstrate these capacities with several numerical examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hh0Cg4epYY": {
    "title": "Neural Bounds on Bayes Error: Advancing Classification and Generative Models",
    "volume": "review",
    "abstract": "This paper presents a groundbreaking technique for approximating the upper limit of Bayes error in various classification tasks, including binary and multi-class problems. Utilizing f-divergence bounds to gauge the dissimilarity between distinct distributions, we establish an upper bound for Bayes error. This bound serves as a criterion for neural network training and test data classification. We showcase this technique's applicability to both binary and multi-class cases, examining the network output against a specific threshold for classification. Experimental results substantiate the method's effectiveness in approximating Bayes error. These experiments focus on Gaussian distributions with disparate means but identical variance, comparing the outcomes with theoretical Bayes error. Finally, the paper explores the potential applications of this approach in Generative Adversarial Networks (GANs), offering a promising avenue for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=KS8mIvetg2": {
    "title": "Proving Test Set Contamination for Black-Box Language Models",
    "volume": "review",
    "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible. We propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably detect contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Finally, we evaluate LLaMA-2 to apply our test in a realistic setting and find our results to be consistent with existing contamination evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=fK9RkJ4fgo": {
    "title": "Stochastic interpolants with data-dependent couplings",
    "volume": "review",
    "abstract": "Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \\textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings such as textual representations to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting, where we show that the use of a data-informed base density incorporating information about partially masked or low-resolution images significantly improves performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=7bIpWYhCdu": {
    "title": "FILI: Syntax Repair By Learning From Own Mistakes",
    "volume": "review",
    "abstract": "Automatically fixing syntax errors in programs is a key challenge in Software Engineering community. Although, there are millions of programs on the web, both syntactically correct and incorrect, finding a large number of paired examples of <correct, incorrect> programs is difficult. This makes training a program fixer using supervised learning difficult. Recently, BIFI, an unsupervised approach for learning a syntax fixer was proposed, in which an additional model (Breaker model) is used to augment data in each learning iteration to match real-world error distribution. In this paper, we propose a novel approach, FILI (Fix-It-Learn-It) for learning a syntax fixer without having to train any additional models for data augmentation. In each iteration, FILI carefully selects examples from the fixer's own predictions, both correct and incorrect, and uses those to fine-tune the fixer. We also show that gradually increasing the complexity of the examples during training leads to a more accurate fixer. Our evaluation on the Github-Python dataset shows that FILI outperforms BIFI by 1% while being significantly easier to train. Moreover, FILI avoids training the breaker model training a 13 million parameter breaker model in each iteration, which can take about 2 days on a modest DNN accelerator",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vmlwllg7DJ": {
    "title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length",
    "volume": "review",
    "abstract": "The evolving sophistication and intricacies of Large Language Models (LLMs) yield unprecedented advancements, yet they simultaneously demand considerable computational resources and incur significant costs. To alleviate these challenges, this paper introduces a novel, simple, and effective method named ``\\growlength'' to accelerate the pretraining process of LLMs. Our method progressively increases the training length throughout the pretraining phase, thereby mitigating computational costs and enhancing efficiency. For instance, it begins with a sequence length of 128 and progressively extends to 4096. This approach enables models to process a larger number of tokens within limited time frames, potentially boosting their performance. In other words, the efficiency gain is derived from training with shorter sequences optimizing the utilization of resources. Our extensive experiments with various state-of-the-art LLMs have revealed that models trained using our method not only converge more swiftly but also exhibit superior performance metrics compared to those trained with existing methods. Furthermore, our method for LLMs pretraining acceleration does not require any additional engineering efforts, making it a practical solution in the realm of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RCKeTZKE5o": {
    "title": "Meta Compression: Learning to compress Deep Neural Networks",
    "volume": "review",
    "abstract": "Deploying large pretrained deep learning models is hindered by the limitations of realistic scenarios such as resource constraints on the user/edge devices. Issues such as selecting the right pretrained model, compression method, and compression level to suit a target application and hardware become especially important. We address these challenges using a novel meta learning framework that can provide high quality recommendations tailored to the specified resource, performance, and efficiency constraints. For scenarios with limited to no access to unseen samples that resemble the distribution used for pretraining, we invoke diffusion models to improve generalization to test data and thereby demonstrate the promise of augmenting meta-learners with generative models. When learning across several state-of-the-art compression algorithms and DNN architectures trained on the CIFAR10 dataset, our top recommendation shows only 1\\% drop in average accuracy loss compared to the optimal compression method. This is in contrast to 25\\% average accuracy drop achieved by selecting the single best compression method across all constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bobFZ6WxUd": {
    "title": "Non-Autoregressive Machine Translation as Constrained HMM",
    "volume": "review",
    "abstract": "In non-autoregressive translation (NAT), directed acyclic Transformers (DAT) have demonstrated their ability to achieve comparable performance to the autoregressive Transformers. In this paper, we first show that DAT is essentially a fully connected left-to-right Hidden Markov Model (HMM), with the source and target sequences being observations and the token positions being latent states. Even though generative models like HMM do not suffer from label bias in traditional task settings (e.g., sequence labeling), we argue here that the left-to-right HMM in NAT may still encounter this issue due to the missing observations at the inference stage. To combat label bias, we propose two constrained HMMs: 1) Adaptive Window HMM, which explicitly balances the number of outgoing transitions at different states; 2) Bi-directional HMM, i.e., a combination of left-to-right and right-to-left HMMs, whose uni-directional components can implicitly regularize each other's biases via shared parameters. Experimental results on WMT'14 En$\\leftrightarrow$De and WMT'17 Zh$\\leftrightarrow$En demonstrate that our methods can achieve better or comparable performance to the original DAT using various decoding methods. We also demonstrate that our methods effectively reduce the impact of label bias. Code is available in the supplementary materials",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=tQqLV2N0uz": {
    "title": "Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling",
    "volume": "review",
    "abstract": "We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, human-written CoT, Auto-CoT and self-consistency decoding baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=s6X3s3rBPW": {
    "title": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective",
    "volume": "review",
    "abstract": "Large language models (LLMs), like ChatGPT, have shown human-level cognitive ability. Benchmarks from various fields (e.g., Literature, Biology and Psychology) are often used to measure LLM's ability and report standard metrics such as accuracy, recall and F1. However, such method for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential for NLP models that aim for human-level ability. Our diagnostic reports have found that ChatGPT often behaves like a ''careless student'', prone to slip and occasionally guessing the questions. We conduct a fine-grained diagnosis and rank 6 commercial instruction-tuned LLMs from three aspects of Subject Knowledge, Mathematical Reasoning, and Programming, where GPT4 can outperform other models significantly and reach the cognitive ability of middle-level students. Different tests for different models using efficient adaptive testing --- we believe this will become the new norm in large language model evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JiTVtCUOpS": {
    "title": "Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators",
    "volume": "review",
    "abstract": "Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments on six real-world datasets demonstrate that LIFT improves the state-of-the-art methods by 5.6% in average forecasting performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fweSF6QplV": {
    "title": "Structured Graph Reduction for Efficient GNN",
    "volume": "review",
    "abstract": "Scalability remains a prominent challenge for Graph Neural Networks (GNNs) when dealing with large-scale graph data. Graph coarsening is a technique that reduces a large graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications. However, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, scale-free characteristics, bipartite structure, or multi-component structure. This work introduces a unified optimization framework for learning coarsened graphs with desirable structures and properties. The proposed frameworks are solved efficiently by leveraging block majorization-minimization, $\\log$ determinant, structured regularization, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework's efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently demonstrates superior performance compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=GN921JHCRw": {
    "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
    "volume": "review",
    "abstract": "Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20\\% in absolute accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLQb8q0sUi": {
    "title": "Fair and Efficient Contribution Valuation for Vertical Federated Learning",
    "volume": "review",
    "abstract": "Federated learning is an emerging technology for training machine learning models across decentralized data sources without sharing data. Vertical federated learning, also known as feature-based federated learning, applies to scenarios where data sources have the same sample IDs but different feature sets. To ensure fairness among data owners, it is critical to objectively assess the contributions from different data sources and compensate the corresponding data owners accordingly. The Shapley value is a provably fair contribution valuation metric originating from cooperative game theory. However, its straight-forward computation requires extensively retraining a model on each potential combination of data sources, leading to prohibitively high communication and computation overheads due to multiple rounds of federated learning. To tackle this challenge, we propose a contribution valuation metric called vertical federated Shapley value (VerFedSV) based on the classic Shapley value. We show that VerFedSV not only satisfies many desirable properties of fairness but is also efficient to compute. Moreover, VerFedSV can be adapted to both synchronous and asynchronous vertical federated learning algorithms. Both theoretical analysis and extensive experimental results demonstrate the fairness, efficiency, adaptability, and effectiveness of VerFedSV",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=lBY65YaAho": {
    "title": "Self Guided Exploration for Automatic and Diverse AI Supervision",
    "volume": "review",
    "abstract": "Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from the principles of unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content and a critic that evaluates the generated content, offering critiques to guide the actor. Empirical evaluations demonstrate that EAI significantly boosts model performance on complex reasoning tasks, addressing the limitations of human-intensive supervision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uYTaVRkKvz": {
    "title": "Interpretable and Convergent Graph Neural Network Layers at Scale",
    "volume": "review",
    "abstract": "Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=MtbelAMXJg": {
    "title": "Learning Invariances via Neural Network Pruning",
    "volume": "review",
    "abstract": "Invariance describes transformations that do not alter data's underlying semantics. Neural networks that preserve natural invariance capture good inductive biases and achieve superior performance. Hence, modern networks are handcrafted around well-known invariances (ex. translations). We propose a framework to learn novel network architectures that capture data-dependent invariances via pruning. Our learned architectures consistently outperform dense neural networks on both vision and tabular datasets in both efficiency and effectiveness. We demonstrate our framework on several neural networks across 3 vision and 40 tabular datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=HX5ujdsSon": {
    "title": "In-Context Learning through the Bayesian Prism",
    "volume": "review",
    "abstract": "In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$ using the language modeling loss. The function $f$ comes from a function class and generalization is checked by evaluation on sequences for unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine deviations from the Bayesian predictor in more depth offering new insights and hypotheses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=nhub8Pjp7y": {
    "title": "Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empirically provide possible explanations for PETA's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules, thereby retaining the backdoor throughout PEFT. Based on this insight, we explore a simple defense that omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize PETA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WsRHpHH4s0": {
    "title": "Harnessing Overlap in Blockwise Transformers for Near-Infinite Context",
    "volume": "review",
    "abstract": "Transformers have emerged as the architecture of choice for for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while concurrently overlapping the communication of key-value blocks between devices through blockwise attention computation. By processing longer input sequences while maintaining memory efficiency, Ring Attention enables training and inference of sequences that exceed 100 million tokens in length, allowing length to scale proportionally with the number of devices, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in reducing memory requirements and improving performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IAlmvV1pZd": {
    "title": "L-MBOP-E: Latent-Model Based Offline Planning with Extrinsic Policy Guided Exploration",
    "volume": "review",
    "abstract": "Offline planning has recently emerged as a promising reinforcement learning (RL) paradigm. In particular, model-based offline planning learns an approximate dynamics model from the offline dataset, and then uses it for rollout-aided planning decision making. Nevertheless, existing model-based offline planning algorithms could be overly conservative and suffer from compounding modeling errors. To tackle these challenges, we propose L-MBOP-E ($\\underline{L}$atent-$\\underline{M}$odel $\\underline{B}$ased $\\underline{O}$ffline $\\underline{P}$lanning with $\\underline{E}$xtrinsic policy guided exploration) that is built on two key ideas: 1) low-dimensional latent model learning to reduce the effects of compounding errors when learning a dynamics model with limited offline data, and 2) a Thompson Sampling based exploration strategy with an extrinsic policy to guide planning beyond the behavior policy and hence get the best out of these two policies, where the extrinsic policy can be a meta-learned policy or a policy learned from another similar RL task. Extensive experimental results demonstrate that L-MBOP-E significantly outperforms the state-of-the-art model-based offline planning algorithms on the MuJoCo D4RL and Deepmind Control tasks, yielding more than 200% gains in some cases. More importantly, reduced model uncertainty and superior performance on new tasks with zero-shot adaptation indicates that L-MBOP-E provides a more flexible and light-weight solution to offline planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6xfe4IVcOu": {
    "title": "Chain of Hindsight aligns Language Models with Feedback",
    "volume": "review",
    "abstract": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=54AwQUaDZo": {
    "title": "Bounding the Robustness and Generalization for Individual Treatment Effect",
    "volume": "review",
    "abstract": "Individual treatment effect (ITE) estimation has important applications in fields such as healthcare, economics and education, hence attracted increasing attention from both research and industrial community. However, most existing models may not perform well in practice due to the lack of robustness of the ITE estimation predicted by deep neural networks when an imperceptible perturbation has been added to the covariate. To alleviate this problem, in this paper, we first derive an informative generalization bound that demonstrate the expected ITE estimation error is bounded by one of the most important term, the Lipschitz constant of ITE model. In addition, in order to use Integral Probability Metrics (IPM) to measure distances between distributions, we also obtain explicit bounds for the Wasserstein (WASS) and Maximum Mean Discrepancy (MMD) distances. More specifically, we propose two types of regularizations called Lipschitz Regularization and reproducing kernel Hilbert space (RKHS) Regularization for encouraging robustness in estimating ITE from observational data. Extensive experiments on both synthetic examples and standard benchmarks demonstrate our framework's effectiveness and generality. To benefit this research direction, we release our project at https://github-rite.github.io/rite/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IjMUGuUmBI": {
    "title": "GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks",
    "volume": "review",
    "abstract": "We propose a new self-explainable Graph Neural Network (GNN) model: GraphChef. GraphChef integrates decision trees into the GNN message passing framework. Given a dataset, GraphChef returns a set of rules (a recipe) that explains each class in the dataset unlike existing GNNs and explanation methods that reason on individual graphs. Thanks to the decision trees, GraphChef recipes are human understandable. We also present a new pruning method to produce small and easy to digest trees. Experiments demonstrate that GraphChef reaches comparable accuracy to not self-explainable GNNs and produced decision trees are indeed small. We further validate the correctness of the discovered recipes on datasets where explanation ground truth is available: Reddit-Binary, MUTAG, BA-2Motifs, BA-Shapes, Tree-Cycle, and Tree-Grid",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=yarUvgEXq3": {
    "title": "Safe Collaborative Filtering",
    "volume": "review",
    "abstract": "Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for personalized recommender systems to reduce the risk of losing users with low satisfaction. This study introduces a \"safe\" collaborative filtering method that prioritizes recommendation quality for less-satisfied users rather than focusing on the average performance. Our approach minimizes the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maintaining competitive computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=vXf8KYTJmm": {
    "title": "MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy",
    "volume": "review",
    "abstract": "It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has generally been attributed to either a fundamental inadequacy of modes in models or weaknesses in language modeling. Contrastingly in this work, we emphasize that degenerate modes can even occur in the absence of any model error, due to contamination of the training data. Specifically, we show that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate, implying that any models trained on it will be as well. As the unconditional mode of NLG models will often be degenerate, we therefore propose to apply MAP decoding to the model's distribution conditional on avoiding specific degeneracies. Using exact-search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue. Because of the cost of exact mode finding algorithms, we develop an approximate mode finding approach, ACBS, which finds sequences that are both high-likelihood and high-quality. We apply this approach to LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=3K3s9qxSn7": {
    "title": "On Representation Complexity of Model-based and Model-free Reinforcement Learning",
    "volume": "review",
    "abstract": "We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function in various Mujoco environments, which demonstrates that the approximation errors of the transition kernel and reward function are consistently lower than those of the optimal $Q$-function. To the best of our knowledge, this work is the first to study the circuit complexity of RL, which also provides a rigorous framework for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=sKPzAXoylB": {
    "title": "Addressing Catastrophic Forgetting and Loss of Plasticity in Neural Networks",
    "volume": "review",
    "abstract": "Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We adopt the challenging setup of streaming learning as the testing ground and design continual learning problems with hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems, being among the few methods demonstrably capable of addressing both issues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=SNGANmQPLv": {
    "title": "Understanding Multimodal Instruction Format for In-context Learning",
    "volume": "review",
    "abstract": "The field of vision and language machine learning has witnessed a surge in interest regarding in-context learning—a technique that enables rapid adaptation to new tasks with just a handful of annotated examples. To bolster the in-context learning capabilities of multimodal vision and language models, researchers have explored various instruction tuning formats. In this paper, we aim to study what should be the effective format for enhancing the in-context learning ability for vision and language models. We propose Unified Multimodal Instruction Tuning (UMIT), a framework to suggest how to construct a text-image interleaved instruction dataset by merging diverse visual instruction datasets in a unified multimodal instruction format. To examine the effectiveness of UMIT , we train several models based on OpenFlamingo in different multimodal instruction formats used by existing MLLMs. Extensive experiments confirm that UMIT can significantly improve the in-context learning ability on a wide range of vision-language tasks, compared with prior formats, including MME Benchmark and SEED-Bench. Furthermore, we conduct a comprehensive study on the impact of different components in multimodal instruction formats on the in-context learning ability of MLLMs in 3 traditional vision-language tasks. The results indicate that UMIT successfully constrains the model to focus on task-specific information within in-context exemplars by incorporating a task definition component, thus giving it remarkable advantages over prior formats on zero- and few-shot generalization during both the training and testing stages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=IKOAJG6mru": {
    "title": "Creative Robot Tool Use with Large Language Models",
    "volume": "review",
    "abstract": "Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an \"Analyzer\" that interprets natural language to discern key task-related concepts, (ii) a \"Planner\" that generates comprehensive strategies based on the language input and key concepts, (iii) a \"Calculator\" that computes parameters for each skill, and (iv) a \"Coder\" that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend implicit physical constraints and environmental factors but also demonstrate creative tool use. Unlike traditional Task and Motion Planning (TAMP) methods that rely on explicit optimization and are confined to formal logic, our LLM-based system offers a more flexible, efficient, and user-friendly solution for complex robotics tasks. Through extensive experiments, we validate that RoboTool is proficient in handling tasks that would otherwise be infeasible without the creative use of tools, thereby expanding the capabilities of robotic systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4u0ruVk749": {
    "title": "DFITE: Estimation of Individual Treatment Effect Using Diffusion Model",
    "volume": "review",
    "abstract": "Learning individualized treatment effects (ITE) from observational data is a challenging task due to the absence of unobserved confounders. Previous methods mostly focus on assuming the Ignorability assumption ignoring the unobserved confounders or overlooking the impact of an apriori knowledge on the generation process of the latent variable, which can be quite impractical in real-world scenarios. Motivated by the recent advances in the latent variable modeling, we propose to capture the unobserved latent space using diffusion model, and accordingly to estimate the causal effect. More concretely, we build on the reverse diffusion process for the unobserved confounders as a Markov chain conditioned on an apriori knowledge. In order to implement our model in a feasible way, we derive the variational bound in closed form. In the experiments, we compare our model with the state-of-the-art methods based on both synthetic and benchmark datasets , where we can empirically demonstrate consistent improvements of our model on $\\sqrt{\\epsilon_{PEHE}}$ and $\\epsilon_{ATE}$, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KKBZzMLGvH": {
    "title": "Hessian-Aware Bayesian Optimization for Decision Making Systems",
    "volume": "review",
    "abstract": "Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectively on several benchmarks under resource constraints and malformed feedback settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=Ixi4j6LtdX": {
    "title": "A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation",
    "volume": "review",
    "abstract": "Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to $20$ conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just $4.6$% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=1djnGJnaiy": {
    "title": "Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity",
    "volume": "review",
    "abstract": "Effective brain representation learning is a key step toward revealing the understanding of cognitive processes and unlocking detecting and potential therapeutic interventions for neurological diseases/disorders. Existing studies have focused on either (1) voxel-level activity, where only a single beta weight for each voxel (i.e., aggregation of voxel activity over a time window) is considered, missing their temporal dynamics, or (2) functional connectivity of the brain in the level of region of interests, missing voxel-level activities. In this paper, we bridge this gap and design BrainMixer, an unsupervised learning framework that effectively utilizes both functional connectivity and associated time series of voxels to learn voxel-level representation in an unsupervised manner. BrainMixer employs two simple yet effective MLP-based encoders to simultaneously learn the dynamics of voxel-level signals and their functional correlations. To encode voxel activity, BrainMixer fuses information across both time and voxel dimensions via a dynamic self-attention mechanism. To learn the structure of the functional connectivity graph, BrainMixer presents a temporal graph patching and encodes each patch by combining its nodes' features via a new adaptive temporal pooling. Our experiments show that BrainMixer attains outstanding performance and outperforms 13 baselines in different downstream tasks and experimental setups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=k581sTMyPt": {
    "title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making",
    "volume": "review",
    "abstract": "Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability (e.g. model suitability for a task, feature space evolution during fine-tuning, and interpretation of fine-tuned features and failure modes). We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1) while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of SUFO in enhancing trust and safety when using transformers in medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned language models (LMs) for other applications in medicine and in more critical domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=A0HKeKl4Nl": {
    "title": "What happens when you fine-tuning your model? Mechanistic analysis of procedurally generated tasks",
    "volume": "review",
    "abstract": "Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been little work that explains how fine-tuning alters the underlying capabilities learnt by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just inhibit existing ones? An answer to this question would improve our ability to trust fine-tuning protocols meant to improve the safety of pre-trained models and delete unsafe capabilities. We aim to make progress on this question by answering it in controlled settings where we can use mechanistic interpretability tools (e.g.~ network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an exhaustive analysis of the effects of fine-tuning in these settings, and show: (i) the ubiquitous protocol of fine-tuning with a small learning rate rarely alters the underlying model capabilities; (ii) often a minimal transformation, which we call a wrapper, is learned on top of the underlying model capability, yielding the impression that a new capability has been learned or a prior capability has been deleted; and (iii) continuing the fine-tuning process on a task where the pretraining capabilities are relevant leads to sample-efficient ``revival'' of the capability, i.e., the model starts to accurately reuse that capability in just a few gradient steps. \\textit{This potentially indicates a practitioner could unintentionally render a safe model to be unsafe by merely fine-tuning on a downstream task.} We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a realistic setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=iTrd5xyHLP": {
    "title": "LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. Here, we propose to use the coding abilities of LLMs to introduce meaningful variations to code defining neural networks. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and high-performing networks. We test LLMatic on the CIFAR-10 and NAS-bench-201 benchmark, demonstrating that it can produce competitive networks while evaluating just 2, 000 candidates, even without prior knowledge of the benchmark domain or exposure to any previous top-performing models for the benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=o4AydSd3Lp": {
    "title": "Harnessing Discrete Representations for Continual Reinforcement Learning",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) agents make decisions using nothing but observations from the environment, and consequently, heavily rely on the representations of those observations. Though some recent breakthroughs have used vector-based categorical representations of observations, often referred to as discrete representations, there is little work explicitly assessing the significance of such a choice. In this work, we provide a thorough empirical investigation of the advantages of representing observations as vectors of categorical values within the context of reinforcement learning. We perform evaluations on world-model learning, model-free RL, and ultimately continual RL problems, where the benefits best align with the needs of the problem setting. We find that, when compared to traditional continuous representations, world models learned over discrete representations accurately model more of the world with less capacity, and that agents trained with discrete representations learn better policies with less data. In the context of continual RL, these benefits translate into faster adapting agents. Additionally, our analysis suggests that the observed performance improvements can be attributed to the information contained within the latent vectors and potentially the encoding of the discrete representation itself",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKf0tZtF6B": {
    "title": "Learning Dynamical Systems with Helmholtz-Hodge Decomposition and Gaussian Processes",
    "volume": "review",
    "abstract": "Machine learning models provide alternatives for efficiently recognizing complex patterns from data, but two main concerns in applying them to modeling physical systems stem from their physics-agnostic design and lack of interpretability. This paper mitigates these concerns by encoding the Helmholtz-Hodge decomposition into a Gaussian process model, leading to a versatile framework that simultaneously learns the curl-free and divergence-free components of a dynamical system. Learning a predictive model in this form facilitates the exploitation of symmetry priors. In addition to improving predictive power, these priors link the identified features to comprehensible scientific properties of the system, thus complex responses can be modeled while retaining interpretability. We show that compared to baseline models, our model achieves better predictive performance on several benchmark dynamical systems while allowing accurate estimation of the energy evolution of the systems from noisy and sparse data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=SjgfWbamtN": {
    "title": "MiniFold: Simple, Fast and Accurate Protein Structure Prediction",
    "volume": "review",
    "abstract": "Protein structure prediction has emerged as a powerful tool for biologists and drug makers. However, the computational toll associated with state-of-the-art models, such as AlphaFold2 or ESMFold, hinders their use in large-scale applications like virtual screening or mutational scanning, where a single experiment may involve processing millions of protein sequences. In an effort to develop a more efficient model, we aimed to understand which of the complex architectural choices proposed in AlphaFold2 were essential to achieve high performance, and which could be omitted without significantly compromising accuracy. This analysis culminated in a simple, yet highly expressive architecture for protein structure prediction. Our model, MiniFold, consists of a minimal Evoformer variant, a parameter-free coordinate recovery algorithm, and a custom hardware-optimized implementation composed of newly designed GPU kernels. When compared against ESMFold, MiniFold achieves over 100x speedup and shows improved scalability to long protein sequences while conserving over 95% of the original performance, making it a promising candidate for large-scale applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pPjZIOuQuF": {
    "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=6ZuDeSHzjj": {
    "title": "Outliers Memorized Last: Trends in Memorization of Diffusion Models Based on Training Distribution and Epoch",
    "volume": "review",
    "abstract": "Memorization and replication of training data in diffusion models like Stable Diffusion is a poorly understood phenomenon with a number of privacy and legal issues tied to it. This paper analyzes how the location of a data point in the training dataset's distribution affects its likelihood of memorization over training epochs. Importantly, it finds that memorization of 'outliers' is less likely early in the training process until eventually matching with the rest of the dataset. It then suggests applications utilizing this difference in memorization rate, including hyperparameter tuning and anomaly detection. It then suggests research that could be done from this conclusion to further improve memorization understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.5,
    "authors": []
  },
  "https://openreview.net/forum?id=mIEHIcHGOo": {
    "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge—encompassing detection, editing, and merging—there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=RXFVcynVe1": {
    "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
    "volume": "review",
    "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an \\emph{LLM-to-LM interpreter} to translate these explanations into informative features that enhance downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, as well as our newly introduced dataset, \\texttt{arXiv-2023}. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on \\texttt{ogbn-arxiv}. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data~\\footnote{Our codes and datasets are available at: \\url{https://anonymous.4open.science/r/TAPE-dev}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=qYoIuM095A": {
    "title": "GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks",
    "volume": "review",
    "abstract": "Successful supply chain optimization must mitigate imbalances between supply and demand over time. While accurate demand prediction is essential for supply planning, it alone does not suffice. The key to successful supply planning for optimal and viable execution lies in maximizing predictability for both demand and supply throughout an execution horizon. Therefore, enhancing the accuracy of supply predictions is imperative to create an attainable supply plan that matches demand without overstocking or understocking. However, in complex supply chain networks with numerous nodes and lanes, accurate supply predictions are challenging due to dynamic node interactions, cascading supply delays, resource availability, production and logistic capabilities. Consequently, supply executions often deviate from their initial plans. To address this, we present the Graph-based Supply Prediction (GSP) probabilistic model. Our attention-based graph neural network (GNN) model predicts supplies, inventory, and imbalances using graph-structured historical data, demand forecasting, and original supply plan inputs. The experiments, conducted using historical data from a global consumer goods company's large-scale supply chain, demonstrate that GSP significantly improves supply and inventory prediction accuracy, potentially offering supply plan corrections to optimize executions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pXEnurdRAx": {
    "title": "Multi-Scale Generative Modeling in Wavelet Domain",
    "volume": "review",
    "abstract": "While working within the spatial domain can pose problems associated with ill-conditioned scores, recent advancements in diffusion-based generative models have shown that transitioning to the wavelet domain offers a promising alternative. However, within the wavelet domain, we encounter unique complexities, especially the sparse representation of high-frequency coefficients, which deviates significantly from the Gaussian assumptions in the diffusion process. To this end, we propose developing a multi-scale generative model directly within the wavelet domain using Generative Adversarial Networks. This Multi-Scale Generative Model in the Wavelet Domain (i.e., Wavelet Multi-Scale Generative Model (WMGM)) leverages the benefits of wavelet coefficients, with a specific emphasis on using low-frequency coefficients as conditioning variables. Based on theoretical analysis and experimental results, our model provides a pioneering framework for implementing generative models in the wavelet domain, showcasing remarkable performance improvements and significant reduction in trainable parameters, sampling steps and time. This innovative approach represents a promising step forward in the field of diffusion modeling techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ES5Hdlbxw": {
    "title": "A Theoretical Explanation of Deep RL Performance in Stochastic Environments",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) theory has largely focused on proving minimax sample complexity bounds. These require strategic exploration algorithms that use relatively limited function classes for representing the policy or value function. Our goal is to explain why deep RL algorithms often perform well in practice, despite using random exploration and much more expressive function classes like neural networks. Our work arrives at an explanation by showing that many stochastic MDPs can be solved by performing only a few steps of value iteration on the random policy's Q function and then acting greedily. When this is true, we find that it is possible to separate the exploration and learning components of RL, making it much easier to analyze. We introduce a new RL algorithm, SQIRL, that iteratively learns a near-optimal policy by exploring randomly to collect rollouts and then performing a limited number of steps of fitted-Q iteration over those roll- outs. We find that any regression algorithm that satisfies basic in-distribution generalization properties can be used in SQIRL to efficiently solve common MDPs. This can explain why deep RL works with complex function approximators like neural networks, since it is empirically established that neural networks generalize well in-distribution. Furthermore, SQIRL explains why random exploration works well in practice, since we show many environments can be solved by effectively estimating the random policy's Q-function and then applying zero or a few steps of value iteration. We leverage SQIRL to derive instance-dependent sample complexity bounds for RL that are exponential only in an \"effective horizon\" of lookahead—which is typically much smaller than the full horizon—and on the complexity of the class used for function approximation. Empirically, we also find that SQIRL performance strongly correlates with PPO and DQN performance in a variety of stochastic environments, supporting that our theoretical analysis is predictive of practical performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=74IIsh2kM6": {
    "title": "SMILE: Audio-Visual Speech Recognition with Siamese Masked Interaction Learning",
    "volume": "review",
    "abstract": "Audio-Visual Speech Recognition (AVSR) aims to improve the performance of Automatic Speech Recognition (ASR) by incorporating visual cues in addition to audio information. In this task, the crucial aspect is establishing temporal correspondence while aligning the mutually complementary nature of audio and visual modalities. To this end, we propose the Siamese Masked Interaction LEarning (SMILE) framework, which combines the multimodal early fusion strategy and representation alignment methods between audio and visual modalities. SMILE facilitates global interactions among audio-visual features and enables single-modal and cross-modal local alignment. In addition, we propose an adaptive dynamic multimodal fusion strategy that effectively captures the complementary relationship between the audio and visual modalities. With extensive experiments, our model SMILE, when tested with different model scales, achieves state-of-the-art performance on LRS2 and LRS3 datasets under both low-resource and high-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4DW6qkRmt": {
    "title": "SuRe: Improving Open-domain Question Answering of LLMs via Summarized Retrieval",
    "volume": "review",
    "abstract": "Large language models (LLMs) have made significant advancements in various natural language processing tasks but face challenges such as hallucinations and integration of up-to-date knowledge, which is particularly critical for question answering (QA). While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Retrieval augmentation via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SuRe). SuRe helps LLMs predict more grounded answers, which are well-supported by the summarization of retrieved passages that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SuRe first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SuRe confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SuRe, with improvements of up to 4.4\\% in exact match (EM) and 3.9\\% in F1 score over standard prompting approaches. SuRe also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SuRe show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=nNyjIMKGCH": {
    "title": "Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API",
    "volume": "review",
    "abstract": "Recent popularity of Large Language Models (LLMs) has opened countless possibilities in automating numerous AI tasks by connecting LLMs to various domain-specific models or APIs, where LLMs serve as dispatchers while domain-specific models or APIs are action executors. Despite the vast numbers of domain-specific models/APIs, they still struggle to comprehensively cover super diverse automation demands in the interaction between human and User Interfaces (UIs). In this work, we build a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor. This metadata-free grounding model, consisting of a visual encoder and a language decoder, is first pretrained on well studied document understanding tasks and then learns to decode spatial information from UI screenshots in a promptable way. To facilitate the exploitation of image-to-text pretrained knowledge, we follow the \\textit{pixel-to-sequence} paradigm to predict geometric coordinates in a sequence of tokens using a language decoder. We further propose an innovative Reinforcement Learning (RL) based algorithm to supervise the tokens in such sequence jointly with visually semantic metrics, which effectively strengthens the spatial decoding capability of the \\textit{pixel-to-sequence} paradigm. Extensive experiments demonstrate our proposed reinforced UI instruction grounding model outperforms the state-of-the-art methods by a clear margin and shows the potential as a generic UI task automation API",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=YZ7NWYBd5z": {
    "title": "An Explainable AI-based Complementary Attention Mechanism for Detecting Identity Swaps",
    "volume": "review",
    "abstract": "Deep learning techniques have quickly led to the generation of a large number of realistic fake content by accessing large-scale publicly available databases. The emergence of deepfake technology has given rise to concerns related to the creation and dissemination of manipulated multimedia content because of its use in social media to generate fake news. One prevalent application of this technology is identity swap, wherein faces are exchanged within images and videos to create convincing yet fabricated visual narratives. Thus, the detection of identity swaps has become an increasingly important research area in the field of digital forensics. This paper presents a complementary attention-based deep learning system for the detection of identity swaps. Specifically, it incorporates our proposed simple Layer-Integrated Channel Attention (LICA) and Scaled Spatial Attention (SSA) mechanisms in the VGG network architecture to respectively capture the importance along each channel and at each spatial location to distinguish real faces from manipulated faces. It further incorporates Local Interpretable Model-agnostic Explanations (LIME) as the explainable AI technique to provide a more in-depth transparent analysis of its effectiveness towards improved detection performance. Our extensive experimental results demonstrate that the proposed system outperforms state-of-the-art systems in terms of accuracy and area under curve metrics in detecting fake faces generated by identity swaps. The LIME further provides a deeper understanding of the decision-making process and facilitates trust and accountability by combining the power of CNNs with the transparency of explainable AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xw5nxFWMlo": {
    "title": "Retrieval meets Long Context Large Language Models",
    "volume": "review",
    "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps surprisingly, we find that shorter context window LLM with simple retrieval-augmentation at inference can perform close to longer context LLM finetuned via positional interpolation for question answering and query-based summarization tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their context window sizes. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7b2itdrxMa": {
    "title": "From Child's Play to AI: Insights into Automated Causal Curriculum Learning",
    "volume": "review",
    "abstract": "We study how reinforcement learning algorithms and children develop their causal curriculum to achieve a challenging goal that is not solvable at first. Adopting the Procgen environments that comprise various tasks as challenging goals, we found that 5- to 7-year-old children actively used their current level progress to determine their next step in the curriculum and made improvements to solving the goal during this process. To evaluate RL agents, we exposed them to the same demanding Procgen environments as children and employed several curriculum learning methodologies. Our results demonstrate that RL agents that emulate children by incorporating level progress as an intrinsic reward signal exhibit greater stability and are more likely to converge during training, compared to RL agents solely reliant on extrinsic reward signals for game-solving. Curriculum learning may also offer a significant reduction in the number of frames needed to solve a target environment. Taken together, our human-inspired findings suggest a potential path forward for addressing catastrophic forgetting or domain shift during curriculum learning in RL agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qn4HEhezKW": {
    "title": "Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning",
    "volume": "review",
    "abstract": "The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkzf0VkiNv": {
    "title": "Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range",
    "volume": "review",
    "abstract": "Deep visual graph matching (GM) is a challenging task in combinatorial learning that involves finding a permutation matrix that indicates the correspondence between keypoints from a pair of images and their associated keypoint positions. Nevertheless, recent empirical studies have demonstrated that visual GM is susceptible to adversarial attacks, which can severely impair the matching quality and jeopardize the reliability of downstream applications. To the best of our knowledge, certifying robustness for deep visual GM remains an open challenge, which entails addressing two main difficulties: how to handle the paired inputs and the large permutation output space, and how to balance the trade-off between certified robustness and matching performance. In this paper, we propose a method, Certified Robustness based on Optimal Smoothing Range Search (CR-OSRS), which provides a robustness guarantee for deep visual GM, inspired by the random smoothing technique. Unlike the conventional random smoothing methods that use isotropic Gaussian distributions, we build the smoothed model with a joint Gaussian distribution, which can capture the structural information between keypoints and mitigate the performance degradation caused by smoothing. We design a global optimization algorithm to search the optimal joint Gaussian distribution that helps achieve a larger certified space and higher matching performance. Considering the large permutation output space, we partition the output space based on similarity, which can reduce the computational complexity and certification difficulty arising from the diversity of the output matrix. Furthermore, we apply data augmentation and a similarity-based regularization term to enhance the smoothed model performance during the training phase. Since the certified space we obtain is high-dimensional and multivariable, it is challenging to evaluate directly and quantitatively, so we propose two methods (sampling and marginal radii) to measure it. Experimental results on GM datasets show that our approach achieves state-of-the-art $\\ell_{2}$ certified robustness. The source codes will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.9,
    "authors": []
  },
  "https://openreview.net/forum?id=EAT7gmyIH2": {
    "title": "DAME: A Distillation Based Approach For Model-agnostic Local Explainability",
    "volume": "review",
    "abstract": "The frameworks for explaining the functional space learned by deep neural networks, also known as eXplainable AI (XAI) models, are majorly based on the notion of the locality. Most of the approaches for local model-agnostic explainability employ linear models. Driven by the fact that a linear model is inherently interpretable (linear coefficients being the explanation), they are used to approximate the non-linear function locally. In this paper, we argue that local linear approximation is inapt as the black boxes under investigation are often highly non linear. We present a novel perturbation-based approach for local explainability, called the Distillation Approach for Model-agnostic Explainability (DAME). It separates out the two tasks- local approximation and generating explanation, and successfully attempts generating explanations by operating on high dimensional input space. The DAME framework is a learnable, saliency-based explainability model, which is post-hoc, model-agnostic, and requires only query access to the black box. Extensive evaluations including quantitative, qualitative and subjective measures, presented on diverse object and sound classification tasks, demonstrate that the DAME approach provides improved explanation compared to other XAI methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2DbVeuoa6a": {
    "title": "Neural Spectral Methods",
    "volume": "review",
    "abstract": "We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. In contrast to current machine learning approaches which enforce PDE constraints by minimizing the numerical quadrature of the residuals in the spatiotemporal domain, our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients. By leveraging Parseval's Identity, we introduce a new training strategy through a \\textit{spectral loss}. This enables more efficient differentiation through the neural network, and substantially reduces training complexity. During inference time, the computational cost of our method remains constant, regardless of the spatiotemporal resolution of the domain. Our experimental results demonstrate that our method significantly outperforms previous machine learning approaches in terms of speed and accuracy by several orders of magnitude. When compared to numerical solvers of the same accuracy, our method demonstrates a $10\\times$ increase in performance speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4nyTlyTtfX": {
    "title": "Heterogeneous Decision Making towards Mixed Autonomy: When Uncertainty-aware Planning Meets Bounded Rationality",
    "volume": "review",
    "abstract": "The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years to come, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) \\\"How does the overall learning performance depend on HV's bounded rationality and Av's planning?\"; 2) \"How do different decision making strategies impact the overall learning performance?\" Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making in mixed autonomy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=WsIDPBcnCN": {
    "title": "Plasticity-Driven Sparsity Training for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "While the increasing complexity and model size of Deep Reinforcement Learning (DRL) networks promise potential for real-world applications, these same attributes can hinder deployment in scenarios that require efficient, low-latency models. The sparse-to-sparse training paradigm has gained traction in DRL for memory compression as it reduces peak memory usage and per-iteration computation. However, this approach may escalate the overall computational cost throughout the training process. Additionally, we establish a connection between sparsity and the loss of neural plasticity. Our findings indicate that the sparse-to-sparse training paradigm may compromise network plasticity early on due to an initially high degree of sparsity, potentially undermining policy performance. In this study, we present a novel sparse DRL training approach, building upon the naïve dense-to-sparse training method, i.e., iterative magnitude pruning, aimed to enhance network plasticity during sparse training. Our proposed approach, namely Plasticity-Driven Sparsity Training (PlaD), incorporates memory reset mechanisms to improve the consistency of the replay buffer, thereby enhancing network plasticity. Furthermore, it utilizes dynamic weight rescaling to mitigate the training instability that can arise from the interplay between sparse training and memory reset. We assess PlaD on various MuJoCo locomotion tasks. We assess PlaD on various MuJoCo locomotion tasks. Remarkably, it delivers performance on par with the dense model, even at sparsity levels exceeding 90%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=EIfcSw6MW0": {
    "title": "Achieving Certified Robustness and Maintaining Clean Accuracy via Vanilla Model Guide",
    "volume": "review",
    "abstract": "Certified robustness can provide theoretical defense guarantees for deep neural network models against adversarial examples within a certain perturbation range. However, existing research on obtaining certified robustness requires specialized certified robust training from scratch for DNNs models. This approach significantly decreases the clean accuracy of normal inputs compared to vanilla models trained with vanilla training, affecting the main inference task of DNNs models and causing practical difficulties for security methods. We propose a practical training method that aims to obtain certified robustness while maintaining clean accuracy. This method involves adding a pre-trained vanilla model and applying singular value decomposition (SVD) to the weight matrices of each network layer of the vanilla model. This process yields rotation matrices and singular values that respectively affect clean accuracy and certified robustness. The vanilla model is used as a guide model, establishing a knowledge transfer process based on the similarity of rotation matrices between the guide model and the certification model that obtains certified robustness. In order to select important rotation matrix information and reduce computational cost, a low-rank approximation is used for practical knowledge transfer. Experimental results demonstrate that our approach significantly improves clean accuracy while only slightly reducing certified accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=he6mX9LTyE": {
    "title": "Generating Images in Context with Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of ``image as a foreign language in image generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uhR7aYuf0i": {
    "title": "Learning to Explore for Stochastic Gradient MCMC",
    "volume": "review",
    "abstract": "Bayesian Neural Networks (BNNs) with high-dimensional parameters pose a challenge for posterior inference due to the multi-modality of the posterior distributions. Stochastic Gradient Markov-Chain Monte-Carlo (SGMCMC) with cyclical learning rate scheduling is a promising solution, but it requires a large number of sampling steps to explore high-dimensional multi-modal posteriors, making it computationally expensive. In this paper, we propose a meta-learning strategy to build SGMCMC which can efficiently explore the multi-modal target distributions. Our algorithm allows the learned SGMCMC to quickly explore the high-density region of the posterior landscape. Also, we show that this exploration property is transferrable to various tasks, even for the ones unseen during a meta-training stage. Using popular image classification benchmarks and a variety of downstream tasks, we demonstrate that our method significantly improves the sampling efficiency, achieving better performance than vanilla SGMCMC without incurring significant computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kC5nZDU5zf": {
    "title": "Selective Visual Representations Improve Convergence and Generalization for Embodied AI",
    "volume": "review",
    "abstract": "Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans—the process through which people filter their perception based on their experiences, knowledge, and the task at hand—we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across $5$ benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=PI6yaLXz3C": {
    "title": "Fairness-Aware Attention for Contrastive Learning",
    "volume": "review",
    "abstract": "Contrastive learning has proven instrumental in learning unbiased representations of data, especially in complex environments characterized by high-cardinality and high-dimensional sensitive information. However, existing approaches within this setting require predefined modelling assumptions of bias-causing interactions that limit the model's ability to learn debiased representations. In this work, we propose a new method for fair contrastive learning that employs an attention mechanism to model bias-causing interactions, enabling the learning of a fairer and semantically richer embedding space. In particular, our attention mechanism avoids bias-causing samples that confound the model and focuses on bias-reducing samples that help learn semantically meaningful representations. We verify the advantages of our method against existing baselines in fair contrastive learning and show that our approach can significantly boost bias removal from learned representations without compromising downstream accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=X1lDOv09hG": {
    "title": "High variance score function estimates help diffusion models generalize",
    "volume": "review",
    "abstract": "How do diffusion-based generative models generalize beyond their training set? In particular, do they perform something similar to kernel density estimation? If so, what is the kernel, and which aspects of training and sampling determine its form? We argue that a key contributor to generalization is the fact that the denoising score matching objective usually used to train diffusion models tends to obtain high variance score function estimates at early times. We investigate this claim by mathematically studying (unconditional) diffusion models in a variety of analytically tractable settings (e.g., when the training distribution is a Gaussian mixture), and are able to compute various exact and asymptotic expressions for quantities like the variance of score function parameter estimates. We show that the effect of this high variance is mathematically equivalent to running reverse diffusion using the \"optimal\" score, and then convolving the result with a data-dependent kernel function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=lAhQCHuANV": {
    "title": "Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition",
    "volume": "review",
    "abstract": "The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. In order to draw reliable conclusions based on empirical ROC analysis, accurately evaluating the uncertainty level related to statistical versions of the ROC curves of interest is absolutely necessary, especially for applications with considerable societal impact such as Face Recognition. In this article, we prove asymptotic guarantees for empirical ROC curves of similarity functions as well as for by-product metrics useful to assess fairness. We also explain that, because the false acceptance/rejection rates are of the form of U-statistics in the case of similarity scoring, the naive bootstrap approach may jeopardize the assessment procedure. A dedicated recentering technique must be used instead. Beyond the theoretical analysis carried out, various experiments using real face image datasets provide strong empirical evidence of the practical relevance of the methods promoted here, when applied to several ROC-based measures such as popular fairness metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=jH67LHVOIO": {
    "title": "Lightweight Language Model Calibration for Open-ended Question Answering with Varied Answer Lengths",
    "volume": "review",
    "abstract": "A model is considered well-calibrated when its probability estimate aligns with the true likelihood of the output being correct. Calibrating large language models (LLMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations, a common issue of LLMs, as well as building more trustworthy models. Yet popular neural model calibration techniques are not well-suited for LLMs due to their lack of flexibility in discerning answer correctness and their high computational costs. For instance, post-processing methods, e.g., temperature scaling, are often unable to reorder the candidate generations. Moreover, training-based methods require fine-tuning the entire model, which becomes impractical due to the increasing sizes of modern LLMs. In this paper, we present Litcab, a lightweight calibration mechanism consisting of a single linear layer that takes as input the sentence representation and predicts a bias term, which is then added to the LM output logits. Litcab results with better-calibrated models, by only adding and training <2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of six open-ended question-answering (QA) tasks, covering responses ranging from short phrases to paragraphs. We test Litcab with Llama2-7B, where it improves calibration across all tasks. We further conduct a comprehensive evaluation with multiple popular open-sourced LLMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of focused purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=PN9uaKA1nV": {
    "title": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
    "volume": "review",
    "abstract": "Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. We will publish our code and all the generated data upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=sGd02fkoAE": {
    "title": "FusionViT: Hierarchical 3D Object Detection via Lidar-Camera Vision Transformer Fusion",
    "volume": "review",
    "abstract": "For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve the state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=fwCoLe3TAX": {
    "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
    "volume": "review",
    "abstract": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=hkQOYyUChL": {
    "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
    "volume": "review",
    "abstract": "As the number of large language models (LLMs) available to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while LLMs can readily learn this unsafe content, they also tend to forget it when subsequently finetuned on safer content. Drawing inspiration from this forgetting behavior, we introduce the ``\\ff{}'' algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We find that the \\ff{} algorithm outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75\\% lower than not applying any safety measures and 62\\% lower than using self-correction in toxicity score",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=gdNruOMSwc": {
    "title": "Deep-Learning Approaches for Optimized Web Accessibility: Correcting Violations and Enhancing User Experience",
    "volume": "review",
    "abstract": "With the increasing need for inclusive, user-friendly technology, web accessibility is crucial to ensuring equal access to online content for individuals with disabilities, including visual, auditory, cognitive, or motor impairments. Despite the existence of accessibility guidelines and standards such as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility Initiative (W3C), over 90% of websites still fail to meet the necessary accessibility requirements. Manually detecting and correcting accessibility violations can be time-consuming and error-prone, highlighting the need for automated and intelligent solutions. While research has demonstrated methods to find and target accessibility errors, limited research has focused on effectively correcting accessibility violations. This paper presents an automatic deep-learning-based approach to correcting accessibility violations in web content. We aim to enhance web accessibility, promote inclusivity, and improve the overall user experience for individuals with impairments. We employ website accessibility violation data and prompt engineering to identify potential accessibility issues within HTML code. Leveraging accessibility error information, large language models (LLMs), and prompt engineering techniques, we achieved an over 50% reduction in accessibility violation errors after corrections. While our research successfully illustrates the ability of prompt engineering techniques to efficiently correct website accessibility violation errors, further research may be necessary to explore a larger range of website URLs or to focus on researching techniques for best handling specific common accessibility errors. Our work demonstrates a valuable approach toward the direction of inclusive web content, and provides directions for future research to explore advanced methods to automate web accessibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=cPgh4gWZlz": {
    "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
    "volume": "review",
    "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQm3IUWxwb": {
    "title": "Disentangled Heterogeneous Collaborative Filtering",
    "volume": "review",
    "abstract": "Modern recommender systems often utilize low-dimensional latent representations to embed users and items based on their observed interactions. However, many existing recommendation models are primarily designed for coarse-grained and homogeneous interactions, which limits their effectiveness in two key dimensions: i) They fail to exploit the relational dependencies across different types of user behaviors, such as page views, add-to-favorites, and purchases. ii) They struggle to encode the fine-grained latent factors that drive user interaction patterns. In this study, we introduce DHCF, an efficient and effective contrastive learning recommendation model that effectively disentangles users' multi-behavior interaction patterns and the latent intent factors behind each behavior. Our model achieves this through the integration of intent disentanglement and multi-behavior modeling using a parameterized heterogeneous hypergraph architecture. Additionally, we propose a novel contrastive learning paradigm that adaptively explores the benefits of multi-behavior contrastive self-supervised augmentation, thereby improving the model's robustness against data sparsity. Through extensive experiments conducted on three public datasets, we demonstrate the effectiveness of DHCF, which significantly outperforms various strong baselines with competitive efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=m3xVPaZp6Z": {
    "title": "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
    "volume": "review",
    "abstract": "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CHGcP6lVWd": {
    "title": "Energy-based Automated Model Evaluation",
    "volume": "review",
    "abstract": "The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real-world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure --- Meta-Distribution Energy (MDE) that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=iS5ADHNg2A": {
    "title": "Deceptive Fairness Attacks on Graphs via Meta Learning",
    "volume": "review",
    "abstract": "We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CTlUHIKF71": {
    "title": "What Matters to You? Towards Visual Representation Alignment for Robot Learning",
    "volume": "review",
    "abstract": "When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through the lens of preference-based learning and optimal transport. Across experiments in X-MAGICAL and in robotic manipulation, we find that RAPL's reward consistently generates preferred robot behaviors with high sample efficiency, and shows strong zero-shot generalization when the visual representation is learned from a different embodiment than the robot's",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=eIYDKNqXuV": {
    "title": "Village-Net clustering: A novel unsupervised manifold clustering method",
    "volume": "review",
    "abstract": "We present \"Village-Net Clustering,\" a novel unsupervised clustering algorithm designed for effectively clustering complex manifold data. The algorithm operates in two primary phases: first, utilizing K-Means clustering, it divides the dataset into distinct \"villages.\" Subsequently, a weighted network is created, where each node represents a village, capturing their proximity relationships. To attain the optimal clustering, we cluster this network using the Walk-likelihood Community Finder (WLCF), a community detection algorithm developed by one of our team members. An important feature of Village-Net Clustering is its ability to autonomously determine the optimal number of cluster. Extensive benchmarking on real datasets with known ground-truth labels showcases its competitive performance, particularly in terms of the normalized mutual information (NMI) score, when compared to state-of-the-art methods. Additionally, the algorithm demonstrates impressive computational efficiency, boasting a time complexity of O(N*k*d), where N signifies the number of instances, k represents the number of villages and d represents the dimension of the dataset, making it well-suited for effectively handling large-scale datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=HvTJLthEGQ": {
    "title": "Zero-shot Clustering of Embeddings with Pretrained and Self-Supervised Learning Encoders",
    "volume": "review",
    "abstract": "In this work, we explore whether pretrained models can provide a useful representation space for datasets they were not trained on, and whether these representations can be used to group novel unlabelled data into meaningful clusters. To this end, we conduct experiments using image representation encoders pretrained on ImageNet using either supervised or self-supervised training techniques. These encoders are deployed on image datasets that were not seen during training, and we investigate whether their embeddings can be clustered with conventional clustering algorithms. We find that it is possible to create well-defined clusters using self-supervised feature encoders, especially when using the agglomerative clustering method, and that it is possible to do so even for very fine-grained datasets such as iNaturalist. We also find indications that the Silhouette score is a good proxy of cluster quality for self-supervised feature encoders when no ground truth is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=aD4YLji1PW": {
    "title": "Genetic Algorithm for Curriculum Generation in Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "As the deployment of autonomous agents increases in real life, there is an increased interest in extending their usage to competitive environments populated by other robots. Self-play in Reinforcement Learning (RL) allows agents to explore and learn competitive strategies. However, the complex dynamics of multi-agent RL interactions introduce instability in training and susceptibility to overfitting. Several game-theoretic approaches address the latter by generating approximate Nash equilibrium strategies to train against. The challenge of learning a policy in a complex and unstable multi-agent environment, the former, is not yet well addressed. This paper aims to address this issue by using a curriculum learning approach. We introduce curriculum design by a genetic algorithm to the multi-agent domain to more efficiently learn a policy that performs well and is stable at Nash equilibrium. Empirical studies show that our approach outperforms several strong baselines across various competitive two-player benchmarks in continuous control settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=PsRL00864k": {
    "title": "Correct and speak: accent reduction with minimum supervision",
    "volume": "review",
    "abstract": "Accent conversion(AC) aims to convert non-native accented speech to native speech by changing the pronunciation pattern and prosody of source speakers while preserving linguistic content and speaker identity. This problem is quite challenging since 1) the parallel data with same speaker speaking the same content in different accent is rarely existed; 2) the accent features not only affect the prosody but also corrupt the pronunciation units in some heavy accents like Indian accent. In this work, we propose a new framework with a correction module and speaking module based on speech generative models in which the accent removal is achieved by correcting the source accented semantic tokens to the target native ones. Specifically, a separate sequence-to-sequence task based on autoregressive decoder-only transformer has been designed to accomplish the correction. Conditioned on this corrected semantic token, a speech generative model based on TF-Codec, trained with large amounts of native speech has been proposed to generate speech with native prosody. Different from multi-stage generation used in other generative models, we use a single-stage autoregressive generation to reduce the complexity and latency of the generation process. To relieve the dependence of the parallel data, we pretrain the correction module with a pretext task in a self-supervised manner using large amounts of native speech to learn the probability space of the target native semantic tokens first so that small amounts of parallel data are needed to learn the mapping of specific corrupted pronunciation units with their native targets. Experimental results show the proposed framework achieved the state-of-the-art performance in terms of accentedness, speech quality and speaker maintanence. With the pretraining, only 15 minutes of parallel data which is not constrained to the same speaker are required to achieve a good correction quality. The proposed generative model also achieves higher speech quality and speaker similarity with lower complexity and latency(50 AR steps/1 sec of audio) compared with multi-stage speech generation methods(75 AR steps+7 NAR steps/1 sec of audio). With less supervision from parallel data, this framework can be easily extended to other accents with low-resource data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzNaCp6Vcg": {
    "title": "PINNACLE: PINN Adaptive ColLocation and Experimental points selection",
    "volume": "review",
    "abstract": "Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or simulations. Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics. Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses. PINNACLE uses information on the interactions among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK). We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and transfer learning problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=kjn99xFUF3": {
    "title": "FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization",
    "volume": "review",
    "abstract": "Federated learning (FL) is an emerging learning paradigm in which a set of distributed clients learns a task under the coordination of a central server. The FedAvg algorithm is one of the most widely used methods to solve FL problems. In FedAvg, the learning rate is a constant rather than changing adaptively. Adaptive gradient methods have demonstrated superior performance over the constant learning rate schedules in non-distributed settings, and they have recently been adapted to FL. However, the majority of these methods are designed for unconstrained settings. Meanwhile, many crucial FL applications, like disease diagnosis and biomarker identification, often rely on constrained formulations such as Lasso and group Lasso. It remains an open question as to whether adaptive gradient methods can be effectively applied to FL problems with constrains. In this work, we introduce \\textbf{FedDA}, a novel adaptive gradient framework for FL. This framework utilizes a restarted dual averaging technique and is compatible with a range of gradient estimation methods and adaptive learning rate schedules. Specifically, an instantiation of our framework \\textbf{FedDA-MVR} achieves gradient complexity $\\tilde{O}(K^{-1}\\epsilon^{-1.5})$ and communication complexity $\\tilde{O}(K^{-0.25}\\epsilon^{-1.25})$ for finding a stationary point $\\epsilon$ in the constrained setting. We conduct experiments over both constrained and unconstrained tasks to confirm the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUKJWr5zOE": {
    "title": "A Differentiable Physical Simulation Framework for Soft Robots on Multiple-Task Learning",
    "volume": "review",
    "abstract": "Learning multiple tasks is challenging for soft robots. Differentiable physics enables efficient gradient-based optimizations of neural network (NN) controllers for soft robot learning. However, existing work typically delivers NN controllers with limited capability and generalizability. We present a practical learning framework that outputs unified NN controllers capable of multiple tasks with significantly improved complexity and diversity. Our framework consists of a high-performance differentiable deformable bodies simulator supporting the material point method (MPM) and mass-spring systems, an automatic differentiation module that enables gradient-based optimizations, and a practical training module for soft robots on learning multiple locomotion tasks with a single NN controller. Using a unified NN controller trained in our framework, we demonstrate that users can interactively control soft robot locomotion and switch among multiple goals with specified velocity, height, and direction instructions. We evaluate our framework with multiple robot designs and challenging locomotion tasks. Experiments show that our learning framework, based on differentiable physics, delivers better results and converges much faster, compared with reinforcement learning frameworks. In addition, we successfully employed our framework on learning manipulation tasks, indicating the potential to extend our framework to tasks beyond locomotion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ujgouOiAA": {
    "title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that our INSTINCT consistently outperforms the existing methods in different tasks, such as in various instruction induction tasks and the task of improving the zero-shot chain-of-thought instruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=kTRGF2JEcx": {
    "title": "Instructing Large Language Models to Identify and Ignore Irrelevant Conditions",
    "volume": "review",
    "abstract": "Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions. Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy. In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions. It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question. Then it prompts LLMs to verify the irrelevant conditions. Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths. Moreover, we propose to select (problem, reasoning paths)-pairs as demonstrations to enhance I$^3$C with few-shot reasoning. We develop I$^3$C-Select that selects the most confusing problems based on the semantic relevance measurement. We conduct extensive experiments on six MWP datasets. I$^3$C can be combined with any CoT prompting methods to improve the performance of solving MWPs. Notably, I$^3$C-Select achieves an accuracy of $93.7$ and $90.9$ on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Auto-CoT by $+19.4$ and $+25.7$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5M2MjyNR2w": {
    "title": "Adaptive Expansion for Hypergraph Learning",
    "volume": "review",
    "abstract": "Hypergraph, with its powerful ability to capture higher-order complex relationships, has attracted substantial attention recently. Consequently, an increasing number of hypergraph neural networks (HyGNNs) have emerged to model the high-order relationships among nodes and hyperedges. In general, most HyGNNs leverage typical expansion methods, such as clique expansion (CE), to convert hypergraphs into graphs for representation learning. However, they still face the following limitations in hypergraph expansion: (i) Some expansion methods expand hypergraphs in a straightforward manner, resulting in information loss and redundancy; (ii) Most expansion methods often employ fixed edge weights while ignoring the fact that nodes having similar attribute features within the same hyperedge are more likely to be connected compared with nodes with dissimilar features. In light of these challenges, we design a novel CE-based \\textbf{Ad}aptive \\textbf{E}xpansion method called \\textbf{AdE} to expand hypergraphs into weighted graphs that preserve the higher-order hypergraph structure information. Specifically, we first introduce a Global Simulation Network to pick two representative nodes for symbolizing each hyperedge in an adaptive manner. We then connect the rest of the nodes within the same hyperedge to the corresponding selected nodes. Instead of leveraging the fixed edge weights, we further design a distance-aware kernel function to dynamically adjust the edge weights to make sure that node pairs having similar attribute features within the corresponding hyperedge are more likely to be connected with large weights. After obtaining the adaptive weighted graphs, we employ graph neural networks to model the rich relationships among nodes for downstream tasks. Extensive theoretical justifications and empirical experiments over five benchmark hypergraph datasets demonstrate that AdE has excellent rationality, generalization, and effectiveness compared to classic expansion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=sP0Aev2Gis": {
    "title": "G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System",
    "volume": "review",
    "abstract": "Text-based delivery addresses, as the data foundation for logistics systems, contain abundant and crucial location information. How to effectively encode the delivery address is a core task to boost the performance of downstream tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural Language Process (NLP) have emerged as the dominant tools for encoding semantic information in text. Though promising, those NLP-based PTMs fall short of encoding geographic knowledge in the delivery address, which considerably trims down the performance of delivery-related tasks in logistic systems such as Cainiao. To tackle the above problem, we propose a domain-specific pre-trained model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in Logistics field. G2PTL combines the semantic learning capabilities of text pre-training with the geographical-relationship encoding abilities of graph modeling. Specifically, we first utilize real-world logistics delivery data to construct a large-scale heterogeneous graph of delivery addresses, which contains abundant geographic knowledge and delivery information. Then, G2PTL is pre-trained with subgraphs sampled from the heterogeneous graph. Comprehensive experiments are conducted to demonstrate the effectiveness of G2PTL through four downstream tasks in logistics systems on real-world datasets. G2PTL has been deployed in production in Cainiao's logistics system, which significantly improves the performance of delivery-related tasks. The code of G2PTL is available at https://huggingface.co/Cainiao-AI/G2PTL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7Ttk3RzDeu": {
    "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
    "volume": "review",
    "abstract": "Summarizing book-length documents ($>$100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving \\$15K and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators. We release code and annotations after blind review to spur more principled research on book-length summarization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.5,
    "authors": []
  },
  "https://openreview.net/forum?id=qT7DXUmX7j": {
    "title": "NP-GL: Extending Power of Nature from Binary Problems to Real-World Graph Learning",
    "volume": "review",
    "abstract": "Nature performs complex computations constantly at clearly lower cost and higher performance than digital computers. It is crucial to understand how to harness the unique computational power of nature in Machine Learning (ML). In the past decade, besides the development of Neural Networks (NNs), the community has also relentlessly explored nature-powered ML paradigms. Although most of them are still predominantly theoretical, a new practical paradigm enabled by the recent advent of CMOS-compatible room-temperature nature-based computers has emerged. By harnessing the nature's power of entropy increase, this paradigm can solve binary learning problems delivering immense speedup and energy savings compared with NNs, while maintaining comparable accuracy. Regrettably, its values to the real world are highly constrained by its binary nature. A clear pathway to its extension to real-valued problems remains elusive. This paper aims to unleash this pathway by proposing a novel end-to-end Nature-Powered Graph Learning (NP-GL) framework. Specifically, through a three-dimensional co-design, NP-GL can leverage the nature's power of entropy increase to efficiently solve real-valued graph learning problems. Experimental results across 4 real-world applications with 6 datasets demonstrate that NP-GL delivers, on average, $6.97\\times 10^3$ speedup and $10^5$ energy consumption reduction with comparable or even higher accuracy than Graph Neural Networks (GNNs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=e5lR6tySR7": {
    "title": "Transformer-Based Large Language Models Are Not General Learners: A Universal Circuit Perspective",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across diverse tasks, evoking perceptions of ``sparks of Artificial General Intelligence (AGI)\" (Bubeck et al., 2023). A key question naturally arises: *Can foundation models lead to AGI?* In this work, we try to answer this question partially by formally considering the capabilities of Transformer-based LLMs (T-LLMs) from the perspective of universal circuits. By investigating the expressive power of realistic T-LLMs as universal circuits, we show that a T-LLM of size $\\operatorname{poly}(n)$ cannot perform all the basic operators of input length $O\\left(\\operatorname{poly}(\\log n)\\right)$. We also demonstrate that a constant-depth-$\\operatorname{poly}(n)$-size log-precision T-LLM cannot faithfully execute prompts of complexity $n$. Our analysis provides a concrete theoretical foundation that T-LLMs can only be universal circuits for limited function classes, or in other words, T-LLMs are not general learners. Furthermore, we exhibit that a constant-depth-$\\operatorname{poly}(n)$-size log-precision T-LLM can memorize $O\\left(\\operatorname{poly}(n)\\right)$ instances, which could partially explain the seeming inconsistency between LLMs' empirical successes and our negative results. To the best of our knowledge, our work takes the first step towards analyzing the limitations of T-LLMs as general learners within a rigorous theoretical framework. Our results promote the understanding of LLMs' capabilities and highlight the need for innovative architecture designs beyond Transformers to break current limitations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WwCirclMvl": {
    "title": "Posterior Sampling via Langevin Monte Carlo for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "In this paper, we consider offline reinforcement learning (RL) problems. Within this setting, posterior sampling has been rarely used, perhaps partly due to its explorative nature. The only work using posterior sampling for offline RL that we are aware of is the model-based posterior sampling of \\cite{uehara2021pessimistic}. However, this framework does not permit any tractable algorithm (not even in the linear models) where simulations of posterior samples become challenging, especially in high dimensions. In addition, the algorithm only admits a weak form of guarantees -- Bayesian sub-optimality bounds which depend on the prior distribution. To address these problems, we propose and analyze the use of Markov Chain Monte Carlo methods for offline RL. We show that for low-rank Markov decision processes (MDPs), using the Langevin Monte Carlo (LMC) algorithm, our algorithm obtains the (frequentist) sub-optimality bound that competes against any comparator policy $\\pi$ and interpolates between $\\tilde{\\mathcal{O}}(H^2 d \\sqrt{C_{\\pi}/ K})$ and $\\tilde{\\mathcal{O}}(H^2 \\sqrt{d C_{\\pi}/ K})$, where $C_{\\pi}$ is the concentrability coefficient of $\\pi$, $d$ is the dimension of the linear feature, $H$ is the episode length, and $K$ is the number of episodes in the offline data. For general MDPs with overparameterized neural network function approximation, we show that our LMC-based algorithm obtains the sub-optimality bounds of $\\tilde{\\mathcal{O}}(H^{2.5} \\tilde{d} \\sqrt{C_{\\pi} /K})$, where $\\tilde{d}$ is the effective dimension of the neural network. Finally, we collaborate our findings with numerical evaluations to demonstrate that LMC-based algorithms could be both efficient and competitive for offline RL in high dimensions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=6cGiRiExUd": {
    "title": "Efficient Point Cloud Matching for 3D Geometric Shape Assembly",
    "volume": "review",
    "abstract": "Learning to assemble geometric shapes into a larger target structure is a fundamental task with various high-level visual applications. In this work, we frame this problem as geometric registration with extremely low overlap. Our goal is to establish accurate correspondences on the mating surface of the shape fragments to predict their relative rigid transformations for assembly. To this end, we introduce Proxy Match Transform (PMT), an approximate high-order feature transform layer that enables reliable correspondences between dense point clouds of shape fragments, while incurring low costs in memory and compute. In our experiments, we demonstrate that Proxy Match Transform surpasses existing state-of-the-art baselines on a popular geometric shape assembly dataset, while exhibiting higher efficiency than other high-order feature transform methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2uwvigLUr8": {
    "title": "From Deterministic to Probabilistic World: Balancing Enhanced Doubly Robust Learning for Debiased Recommendation",
    "volume": "review",
    "abstract": "In recommender systems, selection bias arises from the users' selective interactions with items, which poses a widely-recognized challenge for unbiased evaluation and learning for recommendation models. Recently, doubly robust and its variants have been widely studied to achieve debiased learning of prediction models, which enables unbiasedness when either imputed errors or learned propensities are accurate. However, we find that previous studies achieve unbiasedness using the doubly robust learning approaches are all based on deterministic error imputation model and deterministic propensity model, and these approaches fail to be unbiased when using probabilistic models to impute errors and learn propensities. To tackle this problem, in this paper, we first derive the bias of doubly robust learning methods and provide alternative unbiasedness conditions for probabilistic models. Then we propose a novel balancing enhanced doubly robust joint learning approach, which improves the accuracy of the imputed errors and leads to unbiased learning under probabilistic error imputations and learned propensities. We further derive the generalization error bound when using the probabilistic models, and show that it can be effectively controlled by the proposed learning approach. We conduct extensive experiments on three real-world datasets, including a large-scale industrial dataset, to demonstrate the effectiveness of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=4bat0pSQBq": {
    "title": "FLOOD SIMULATION WITH PHYSICS-INFORMED MESSAGE PASSING",
    "volume": "review",
    "abstract": "Flood modeling is an important tool for supporting preventive and emergency measures to mitigate flood risks. Recently, there has been an increasing interest in exploring machine learning-based models as an alternative to traditional hydrodynamic models for flood simulation to address challenges such as scalability and accuracy. However, current ML approaches are ineffective at modeling early stages of flooding events, limiting their ability to simulate the entire evolution of the flood. Another key challenge is how to incorporate physics domain-knowledge into these data-driven models. In this paper, we address these challenges by introducing a physics-inspired graph neural network for flood simulation. Given a (geographical) region and precipitation data, our model predicts water depths in an autoregressive fashion. We propose a message-passing framework inspired by the conservation of momentum and mass expressed in the shallow-water equations, which describe the physical process of a flooding event. Empirical results on a dataset covering 9 regions and 7 historical precipitation events demonstrate that our model outperforms the best baseline, and is able to capture the propagation of water flow better, especially at the very early stage of the flooding event",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=HrTGl8AhnS": {
    "title": "PACIA: Parameter-Efficient Adapter for Few-Shot Molecular Property Prediction",
    "volume": "review",
    "abstract": "Molecular property prediction (MPP) plays a crucial role in biomedical applications, but it often encounters challenges due to a scarcity of labeled data. Existing works commonly adopt gradient-based strategy to update a large amount of parameter for property-level adaptation. However, the increase of adaptive parameters can cause overfitting and lead to poor performance. Observing that graph neural network (GNN) performs well as both encoder and predictor, we propose PACIA, a parameter-efficient GNN adapter for few-shot MPP. We design a unified adapter to generate a few adaptive parameters to modulate the message passing process of GNN. We then adopt hierarchical adaptation mechanism to adapt the encoder on property-level and the predictor on molecule-level by the unified GNN adapter. Extensive results show that PACIA obtains the state-of-the-art performance in few-shot MPP problems, and our proposed hierarchical adaptation mechanism is rational and effective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEGUT3OGCW": {
    "title": "Provable Repair of Vision Transformers: Last Layer is All You Need",
    "volume": "review",
    "abstract": "Vision Transformers have emerged as state-of-the-art image recognition tools, but may still exhibit incorrect behavior. Incorrect image recognition can have disastrous consequences in safety-critical real-world applications such as self-driving automobiles. In this paper, we present Provable Repair of Vision Transformers (PRoViT), a provable repair approach that guarantees the correct classification of images in a repair set for a given Vision Transformer without modifying its ar- chitecture. PRoViT avoids negatively affecting correctly classified images (draw- down) by minimizing the changes made to the Vision Transformer's parameters and original output. We observe that for Vision Transformers, unlike for other architectures such as ResNet or VGG, editing just the parameters in the last layer achieves correctness guarantees and very low drawdown. We introduce a novel method for editing these last-layer parameters that enables PRoViT to efficiently repair state-of-the-art Vision Transformers for thousands of images, far exceeding the capabilities of prior provable repair approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=jhCzPwcVbG": {
    "title": "LLMZip: Lossless Text Compression using Large Language Models",
    "volume": "review",
    "abstract": "We design a lossless compression algorithm for compressing English text by using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. Specifically, the proposed LLMZip algorithm uses the conditional probabilities at the output of the large language model in conjunction with Arithmetic Coding. We show that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h. We show that it is possible to marginally improve the compression performance further by first extracting a summary from the document and compressing the text by conditioning on the summary. Finally, we investigate the compression performance of LLMZip when the summary (side information) is available both at the encoder and decoder. We show that the LLM is able to exploit the available side information to significantly improve the compression performance. As an important byproduct, we provide new estimates of an asymptotic upper bound on the entropy of English which is significantly smaller than currently available estimates in \\cite{cover1978convergent}, \\cite{lutati2023focus}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UK7Hs7f0So": {
    "title": "VMFTransformer: An Angle-Preserving and Auto-Scaling Machine for Multi-horizon Probabilistic Forecasting",
    "volume": "review",
    "abstract": "Time series forecasting has historically been a key area of academic research and industrial applications. As deep learning develops, the major research methodologies of time series forecasting can be divided into two categories, i.e., iterative and direct methods. In the iterative methods, since a small amount of error is produced at each time step, the recursive structure can potentially lead to large error accumulations over longer forecasting horizons. Although the direct methods can avoid this puzzle involved in the iterative methods, it faces abuse of conditional independence among time points. This impractical assumption can also lead to biased models. To solve these challenges, we propose a direct approach for multi-horizon probabilistic forecasting, which can effectively characterize the dependence across future horizons. Specifically, we consider the multi-horizon target as a random vector. The direction of the vector embodies the temporal dependence, and the length of the vector measures the overall scale across each horizon. Therefore, we respectively apply the von Mises-Fisher (VMF) distribution and the truncated normal distribution to characterize the angle and the magnitude of the target vector in our model. We evaluate the performance of our framework on three benchmarks. Extensive results demonstrate the superiority of our framework over six state-of-the-art methods and show the remarkable versatility and extensibility for different time series forecasting tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ElykcDu5YK": {
    "title": "Leveraging Previous Tasks in Optimizing Risk Measures with Gaussian Processes",
    "volume": "review",
    "abstract": "Research on optimizing the risk measure of a blackbox function using Gaussian processes, especially Bayesian optimization (BO) of risk measures, has become increasingly important due to the inevitable presence of uncontrollable variables in real-world applications. Nevertheless, existing works on BO of risk measures start the optimization from scratch for every new task without considering the results of previous tasks. In contrast, its vanilla BO counterpart has received a thorough investigation on utilizing previous tasks to speed up the current task through the body of works on meta-BO which, however, have not considered risk measures. To bridge this gap, this paper presents the first algorithm for meta-BO of risk measures (i.e., value-at-risk (VaR) and the conditional VaR) by introducing a novel adjustment to the upper confidence bound acquisition function. Our proposed algorithm exhibits two desirable properties: (i) invariance to scaling and vertical shifting of the blackbox function and (ii) robustness to previous harmful tasks. We provide a theoretical performance guarantee for our algorithm and empirically demonstrate its performance using several synthetic function benchmarks and real-world objective functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=qxGXjWxabq": {
    "title": "Canonpipe: Data Debugging with Shapley Importance over Machine Learning Pipelines",
    "volume": "review",
    "abstract": "When a machine learning (ML) model exhibits poor quality (e.g., poor accuracy or fairness), the problem can often be traced back to errors in the training data. Being able to discover the data examples that are the most likely culprits is a fundamental concern that has received a lot of attention recently. One prominent way to measure \"data importance\" with respect to model quality is the Shapley value. Unfortunately, existing methods only focus on the ML model in isolation, without considering the broader ML pipeline for data preparation and feature extraction, which appears in the majority of real-world ML code. This presents a major limitation to applying existing methods in practical settings. In this paper, we propose Canonpipe, a method for efficiently computing Shapley-based data importance over ML pipelines. We introduce several approximations that lead to dramatic improvements in terms of computational speed. Finally, our experimental evaluation demonstrates that our methods are capable of data error discovery that is as effective as existing Monte Carlo baselines, and in some cases even outperform them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRXxnoqDHH": {
    "title": "MoAT: Multi-Modal Augmented Time Series Forecasting",
    "volume": "review",
    "abstract": "Time series forecasting plays a pivotal role in various domains, facilitating optimized resource allocation and strategic decision-making. However, the scarcity of training samples often hinders the accuracy of the forecasting task. To address this, we explore the potential of leveraging information from different modalities that are commonly associated with time series data. In this paper, we introduce MoAT, a novel multi-modal augmented time series forecasting approach that strategically integrates both feature-wise and sample-wise augmentation methods to enrich multi-modal representation learning. It further enhances prediction accuracy through joint trend-seasonal decomposition across all modalities and fuses the information for the final prediction. Extensive experiments show that MoAT outperforms state-of-the-art methods, resulting in a substantial reduction in mean squared error ranging from 6.5% to 71.7%, which demonstrates the effectiveness and robustness in addressing the limitations imposed by data scarcity. The datasets and code are available at https://anonymous.4open.science/r/MoAT-201E",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5t57omGVMw": {
    "title": "Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances",
    "volume": "review",
    "abstract": "Solving a linear system ${\\bf Ax}={\\bf b}$ is a fundamental scientific computing primitive, and numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved but are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used instead. We consider the common setting in which many related linear systems are solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation~(SOR), a standard solver whose parameter $\\omega$ has a strong impact on its runtime. For this method, we prove that a bandit algorithm—using only the number of iterations as feedback—can select parameters for a sequence of instances such that the overall cost is almost as good as that the best fixed $\\omega$ would have obtained. Furthermore, when given additional structural information, we show that a {\\em contextual} bandit method approaches the performance of the {\\em instance-optimal} policy, which selects the best $\\omega$ for each instance. Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqqE1yS3RY": {
    "title": "Towards Better Evaluation of GNN Expressiveness with BREC Dataset",
    "volume": "review",
    "abstract": "Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, unifying all kinds of models into one framework is untractable, making it hard to measure and compare their expressiveness quantitatively. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100\\% accuracy), granularity (models tend to be either 100\\% correct or near random guess), and scale (only several essentially different graphs in each dataset). To address these limitations, we propose a new expressiveness dataset, **BREC**, including 400 pairs of non-isomorphic graphs carefully selected from four primary categories (Basic, Regular, Extension, and CFI). These graphs have higher difficulty (up to 4-WL-indistinguishable), finer granularity (can compare models between 1-WL and 3-WL), and a larger scale (400 pairs or extend to 319600 pairs or even more). Further, we synthetically test 23 models with higher-than-1-WL expressiveness on our BREC dataset. Our experiment gives the first thorough measurement of the expressiveness of those state-of-the-art beyond-1-WL GNN models and reveals the gap between theoretical and practical expressiveness. We expect this dataset to serve as a benchmark for testing the expressiveness of future GNNs. Dataset and evaluation codes are released at: https://github.com/brec-iclr2024/brec-iclr2024",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=4kLVvIh8cp": {
    "title": "Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation. Our work extends the previous instance-dependent results within simpler function classes, such as linear and differentiable function to a more general framework. To the best of our knowledge, this is the first statistically optimal algorithm for nonlinear offline RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=TC9r8gsaoh": {
    "title": "Nuisance-Robust Weighting Network for End-to-End Causal Effect Estimation",
    "volume": "review",
    "abstract": "We combine the two major approaches to causal inference: the conventional statistical approach based on weighting and the end-to-end learning with adversarial networks. Causal inference concerns the expected loss in a distribution different from the training distribution due to intervening on the input variables. Recently, the representation balancing approach with neural networks has repeatedly demonstrated superior performance for complex problems, owing to its end-to-end modeling by adversarial formulation. However, some recent work has shown that the limitation lies in the unrealistic theoretical assumption of the invertibility of the representation extractor. This inherent difficulty stems from the fact that the representation-level discrepancy in representation balancing accounts only for the uncertainty of the later layers than the representation, i.e., the hypothesis layers and the loss. Therefore, we shed light once again on the conventional weighting-based approach, retaining the spirit of end-to-end learning. Most conventional statistical methods are based on inverse probability weighting using propensity scores, which involves nuisance estimation of propensity as an intermediate step. They often suffer from inaccurate estimation of the propensity scores and instability due to large weights. One might be tempted to jointly optimize the nuisance and the target, though it may lead to an optimistic evaluation, e.g., avoiding noisy instances by weighting less when noise levels are heterogeneous. In this paper, we propose a simple method that amalgamates the strengths of both approaches: adversarial joint optimization of the nuisance and the target. Our formulation follows the pessimistic evaluation principle in offline reinforcement learning, which brings provable robustness to the estimation uncertainty of the nuisance and the instability due to extreme weights. Our method performed consistently well under challenging settings with heterogeneous noise. Our code is available online: https://anonymous.4open.science/r/NuNet-002A",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jf5gplvglq": {
    "title": "Skill-Mix: a Flexible and Expandable Family of Evaluations for AI Models",
    "volume": "review",
    "abstract": "As the role of LLMs shifts from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. This capability to combine skills plays an important role in (human) pedagogy and also in a recent paper on emergence phenomena (Arora & Goyal, 2023). Our paper introduces an evaluation, Skill-Mix, to measure this capability. Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text it has not seen in the training set. The paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using the open LLaMA-2 70b model as well as GPT-4. Administering a version of Skill-Mix to popular chatbots gave results that, while generally in line with prior expectations, contained surprises. We found sizeable differences in capabilities among models ---including suspected cases of ``cramming for the leaderboard''--- that had not been revealed by the (much simpler) evaluations used in popular LLM leaderboards. Our methodology can flexibly change to future models and model capabilities, by expanding the set of skills being tested and increasing $k$. We hope Skill-Mix (which will be publicly released, including all prompts and code) may grow into an eco-system of open evaluations for AI capabilities, including in multi-modal settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=yroyhkhWS6": {
    "title": "A Quadratic Synchronization Rule for Distributed Deep Learning",
    "volume": "review",
    "abstract": "In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for $H$ steps without synchronizing with others, hence reducing communication frequency. While $H$ has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper $H$ value can lead to generalization improvement. Yet, selecting a proper $H$ is elusive. This work proposes a theory-grounded method for determining $H$, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting $H$ in proportion to $\\frac{1}{\\eta^2}$ as the learning rate $\\eta$ decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared to the standard data parallel training, QSR enables Local AdamW to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.16% or 0.84% higher top-1 validation accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=e2YOVTenU9": {
    "title": "ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor",
    "volume": "review",
    "abstract": "Deep neural network (DNN) models, despite their impressive performance, are vulnerable to exploitation by attackers who attempt to adapt them to other tasks for their own benefit. Current defense strategies mainly address this vulnerability at the model parameter level, leaving the potential of architectural-level defense largely unexplored. This paper, for the first time, addresses the issue of model protection by reducing transferability at the architecture level. Specially, we present a novel neural architecture search (NAS)-enabled algorithm that employs zero-cost proxies and evolutionary search, to design model architectures with low transferability. Our method, namely ArchLock, aims to achieve high performance on the source task, while degrading the performance on target tasks, i.e., locking the transferability of a DNN model. To achieve efficient cross-task search without having access to the training data owned by the attackers, we utilize zero-cost proxies to speed up architecture evaluation and simulate potential target task embeddings to assist cross-task search with a binary performance predictor. Extensive experiments on NAS-Bench-201 and TransNAS-Bench-101 demonstrate that ArchLock reduces transferability by up to 30\\% and 50%, respectively, with negligible performance degradation on source tasks (<2%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=fjRM5ozPv9": {
    "title": "Local-Forward: Towards Biological Plausibility in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "A lasting critique of deep learning as a model for biological intelligence and learning is the biological implausibility of backpropagation. Backpropagation requires caching local outputs and propagating a global error via derivatives, neither of which are known to be implemented by biological neurons. In reinforcement learning, building more biologically plausible agents would allow us to better model human cognition and social behavior, and improve computational efficiency. We propose Local-Forward, a new temporal-difference learning algorithm (and associated architecture) that trains neural networks to predict Q-values. Rather than backpropagating error derivates, we rely on updates that are local to each layer of the architecture and additionally use forward connections in time to pass information from upper layers to lower layers via activations. Our approach builds on the recently proposed Forward-Forward algorithm, as well as recurrence and attention in neural architectures. This approach no longer suffer the aforementioned contradictions with biology. Furthermore, as a proof-of-concept, we train reinforcement learning agents with Local-Forward to solve control tasks in the MinAtar environments, and show that our method's potential warrants further investigation because it opens avenues for more computational efficient training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDMM4ZX1UB": {
    "title": "Exploiting Code Symmetries for Learning Program Semantics",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) hold significant potential for automating program analysis, but current code LLMs face challenges in grasping program semantics. Our paper addresses this by formalizing program semantics through code symmetries and integrating them into LLM architectures for code analysis. We introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, enabling precise reasoning within LLMs. Our solution, SymC, employs a novel variant of group-equivariant self-attention that is provably equivariant to code symmetries. We extensively evaluate SymC on four program analysis tasks, comparing it to eight baselines against eight code transformations. Our results show that SymC generalizes to unseen code transformations, outperforming the state-of-the-art code models by 30.7%. SymC, by design, stays invariant to semantics-preserving permutations, while state-of-the-art code models like WizardCoder and GPT-4 violate these invariances at a high rate (i.e., 14% and 43%, respectively)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=j2AWbl4L3K": {
    "title": "Weight Uncertainty in Individual Treatment Effect",
    "volume": "review",
    "abstract": "The estimation of individual treatment effects (ITE) has recently gained significant attention from both the research and industrial communities due to its potential applications in various fields such as healthcare, economics, and education. However, the sparsity of observational data often leads to a lack of robustness and over-fitting in most existing methods. To address this issue, this paper investigates the benefits of incorporating uncertainty modeling in the process of optimizing parameters for robust ITE estimation. Specifically, we derive an informative generalization bound that connects to Bayesian inference and propose a variational bound in closed form to learn a probability distribution on the weights of a hypothesis and representation function. Through experiments on one synthetic dataset and two benchmark datasets, we demonstrate the effectiveness of our proposed model in comparison to state-of-the-art methods. Moreover, we conduct experiments on a real-world dataset in recommender scenarios to verify the benefits of uncertainty in causal inference. The results of our experiments provide evidence of the practicality of our model, which aligns with our initial expectations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=DE7IVrk8Ks": {
    "title": "Latent Shattering: Turning Unconditional Pretrained Generators Into Conditional Models By Imposing Latent Structure",
    "volume": "review",
    "abstract": "Deep generative models, such as GANs and VAEs, have gained substantial attention for their ability to synthesize realistic data. Pretrained generative models are often unconditional, thus do not easily allow the user to specify the class of the output. Yet supporting conditional generation offers inherent benefits for many tasks. Due to current models requiring huge data sets and often prohibitively expensive computational resources for training, it is desirable to have a lightweight method that can convert pretrained unconditional generators into conditional models without retraining. Previous research into this problem is limited, typically assuming either access to classifiers that identify which regions of the generator's latent space correspond to specific classes, access to labeled data, or even retraining of the generative model itself. These strict requirements pose a serious limitation. In this work, we propose LASH, a fresh approach at the conversion of unconditional generators into conditional models in a completely unsupervised manner without requiring retraining nor access to any real data. Instead, the key principle of LASH is to identify points in the generator's latent space that are mapped to low-density regions of the output space. The insight is that by removing these points, LASH \"shatters\" the latent space into distinct clusters where each cluster corresponds to a semantically meaningful mode in the output space. We demonstrate that these modes correspond to distinct real-world classes. Lastly, LASH utilizes a simple Gaussian mixture model to adaptively sample from these clusters, supporting unsupervised conditional generation. Through a series of experiments on MNIST, FashionMNIST, and CelebA, we demonstrate that LASH significantly outperforms existing methods in unsupervised conditional sampling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=DVA0NDUdCQ": {
    "title": "Efficient Large Language Models Fine-Tuning on Graphs",
    "volume": "review",
    "abstract": "Learning from Text-Attributed Graphs (TAGs) has attracted significant attention due to its wide range of real-world applications. The rapid evolution of large language models (LLMs) has revolutionized the way we process textual data, which indicates a strong potential to replace shallow text embedding generally used in Graph Neural Networks (GNNs). However, we find that existing LLM approaches that exploit text information in graphs suffer from inferior computation and data efficiency. In this work, we introduce a novel and efficient approach for the end-to- end fine-tuning of Large Language Models (LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost and memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover, it transfers the rick knowledge in LLMs to downstream graph learning tasks effectively with limited labeled data in semi-supervised learning. Its superior computation and data efficiency are demonstrated through comprehensive experiments, offering a promising solution for a wide range of LLMs and graph learning tasks on TAGs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RwhRZojoYw": {
    "title": "On information dropping and oversmoothing in graph neural networks",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) are widespread in graph representation learning. *Random dropping* approaches, notably DropEdge and DropMessage, claim to alleviate the key issues of overfitting and oversmoothing by randomly removing elements of the graph representation. However, their effectiveness is largely unverified. In this work, we find experimentally that they have a limited effect in reducing oversmoothing, contrary to what is typically assumed in the literature. These approaches are also non-parametric and motivate us to question if *learned* dropping can alleviate the propagation of redundant or noisy edges. We propose a new information-theoretic approach, in which we learn to perform dropping on the data exchanged by nodes during message passing via optimizing an information bottleneck. Our approach is superior to previous dropping methods in oversmoothing reduction and has promising performance in the case of deep GNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ibggY9ZJ1T": {
    "title": "HuRef: HUman-REadable Fingerprint for Large Language Models",
    "volume": "review",
    "abstract": "Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations through fine-tuning or continued pretraining. In this study, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly changes it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model. We make these invariant terms human-readable by mapping them to a Gaussian vector using a convolutional encoder and then converting it into a natural image with StyleGAN2. The encoder discriminates between invariants from different base models and ensures Gaussian output through adversarial training, while StyleGAN2 transforms Gaussian vectors into dog images. Consequently, our method generates a dog image as an identity fingerprint for an LLM, where the dog's appearance strongly indicates the LLM's base model. Specifically, if the LLM is adapted from another base model, the generated dog highly resembles that model; otherwise if trained independently from scratch, it exhibits a unique dog image distinct from other models. Experimental results across various LLMs demonstrate the effectiveness of our method, the generated dog image remains invariant to different training steps, including SFT, RLHF, or even continued pretraining with augmented vocabulary in a new language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=WM5G2NWSYC": {
    "title": "Projected Subnetworks Scale Adaptation",
    "volume": "review",
    "abstract": "Large models support great zero-shot and few-shot capabilities. However, updating these models on new tasks can break performance on previous seen tasks and their zero/few-shot unseen tasks. Our work explores how to update zero/few-shot learners such that they can maintain performance on seen/unseen tasks of previous tasks as well as new tasks. By manipulating the parameter updates of a gradient-based meta learner as the projected task-specific subnetworks, we show improvements for large models to retain seen and zero/few shot task performance in online settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fwJeVYGcbz": {
    "title": "Multiple Modes for Continual Learning",
    "volume": "review",
    "abstract": "Adapting model parameters to incoming streams of data is a crucial factor to deep learning scalability. Interestingly, prior continual learning strategies in online settings inadvertently anchor their updated parameters to a local parameter subspace to remember old tasks, else drift away from the subspace and forget. From this observation, we formulate a trade-off between constructing multiple parameter modes and allocating tasks per mode. Mode-Optimized Task Allocation (MOTA), our contributed adaptation strategy, trains multiple modes in parallel, then optimizes task allocation per mode. We empirically demonstrate improvements over baseline continual learning strategies and across varying distribution shifts, namely sub-population, domain, and task shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yID2fdta1Z": {
    "title": "Robust Graph Neural Networks via Unbiased Aggregation",
    "volume": "review",
    "abstract": "The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses. In this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to understand their robustness and limitations. Our novel analysis of estimation bias motivates the design of a robust and unbiased graph signal estimator. We then develop an efficient Quasi-Newton iterative reweighted least squares algorithm to solve the estimation problem, which unfolds as robust unbiased aggregation layers in GNNs with a theoretical convergence guarantee. Our comprehensive experiments confirm the strong robustness of our proposed model, and the ablation study provides a deep understanding of its advantages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=J4zh8rXMm9": {
    "title": "Flashback: Understanding and Mitigating Forgetting in Federated Learning",
    "volume": "review",
    "abstract": "In the realm of Federated Learning (FL), the convergence and effectiveness of learning algorithms can be severely hampered by the phenomenon of forgetting—where knowledge obtained in one round becomes diluted or lost in subsequent rounds. Such a challenge is a result of severe data heterogeneity across clients. Although FL algorithms like FedAvg have been pivotal, they often falter in scenarios of high data heterogeneity. This work delves into the nuances of this problem, establishing the critical role forgetting plays in the inefficient learning of FL in the context of severe data heterogeneity. Knowledge loss occurs in both the local update and the aggregation step; addressing one phase without considering the other will not mitigate forgetting. We introduce a novel metric that offers a granular measurement of forgetting at every round while ensuring that the occurrence of forgetting is distinctly recognized and not obscured by the simultaneous acquisition of new class-specific knowledge. Leveraging these insights, we propose Flashback, an FL algorithm that integrates a novel dynamic distillation approach. The knowledge of different models is estimated and the distillation loss is adapted accordingly. This adaptive distillation is applied both at the local and global update phases, ensuring models retain essential knowledge across rounds while also assimilating new knowledge. Our approach seeks to robustly mitigate the detrimental effects of forgetting, paving the way for more efficient and consistent FL algorithms, especially in environments of high data heterogeneity. By effectively mitigating forgetting, Flashback achieves faster convergence to target accuracy outperforming baselines, by being up to 88.5$\\times$ faster and at least 4.6$\\times$ faster across the different benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=10BTKkFfhl": {
    "title": "Efficient Backdoor Mitigation in Federated Learning with Contrastive Loss",
    "volume": "review",
    "abstract": "Due to the data-driven nature of deep neural networks and privacy concerns around user data, a backdoor could be easily injected into deep neural networks in federated learning without attracting the attention of users. An affected global model operates normally as a clean model in regular tasks and behaves differently when the trigger is presented. In this paper, we propose a novel reverse engineering approach to detect and mitigate the backdoor attack in federated learning by adopting a self-supervised Contrastive learning loss. In contrast to existing reverse engineering techniques, such as Neural Cleanse, which involve iterating through each class in the dataset, we employ the contrastive loss as a whole to identify triggers in the backdoored model. Our method compares the last-layer feature outputs of a potentially affected model with these from a clean one preserved beforehand to reconstruct the trigger under the guidance of the contrastive loss. The reverse-engineered trigger is then applied to patch the affected global model to remove the backdoor. If the global model is free from backdoors, the Contrastive loss will lead to either a blank trigger or one with random pattern. We evaluated the proposed method on three datasets under two backdoor attacks and compared it against three existing defense methods. Our results showed that while many popular reverse engineering algorithms were successful in centralized learning settings, they had difficulties detecting backdoors in federated learning, including Neural Cleanse, TABOR, and DeepInspect. Our method successfully detected backdoors in federated learning and was more time-efficient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=MGWsPGogLH": {
    "title": "Turing Complete Transformers: Two Transformers Are More Powerful Than One",
    "volume": "review",
    "abstract": "This paper presents Find+Replace transformers, a family of multi-transformer architectures that can provably do things no single transformer can, and which outperforms GPT-4 on several challenging tasks. We first establish that traditional transformers and similar architectures are not Turing Complete, while Find+Replace transformers are. Using this fact, we show how arbitrary programs can be compiled into Find+Replace transformers, potentially aiding interpretability research. We also demonstrate the superior performance of Find+Replace transformers over GPT-4 on a set of composition challenge problems. This work aims to provide a theoretical basis for multi-transformer architectures, and to encourage their further exploration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=vUgeBN7F9l": {
    "title": "PolyFormer: Scalable Graph Transformer via Polynomial Attention",
    "volume": "review",
    "abstract": "Graph Transformers have demonstrated superior performance in graph representation learning. However, many current methods focus on attention mechanisms between node pairs, limiting their scalability and expressiveness on node-level tasks. While the recent NAGphormer attempts to address scalability by employing node tokens in conjunction with vanilla multi-head self-attention, these tokens, which are designed in the spatial domain, suffer from restricted expressiveness. On the other front, some approaches have explored encoding eigenvalues or eigenvectors in the spectral domain to boost expressiveness, but these methods incur significant computational overhead due to the requirement for eigendecomposition. To overcome these limitations, we first introduce node tokens using various polynomial bases in the spectral domain. Then, we propose a tailored polynomial attention mechanism, PolyAttn, which serves as a node-wise graph filter and offers powerful representation capabilities. Building on PolyAttn, we present PolyFormer, a graph Transformer model specifically engineered for node-level tasks, offering a desirable balance between scalability and expressiveness. Extensive experiments demonstrate that our proposed methods excel at learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs containing up to 100 million nodes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LTHWoQ9ac1": {
    "title": "Cost Adaptive Recourse Recommendation by Adaptive Preference Elicitation",
    "volume": "review",
    "abstract": "Algorithmic recourse recommends a cost-efficient action to a subject to reverse an unfavorable machine learning classification decision. Most existing methods in the literature generate recourse under the assumption of complete knowledge about the cost function. In real-world practice, subjects could have distinct preferences, leading to incomplete information about the underlying cost function of the subject. This paper proposes a two-step approach that integrates preference learning to the recourse generation problem. In the first step, we design a question-answering framework to refine the confidence set of the Mahalanobis matrix cost of the subject sequentially. Then we generate recourse by utilizing two methods: gradient-based and graph-based cost-adaptive recourse that ensures validity while considering the whole confidence set of the cost matrix. The numerical evaluation demonstrates the benefits of our approach over state-of-the-art baselines in delivering cost-efficient recourse recommendations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUEe72DwPG": {
    "title": "Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa",
    "volume": "review",
    "abstract": "Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30\\%) making the model easier to use, 2) improve the more performant method (up to 32.2\\%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3\\%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDGKPbF0VQ": {
    "title": "Improving Language Models with Advantage-based Offline Policy Gradients",
    "volume": "review",
    "abstract": "Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=mlJLVigNHp": {
    "title": "RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation",
    "volume": "review",
    "abstract": "Retrieval-augmented language models improve language models (LMs) by retrieving documents and prepending them in-context. However, these documents, often spanning hundreds of words, make inference substantially less efficient. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieve the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summary by synthesizing information from multiple documents. Both are trained to achieve performance gain in LMs when we prepend the generated summary from the compressor to LMs' input, while minimizing the summary length. When retrieved documents are irrelevant to the input or offer no additional information to LM, our compressors output an empty string, enabling selective augmentation. We evaluate our approach on the language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide a summary largely faithful to the retrieved documents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=VRCh74Liu9": {
    "title": "Federated Generalization via Information-Theoretic Distribution Diversification",
    "volume": "review",
    "abstract": "Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic generalization framework for FL. Specifically, it quantifies generalization errors by evaluating the information entropy of local distributions and discerning discrepancies across these distributions. Inspired by our deduced generalization bounds, we introduce a weighted aggregation approach and a duo of client selection strategies. These innovations aim to bolster FL's generalization prowess by encompassing a more varied set of client data distributions. Our extensive empirical evaluations reaffirm the potency of our proposed methods, aligning seamlessly with our theoretical construct",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=rT2KyF8SFM": {
    "title": "Defender of privacy and fairness: tiny but reversible generative model via mutually collaborative knowledge distillation",
    "volume": "review",
    "abstract": "Sharing vast amounts of data to train powerful artificial intelligence (AI) models raises public interest concerns such as privacy and fairness. While reversible anonymization techniques are very effective for privacy preservation and fairness enhancement, these methods rely on heavy reversible generative models, making them only suitable to run in the cloud or on a server independent from the image source. For example, data transmission might be under the privacy threats such as channel eavesdropping. Therefore, we propose a novel mutually collaborative knowledge distillation strategy to train a tiny and reversible generative model. This enables us to build a synthesis-based privacy and fairness protection system in embedded devices for anonymizing privacy-sensitive data and thus improve security protection capabilities from the source. The proposed mutually collaborative knowledge distillation method exploits the reversibility of the generative model. By pairing the teacher encoder (decoder) with the student decoder (encoder), we train the student decoder (encoder) by reconstructing the image space (latent space) from the prior image space (latent space). This results in tiny-size student models that can be embedded into devices. We deploy and evaluate our system on NVIDIA Jetson TX2 devices, which operate in real-time. Extensive experiments demonstrate that our system effectively anonymizes face images and thus protects privacy and also improves fairness while minimizing the impact on downstream tasks. Our code will be publicly available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rkplYfqUr0": {
    "title": "Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions",
    "volume": "review",
    "abstract": "Language model (LM) prompting—a popular paradigm for solving NLP tasks—has been shown to be susceptible to miscalibration and brittleness to slight prompt variations, caused by its discriminative prompting approach, i.e., predicting the label given the input. To address these issues, we propose Gen-Z—a generative prompting framework for zero-shot text classification. GEN-Z is generative, as it measures the LM likelihood of input text, conditioned on natural language descriptions of labels. The framework is multivariate, as label descriptions allow us to seamlessly integrate additional contextual information about the labels to improve task performance. On various standard classification benchmarks, with six open-source LM families, we show that zero-shot classification with simple contextualization of the data source of the evaluation set consistently outperforms both zero-shot and few-shot baselines while improving robustness to prompt variations. Further, our approach enables personalizing classification in a zero-shot manner by incorporating author, subject, or reader information in the label descriptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0sO2euxhUQ": {
    "title": "Learning Latent Structural Causal Models",
    "volume": "review",
    "abstract": "Causal learning has long concerned itself with the recovery of underlying causal mechanisms. Such causal modelling enables better explanations of out-of-distribution data. Prior works on causal learning assume that the causal variables are given. However, in machine learning tasks, one often operates on low-level data like image pixels or high-dimensional vectors. In such settings, the entire Structural Causal Model (SCM) -- structure, parameters, \\textit{and} high-level causal variables -- is latent and needs to be learnt from low-level data. We treat this problem as Bayesian inference of the latent SCM, given low-level data. We present BIOLS, a tractable approximate inference method which performs joint inference over the causal variables, structure and parameters of the latent SCM from known interventions. Experiments are performed on synthetic datasets and a causal benchmark image dataset to demonstrate the efficacy of our approach. We also demonstrate the ability of BIOLS to generate images from unseen interventional distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=62K7mALO2q": {
    "title": "In-Context Learning Dynamics with Random Binary Sequences",
    "volume": "review",
    "abstract": "Large language models (LLMs) trained on huge corpora of text datasets demonstrate complex, emergent capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often unclear, and different prompts can elicit different capabilities through in-context learning. We propose a Cognitive Interpretability framework that enables us to analyze in-context learning dynamics to understand latent concepts in LLMs underlying behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would require. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate pseudo-random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from pseudo-random behaviors to deterministic repetition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ueQ6T58ZAK": {
    "title": "Dynamic Representation of Optimal Transport via Ensemble Systems",
    "volume": "review",
    "abstract": "Optimal transport has gained widespread recognition in diverse areas from economics and fluid mechanics, lately, to machine learning. However, its connection and potential applications to the domain of dynamical systems and control remain underexplored. To fill this gap, we establish an ensemble-systems interpretation for modeling the optimal transport process. We interpret displacement interpolation of the transport between continuous distributions as a dynamic process and show that this can be modeled as an ensemble control system. This is achieved by establishing moment kernel representations for describing the dynamics of optimal transport and ensemble systems. This methodology further gives rise to an optimal transport based algorithm for learning controls for ensemble systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=XsHqr9dEGH": {
    "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
    "volume": "review",
    "abstract": "Recent work by Power et al. (2022) highlighted a surprising \"grokking\" phenomenon in learning arithmetic tasks: a neural net first \"memorizes\" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy. Even in the absence of weight decay, we show that grokking can still happen when the late phase implicit bias is driven by other regularization mechanisms, such as implicit margin maximization or sharpness reduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=52fz5sUAy2": {
    "title": "Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference for Recommendation",
    "volume": "review",
    "abstract": "The interaction between users and recommender systems is not only affected by selection bias but also the neighborhood effect, i.e., the interaction between a user and an item is affected by the interactions between other users and other items, or between the same user and other items, or between other users and the same item. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but the lack of consideration of neighborhood effects can lead to biased estimates and suboptimal performance of the prediction model. In this paper, we formally formulate the neighborhood effect as an interference problem from the perspective of causal inference and introduce a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effects. In addition, we further develop two novel estimators for the ideal loss. We theoretically establish the connection between the proposed methods and previous methods ignoring the neighborhood effect and show that the proposed methods achieve unbiased learning when both selection bias and neighborhood effects are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3yyGlNHnlj": {
    "title": "GraphECL: Towards Efficient Contrastive Learning for Graphs",
    "volume": "review",
    "abstract": "Due to the inherent label scarcity, learning useful representations on graphs with no supervision is of great benefit. Yet, existing graph self-supervised learning methods overlook the scalability challenge and fail to conduct fast inference of representations in latency-constrained applications due to the intensive message passing of graph neural networks. In this paper, we present GraphECL, a simple and efficient contrastive learning paradigm for graphs. To achieve inference acceleration, GraphECL does not rely on graph augmentations but introduces cross-model contrastive learning, where positive samples are obtained through \\MLP and \\GNN representations from the central node and its neighbors. We provide theoretical analysis on the design of this cross-model framework and discuss why our \\MLP can still capture structure information and enjoys better downstream performance as \\GNN. Extensive experiments on common real-world tasks verify the superior performance of \\simper compared to state-of-the-art methods, highlighting its intriguing properties, including better inference efficiency and generalization to both homophilous and heterophilous graphs. On large-scale datasets such as Snap-patents, the \\MLP learned by GraphECL is 286.82x faster than GCL methods with the same number of \\GNN layers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=dnaCBAP7X2": {
    "title": "An Implicit Watermark Framework for Adversary Identification",
    "volume": "review",
    "abstract": "Security of deep neural networks based machine learning systems has been an emerging research topic, especially after the discovery of adversarial attacks. In general, however, it is very difficult to build a machine learning system that is resistant to different types of attacks. Instead of directly improving the robustness of neural networks, Cheng et al. proposed the first framework to trace the first compromised model under the black-box adversarial attack in a forensic view. However, the black-box assumption has limited the usage of the framework since users will require detailed model information to facilitate their own use in the modern MLaaS system. In this paper, instead of considering the limited black-box attacks, we investigate more general and harder white-box setting where all users will have full access to model. Explicit modification on the model architecture during the inference will be no longer effective because those mechanisms could be easily bypassed by adversary. To address this challenge, a novel identification framework is proposed that can achieve high tracking accuracy to trace the source of white-box adversarial attack. Specifically, to differentiate adversarial examples generated from different copies, we first design an implicit watermark from backdooring before the model distribution. Then we design a data-free method to identify the adversary with only adversarial example available. Extensive experiments on different attacks including both white-box and black-box attacks, datasets, and model architectures verify the effectiveness of the proposed method. Our code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=22pyNMuIoa": {
    "title": "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization",
    "volume": "review",
    "abstract": "Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mCnWT9OVvK": {
    "title": "Understanding Retrieval Augmentation for Long-Form Question Answering",
    "volume": "review",
    "abstract": "We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the *attribution* of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our controlled study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further reveal novel attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=JnRStoIuTe": {
    "title": "Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning",
    "volume": "review",
    "abstract": "Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and dataset distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed, yet expensive, strategies for identifying the most informative training examples out of large datasets. In this work, we revisit these methods to understand if the additional computational costs associated with such strategies are justified from the perspective of time-to-accuracy, which has become a critical efficiency measure of deep neural network training over large datasets. Surprisingly, we find that many of the recently proposed methods underperform what we call Repeated Sampling of Random Subsets (RSRS or RS2), a powerful yet overlooked extension of the standard random baseline that learns from repeatedly sampled data throughout training instead of a fixed random subset. We test RS2 against thirty-two state-of-the-art data pruning and distillation methods across four datasets including ImageNet. Our results demonstrate that RS2 significantly reduces time-to-accuracy, particularly in practical regimes where accuracy, but not runtime, is similar to that of training on full dataset. For example, when training ResNet-18 on ImageNet, with 10\\% of the dataset each epoch RS2 reaches an accuracy of 66\\% versus 69\\% when training with the full dataset. The best competing method achieves only 55\\% while training 1.6$\\times$ slower than RS2. Beyond the above meta-study, we discuss the theoretical properties of RS2 such as its convergence rate and generalization error. Our primary goal is to highlight that future works that aim to minimize total training cost by using subset selection, need to consider 1) the total computation cost (including preparing the subset) and 2) should aim to outperform a simple extension of random sampling (i.e., RS2)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=kGteeZ18Ir": {
    "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
    "volume": "review",
    "abstract": "Recent work has showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like \"_You are Julius Caesar. Compose a rap about Climate Change._\" However, it remains unclear how these persona assignments indirectly influence LLMs' core capabilities. We present the first extensive study of this in the context of LLMs' ability to perform basic reasoning. Our study encompasses 16 personas spanning 5 diverse groups (race, gender, religion, disability, and political affiliation), across 24 reasoning datasets in diverse domains such as mathematics, history, law, ethics, and more. Our findings unveil that while LLMs, such as ChatGPT, overtly reject stereotypes when explicitly asked (\"_Are Black people inept at mathematics?_\"), they tend to manifest implicit stereotypical and often erroneous presumptions when prompted to take on a persona (e.g., abstentions in rationales such as \"_As a Black person, I am unable to answer this question as it requires math knowledge_\"). This results in substantial disparities in reasoning performance among personas. This inherent 'deep' bias permeates extensively, leading to a statistically significant performance drop in over 95\\% of our datasets for certain personas, with as much as 70\\% relative drop in accuracy on select datasets. Beyond explicit abstentions, these models also have implicitly biased reasoning not evident in their responses. We find that simple prompt-based mitigation approaches have minimal impact. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs---a trend on the rise---can surface their deep-rooted biases and have unforeseeable and detrimental side-effects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=AZW3qlCGTe": {
    "title": "Enhancing Instance-Level Image Classification with Set-Level Labels",
    "volume": "review",
    "abstract": "Instance-level image classification tasks have traditionally relied on single-instance labels to train models, e.g., few-shot learning and transfer learning. However, set-level coarse-grained labels that capture relationships among instances can provide richer information in real-world scenarios. In this paper, we present a novel approach to enhance instance-level image classification by leveraging set-level labels. We provide a theoretical analysis of the proposed method, including recognition conditions for fast excess risk rate, shedding light on the theoretical foundations of our approach. We conducted experiments on two distinct categories of datasets: natural image datasets and histopathology image datasets. Our experimental results demonstrate the effectiveness of our approach, showcasing improved classification performance compared to traditional single-instance label-based methods. Notably, our algorithm achieves 13\\% improvement in classification accuracy compared to the strongest baseline on the histopathology image classification benchmarks. Importantly, our experimental findings align with the theoretical analysis, reinforcing the robustness and reliability of our proposed method. This work bridges the gap between instance-level and set-level image classification, offering a promising avenue for advancing the capabilities of image classification models with set-level coarse-grained labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Ad81awoBVS": {
    "title": "Rotation has two sides: Evaluating Data Augmentation for Deep One-class Classification",
    "volume": "review",
    "abstract": "One-class classification (OCC) involves predicting whether a new data is normal or anomalous based solely on the data from a single class during training. Various attempts have been made to learn suitable representations for OCC within a self-supervised framework. Notably, discriminative methods that use geometric visual transformations, such as rotation, to generate pseudo-anomaly samples have exhibited impressive detection performance. Although rotation is commonly viewed as a distribution-shifting transformation and is widely used in the literature, its effectiveness remains a mystery. In this study, we make a surprising observation: there exists a strong linear relationship (Pearson's Correlation, $r > 0.9$) between the accuracy of rotation prediction and the performance of OCC. This suggests that a classifier that effectively distinguishes different rotations is more likely to excel in OCC, and vice versa. The root cause of this phenomenon can be attributed to the transformation bias in the dataset, where representations learned from transformations already present in the dataset tend to be less effective, making it essential to accurately estimate the transformation distribution before utilizing pretext tasks involving these transformations for reliable self-supervised representation learning. To the end, we propose a novel two-stage method to estimate the transformation distribution within the dataset. In the first stage, we learn general representations through standard contrastive pre-training. In the second stage, we select potentially semantics-preserving samples from the entire augmented dataset, which includes all rotations, by employing density matching with the provided reference distribution. By sorting samples based on semantics-preserving versus shifting transformations, we achieve improved performance on OCC benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=I07KLz6Em1": {
    "title": "QuantEase: Optimization-based Quantization for Large Language Models",
    "volume": "review",
    "abstract": "With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-the-art performance regarding perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements of up to 15% over methods such as GPTQ. Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=6Ey8mAuLiw": {
    "title": "On the Power of Multitask Representation Learning with Gradient Descent",
    "volume": "review",
    "abstract": "Representation learning, particularly multi-task representation learning, has gained widespread popularity in various deep learning applications, ranging from computer vision to natural language processing, due to its remarkable generalization performance. Despite its growing use, our understanding of the underlying mechanisms remains limited. In this paper, we provide a theoretical analysis elucidating why multi-task representation learning outperforms its single-task counterpart in scenarios involving over-parameterized two-layer convolutional neural networks trained by gradient descent. Our analysis is based on a data model that encompasses both task-shared and task-specific features, a setting commonly encountered in real-world applications. We also present experiments on synthetic and real-world data to illustrate and validate our theoretical findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jTSKkcbEsj": {
    "title": "Pushing Boundaries: Mixup's Influence on Neural Collapse",
    "volume": "review",
    "abstract": "Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to augment the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame (ETF), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this study, we delve into the last-layer activations of training data for deep networks subjected to mixup, aiming to uncover insights into its operational efficacy. Our investigation, spanning various architectures and dataset pairs, reveals that mixup's last-layer activations predominantly converge to a distinctive configuration. In this configuration, activations from mixed-up examples of identical classes align with the classifier, while those from different classes delineate channels along the decision boundary. To validate our empirical observations, we further conduct a theoretical analysis under the assumption of an unconstrained features model, utilizing the mixup loss. Through this, we characterize and derive the optimal last-layer features, culminating in a configuration consistent with our experimental findings, thereby shedding light on the intricate workings of mixup in the training of deep networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2XBBumBGeP": {
    "title": "sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows",
    "volume": "review",
    "abstract": "Noise poses a widespread challenge in signal processing, particularly when it comes to denoising images. Although convolutional neural networks (CNNs) have exhibited remarkable success in this field, they are predicated upon the belief that noise follows established distributions, which restricts their practicality when dealing with real-world noise. To overcome this limitation, several efforts have been taken to collect noisy image datasets from the real world. Generative methods, employing techniques such as generative adversarial networks (GANs) and normalizing flows (NFs), have emerged as a solution for generating realistic noisy images. Recent works model noise using camera metadata, however requiring metadata even for sampling phase. In contrast, in this work, we aim to estimate the underlying camera settings, enabling us to improve noise modeling and generate diverse noise distributions. To this end, we introduce a new NF framework that allows us to both classify noise based on camera settings and generate various noisy images. Through experimental results, our model demonstrates exceptional noise quality and leads in denoising performance on benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=N0gLRTmmO5": {
    "title": "Open-Ended Learning in General-Sum Games: The Role of Diversity in Correlated Equilibrium",
    "volume": "review",
    "abstract": "The primary in this work focuses on the challenging and crucial task of identifying and selecting equilibria for $n$-player general-sum games. PSRO serves as a comprehensive framework for tackling complex games by leveraging the concept of the meta-game. However, prior research on PSRO mainly concentrates on solving two-player zero-sum games. Extended approaches such as JPRSO and $\\alpha$-Rank can address multi-player general-sum games, and these methods theoretically ensure uniqueness and convergence. Nonetheless, a noticeable gap often exists between the joint policy distribution derived by the solver and the target equilibrium, which can undermine the robustness of the joint policy. Within the PSRO framework, diversity characterizes the distinctions among policies within the population, representing the exploration of the policy space by players. Consequently, allocating greater sampling probabilities (meta-strategy) to more diverse policies encourages players to employ more exploratory policies, thereby mitigating the risk of exploitation. We begin by incorporating diversity measures into solving equilibria for $n$-player meta-games and introduce a novel equilibrium concept, called Diverse (C)CE, the objective of which is to maximize sum of expectations of each player's diversity. In alignment with this, we present a policy training algorithm, Diverse Correlated Oracle (DCO), which effectively associates policy dynamics with the joint policy distribution. The experimental results conducted on a range of multi-player, general-sum games demonstrate that our algorithm outperforms JPSRO and $\\alpha$-Rank and enhances the approximation of the joint policy distribution towards the target equilibrium by notably reducing the gap",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=li1Z0OQfnA": {
    "title": "On Local Equilibrium in Non-Concave Games",
    "volume": "review",
    "abstract": "While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategies, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parametrized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support, in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed *local correlated equilibrium*, which generalizes local Nash equilibrium. Importantly, we show that this solution concept captures the convergence guarantees of Online Gradient Descent and no-regret learning, which we show efficiently converge to this type of equilibrium in non-concave games with smooth utilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RvUVMjfp8i": {
    "title": "A Benchmark on Robust Semi-Supervised Learning in Open Environments",
    "volume": "review",
    "abstract": "Semi-supervised learning (SSL) has emerged as a promising paradigm to alleviate the dependency on abundant labeled data by harnessing the power of unlabeled data. Although many SSL algorithms have been proposed, their performance in practical applications is not robust because the assumption that labeled and unlabeled data are consistent does not hold. In open environments, the sources of labeled and unlabeled data may differ, leading to inconsistent data distributions and even data spaces. This paper points out that previous research on robust SSL has approached the problem from a static perspective, thereby only achieving local robustness rather than global robustness. We reshape the research framework of robust SSL by using the Robustness Analysis Curve (RAC) and the associated metrics defined based on it. Based on these metrics, we build a benchmark that encompasses three types of open environments: inconsistent data distributions, inconsistent label spaces, and inconsistent feature spaces to assess the performance of widely used statistical and deep SSL algorithms with tabular, image, and text datasets. This paper also conducted a detailed analysis, based on experimental results and theory, on how to make SSL algorithms more robust in open environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8dN7gApKm3": {
    "title": "Uncertainty-aware Graph-based Hyperspectral Image Classification",
    "volume": "review",
    "abstract": "Hyperspectral imaging (HSI) technology captures spectral information across a broad wavelength range, providing richer pixel features compared to traditional color images with only three channels. Although pixel classification in HSI has been extensively studied, especially using graph convolution neural networks (GCNs), quantifying epistemic and aleatoric uncertainties associated with the HSI classification (HSIC) results remains an unexplored area. These two uncertainties are effective for out-of-distribution (OOD) and misclassification detection, respectively. In this paper, we adapt two advanced uncertainty quantification models, evidential GCNs (EGCN) and graph posterior networks (GPN), designed for node classifications in graphs, into the realm of HSIC. We first analyze theoretically the limitations of a popular uncertainty cross-entropy (UCE) loss function when learning EGCNs for epistemic uncertainty estimation. To mitigate the limitations, we propose two regularization terms. One leverages the inherent property of HSI data where pixel features can be decomposed into weighted sums of various material features, and the other is the total variation (TV) regularization to enforce the spatial smoothness of the evidence with edge-preserving. We demonstrate the effectiveness of the proposed regularization terms on both EGCN and GPN on three real-world HSIC datasets for OOD and misclassification detection tasks. The code is available at \\url{https://anonymous.4open.science/r/HSI_torch-1586/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=qrv4wcmmxe": {
    "title": "Zero-shot Human-Object Interaction Detection via Conditional Multi-Modal Prompts",
    "volume": "review",
    "abstract": "Human Object Interaction (HOI) detection is the task of locating and inferring the relationships between all possible human-object combinations. One of the most challenging issues is the extensive labor required for the annotation of combinatorial space of possible HOI interactions. Most existing HOI detectors rely on full annotations of all predefined interactions, resulting in a lack of generalisation for unseen combinations and actions. Inspired by the powerful generalisation ability of the large Vision-Language Models (VLM), we propose a Prompt-based zero-shot human-object Interaction Detection framework, namely PID, which can improve alignment between the vision and language representations using conditional multi-modal prompts. Specifically, different from traditional prompt-learning methods, we propose learning decoupled visual and language prompts for spatial-aware visual feature extraction and interaction classification, respectively. Furthermore, we introduce constraints for multi-modal prompts to alleviate the problem of overfitting to seen concepts in prompt learning process, thus improving the suitability for zero-shot settings. Extensive experiments demonstrate the prominence of our detector with conditional multi-modal prompts, outperforming previous state-of-the-art on unseen classes of various zero-shot settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=SaSK9M66KK": {
    "title": "Pick and Adapt: An Iterative Approach for Source-Free Domain Adaptation",
    "volume": "review",
    "abstract": "Domain adaptation plays a pivotal role in deploying models when inference data distribution is different from the training data. It becomes particularly challenging in source-free domain adaptation (SFDA) scenarios, where access to the source domain data is restricted due to data privacy concern. To tackle such cases, existing approaches often resort to generating source-like data for standard unsupervised domain adaptation or endeavor to fine-tune a model pre-trained on a source domain using self-supervised training techniques. Instead, our approach strikes a different path by theoretically analyzing into an empirical risk bound for SFDA. We identify the population risk and domain drift as the major factors from the risk bound. Subsequently, we introduce a top-k importance sampling to purify the pseudo labeling and thus reduce the population risk. We further present a nearest neighbor voting based semantic domain alignment to mitigate the domain drift. An iterative optimization is finally proposed to combine the above two steps for multiple rounds. Extensive experiments across three widely applied domain adaptation datasets, i.e., Office-Home, DomainNet, and VisDA-C, demonstrate the consistently advantageous performance over the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ba5KGabRe8": {
    "title": "XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently made impressive strides in natural language understanding tasks. Despite their remarkable performance, understanding their decision-making process remains a big challenge. In this paper, we look into bringing some transparency to this process by introducing a new explanation dataset for question answering (QA) tasks that integrates knowledge graphs (KGs) in a novel way. Our dataset includes 12,102 question-answer-explanation (QAE) triples. Each explanation in the dataset links the LLM's reasoning to entities and relations in the KGs. The explanation component includes a $\\textit{why-choose}$ explanation, a $\\textit{why-not-choose}$ explanation, and a set of $\\textit{reason-elements}$ that underlie the LLM's decision. We leverage KGs and graph attention networks (GAT) to find the $\\textit{reason-elements}$ and transform them into $\\textit{why-choose}$ and $\\textit{why-not-choose}$ explanations that are comprehensible to humans. Through quantitative and qualitative evaluations, we demonstrate the potential of our dataset to improve the in-context learning of LLMs, and enhance their interpretability and explainability. Our work contributes to the field of explainable AI by enabling a deeper understanding of the LLMs decision-making process to make them more transparent and thereby, potentially more reliable, to researchers and practitioners alike. Our dataset is available at: http://anonymous.4open.science/r/XplainLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=TlyiaPXaVN": {
    "title": "Generative Adversarial Equilibrium Solvers",
    "volume": "review",
    "abstract": "We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies. Pseudo-games are a generalization of games in which players' actions affect not only the payoffs of other players but also their feasible action spaces. Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances. We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances. We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games, and apply the framework to finding Nash equilibria in normal-form games, CE in Arrow-Debreu competitive economies, and GNE in an environmental economic model of the Kyoto mechanism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4e0ItHjNo9": {
    "title": "Rethinking Counterfactual Fairness: On Which Individuals to Enforce, and How?",
    "volume": "review",
    "abstract": "Fairness in human and algorithmic decision-making is crucial in areas such as criminal justice, education, and social welfare. Recently, counterfactual fairness has drawn increasing research interest, suggesting that decision-making for individuals should remain the same when intervening with different values on the protected attributes. Nevertheless, the question of \"which attributes and individuals should be protected\" is rarely discussed in the existing counterfactual fairness literature. For example, when considering leg disability as a protected attribute, the algorithms should not treat individuals with leg disabilities differently in college admissions, but one may naturally take into this factor for the purpose of selecting runner athletes. In other words, when and how to enforce fairness is expected to depend on the causal relation between the protected attribute and the outcome of interest. Formally, this paper proposes principal counterfactual fairness using the concept of principal stratification from the causal inference literature, focusing on whether an algorithm is counterfactually fair for individuals whose protected attribute has no individual causal effect on the outcome of interest. To examine whether an algorithm satisfies principal counterfactual fairness, we derive the statistical bounds, and propose a post-processing approach to achieving principal counterfactual fairness with minimal individual decision changes. Experiments are conducted using synthetic and real-world datasets to verify the effectiveness of our methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LEuuOaZNOT": {
    "title": "Learning Boolean functions with neural networks",
    "volume": "review",
    "abstract": "Many works have shown learnability of functions on the Boolean hypercube via gradient descent. These analyses of gradient descent use the convexity of the problem to establish guarantees despite the fact that most loss functions are highly non-convex. In addition, the analyses explicitly show that the hypothesis class can approximate the target function; this is known as a representation theorem. In this work we give gradient descent guarantees for learning functions on the Boolean hypercube on both the mean squared and hinge losses with $2$-layer neural networks with a single hidden non-linear layer. Furthermore, all of our analyses apply to the ReLU activation function. Moreover, on both losses, we don't make use of any convexity of the problem, and don't explicitly prove a representation theorem. A representation theorem is a consequence of our analysis. In the hinge loss setting to learn size $k$ parities, with dimension $n$, and $\\epsilon$ error, we obtain bounds of $n^{O(k)}poly(\\frac{1}{\\epsilon})$ and $n^{O(k)}\\log(\\frac{1}{\\epsilon})$ for network width and samples, and iterations needed, respectively. This upper bound matches the SQ lower bounds of $n^{\\Omega(k)}$. In the mean squared loss setting, given that the Fourier spectrum of an activation function has non-zero Fourier coefficients up to degree $k$, and given that the best degree $k$ polynomial approximation of the target function is $\\epsilon_0$ in mean squared loss, we give guarantees for network width and samples, and iterations needed of $n^{O(k)}poly(\\frac{1}{\\epsilon})$ and $n^{O(k)}\\log(\\frac{1}{\\epsilon})$ respectively for an error of $\\epsilon+ \\epsilon_0$. To the best of our knowledge, our bounds of $n^{O(k)}\\log(\\frac{1}{\\epsilon})$ iterations needed for learning degree $k$ polynomials on both losses are better than any previous bounds in the Boolean setting, which is a consequence of not using any convexity of the problem in our analysis. Specifically, in other works in the Boolean setting, the bound on iterations is $n^{O(k)}poly(\\frac{1}{\\epsilon})$. Moreover, as a corollary to our agnostic learning guarantee, we establish that lower degree Fourier components are learned before higher degree ones, a phenomenon observed experimentally. Finally, as a corollary to our mean squared loss guarantee, we show that neural networks with sparse hidden ReLU units as target functions can be efficiently learned with gradient descent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjygLD0AkT": {
    "title": "Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection",
    "volume": "review",
    "abstract": "While likelihood is attractive in theory, its estimates by deep generative models (DGMs) are often broken in practice, and perform poorly for OOD Detection. Various recent works started to consider alternative summary statistics and achieved better performances. However, such recipes do not come with provable guarantees, nor is it clear that their choices extract sufficient information. We attempt to change this by conducting a case study on variational autoencoders (VAEs). First, we introduce the *likelihood path (LPath) principle*, generalizing the likelihood principle. This narrows the search for informative summary statistics down to the *minimal sufficient statistics* of VAEs' conditional likelihoods. Second, introducing new theoretic tools such as *essential support*, *essential distance* and *co-Lipschitzness*, we obtain non-asymptotic provable OOD detection guarantees for certain distillation of the minimal sufficient statistics. The corresponding LPath algorithm demonstrates SOTA performances, even using simple and small VAEs with poor likelihood estimates. To our best knowledge, this is the first provable unsupervised OOD method that delivers excellent empirical results, better than any other VAEs based techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzvIWvC9MG": {
    "title": "Generative Adversarial Inverse Multiagent Learning",
    "volume": "review",
    "abstract": "In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to find parameters of a game's payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as a generative-adversarial (i.e., min-max) optimization problem, based on which we develop polynomial-time algorithms the solve them, the former of which relies on an exact first-order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent apprenticeship learning in polynomial time and number of samples, where we seek a simulacrum, i.e., parameters and an associated equilibrium, which replicate observations in expectation. We find that our approach outperforms other widely-used methods in predicting prices in Spanish electricity markets based on time-series data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rPpRyGVVnt": {
    "title": "Learning to Play Atari in a World of Tokens",
    "volume": "review",
    "abstract": "Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context, resulting in more accurate world models. However, for complex reasoning and planning tasks, these methods primarily rely on continuous representations. This complicates modeling of discrete properties of the real world such as disjoint object classes between which interpolation is not plausible. In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior. We incorporate a transformer-decoder for auto-regressive world modeling and a transformer-encoder for learning behavior by attending to task-relevant cues in the discrete representation of the world model. For handling partial observability, we aggregate information from past time steps as memory tokens. DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=992eLydH8G": {
    "title": "Do Pre-trained Transformers Really Learn In-context by Gradient Descent?",
    "volume": "review",
    "abstract": "Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore necessitate further investigation to validate their applicability in reality. We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order-sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting. Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ervzSmtQyY": {
    "title": "Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data",
    "volume": "review",
    "abstract": "Traditional deep learning (DL) suffers from two core problems. First, it assumes training samples are independent and identically distributed (iid), however, in many real-world datasets samples are grouped by measurements made on the same sample (e.g., study participant, cell, tissue) violating this assumption. On such clustered data, traditional DL suffers from reduced prediction performance, lack of generalization, poor interpretability, and cluster confounding causing Type 1 and 2 errors. Second, traditional model fitting prioritizes only overall training data accuracy, which is biased towards the most common group, with often unfair, lower accuracy on samples from underrepresented groups. When DL is used to guide critical decisions (e.g., loan approvals or determining health insurance rates) such biases can significantly impact one's quality of life. To address both of these challenges simultaneously, we introduce a fairness-enhancing mixed effects deep learning (MEDL) framework. MEDL separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) through the introduction of: 1) a cluster adversary which encourages the learning of cluster-invariant FE, 2) a Bayesian neural network which quantifies the RE, and a mixing function combining the FE an RE into a mixed-effect prediction. We marry this MEDL with adversarial debiasing, which promotes equality-of-odds fairness across FE, RE, and ME predictions for fairness-sensitive variables. An empirical evaluation spanning three datasets: two from the census/finance sector and one from the healthcare sector is performed. The former focuses on income classification, while the latter is a healthcare dataset that predicts forthcoming hospitalization duration, a regression task. The proposed framework boosts fairness across all sensitive variables—increasing fairness up to 82% for age, 43% for race, 86% for sex, and 27% for marital-status. While enhancing fairness, our method also preserves the intrinsic improved performance and interpretability advantages of MEDL. Moreover, the proposed method is agnostic to dataset type and task (regression or classification) with broad potential applicability by the community. To facilitate these benefits and further community extension, we have made our implementation available through GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BhxsjonZ0z": {
    "title": "FedOD: Federated Outlier Detection via Neural Approximation",
    "volume": "review",
    "abstract": "Outlier detection (OD) is a crucial machine learning task with key applications in various sectors such as security, finance, and healthcare. Preserving data privacy has been increasingly important for OD due to the sensitivity of the data involved. While federated learning (FL) offers the potential in protecting data privacy, it is not yet available for most classical OD algorithms, such as those based on distance and density estimation. To address this, we introduce FedOD, the first FL-based system designed for general OD algorithms. FedOD effectively overcomes the privacy and efficiency challenges inherent in classical OD algorithms by automatically decomposing these algorithms into a set of basic operators and approximating their behaviors using neural networks. Given the inherent compatibility of neural networks with FL, the approximated OD algorithms also become capable of privacy-preserving learning without data exchange. With this design, FedOD supports over 20 popular classical OD algorithms and is readily extendable to other fields like classification. Evaluation on more than 30 benchmark and synthetic datasets demonstrates FedOD's accuracy and efficacy over state-of-the-art baselines---compared to existing OD systems, FedOD achieves up to 11x reduction in errors and 10x improvement in performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.3,
    "authors": []
  },
  "https://openreview.net/forum?id=4N7v4w2r3b": {
    "title": "Robustness Evaluation of Proxy Models against Adversarial Optimization",
    "volume": "review",
    "abstract": "Ensuring the robustness of neural network proxies to optimization pressure is crucial as machine learning applications expand across diverse domains. However, research on proxy robustness remains limited and largely unexplored. In this paper, we introduce a comprehensive benchmark for investigating the robustness of neural network proxies under various sources of optimization pressure in the text domain. Through extensive experiments using our benchmark, we uncover previously unknown properties of the proxy gaming problem and highlight serious issues with proxy reward models currently used to fine-tune or monitor large language models. Furthermore, we explore different approaches to enhance proxy robustness and demonstrate the potential of adversarial training to improve alignment between proxy and gold models. Our findings suggest that proxy robustness is a solvable problem that can be incrementally improved, laying the groundwork for future research in this important area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=uXbqFnQfH4": {
    "title": "Multi-Objective Multi-Solution Transport",
    "volume": "review",
    "abstract": "In the realm of multi-objective optimization, we introduce ''Multi-objective multi-solution Transport (MosT)'', a novel solution for optimizing multiple objectives that employs multiple solutions. The essence lies in achieving diverse trade-offs among objectives, where each solution performs as a domain expert, focusing on specific objectives while collectively covering all of them. Traditional methods often struggle, especially when the number of objectives greatly outnumbers the number of solutions, leading to either subpar solutions or objectives that have been essentially ignored. MosT addresses this by formulating the problem as a bi-level optimization of weighted objectives, where the weights are defined by an optimal transport between the objectives and solutions. Our newly developed algorithm not only ensures theoretical convergence to various Pareto front solutions but is also adaptive to cases where objectives outnumber solutions. We further enhance its efficiency by introducing a solution-specialization curriculum. With proven applications in federated learning, fairness-accuracy trade-offs, and standard MOO benchmarks, MosT distinctly outperforms existing methods, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across objectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=k8Y71G7Xpz": {
    "title": "FORKS: Fast Second-Order Online Kernel Learning using Incremental Sketching",
    "volume": "review",
    "abstract": "Online Kernel Learning (OKL) has attracted considerable research interest due to its promising predictive performance. Second-order methods are particularly appealing for OKL as they often offer substantial improvements in regret guarantees. However, existing approaches like PROS-N-KONS suffer from at least quadratic time complexity with respect to the budget, rendering them unsuitable for meeting the real-time demands of large-scale online learning. Additionally, current OKL methods are typically prone to concept drifting in data streams, making them vulnerable in adversarial environments. To address these issues, we introduce FORKS, a fast incremental sketching approach for second-order online kernel learning. FORKS maintains an efficient time-varying explicit feature mapping that enables rapid updates and decomposition of sketches using incremental sketching techniques. Theoretical analysis demonstrates that FORKS achieves a logarithmic regret guarantee, on par with other second-order approaches, while maintaining a linear time complexity w.r.t. the budget. We validate the performance of FORKS through extensive experiments conducted on real-world datasets, demonstrating its superior scalability and robustness against adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pe0Vdv7rsL": {
    "title": "Graph Transformers on EHRs: Better Representation Improves Downstream Performance",
    "volume": "review",
    "abstract": "Following the success of transformer-based methods across various machine learning applications, their adoption to healthcare predictive tasks using electronic health records (EHR) has also expanded extensively. Similarly, graph-based methods have been shown to be very effective in capturing inherent graph-type relationships in EHRs, leading to improved downstream performance. Although integrating these two families of approaches seems like a natural next step, in practice, creating such a design is challenging and has not been done. This is partly due to known EHR problems, such as high sparsity, making extracting meaningful temporal representations of medical visits challenging. In this study, we propose GT-BEHRT, a new approach that leverages temporal visit embeddings extracted from a graph transformer and uses a BERT-based model to obtain more robust patient representations, especially on longer EHR sequences. The graph-based approach allows GT-BEHRT to implicitly capture the intrinsic graphical relationships between medical observations, while the BERT model extracts the temporal relationships between visits, loosely mimicking the clinicians' decision-making process. As part of our method, we also present a two-step pre-training strategy for learning better graphical and temporal representations. Our proposed method achieves state-of-the-art performance in a variety of standard medical predictive tasks, demonstrating the versatility of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=a01qbkxbve": {
    "title": "O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data, advancing the capability of solving downstream tasks. Empirical results under two interactive decision-making benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the decision-making capabilities of LLMs through the offline discovery and distillation process, and consistently outperform baselines across various LLMs with both text-based-policy and code-based-policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y84b6FahMD": {
    "title": "Counterfactual Fairness from Partially DAGs: A General Min-Max Optimization Framework",
    "volume": "review",
    "abstract": "Developing fair automated machine learning algorithms is critical in making safe and trustworthy decisions. Many causality-based fairness notions have been proposed to address the above issues by quantifying the causal connections between sensitive attributes and decisions, and when the true causal graph is fully known, certain algorithms that achieve counterfactual fairness have been proposed. However, when the true causal graph is unknown, it is still challenging to effectively and well exploit partially directed acyclic graphs (PDAGs) to achieve counterfactual fairness. To tackle the above issue, a recent work suggests using non-descendants of sensitive attribute for fair prediction. Interestingly, in this paper, we show it is actually possible to achieve counterfactual fairness even using the descendants of the sensitive attribute for prediction, by carefully control the possible counterfactual effects of the sensitive attribute. We propose a general min-max optimization framework that can effectively achieve counterfactual fairness with promising prediction accuracy, and can be extended to maximally oriented PDAGs (MPDAGs) with added background knowledge. Specifically, we first estimate all possible counterfactual treatment effects of sensitive attribute on a given prediction model from all possible adjustment sets of sensitive attributes. Next, we propose to alternatively update the prediction model and the corresponding possible estimated causal effects, where the prediction model is trained via a min-max loss to control the worst-case fairness violations. Extensive experiments on synthetic and real-world datasets verifying the effectiveness of our methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=dwzLn78jq7": {
    "title": "On the Scalability and Memory Efficiency of Semidefinite Programs for Lipschitz Constant Estimation of Neural Networks",
    "volume": "review",
    "abstract": "Lipschitz constant estimation plays an important role for understanding generalization, robustness, and fairness in deep learning. Unlike naive bounds based on the network weight norm product, semidefinite programs (SDPs) have shown great promise in providing less conservative Lipschitz bounds with polynomial-time complexity guarantees. However, due to the memory consumption and running speed, standard SDP algorithms cannot scale to modern neural network structures. In this paper, we transform the SDPs for Lipschitz constant estimation into an eigenvalue problem, which aligns with the modern large optimization paradigms based on first-order methods. This is amenable to autodiff frameworks such as PyTorch and TensorFlow, requiring significantly less memory than standard SDP algorithms. The transformation also allows us to leverage various existing numerical techniques for eigenvalue optimization, opening the way for further memory improvement and computational speedup. The essential technique of our eigenvalue-problem transformation is to introduce redundant quadratic constraints and then utilize both Lagrangian and Shor's SDP relaxations. Numerical examples demonstrate that our technique is more scalable than existing approaches. For networks that existing SDP solvers cannot handle, we improve the Lipschitz constant estimation by up to 58\\% compared to the weight matrix norm product bound",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=dexKVPmPOg": {
    "title": "Efficient Recomputation of Marginal Likelihood upon Adding Training Data in Gaussian Processes and Simulator Fusion",
    "volume": "review",
    "abstract": "To reduce generalization loss in line with the bias-variance trade-off, machine learning engineers should construct models based on their knowledge of the modeling target and, as training data increases, choose more flexible models with reduced dependence on that knowledge if that knowledge is unreliable. To achieve this automatically, methods have been proposed to determine the amount of model's assumed prior knowledge directly from training data, rather than relying solely on an engineer's intuition. A widely studied approach involves using both a flexible model and a knowledge-dependent simulator, selectively incorporating simulator-generated data into the flexible model's training data. While neural networks have been used as flexible models, Gaussian processes are also candidates due to their flexibility and ability to output prediction uncertainty. However, direct methods for adding simulator-generated data to Gaussian process training data remain unstudied. The Subset of Data (SoD) method, the closest alternative, often adds inappropriate data due to its assumption about the true distribution. The log marginal likelihood, grounded in theory, determines the inclusion of generated data. However, its computation in Gaussian processes is costly. We propose a faster method considering the Cholesky factor and matrix element dependencies. Experiments indicate that, in terms of MSE, metrics using exact negative log likelihood outperform Subset of Data and other basic methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=2C3CWCPxNS": {
    "title": "Preconditioning for Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "Physics-informed neural networks (PINNs) have shown promise in solving complex partial differential equations (PDEs). However, certain training pathologies have emerged, compromising both convergence and prediction accuracy in practical applications. In this paper, we propose to use condition number as an innovative metric to diagnose and rectify the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We delineate a theory that elucidates the relationship between reduced condition numbers and improved error control, as well as better convergence. Subsequently, we present an algorithm that leverages preconditioning to enhance the condition number. Evaluations on 16 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. Furthermore, in 2 distinct cases, our approach pioneers a solution, slashing relative errors from roughly $100\\\\%$ to below $6\\\\%$ and $21\\\\%$, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNLAkjUm19": {
    "title": "On the Role of Discrete Tokenization in Visual Representation Learning",
    "volume": "review",
    "abstract": "In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using unmasked portions. A notable subset of MIM methodologies employs discrete visual tokens as reconstruction target. This study explores the role of discrete visual tokens in MIM, with the aim of decoding their potential benefits and inherent constraints. Building upon the connection between MIM and contrastive learning, we provide comprehensive explanations on how discrete tokenization affects generalization performance of MIM. Furthermore, we introduce a novel metric designed to quantify the proficiency of discrete visual tokens in the MIM framework. Inspired by this metric, we contribute an accessible tokenizer design and demonstrate its superior performance across various benchmark datasets and ViT backbones",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZEXgtMNNo": {
    "title": "Large Language Models as Automated Aligners for benchmarking Vision-Language Models",
    "volume": "review",
    "abstract": "With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc. The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ofzeypWosV": {
    "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
    "volume": "review",
    "abstract": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to 1) achieve superior compression in the token length, and 2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art zero-shot TTS baselines regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=E4hK8t7Fts": {
    "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
    "volume": "review",
    "abstract": "Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM~2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VgBjsOC8k": {
    "title": "Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels",
    "volume": "review",
    "abstract": "Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of Gaussian (DoG) functions, and their first and second-order derivatives. Notably, we classify over 95\\% and 90\\% of the filters from state-of-the-art ConvNeXtV2 and ConvNeXt models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long proposed for the vision systems of mammals. Our results thus deepen our understanding of the emergent properties of trained DS-CNNs and provide a bridge between artificial and biological visual processing systems. More broadly, they pave the way for more interpretable and biologically-inspired neural network designs in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0j9ZDzMPqr": {
    "title": "UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models",
    "volume": "review",
    "abstract": "Node representation learning, such as Graph Neural Networks (GNNs), has become one of the important learning methods in machine learning, and the demand for reliable explanation generation is growing. Despite extensive research on explanation generation for supervised node representation learning, explaining unsupervised models has been less explored. To address this gap, we propose a method for generating counterfactual (CF) explanations in unsupervised node representation learning, aiming to identify the most important subgraphs that cause a significant change in the $k$-nearest neighbors of a node of interest in the learned embedding space upon perturbation. The $k$-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-$k$ link prediction and clustering. Furthermore, we introduce a Monte Carlo Tree Search (MCTS)-based explainability method for generating expressive CF explanations for **U**nsupervised **N**ode **R**epresentation learning methods, which we call **UNR-Explainer**. The proposed method demonstrates improved performance on six datasets for both unsupervised GraphSAGE and DGI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=BmhWGsOyDu": {
    "title": "Reinforcement Learning for Large Group Systems using Hierarchical Kernel Representations",
    "volume": "review",
    "abstract": "Policy learning for targeted coordination of massive-scale populations of, in the limit a continuum spectrum of, intelligent agents has been a missing component in reinforcement learning research. The purpose of this work is to fill in this literature gap by addressing the major challenge: the curse of dimensionality caused by the huge population size. To this end, we formulate such an intelligent agent population as a parameterized deterministic dynamical system, referred to as a group system, and then introduce the novel moment representation to the system. Under this representation, we propose a nested reinforcement learning algorithm to learn the optimal policy for the system hierarchically. As a significant advantage, each hierarchy preserves the optimality of all its lower-level children, which then leads to the fast convergence of the nested algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ZBEs9CJiWs": {
    "title": "Optimizing Interpersonal Communication by Simulating Audiences with Large Language Models",
    "volume": "review",
    "abstract": "How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage current Large Language Models (LLMs) to help us communicate better. Specifically, we propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve, 1) explores the solution space by first producing a diverse set of advice relevant to the scenario, 2) generates potential candidates conditioned on subsets of the advice, and 3) simulates the reactions from various audiences, selecting both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal communication. For each scenario, we collect a dataset of human evaluations across candidates and baselines and showcase that our framework's chosen candidate is preferred over popular baseline generation mechanisms including Chain-of-Thought. We also find that audience simulations achieve reasonably high agreement with human raters across $5$ of the $8$ scenarios. Furthermore, we demonstrate the generality of our framework by applying it to real-world scenarios described by users on web forums. Viewing LLMs as a library of shared experiences and opinions, our approach draws on this library to integrate cultural and individual experience and ultimately help people communicate better",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=aCgybhcZFi": {
    "title": "Enhancing Neural Network Transparency through Representation Analysis",
    "volume": "review",
    "abstract": "In this paper, we introduce and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase these methods can provide traction on a wide range of safety-relevant problems, including truthfulness, memorization, power-seeking, and more, demonstrating the promise of representation-centered transparency research. We hope this work catalyzes further exploration into RepE and fosters advancements in the transparency and safety of AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=x8VNtpCu1I": {
    "title": "Are Bert Family Good Instruction Followers? A Study on Their Potential And Limitations",
    "volume": "review",
    "abstract": "Language modeling at scale has proven very effective and brought unprecedented success to natural language models. Many typical representatives, especially decoder-only models, e.g., BLOOM and LLaMA, and encoder-decoder models, e.g., Flan-T5 and AlexaTM, have exhibited incredible instruction-following capabilities while keeping strong task completion ability. These large language models can achieve superior performance in various tasks and even yield emergent capabilities, e.g., reasoning and universal generalization. Though the above two paradigms are mainstream and well explored, the potential of the BERT family, which are encoder-only based models and have ever been one of the most representative pre-trained models, also deserves attention, at least should be discussed. In this work, we adopt XML-R to explore the effectiveness of the BERT family for instruction following and zero-shot learning. We first design a simple yet effective strategy to utilize the encoder-only models for generation tasks and then conduct multi-task instruction tuning. Experimental results demonstrate that our fine-tuned model, Instruct-XMLR, outperforms Bloomz on all evaluation tasks and achieves comparable performance with mT0 on most tasks. Surprisingly, Instruct-XMLR also possesses strong task and language generalization abilities, indicating that Instruct-XMLR can also serve as a good instruction follower and zero-shot learner. Besides, Instruct-XMLR can accelerate decoding due to its non-autoregressive generation manner, achieving around 3 times speedup compared with current autoregressive large language models. Although we also witnessed several limitations through our experiments, such as the performance decline in long-generation tasks and the shortcoming of length prediction, Instruct-XMLR can still become a good member of the family of current large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=whxKU5YcH6": {
    "title": "SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "Graph-level representation learning is important in a wide range of applications. However, existing graph-level models are generally built on i.i.d. assumption for both training and testing graphs, which is not realistic in an open world, where models can encounter out-of-distribution (OOD) testing graphs that are from different distributions unknown during training. A trustworthy model should not only produce accurate predictions for in-distribution (ID) data, but also detect OOD graphs to avoid unreliable prediction. In this paper, we present SGOOD, a novel graph-level OOD detection framework. We find that substructure differences commonly exist between ID and OOD graphs. Hence, SGOOD explicitly utilizes substructures to learn powerful representations to achieve superior performance. Specifically, we build a super graph of substructures for every graph, and design a two-level graph encoding pipeline that works on both original graphs and super graphs to obtain substructure-enhanced graph representations. To further distinguish ID and OOD graphs, we develop three graph augmentation techniques that preserve substructures and increase expressiveness. Extensive experiments against 10 competitors on numerous graph datasets demonstrate the superiority of SGOOD, often surpassing existing methods by a significant margin. The code is available at https://anonymous.4open.science/r/SGOOD-0958",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=hdCDVSPQ7v": {
    "title": "Jorge: Approximate Preconditioning for GPU-Efficient Second-Order Optimization",
    "volume": "review",
    "abstract": "Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate the distinct advantages of using Jorge, outperforming state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple deep learning models, both in terms of sample efficiency and wall-clock time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=V2cBKtdC3a": {
    "title": "Exploring the Promise and Limits of Real-Time Recurrent Learning",
    "volume": "review",
    "abstract": "Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10B frames. To scale to such challenging tasks, we focus on certain well-known neural architectures with element-wise recurrence, allowing for tractable RTRL without approximation. Importantly, we also discuss rarely addressed limitations of RTRL in real-world applications, such as its complexity in the multi-layer case",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=1p4q1cXOX9": {
    "title": "Attribute-Enhanced Similarity Ranking for Sparse Link Prediction",
    "volume": "review",
    "abstract": "Link prediction is a fundamental problem in graph data. In its most realistic setting, the problem consists of predicting missing or future links between random pairs of nodes from the set of disconnected pairs. Graph Neural Networks (GNNs) have become the predominant framework for link prediction. GNN-based methods treat link prediction as a binary classification problem and handle the extreme class imbalance---real graphs are very sparse---by sampling (uniformly at random) a balanced number of disconnected pairs not only for training but also for evaluation. However, we show that the reported performance of GNNs for link prediction in the balanced setting does not translate to the more realistic imbalanced setting and that simpler topology-based approaches are often better at handling sparsity. These findings motivate Gelato, a similarity-based link-prediction method that applies (1) graph learning based on node attributes to enhance a topological heuristic, (2) a ranking loss for addressing class imbalance, and (3) a negative sampling scheme that efficiently selects hard training pairs via graph partitioning. Experiments show that Gelato is more accurate and faster than GNN-based alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oIwoBDsJJI": {
    "title": "Measuring Graph Similarity Using Transfer Cost of Forster Distributions",
    "volume": "review",
    "abstract": "In recent years, optimal transport-based distance metrics have shown to be effective similarity and dissimilarity measures for tackling learning problems involving network data. Prominent examples range from graph classification and community detection to object matching. However, the high computational complexity of calculating optimal transport costs substantially confines their applications to large-scale networks. To address this challenge, in this paper, we introduce a probability distribution on the set of edges of a graph, referred to as the Foster distribution of the graph, by extending Foster's theorem from electrical to general networks. Then, we represent Foster distributions as probability measures on the real line and estimate the Wasserstein metric between the corresponding probability measures to quantify graph similarity. The applicability of the proposed approach is corroborated on diverse graph-structured datasets, through which we particularly demonstrate the high efficiency of computing the proposed graph distance for sparse graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=YH5w12OUuU": {
    "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting",
    "volume": "review",
    "abstract": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. On the other hand, for natural language processing, Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal, and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO, with 20%-60% improvement over state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets. This compelling finding highlights TEMPO's potential to constitute a foundational model building framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=u3dX2CEIZb": {
    "title": "Scaling physics-informed hard constraints with mixture-of-experts",
    "volume": "review",
    "abstract": "Imposing known physical constraints, such as conservation laws, during the training of neural networks introduces an inductive bias that can improve accuracy, convergence, and data efficiency for physical problems. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve accuracy by integrating constrained optimization within neural network training, enabling a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems, as it requires solving the differentiable optimization problem over a large number of points in the spatiotemporal domain. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE). Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an ``expert'' through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy on challenging non-linear systems, concurrently improving training stability and requiring significantly less computation time during both training and inference stages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4stB7DFLp6": {
    "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
    "volume": "review",
    "abstract": "Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval before instruction tuning. Specifically, we continue to pretrain the 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. The obtained foundation model, Retro 48B, largely outperforms the original 43B GPT in terms of perplexity. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on zero-shot question answering (QA) tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA tasks, and 10% over GPT across 4 challenging long-form QA tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. We hypothesize that pretraining with retrieval makes its decoder good at incorporating context for QA. Our results highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xiGwCVzsCi": {
    "title": "Discrimination-free Pricing with Privatized Sensitive Attributes",
    "volume": "review",
    "abstract": "Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continue to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic biasness has introduced various fairness concepts, including demographic parity and equalized odds, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement in this field, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing. In particular, the regulatory bodies are increasingly emphasizing transparency in pricing algorithms and imposing constraints for insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose a straightforward method for constructing fair models that align with the specific fairness criteria unique to the insurance pricing domain. Notably, our approach only relies on privatized sensitive attributes and offers statistical guarantees. Further, it does not require insurers to have direct access to sensitive attributes, and it can be tailored to accommodate varying levels of transparency as required. This methodology seeks to meet the growing demands for privacy and transparency set forth by regulators while ensuring fairness in insurance pricing practices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=BoMvv7ypDF": {
    "title": "Recursive Score Estimation Accelerates Diffusion-Based Monte Carlo",
    "volume": "review",
    "abstract": "To sample from a general target distribution $p_*\\propto e^{-f_*}$ beyond the isoperimetric condition, \\citet{huang2023monte} proposed to perform sampling through reverse diffusion, giving rise to *Diffusion-based Monte Carlo* (DMC). Specifically, DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an *exponential dependency* on the error tolerance $\\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of the original DMC algorithm originates from its redundant design of score estimation, and proposed a more efficient DMC algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation and sampling subproblems accordingly, which are correlated in a recursive manner. Importantly, we show that with a proper design of the segment decomposition, all sampling subproblems will only need to tackle a strongly log-concave distribution, which can be very efficient to solve using the standard sampler (e.g., Langevin Monte Carlo) with a provably rapid convergence rate. As a result, we prove that the gradient complexity of RS-DMC only has a *quasi-polynomial dependency* on $\\epsilon$, which significantly improves exponential gradient complexity in \\citet{huang2023monte}. Furthermore, under commonly used dissipative conditions, our algorithm is provably much faster than the popular Langevin-based algorithms. Our algorithm design and theoretical framework illuminate a novel direction for addressing sampling problems, which could be of broader applicability in the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=W98SiAk2ni": {
    "title": "Ensemble Systems Representation for Function Learning over Manifolds",
    "volume": "review",
    "abstract": "Function learning concerns with the search for functional relationships among datasets. It coincides with the formulations of various learning problems, particularly supervised learning problems, and serves as the prototype for many learning models, e.g., neural networks and kernel machines. In this paper, we propose a novel framework to tackle function learning tasks from the perspective of ensemble systems theory. Our central idea is to generate function learning algorithms by using flows of continuous-time ensemble systems defined on infinite-dimensional Riemannian manifolds. This immediately gives rise to the notion of natural gradient flow that enables the generated algorithms to tackle function learning tasks over manifolds. Moreover, we rigorously investigate the relationship between the convergence of the generated algorithms and the dynamics of the ensemble systems with and without an external forcing or control input. We show that by turning the penalty strengths into control inputs, the algorithms are able to converge to any function over the manifold, regardless of the initial guesses, providing {\\em ensemble controllability} of the systems. In addition to the theoretical investigation, concrete examples are also provided to demonstrate the high efficiency and excellent generalizability of these \"continuous-time\" algorithms compared with classical \"discrete-time\" algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ANvmVS2Yr0": {
    "title": "Generalization in diffusion models arises from geometry-adaptive harmonic representation",
    "volume": "review",
    "abstract": "High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the ``true'' density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates the existence of powerful inductive biases in the DNN architecture and/or training algorithm. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image regions. We show that trained denoisers are inductively biased towards these geometry-adaptive harmonic representations by demonstrating that they arise even when the network is trained on image classes such as low-dimensional manifolds for which the harmonic basis is suboptimal. Additionally, we show that the denoising performance of the networks is near-optimal when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CQF8mTF7qx": {
    "title": "Simplicity Bias of SGD via Sharpness Minimization",
    "volume": "review",
    "abstract": "The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features (Huh et al., 2021). Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) towards flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low rank) is not well understood. In this work, we take a step towards bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property --- a local geodesic convexity --- of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bvjcMvMn7B": {
    "title": "Structural Fairness-aware Active Learning for Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) have seen significant achievements in semi-supervised node classification. Yet, their efficacy often hinges on access to high-quality labeled node samples, which may not always be available in real-world scenarios. While active learning is commonly employed across various domains to pinpoint and label high-quality samples based on data features, graph data present unique challenges due to their intrinsic structures that render nodes non-i.i.d. Furthermore, biases emerge from the positioning of labeled nodes; for instance, nodes closer to the labeled counterparts often yield better performance. To better leverage graph structure and mitigate structural bias in active learning, we present a unified optimization framework (SCARCE), which is also easily incorporated with node features. Extensive experiments demonstrate that the proposed method not only improves the GNNs performance but also paves the way for more fair results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FWJAmwE0xH": {
    "title": "Neural-Symbolic Recursive Machine for Systematic Generalization",
    "volume": "review",
    "abstract": "Current learning models often struggle with human-like systematic generalization; learning compositional rules from limited data and extrapolating them to unseen combinations. To address this, we introduce Neural-Symbolic Recursive Machine (NSR), a model whose core representation is a Grounded Symbol System (GSS ), with its combinatorial syntax and semantics emerging entirely from the training data. The NSR adopts a modular approach, incorporating neural perception, syntactic parsing, and semantic reasoning, which are jointly learned through a deduction-abduction algorithm. We establish that NSR possesses sufficient expressiveness to handle a variety of sequence-to-sequence tasks and attains superior systematic generalization, thanks to the inductive biases of equivariance and recursiveness inherent in each module. We assess NSR 's performance against four rigorous benchmarks designed to test systematic generalization: SCAN for semantic parsing, PCFG for string manipulation, HINT for arithmetic reasoning, and a task involving compositional machine translation. Our results indicate that NSR outperforms existing neural or hybrid models in terms of generalization and transferability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ITq4ZRUT4a": {
    "title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation",
    "volume": "review",
    "abstract": "Evaluating text-to-image models is notoriously difﬁcult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not assert that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG is an automatic, graph-based QG/A that is modularly implemented to be adaptable to any QG/A module. DSG produces atomic and unique questions organized in dependency graphs, which (i) ensure appropriate semantic coverage and (ii) sidestep inconsistent answers. With extensive experimentation and human evaluation on a range of model conﬁgurations (LLM, VQA, and T2I), we empirically demonstrate that DSG addresses the challenges noted above. Finally, we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide range of ﬁne-grained semantic categories with a balanced distribution. We will release the DSG-1k prompts and the corresponding DSG questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wu9nGGYvAX": {
    "title": "Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations",
    "volume": "review",
    "abstract": "Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn abstract, generalizable same-different visual relations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=n9xeGcI4Yg": {
    "title": "The Consensus Game: Language Model Generation via Equilibrium Search",
    "volume": "review",
    "abstract": "When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate answers). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game—which we term the concensus game—in which a generator seeks to communicate an abstract correctness parameter using natural language sentences to a discriminator. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call equilibrium-ranking. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and assistive dialog), equilibrium-ranking consistently improves performance over existing LM decoding procedures. These improvements are sometimes substantial—on multiple benchmarks, we observe that applying equilibrium-ranking to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uiFuqvkpAt": {
    "title": "Vector Quantized Representations for Efficient Hierarchical Delineation of Behavioral Repertoires",
    "volume": "review",
    "abstract": "Understanding animal behaviors and their neural underpinnings requires precise kinematic measurements plus analytical methods to parse these continuous, multidimensional measurements into interpretable, organizational descriptions. Existing approaches can identify stereotyped behavioral motifs, given 2D or 3D keypoint-based data but are limited in their interpretability, computational efficiency, and/or ability to seamlessly integrate new behavioral measurements. In this paper, we propose an end-to-end behavioral analysis approach that dissects continuous body movements into sequences of discrete latent variables using vector quantization (VQ). The discrete latent space naturally defines an interpretable deep behavioral repertoire composed of hierarchically organized behavioral motifs. Using recordings of freely moving rodents, we demonstrate that the proposed framework faithfully supports standard behavioral analysis tasks and enables a series of new applications stemming from the discrete information bottleneck, including realistic synthesis of animal body movements and cross-species behavioral mapping",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Oy1NtlFDmD": {
    "title": "STRUCTDROP: A STRUCTURED RANDOM ALGORITHM TOWARDS EFFICIENT LARGE-SCALE GRAPH TRAINING",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have gained considerable success in graph-based learning tasks, yet training GNNs on large graphs is still inefficient. The root cause is the graph-based sparse operations are difficult to accelerate with commodity hardware. Prior art reduces the computation cost of sparse matrix based operations (e.g., linear) via sampling-based approximation. However, two under-explored pain points still persist in this paradigm. Inefficiency Issue: The random-based sampling approaches have the non-zero entries randomly distributing over adjacency matrix, which slows down memory access process and is difficult to accelerate with commodity hardware. Under-fitting Problem: The previous sampling methods only utilize the same subset of nodes during the training, which may cause the under-fitting problem on other remain nodes. Aiming to systematically address these two pain points, we propose StructuredDropout, a.k.a, StructDrop. This method involves the selective random sampling of columns and rows from a sparse matrix for computation. Comprehensive experiments validate the efficiency and generalization of our framework: StructDrop achieves up to 5.09x speedup for a single sparse operation and 6.48x end-to-end speedup with negligible accuracy loss or even better accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPOVOwsDOO": {
    "title": "Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication",
    "volume": "review",
    "abstract": "Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD is a technique that transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend the knowledge distillation paradigm by introducing an interactive communication process to help student models of downstream tasks learn effectively from pre-trained foundation models. Our design is inspired by the way humans learn from teachers who can explain knowledge in a way that meets the students' needs. Specifically, we let each model (i.e., student and teacher) train two components: (1) an encoder which encodes the model's hidden states to a message in a shared message space with other models and (2) a decoder which decodes any message to its own hidden states. With encoder and decoder, not only can the teacher model transfer rich information by encoding its hidden states to messages, but also the student model can send messages with information of downstream tasks to teacher so that the teacher can interpret and generate responses. With this interactive communication process, knowledge passing from teacher to student can be tailored to the student's model capacity and downstream tasks' distributions. We conducted experiments on benchmark datasets for computer vision and recommendation tasks to show that our communication mechanism outperforms state-of-the-art distillation techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=3EWTEy9MTM": {
    "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
    "volume": "review",
    "abstract": "Generating a sequence of intermediate steps, \\emph{a.k.a.}, a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=pmweVpJ229": {
    "title": "Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy",
    "volume": "review",
    "abstract": "Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides $\\varepsilon$-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by $(\\varepsilon,\\delta)$-approximate DP. In practice, however, one needs to apply approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus re-introducing the unappealing $\\delta$-approximation error into the privacy guarantees. To bridge this gap, we propose the Approximate SAample Perturbation (abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to its Wasserstein-infinity ($W_\\infty$) distance from a reference distribution that satisfies pure DP or pure Gaussian DP (i.e., $\\delta=0$). We then leverage a Metropolis-Hastings algorithm to generate the sample and prove that the algorithm converges in W$_\\infty$ distance. We show that by combining our new techniques with a careful localization step, we obtain the first nearly linear-time algorithm that achieves the optimal rates in the DP-ERM problem with strongly convex and smooth losses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=LYGHdwyXUb": {
    "title": "Efficient Multi-task Reinforcement Learning via Selective Behavior Sharing",
    "volume": "review",
    "abstract": "Multi-task Reinforcement Learning (MTRL) offers several avenues to address the issue of sample efficiency through information sharing between tasks. However, prior MTRL methods primarily exploit data and parameter sharing, overlooking the potential of sharing learned behaviors across tasks. The few existing behavior-sharing approaches falter because they directly imitate the policies from other tasks, leading to suboptimality when different tasks require different actions for the same states. To preserve optimality, we introduce a novel, generally applicable behavior-sharing formulation that selectively leverages other task policies as the current task's behavioral policy for data collection to efficiently learn multiple tasks simultaneously. Our proposed MTRL framework estimates the shareability between task policies and incorporates them as temporally extended behaviors to collect training data. Empirically, selective behavior sharing improves sample efficiency on a wide range of manipulation, locomotion, and navigation MTRL task families and is complementary to parameter sharing. Result videos are available at [https://sites.google.com/view/qmp-mtrl](https://sites.google.com/view/qmp-mtrl)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsJwmWvE6Q": {
    "title": "Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms",
    "volume": "review",
    "abstract": "We study the problem of residual error estimation for matrix and vector norms using a linear sketch. Such estimates can be used, for example, to quickly assess how useful a more expensive low-rank approximation computation will be. The matrix case concerns the Frobenius norm and the task is to approximate the $k$-residual $\\|A - A_k\\|_F$ of the input matrix $A$ within a $(1+\\epsilon)$-factor, where $A_k$ is the optimal rank-$k$ approximation. We provide a tight bound of $\\Theta(k^2/\\epsilon^4)$ on the size of bilinear sketches, which have the form of a matrix product $SAT$. This improves the previous $O(k^2/\\epsilon^6)$ upper bound in (Andoni et al. SODA 2013) and gives the first non-trivial lower bound, to the best of our knowledge. In our algorithm, our sketching matrices $S$ and $T$ can both be sparse matrices, allowing for a very fast update time. We demonstrate that this gives a substantial advantage empirically, for roughly the same sketch size and accuracy as in previous work. For the vector case, we consider the $\\ell_p$-norm for $p>2$, where the task is to approximate the $k$-residual $\\|x - x_k\\|_p$ up to a constant factor, where $x_k$ is the optimal $k$-sparse approximation to $x$. Such vector norms are frequently studied in the data stream literature and are useful for finding frequent items or so-called heavy hitters. We establish an upper bound of $O(k^{2/p}n^{1-2/p}\\operatorname{poly}(\\log n))$ for constant $\\epsilon$ on the dimension of a linear sketch for this problem. Our algorithm can be extended to the $\\ell_p$ sparse recovery problem with the same sketching dimension, which seems to be the first such bound for $p > 2$. We also show an $\\Omega(k^{2/p}n^{1-2/p})$ lower bound for the sparse recovery problem, which is tight up to a $\\mathrm{poly}(\\log n)$ factor",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kIPEyMSdFV": {
    "title": "Reverse Diffusion Monte Carlo",
    "volume": "review",
    "abstract": "The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the application of reverse diffusion to Monte Carlo sampling. It is shown that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the posterior distribution, we derive a novel Monte Carlo sampling algorithm from the reverse diffusion process, which is distinct from traditional Markov Chain Monte Carlo (MCMC) methods. We calculate the error requirements and sample size for the posterior distribution, and use the result to derive an algorithm that can approximate the target distribution to any desired accuracy. Additionally, by estimating the log-Sobolev constant of the posterior distribution, we show under suitable conditions the problem of sampling from the posterior can be easier than direct sampling from the target distribution using traditional MCMC techniques. For Gaussian mixture models, we demonstrate that the new algorithm achieves significant improvement over the traditional Langevin-style MCMC sampling methods both theoretically and practically. Our algorithm offers a new perspective and solution beyond classical MCMC algorithms for challenging complex distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICwdNpmu2d": {
    "title": "LLM-based Stock Market Trend Prediction",
    "volume": "review",
    "abstract": "LLM-based Stock Market Trend Prediction Investor sentiment, which is driven by 'intriguing factors' such as news articles and options volume, has been historically resistant to effective use in quantitative methods for predictive market analysis. The emerging science of large language models (LLMs), however, offers a potential solution to this problem. In this paper, we describe our initial experiments with a novel system which prompts available LLMs in a way which allows us to link responses with features in the otherwise more traditional quantitative methods. The results show high accuracy in predicting market moves. We describe the experiments and our initial thoughts about next steps in the paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.5,
    "authors": []
  },
  "https://openreview.net/forum?id=7eYmijcuqO": {
    "title": "On the Dynamics of Learning Time-Aware Behavior with RNNs",
    "volume": "review",
    "abstract": "Recurrent Neural Networks (RNNs) have shown great success in modeling time-dependent patterns, but there is limited research on how they develop representations of temporal features during training. To address this gap, we use timed automata (TA) to introduce a family of supervised learning tasks modeling behavior dependent on hidden temporal variables whose complexity is directly controllable. Building upon past studies from the perspective of dynamical systems theory, we train RNNs to emulate a new class of TA called temporal flipflops, and we find they undergo *phase transitions during training* characterized by sudden and rapid discovery of the hidden time-dependent features. In the case of periodic \"time-of-day\" aware flipflop, we show that the RNNs learn stable periodic cycles that encode time modulo the period of the transition rules. We then use fixed point stability analysis to monitor changes in the RNN dynamics during training, and we observe that the phase transition coincides with a *bifurcation* from which stable periodic behavior emerges. We also show that these cycles initially lose stability if the RNN is later trained on the same TA task but with a different period, and we explain this result through analysis of a simple differential equation for learning oscillations via gradient flow. Through this work, we demonstrate how dynamical systems theory can provide insights into not only learned representations, but also the dynamics and pathologies of the learning process itself",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMh4W72Hhe": {
    "title": "Improving Branching in Neural Network Verification with Bound Implication Graph",
    "volume": "review",
    "abstract": "Many state-of-the-art neural network verifiers for ReLU networks rely on Branch and Bound (BaB)-based methods. They branch ReLUs into positive (active) and negative (inactive) parts, and bound each subproblem independently. Since the cost of verification heavily depends on the number of subproblems, reducing the total number of branches is the key to verifying neural networks efficiently. In this paper, we consider \\emph{bound implications} during branching - i.e., when one or more ReLU neurons are branched into the active (or inactive) case, they may imply that a set of other neurons from any layers become active or inactive, or have their bounds tightened. These implications can eliminate subproblems and improve bounds. We propose a scalable method to find implications among all neurons within tens of seconds even for large ResNets, by reusing pre-computed variables in popular bound-propagation-based verification methods such as $\\alpha$-CROWN, and solving a cheap linear programming problem. Then, we build the bound implication graph (BIG) which connects neurons with bound implications, and it can be used by any BaB-based verifier to reduce the number of branching needed. When evaluated on a set of popular verification benchmarks and a new benchmark consisting of harder verification problems, BIG consistently reduces the verification time and verifies more problems than state-of-the-art verification tools",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qaJxPhkYtD": {
    "title": "Counting Graph Substructures with Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) are powerful representation learning tools that have achieved remarkable performance in various important tasks. However, their ability to count substructures, which play a crucial role in biological and social networks, remains uncertain. In this work, we fill this gap and characterize the representation power of GNNs in terms of their ability to produce powerful representations that count graph substructures. In particular, we study the message-passing operations of GNNs with random stationary input and show that they can produce permutation equivariant representations that are associated with high-order statistical moments. Using these representations, we prove that GNNs can learn how to count cycles, quasi-cliques, and the number of connected components in a graph. We also provide new insights into the generalization capacity of GNNs. Our analysis is constructive and enables the design of a generic GNN architecture that shows remarkable performance in four distinct tasks: cycle detection, cycle counting, graph classification, and molecular property prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zBgAlcIoZP": {
    "title": "Dynamic Mode Decomposition-inspired Autoencoders for Reduced-order Modeling and Control of PDEs : Theory and Design",
    "volume": "review",
    "abstract": "Modeling and controlling complex spatiotemporal dynamical systems driven by partial differential equations (PDEs) often necessitate dimensionality reduction techniques to construct lower-order models for computational efficiency. This paper studies a deep autoencoding learning method for controlling dynamical systems governed by spatiotemporal PDEs. We first analytically show that an optimization objective for learning a linear autoencoding reduced-order model can be formulated, yielding a solution that closely resembles the result obtained through the $\\textit{dynamic mode decomposition with control}$ algorithm. Subsequently, we extend this linear autoencoding architecture to a deep autoencoding framework, enabling the development of a nonlinear reduced-order model. Furthermore, we leverage the learned reduced-order model to design controllers using stability-constrained deep neural networks. Our framework operates without prior knowledge of the governing equations of the underlying system, relying solely on time series data of observations and actuations. Empirical analyses are presented to validate the efficacy of our approach in both modeling and controlling spatiotemporal dynamical systems, exemplified through applications to reaction-diffusion systems and fluid flow systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=GwKNdRc9Bj": {
    "title": "Exploiting Action Distances for Reward Learning from Human Preferences",
    "volume": "review",
    "abstract": "Preference-based Reinforcement Learning (PbRL) with binary preference feedbacks over trajectory pairs has proved to be quite effective in learning complex preferences of a human in the loop in domains with high dimensional state spaces and action spaces. While the human preference is primarily inferred from the feedback provided, we propose that the policy being learned (jointly with the reward model) during training can provide valuable learning signal about the structure of the state space that can be leveraged by the reward learning process. We introduce an action distance measure based on the policy and use it as an auxiliary prediction task for reward learning to influence its embedding space. This measure not only provides insight into the transition dynamics of the environment but also informs about the reachability of states and the overall state space structure. We evaluate the performance and sample efficiency of our approach using a combination of six tasks in Meta-World domains with simulated oracles. We also conduct human in the loop evaluation on three tasks to confirm our findings from oracular experiments. We demonstrate that the proposed simple auxiliary task for constraining reward model's embedding space can provide strong empirical improvements to sample efficiency and accelerate policy learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Y2Txh5uGRe": {
    "title": "Text2Data: Low-Resource Data Generation with Textual Control",
    "volume": "review",
    "abstract": "Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, like molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding both generation quality and controllability across various modalities, including molecules, motions and time series, when compared to existing baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=M9nKQX5nYF": {
    "title": "On the Effect of Defection in Federated Learning and How to Prevent It",
    "volume": "review",
    "abstract": "Federated learning is a machine learning protocol that enables a large population of agents to collaborate. These agents communicate over multiple rounds to produce a single, consensus model. Despite this collaborative framework, there are instances where agents may choose to defect permanently—essentially withdrawing from the collaboration—if they are content with their instantaneous model in that round. This work demonstrates the detrimental impact such defections can have on the final model's robustness and ability to generalize. We also show that current federated optimization algorithms fall short in disincentivizing these harmful defections. To address this, we introduce a novel optimization algorithm with theoretical guarantees to prevent defections while ensuring asymptotic convergence to an effective solution for all participating agents. We also provide numerical experiments to corroborate our findings and demonstrate the effectiveness of our algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=psDvcWtFdE": {
    "title": "DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee",
    "volume": "review",
    "abstract": "Mixed-integer linear programming (MILP) stands as a notable NP-hard problem pivotal to numerous crucial industrial applications. The development of effective algorithms, the tuning of solvers, and the training of machine learning models for MILP resolution all hinge on access to extensive, diverse, and representative data. Yet compared to the abundant naturally occurring data in image and text realms, MILP is markedly data deficient, underscoring the vital role of synthetic MILP generation. We present DIG-MILP, a deep generative framework based on variational auto-encoder (VAE), adept at extracting deep-level structural features from highly limited MILP data and producing instances that closely mirror the target data. Notably, by leveraging the MILP duality, DIG-MILP guarantees a correct and complete generation space as well as ensures the boundedness and feasibility of the generated instances. Our empirical study highlights the novelty and quality of the instances generated by DIG-MILP through two distinct downstream tasks: (S1) Data sharing, where solver solution times correlate highly positive between original and DIG-MILP-generated instances, allowing data sharing for solver tuning without publishing the original data; (S2) Data Augmentation, wherein the DIG-MILP-generated instances bolster the generalization performance of machine learning models tasked with resolving MILP problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=nSDOkm0SKo": {
    "title": "Analyzing Complex Interdependencies in Financial Markets: A Neural Network-Based Approach for News Impact Assessment",
    "volume": "review",
    "abstract": "Analyzing Complex Interdependencies in Financial Markets: A Neural Network-Based Approach for News Impact Assessment In the ever-evolving landscape of financial markets, the intricate web of interdependencies among companies, driven by supply chain intricacies and competitive dynamics, has become a central concern for investors and analysts alike. Our research endeavors to shed light on these intricate relationships and their susceptibility to external news events. In this study, we examine a hypothetical scenario where Company A relies on Companies B and C, Company B depends on Company D, and Company C's fortunes are intertwined with those of Companies E and F, all while these companies are directly reliant on finite natural resources. We use this scenario to illustrate the profound impact of news pertaining to any one of these companies, be it Company A, B, C, or their competitors, on the entire ecosystem. The ripple effect extends through supply chains and demand chains, with repercussions resonating both directly and indirectly. Of importance, we show how emerging ML techniques can model and predict such effects. To navigate this complex terrain, we introduce a novel approach based on constructing dependency graphs for each company using a suitable methodology akin to BFS. This method involves expanding the nodes in the graph to represent companies, scrutinizing their lists of competitors, suppliers, and clients, with terminal nodes denoting natural resources often owned by government entities. Our research harnesses the wealth of sentiment and dependency information extracted from news articles covering a diverse array of companies. These companies are integrated as nodes into our data model. Through the aggregation of stock values for these nodes during successive news intervals, coupled with a meticulous analysis of news sentiment's influence on each node and the deduction of intricate relationships among them, we present a comprehensive view of the interplay between news events and the financial market landscape. The culmination of our efforts culminates in the integration of this analysis into a neural network-based stock trend prediction model. The objective is to assess the effectiveness of our approach in gauging the impact of news on associated companies, providing investors and analysts with a powerful tool to navigate the complex and interconnected world of financial markets. This research not only contributes to a deeper understanding of market dynamics but also offers practical insights for informed decision-making in an increasingly volatile financial landscape",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgVEz6wwbM": {
    "title": "What's the Magic Word? A Control Theory of LLM Prompting",
    "volume": "review",
    "abstract": "Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of a self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k - \\epsilon$ controllability to characterize LLM steerability. We compute the $k-\\epsilon$ controllability of a panel of large language models, including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language modeling tasks. Remarkably, we find that magic words of 10 tokens or less exist for over 97\\% of WikiText instances surveyed for each model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=SuUh5aRbbu": {
    "title": "End-to-end Story Plot Generator",
    "volume": "review",
    "abstract": "Story plots, while short, carry most of the essential information of a full story that may contain tens of thousands of words. We study the problem of automatic generation of story plots, which includes story premise, character descriptions, plot outlines, etc. To generate a single engaging plot, existing plot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands of calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot, which is costly and takes at least several minutes. Moreover, the hard-wired nature of the method makes the pipeline non-differentiable, blocking fast specialization and personalization of the plot generator. In this paper, we overcome these issues with an end-to-end story plot generator, which is (1) faster and cheaper to generate and (2) end-to-end fine-tunable with human feedback. Compared to DOC, our work replaces expensive OpenAI API calls with Llama2 models via careful prompt designs, which leads to the cheap generation of high-quality training datasets. We then perform supervised fine-tuning (SFT) using approximately 13000 story plots to obtain an end-to-end model. The end-to-end model can generate story plots of comparable quality to the previous DOC method and is $>10\\times$ faster (1k tokens in only 30 seconds on average). Furthermore, fine-tuned with RLHF on several different reward models for different aspects of story quality, our model achieves 60.0\\% winning rate against the model after SFT in the aspect of suspense and surprise",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ohdVLirfbz": {
    "title": "Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks",
    "volume": "review",
    "abstract": "Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work (Richards and Kuzborskij,2021; Lei et al.,2022) by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\\sqrt{n})$ for GD algorithms in both two-layer and three-layer NNs. This sheds light on sufficient or necessary conditions for under-parameterized and over-parameterized NNs trained by GD to attain the desired risk rate of $O(1/\\sqrt{n})$. Moreover, we demonstrate that as the scaling parameter increases or the network complexity decreases, less over-parameterization is required for GD to achieve the desired error rates. Additionally, under a low-noise condition, we obtain a fast risk rate of $O(1/n)$ for GD in both two-layer and three-layer NNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=w1JanwReU6": {
    "title": "Are Models Biased on Text without Gender-related Language?",
    "volume": "review",
    "abstract": "In the large language models era, it is imperative to measure and understand how gender biases present in the training data influence model behavior. Previous works construct benchmarks around known stereotypes (e.g., occupations) and demonstrate high levels of gender bias in large language models, raising serious concerns about models exhibiting undesirable behaviors. We expand on existing literature by asking the question: \\textit{Do large language models still favor one gender over the other in non-stereotypical settings?} To tackle this question, we restrict language model evaluation to a \\textit{neutral} subset, in which sentences are free of pronounced word-gender associations. After characterizing these associations in terms of pretraining data statistics, we use them to (1) create a new benchmark with low gender-word associations, and (2) repurpose popular benchmarks in the gendered pronoun setting | WinoBias and \\Winogender |, removing pronounced gender-correlated words. Surprisingly, when testing $20+$ models (e.g., Llama-2, Pythia, and OPT) in the proposed benchmarks, we still detect critically high gender bias across all tested models. For instance, after adjusting for strong word-gender associations, we find that all models still exhibit clear gender preferences in about $60\\%$-$95\\%$ of the sentences, representing a small change (up to $5\\%$) from the original \\textit{stereotypical} setting. By demonstrating that measured bias is not necessarily due to the presence of highly gender-associated words, our work highlights important questions about bias evaluation as well as potentially underlying model biases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=op19LjpHkH": {
    "title": "Decoupled Actor-Critic",
    "volume": "review",
    "abstract": "Actor-Critic methods are in a stalemate of two seemingly irreconcilable problems. Firstly, critic proneness towards overestimation requires sampling temporal-difference targets from a conservative policy optimized using lower-bound Q-values. Secondly, well-known results show that policies that are optimistic in the face of uncertainty yield lower regret levels. To remedy this dichotomy, we propose Decoupled Actor-Critic (DAC). DAC is an off-policy algorithm that learns two distinct actors by gradient backpropagation: a conservative actor used for temporal-difference learning and an optimistic actor used for exploration. We test DAC on DeepMind Control tasks in low and high replay ratio regimes and ablate multiple design choices. Despite minimal computational overhead, DAC achieves state-of-the-art performance and sample efficiency on locomotion tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=J2pMoN2pon": {
    "title": "How do skip connections affect Graph Convolutional networks with graph sampling? A theoretical analysis on generalization",
    "volume": "review",
    "abstract": "Skip connections enable deep Graph Convolutional Networks (GCNs) to overcome oversmoothing, while graph sampling reduces computational demands by selecting a submatrix of the graph adjacency matrix during neighborhood aggregation. Learning deep GCNs with graph sampling has shown empirical success across various applications, but a theoretical understanding of the generalization guarantees remains limited, with existing analyses ignoring either graph sampling or skip connections. This paper presents the first generalization analysis of GCNs with skip connections using graph sampling. Our analysis demonstrates that the generalization accuracy of the learned model closely approximates the highest achievable accuracy within a broad class of target functions dependent on the proposed sparse effective adjacency matrix, denoted by $A^*$. Thus, graph sampling maintains generalization performance when $A^*$ accurately models data correlations. Notably, our findings reveal that skip connections lead to different sampling requirements across layers. In a two-hidden-layer GCN, the generalization is more affected by the sampled matrix deviations from $A^*$ of the first layer than the second layer. To the best of our knowledge, this marks the first theoretical characterization of skip connections' role in sampling requirements. We validate our theoretical results on benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=dFcXJgnrGB": {
    "title": "PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning",
    "volume": "review",
    "abstract": "Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex and often contextualized situations, e.g. ``scheduling a doctor's appointment without a phone''. While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (constrained) language-based planning capabilities. More concretely, we develop *symbolic procedural knowledge distillation* to enhance the commonsense knowledge in small language models and an*inference-time algorithm* to facilitate more structured and accurate reasoning. In addition, we introduce a new related task, *Replanning*, that requires a revision of a plan to cope with a constrained situation. In both the planning and replanning settings, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models' capabilities. Finally, we showcase successful application of PlaSma in an embodied environment, VirtualHome",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=PfPnugdxup": {
    "title": "From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction",
    "volume": "review",
    "abstract": "The role of machine learning in computing atomic properties is expanding rapidly for a wide range of applications from healthcare to climate change. One important ingredient that has enabled this development is the creation of large and diverse molecular datasets. Given the extreme computational cost of these datasets, an important question moving forward is: Can we limit the need for exhaustive large dataset creation by pre-training a foundation style model over multiple chemical domains to generate transferable atomic representations for downstream fine-tuning tasks? Generalization across the entire molecular space is challenging due to the range and complexity of atomic interactions that exist. In this paper, we present Joint Multi-domain Pre-training (JMP), a robust supervised pre-training strategy that utilizes data from multiple chemical domains, $\\sim$120 million examples in total. We demonstrate state-of-the-art results across many targets of the rMD17, QM9, MatBench, QMOF, SPICE, and MD22 datasets. Finally, we conduct ablations to study the impact of different components of JMP on downstream performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FTSUDBM6lu": {
    "title": "Patch Ranking Map: Explaining Relations among Top-Ranked Patches, Top-Ranked Features and Decisions of Convolutional Neural Networks for Image Classification",
    "volume": "review",
    "abstract": "Since a conventional Convolutional Neural Network (CNN) using a large number of extracted features is not fully explainable and not very memory-efficient, we develop an explainable and efficient CNN model consisting of convolutional layers, a new feature selection (FS) layer, a classifier, and a novel ``Patch Ranking Map\" (PRM). The PRM contains top-ranked image patches that have important associations with decisions of the CNN. Top-ranked common features selected by different FS methods are used to generate two newly defined matrices: the ``feature accumulation matrix\" and the ``feature ranking matrix\". Different from a typical heatmap, these two matrices are used to rank image patches in the PRM to effectively explain the relationship among an input image, top-ranked features, top-ranked feature maps, and the final classification decision. Simulation results using the Alzheimer's MRI preprocessed dataset for 4-class image classification with $6,400$ $128\\times128$ images indicate that the three PRMs based on three robust top-ranked common feature sets generated by seven different FS methods have the same top two most important patches associated with Alzheimer's disease diagnosis. In addition, $8\\times8$ patches of a $128\\times128$ image at the 7th and 12th patch rows and at the 8th and 9th patch columns are most informative because they have the top two most important patches and the top two most top-ranked common row-wise and column-wise features. The relationship among brain regions associated with Alzheimer's disease, the top-ranked patches, the top patch rows, and the top patch columns will be analyzed based on research results in brain informatics and medical informatics. The simulations also show that the trained CNN with FS can have higher classification accuracy and smaller model size than the conventional CNN without FS. More effective and efficient optimization algorithms will be developed to select the top (most informative) features and rank patches for building an accurate and efficient CNN model with more explainable decisions that can be captured by the PRM for various image classification applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=HC26cxtI96": {
    "title": "The Fine-Grained Chip Placement with Hybrid Action Spaces and Feature Fusion",
    "volume": "review",
    "abstract": "Chip placement is an essential and time-consuming step in the physical design process. Deep reinforcement learning, as an emerging field, has gained significant attention due to its ability to replace weeks of expert model design. We devise a fusion-based reinforcement learning framework to address the limited representation problem of both graph networks and CNN networks. Furthermore,the structure of PDQN in the hybrid action space allows for precise coordinate placement, compared to other RL-based structures in placement. The experimental results can demonstrate the effectiveness of our model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.0,
    "authors": []
  },
  "https://openreview.net/forum?id=M6XWoEdmwf": {
    "title": "AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents",
    "volume": "review",
    "abstract": "We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is uniquely scalable and applicable to a wide range of problems. We demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a novel hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments. We evaluate our agent on three goal-conditioned domains and study how its individual improvements connect to create a generalist policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=nhgTmx1TZJ": {
    "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
    "volume": "review",
    "abstract": "Language models (LMs) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LMs techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LMs. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released\\footnote{\\url{https://uniaudio666.github.io/demo_UniAudio/}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Zc2aIcucwc": {
    "title": "Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets",
    "volume": "review",
    "abstract": "Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=D8DAQhpznu": {
    "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Selective Classification",
    "volume": "review",
    "abstract": "To maintain user trust, large language models (LLMs) should signal low confidence on examples they get incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but state-of-the-art LLMs such as GPT-4 and Claude do not provide access to these probabilities. We first study eliciting confidence linguistically---asking an LLM for its confidence in its answer---but we find that this leaves a lot of room for improvement (79\\% AUC on GPT-4 averaged across 12 question-answering datasets---only 5\\% above a random baseline). We then explore using a \\emph{surrogate} confidence model---using a model where we do have probabilities to evaluate the original model's confidence in a given question. Surprisingly, even though these probabilities come from a different model, this method leads to higher AUC than linguistic confidences on 10 out of 12 datasets. Our best method mixing linguistic confidences and surrogate model probabilities gives state-of-the-art performance on all 12 datasets (85\\% average AUC on GPT-4)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=w50MQ9Vfty": {
    "title": "Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference",
    "volume": "review",
    "abstract": "Interference is ubiquitous when conducting causal experiments over social networks. Except for certain network structures, causal inference on the network in the presence of interference is difficult due to the entanglement between the treatment assignments and the interference levels. In this article, we conduct causal inference under interference on an observed, sparse but connected network, and we propose a novel design of experiments based on an independent set. Compared to conventional designs, the independent-set design focuses on an independent subset of data and controls their interference exposures through the assignments to the rest (auxiliary set). The independent-set design enhances the performance of causal estimators by trading sample quantity for sample quality. We show the capacity of our approach for various causal inference tasks, justify its superiority over conventional methods, and illustrate the empirical performance through simulations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=gPKTTAfYBp": {
    "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores",
    "volume": "review",
    "abstract": "Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)---which allows long convolutions to run in $O(N\\log N)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms---1) partial convolutions and 2) frequency-sparse convolutions---which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 8.7$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity and M2-BERT-base to achieve 3.3 points higher GLUE score---matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models---yielding the first DNA model that can process the longest human genes (2.3M base pairs)---and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=AKAlVyunxA": {
    "title": "SHINE: Shielding Backdoors in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "Recent studies have discovered that similar to supervised classifiers, a deep reinforcement learning (DRL) policy is also vulnerable to backdoor attacks. Existing defenses against backdoor attacks either do not consider RL's unique mechanism or make unrealistic assumptions, resulting in limited defense efficacy, practicability, and generalizability. In this work, we propose SHINE, a novel backdoor shielding method for DRL. SHINE first leverages policy explanation techniques to identify the backdoor triggers and then designs a policy retraining algorithm to eliminate the negative impact of the triggers on backdoored agents. We theoretically prove that SHINE guarantees to improve a backdoored agent's performance in a poisoned environment while ensuring its performance difference in the clean environment before and after shielding is bounded. We further conduct extensive experiments that evaluate SHINE against three mainstream DRL backdoor attacks in various benchmark RL environments. Our results show that SHINE significantly outperforms existing defenses in mitigating these backdoor attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=oDdzXQzP2F": {
    "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
    "volume": "review",
    "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Cx6Jn6gKHz": {
    "title": "Can adversarial samples benefit few-shot unsupervised implicit neural shape representation learning ?",
    "volume": "review",
    "abstract": "Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. While recent methods rely on smoothness priors to regularize the learning, our method introduces a regularization term that leverages adversarial samples around the shape to improve the learned SDFs. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve SDF learning with respect to baselines and the state-of-the-art using synthetic and real data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=O9gstAazBM": {
    "title": "Efficient Model-Agnostic Multi-Group Equivariant Networks",
    "volume": "review",
    "abstract": "Constructing model-agnostic group equivariant networks, such as equitune (Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be computationally expensive for large product groups. We address this by providing efficient model-agnostic equivariant designs for two related problems: one where the network has multiple inputs each with potentially different groups acting on them, and another where there is a single input but the group acting on it is a large product group. For the first design, we initially consider a linear model and characterize the entire equivariant space that satisfies this constraint. This characterization gives rise to a novel fusion layer between different channels that satisfies an invariance-symmetry (IS) constraint, which we call an IS layer. We then extend this design beyond linear models, similar to equitune, consisting of equivariant and IS layers. We also show that the IS layer is a universal approximator of invariant-symmetric functions. Inspired by the first design, we use the notion of the IS property to design a second efficient model-agnostic equivariant design for large product groups acting on a single input. For the first design, we provide experiments on multi-image classification where each view is transformed independently with transformations such as rotations. We find equivariant models are robust to such transformations and perform competitively otherwise. For the second design, we consider three applications: language compositionality on the SCAN dataset to product groups; fairness in natural language generation from GPT-2 to address intersec- tionality; and robust zero-shot image classification with CLIP. Overall, our methods are simple and general, competitive with equitune and its variants, while also being computationally more efficient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nqir5R4ACn": {
    "title": "Simple Data Sharing for Multi-Tasked Goal-Oriented Problems",
    "volume": "review",
    "abstract": "Many important sequential decision problems -- from robotics, games to logistics -- are multi-tasked and goal-oriented. In this work, we frame them as Contextual Goal Oriented (CGO) problems, a goal-reaching special case of the contextual Markov decision process. CGO is a framework for designing multi-task agents that can follow instructions (represented by contexts) to solve goal-oriented tasks. We show that CGO problem can be systematically tackled using datasets that are commonly obtainable: an unsupervised interaction dataset of transitions and a supervised dataset of context-goal pairs. Leveraging the goal-oriented structure of CGO, we propose a simple data sharing technique that can provably solve CGO problems offline under natural assumptions on the datasets' quality. While an offline CGO problem is a special case of offline reinforcement learning (RL) with unlabelled data, running a generic offline RL algorithm here can be overly conservative since the goal-oriented structure of CGO is ignored. In contrast, our approach carefully constructs an augmented Markov Decision Process (MDP) to avoid introducing unnecessary pessimistic bias. In the experiments, we demonstrate our algorithm can learn near-optimal context-conditioned policies in simulated CGO problems, outperforming offline RL baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekeyCgeRfC": {
    "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
    "volume": "review",
    "abstract": "In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a *teaching sequence*, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement *two distinct* algorithms to solve a *single* task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mSSi0zYkEA": {
    "title": "Initializing the Layer-wise Learning Rate",
    "volume": "review",
    "abstract": "The standard method to assign learning rates has been to rely on the optimizer and to use a single, global learning rate across all its layers. We propose to assign individual learning rates as well, according to the layer-wise gradient magnitude at initialization. Even if individual layers are initialized to preserve gradient variance, architectural characteristics result in uneven gradient magnitude even when the network has not started training. We interpret this gradient magnitude as a measure of architecture-induced convergence bias, and adjust the layer-wise learning rate opposite to its gradient magnitude at initialization. This relative learning rate is maintained throughout the entire training scheme. Experiments on convolutional and transformer architectures on ImageNet-1k show improved accuracy and training stability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=CARclfc9ci": {
    "title": "Relational Convolutional Networks: A framework for learning representations of hierarchical relations",
    "volume": "review",
    "abstract": "A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of *hierarchical* relations, proposing an architectural framework we call \"relational convolutional networks\". Given a sequence of objects, a \"multi-dimensional inner product relation\" module produces a relation tensor describing all pairwise relations. A \"relational convolution\" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demonstrate how relational convolutional networks can provide an effective framework for modeling relational tasks that have hierarchical structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8Q3XTUJP9": {
    "title": "How does overparametrization affect features?",
    "volume": "review",
    "abstract": "Overparametrization, the condition where models have more parameters than necessary to fit their training loss, is a crucial factor for the success of deep learning. However, the characteristics of the features learned by overparametrized networks are not well understood. In this work, we explore this question by comparing models with the same architecture but different widths. We first examine the expressivity of the features of these models, and show that the feature space of overparametrized networks cannot be spanned by concatenating many underparametrized features, and vice versa. This reveals that both overparametrized and underparametrized networks acquire some distinctive features. We then evaluate the performance of these models, and find that overparametrized networks outperform underparametrized networks, even when many of the latter are concatenated. We corroborate these findings using a VGG-16 and ResNet18 on CIFAR-10 and a Transformer on the MNLI classification dataset. Finally, we propose a toy setting to explain how overparametrized networks can learn some important features that the underparamaterized networks cannot learn",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4g02l2N2Nx": {
    "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry",
    "volume": "review",
    "abstract": "Linear attentions have shown promise for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) `inetuned-conversion of task-specific Transformers into linear versions that recover task performance, and (3) pretrained-conversion of Transformers, such as language models, into linear versions readily finetunable on downstream tasks. However, linear attentions often underperform compared to standard softmax attention. To close this performance gap, we study the behaviors of softmax and linear attentions in various train-from-scratch and finetuned-conversion settings. We find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or spiky) weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple, trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99\\% of standard Transformer performance in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions by up to 6 perplexity points on WikiText-103 when training causal GPT models from scratch, and up to 8.7 GLUE score points when converting finetuned bidirectional BERT models. Hedgehog also enables direct pretrained-conversion, achieving a new state-of-the-art WikiText-103 perplexity of 16.7 for 125M decoder-only Transformers by converting pretrained GPT-2 into a linear attention Transformer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=gLZeEpfVjy": {
    "title": "Understanding and Robustifying Sub-domain Alignment for Domain Adaptation",
    "volume": "review",
    "abstract": "In unsupervised domain adaptation (UDA), aligning source and target domains improves the predictive performance of learned models on the target domain. A common methodological improvement in alignment methods is to divide the domains and align sub-domains instead. These sub-domain-based algorithms have demonstrated great empirical success but lack theoretical support. In this work, we establish a rigorous theoretical understanding of the advantages of these methods that have the potential to enhance their overall impact on the field. Our theory uncovers that sub-domain-based methods optimize an error bound that is at least as strong as non-sub-domain-based error bounds and is empirically verified to be much stronger. Furthermore, our analysis indicates that when the marginal weights of sub-domains shift between source and target tasks, the performance of these methods may be compromised. We therefore implement an algorithm to robustify sub-domain alignment for domain adaptation under sub-domain shift, offering a valuable adaptation strategy for future sub-domain-based methods. Empirical experiments across various benchmarks validate our theoretical insights, prove the necessity for the proposed adaptation strategy, and demonstrate the algorithm's competitiveness in handling label shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=XNa6r6ZjoB": {
    "title": "Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers",
    "volume": "review",
    "abstract": "An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the *Abstractor*. At the core of the Abstractor is a variant of attention called *relational cross-attention*. The approach is motivated by an architectural inductive bias for relational learning that disentangles relational information from extraneous features about individual objects. This enables explicit relational reasoning, supporting abstraction and generalization from limited data. The Abstractor is first evaluated on simple discriminative relational tasks and compared to existing relational architectures. Next, the Abstractor is evaluated on purely relational sequence-to-sequence tasks, where dramatic improvements are seen in sample efficiency compared to standard Transformers. Finally, Abstractors are evaluated on a collection of tasks based on mathematical problem solving, where modest but consistent improvements in performance and sample efficiency are observed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=OF5x1dzWSS": {
    "title": "Doubly Robust Instance-Reweighted Adversarial Training",
    "volume": "review",
    "abstract": "Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes can be found in the Supplement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=84fOBZlOiV": {
    "title": "Estimating uncertainty from feed-forward network based sensing using quasilinear approximation",
    "volume": "review",
    "abstract": "Artificial neural networks are increasingly integrated into both sensing hardware (e.g., \"smart sensors\") and dedicated decision-making circuits that operate on this information. As this technology is deployed in safety-critical environments (pedestrian-detection, power management, and flight-controls) it is critical to assess the real-time confidence of information built on these networks. However, while stand-alone confidence of sensing (e.g. object detection) neural networks are common, tools are much more limited for integrating such information into formal estimation of latent variables upstream of the sensor. To make this distinction clear, consider the common problem of target-tracking from a mobile camera. The geographic position of the target is a function of the camera position and orientation in addition to position within the image, whereas the neural network only reports confidence in pixel-space. Likewise, optimally leveraging an image-sequence requires consideration of uncertainty in the camera and target dynamics, as well as the sensing neural network. As we will demonstrate, fusing dynamical system models with large sensing networks presents a major computational challenge. Specifically, popular approaches such as first-order (Jacobian) linearization prove inaccurate, whereas nonlinear sampling-based approaches, while effective, are intractable for high-dimensional measurements such as images. In this work, we borrow an analytic approach from control engineering, quasilinear system approximation, to propagate the dynamics of environmental uncertainty through feedforward neural network architectures. The approximation enables direct Bayesian (i.e., Kalman-style) filtering to estimate latent variables, thus obviating the need for taxing sampling-based approaches. Thus, the proposed framework may enable real-time confidence estimation in high-dimensional network-based sensing deployments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FjifPJV2Ol": {
    "title": "SOLVING SCHRODINGER BRIDGE PROBLEM VIA STOCHASTIC ACTION MINIMIZATION",
    "volume": "review",
    "abstract": "The Schrodinger bridge problem is a classical entropy-regularized optimal transport problem that seeks to find optimal diffusion trajectories that transform one probability distribution into another. Although mathematical theory has reached a mature stage, the ongoing research in algorithmic advancements remains a dynamic field, driven by recent innovations in diffusion models. We introduce stochastic Lagrangian and stochastic action as viable alter- native for serving as a direct loss function. We demonstrate the feasibility of incorporating all the vital physical constraints necessary to solve the problem directly into the Lagrangian, providing an intuitive grasp of the loss function and streamlining the training process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=YCWjhGrJFD": {
    "title": "Training Diffusion Models with Reinforcement Learning",
    "volume": "review",
    "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Yp01vcQSNl": {
    "title": "DIRECTIONALITY IN GRAPH TRANSFORMERS",
    "volume": "review",
    "abstract": "We study how one can capture directionality in graph transformers, for learning over directed graphs. Most existing graph transformers do not take edge direction into account. We therefore introduce a novel graph transformer architecture that explicitly takes into account the edge directionality. To achieve this, we make use of dual encodings to represent both potential roles, i.e., source or target, of each pair of vertices linked by a directed edge. These dual encodings are learned by leveraging the latent adjacency information extracted from a novel directional attention module, localized with $k$-hop neighborhood information. We also study alternative approaches to incorporating directionality into other graph transformers to enhance their performance on directed graph learning tasks. To evaluate the importance of edge direction, we empirically characterize via randomization whether direction really matters for the downstream task. We propose two new directional graph datasets where direction is intrinsically related to learning. Via experiments on directional graph datasets, we show that our approach yields state-of-the-art results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=OnLAftJkhV": {
    "title": "Latent Conservative Objective Models for Offline Data-Driven Crystal Structure Prediction",
    "volume": "review",
    "abstract": "In computational chemistry, crystal structure prediction (CSP) is an optimization problem that involves discovering the lowest energy stable crystal structure for a given chemical formula. This problem is challenging as it requires discovering globally optimal designs with the lowest energies on complex manifolds. One approach to tackle this problem involves building simulators based on density functional theory (DFT) followed by running search in simulation, but these simulators are painfully slow. In this paper, we study present and study an alternate, data-driven approach to crystal structure prediction: instead of directly searching for the most stable structures in simulation, we train a surrogate model of the crystal formation energy from a database of existing crystal structures, and then optimize this model with respect to the parameters of the crystal structure. This surrogate model is trained to be conservative so as to prevent exploitation of its errors by the optimizer. To handle optimization in the non-Euclidean space of crystal structures, we first utilize a state-of-the-art graph variational auto-encoder (CD-VAE) to convert a crystal structure into a vector-based search space and then optimize a conservative surrogate model of the crystal energy, trained on top of this vector representation. We show that our approach, dubbed LCOMs (latent conservative objective models), performs comparably to the best current approaches in terms of success rate of structure prediction, while also drastically reducing computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=D2eOVqPX9g": {
    "title": "Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning",
    "volume": "review",
    "abstract": "Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-time error analysis. Notably, we establish that FedSARSA converges to a policy that is near-optimal for all agents, with the extent of near-optimality proportional to the level of heterogeneity. Furthermore, we prove that FedSARSA leverages agent collaboration to enable linear speedups as the number of agents increases, which holds for both fixed and adaptive step-size configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jQ596tXT3k": {
    "title": "Explaining the Out-of-Distribution Detection Paradox through Likelihood Peaks",
    "volume": "review",
    "abstract": "Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having high likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass, which can occur if the density is sharply peaked. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur on data confined to low dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Moreover, we provide an efficient method for estimating LID from a normalizing flow model, improving upon existing estimators, and enabling state-of-the-art OOD detection performance with respect to comparable flow-based benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=fe6ANBxcKM": {
    "title": "Federated Q-Learning: Linear Regret Speedup with Low Communication Cost",
    "volume": "review",
    "abstract": "In this paper, we consider federated reinforcement learning for tabular episodic Markov Decision Processes (MDP) where, under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. While linear speedup in the number of agents has been achieved for some metrics, such as convergence rate and sample complexity, in similar settings, it is unclear whether it is possible to design a *model-free* algorithm to achieve linear *regret* speedup with low communication cost. We propose two federated Q-Learning algorithms termed as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the corresponding total regrets achieve a linear speedup compared with their single-agent counterparts, while the communication cost scales logarithmically in the total number of time steps $T$. Those results rely on an event-triggered synchronization mechanism between the agents and the server, a novel step size selection when the server aggregates the local estimates of the state-action values to form the global estimates, and a set of new concentration inequalities to bound the sum of non-martingale differences. This is the first work showing that linear regret speedup and logarithmic communication cost can be achieved by model-free algorithms in federated reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=MeHmwCDifc": {
    "title": "The Trickle-down Impact of Reward Inconsistency on RLHF",
    "volume": "review",
    "abstract": "Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs --- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments --- and their impact on the downstream RLHF model. In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training? We propose **Contrast Instruction** -- a benchmarking strategy for the consistency of RM. Each example in **Contrast Instruction** features a pair of lexically similar instructions with different ground truth responses. A consistent RM is expected to rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on \\contrast{} compared to average humans. To show that RM consistency can be improved efficiently without using extra training budget, we propose two techniques **ConvexDA** and **RewardFusion**, which enhance reward consistency through extrapolation during the RM training and inference stage, respectively. We show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=nZ7rpEp6wj": {
    "title": "Multi-Resolution Learning with DeepONets and Long Short-Term Memory Neural Networks",
    "volume": "review",
    "abstract": "Deep operator networks (DeepONets, DONs) offer a distinct advantage over traditional neural networks in their ability to be trained on multi-resolution data. This property becomes especially relevant in real-world scenarios where high-resolution measurements are difficult to obtain, while low-resolution data is more readily available. Nevertheless, DeepONets alone often struggle to capture and maintain dependencies over long sequences compared to other state-of-the-art algorithms. We propose a novel architecture, named DON-LSTM, which extends the DeepONet with a long short-term memory network (LSTM). Combining these two architectures, we equip the network with explicit mechanisms to leverage multi-resolution data, as well as capture temporal dependencies in long sequences. We test our method on long-time-evolution modeling of multiple non-linear systems and show that the proposed multi-resolution DON-LSTM achieves significantly lower generalization error and requires fewer high-resolution samples compared to its vanilla counterparts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=qVILwUxjLG": {
    "title": "Non-stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling",
    "volume": "review",
    "abstract": "Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Kwm1OyINXt": {
    "title": "Deep probabilistic 3D angular regression for directional dark matter detectors",
    "volume": "review",
    "abstract": "Modern detectors of elementary particles are approaching a fundamental sensitivity limit where individual quanta of charge can be localized and counted in 3D. This enables novel detectors capable of unambiguously demonstrating the particle nature of dark matter by inferring the 3D directions of elementary particles from complex point cloud data. The most complex scenario involves inferring the initial directions of low-energy electrons from their tortuous trajectories. To address this problem we develop and demonstrate the first probabilistic deep learning model that predicts 3D directions using a heteroscedastic von Mises-Fisher distribution that allows us to model data uncertainty. Our approach generalizes the cosine distance loss which is a special case of our loss function in which the uncertainty is assumed to be uniform across samples. We utilize a sparse 3D convolutional neural network architecture and develop approximations to the negative log-likelihood loss which stabilize training. On a simulated Monte Carlo test set, our end-to-end deep learning approach achieves a mean cosine distance of $0.104$ $(26^\\circ)$ compared to $0.556$ $(64^\\circ) $ achieved by a non-machine learning algorithm. We demonstrate that the model is well-calibrated and allows selecting low-uncertainty samples to improve accuracy. This advancement in probabilistic 3D directional learning could significantly contribute to directional dark matter detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=eeaKRQIaYd": {
    "title": "Unsupervised Sign Language Translation and Generation",
    "volume": "review",
    "abstract": "Sign language translation and generation are crucial in facilitating communication between the deaf and hearing communities.However, the scarcity of parallel sign language video-to-text data poses a considerable challenge to developing effective sign language translation and generation systems.Motivated by the success of unsupervised neural machine translation (UNMT), this paper introduces an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. Inspired by UNMT, USLNet comprises two main components: single-modality reconstructing modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences.To address the issues, we propose a sliding window method to align variable-length text with video sequences.To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both text and sign language video in a unified manner.Experimental results on the BBC-Oxford Sign Language datasets (BOBSL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1TKGCrSL7": {
    "title": "Cross-modality debiasing: using language to mitigate sub-population shifts in imaging",
    "volume": "review",
    "abstract": "Sub-population shift is a specific type of domain shift that highlights changes in data distribution within specific sub-groups or populations between training and testing. Sub-population shift accounts for a significant source of algorithmic bias and calls for distributional robustness. Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper, we propose leveraging the connection of robustness among different modalities and reshaping the distributional robustness of one modality with another. Specifically, in the context of the distributional robustness of CLIP, we propose to leverage natural language inputs to debias the image feature representations, to improve worst-case performance on sub-populations. Our extensive empirical studies show that image representations debiased by natural language can achieve significant performance improvement and reduction of performance instability under sub-population shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=edETIhDTwL": {
    "title": "Decompose Time and Frequency Dependencies: Multivariate Time Series Physiological Signal Emotion Recognition",
    "volume": "review",
    "abstract": "In this study, we proposed a transformer based end-to-end solution to capture the relationship between the physiological signals and affective changes. We first convert the physiological signal emotion recognition prediction task to a sequence-to-sequence multivariate time series prediction task. We utilize the state-of-the-art (SOTA) self-attention mechanism to decompose the physiological signals into separate frequency domain and time domain representations, and capture the channel dependencies via Two-Stage Attention (TSA). Meanwhile, we implement the multitask learning framework to better predict the valence and arousal affective states individually. We evaluate our system on the Continuously Annotated Signals of Emotion (CASE) dataset used in the Emotion Physiology and Experience Collaboration (EPiC) challenge, and our proposed system outperform all the challenge participants in all four test scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=PKsTHJXn4d": {
    "title": "Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation",
    "volume": "review",
    "abstract": "Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts; however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, thus making our method independent from the underlying model's representation. For such models, we first learn a behavior representation and subsequently use it to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. We evaluate our method in a multi-agent search-and-rescue environment and demonstrate the effectiveness of our explanations for agents executing various behaviors. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those produced by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RVaUSKSh9t": {
    "title": "Continual Graph Learning for Thermal Analysis of Composite Materials under Interface Variations",
    "volume": "review",
    "abstract": "Thermal analysis is an important topic in many fields, such as building, machinery, and microelectronics. As the types of materials in a system are increasingly diverse, conventional numerical methods or machine learning-based surrogate models face tremendous challenges in computation cost and accuracy. Furthermore, a realistic system usually suffers from random fabrication variations that induce significant errors in model prediction. To overcome these issues, we propose Graph Neural Networks (GNN) as a framework for thermal analysis of composite materials with diverse thermal conductivity and thermal interface variations. Using chiplets in microelectronics as the study case, we first partition the system into sub-blocks based on their material property. Then we develop a physics-constrained GNN as the aggregator to integrate local models of each sub-block into a system, with the edge to represent the thermal interaction. In the presence of interface variations, we introduce continual adaptation of the GNN model, using a minimum number of training samples. Compared with previous solutions, our GNN model is robust for various material and interface conditions, and efficient in the prediction of hot-spot",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=XJiN1VkgA0": {
    "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the \"dispersion\" of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty metrics, applying them to *selective* NLG where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=e3tveFVmoH": {
    "title": "Stochastic two points method for deep model gradient free optimization",
    "volume": "review",
    "abstract": "Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms standard methods across various model types and scales, with 2$\\times$ speed-up in training over most conducted tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZVi81SH1Ob": {
    "title": "Neural Collapse meets Differential Privacy: Curious behaviors of NoisySGD with Near-Perfect Representation Learning",
    "volume": "review",
    "abstract": "In recent studies, it has been demonstrated that large-scale representation learning through pre-training on gigantic datasets significantly enhances differentially private learning for downstream tasks. By training on Google's proprietary JFT dataset, one can achieve an unprecedented 83% Top 1 accuracy on ImageNet with strong privacy parameters $(0.5,8\\times 10^{-7})$-DP, even given the high dimensionality of the feature space. While the exact behaviors of NoisySGD in these scenarios remain theoretically challenging to analyze, we explore an idealized setting using a layer-peeled model for representation learning, which results in interesting phenomena of the learned features known as neural collapse. Under this setting, we have observed several notable behaviors of NoisySGD. Specifically, we demonstrate that under perfect neural collapse, the misclassification error is unaffected by the dimension of the features. This dimension-independent result holds with any learning rate and even with class imbalance and is not influenced by the nature of the loss functions. Nevertheless, a dimension dependency emerges when introducing minor perturbations in either the feature or model space. To address this dependency under perturbation, we suggest several strategies, such as pre-processing features or employing principal component analysis to reduce feature dimensions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ip5LHJs6QX": {
    "title": "Efficient Modulation for Vision Networks",
    "volume": "review",
    "abstract": "In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the abstracted modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Bene- fiting from the prominent representational ability of modulation mechanism and the efficiency of efficient modulation design, our network can accomplish better accuracy-efficiency trade-offs and set new state-of-the-art performance for efficient networks. When integrating EfficientMod block with the vanilla self-attention block, we obtain the hybrid architecture and further improve the performance without sacrificing the efficiency. We carry out comprehensive experiments to verify EfficientMod's performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than the prior state-of-the-art approach EfficientFormerV2-s2 without any training tricks and is 25% faster on GPU. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Codes and checkpoints are available in the supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fB1iiH9xo7": {
    "title": "Pre-training LiDAR-based 3D Object Detectors through Colorization",
    "volume": "review",
    "abstract": "Accurate 3D object detection and understanding for self-driving cars heavily relies on LiDAR point clouds, necessitating large amounts of labeled data to train. In this work, we introduce an innovative pre-training approach, Grounded Point Colorization (GPC), to bridge the gap between data and labels by teaching the model to colorize LiDAR point clouds, equipping it with valuable semantic cues. To tackle challenges arising from color variations and selection bias, we incorporate color as \"context\" by providing ground-truth colors as hints during colorization. Experimental results on the KITTI and Waymo datasets demonstrate GPC's remarkable effectiveness. Even with limited labeled data, GPC significantly improves fine-tuning performance; notably, on just 20% of the KITTI dataset, GPC outperforms training from scratch with the entire dataset. In sum, we introduce a fresh perspective on pre-training for 3D object detection, aligning the objective with the model's intended role and ultimately advancing the accuracy and efficiency of 3D object detection for autonomous vehicles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eo7kv0sllr": {
    "title": "An Emulator for Fine-tuning Large Language Models using Small Language Models",
    "volume": "review",
    "abstract": "Widely used language models (LMs) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage using more targeted examples of specific behaviors and/or human preferences. While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been rigorously tested. In this paper, we test this hypothesis with a novel methodology for scaling these two stages independently, essentially asking, *What would happen if we combined the knowledge learned by a large model during pre-training with the knowledge learned by a small model during fine-tuning (or vice versa)?* Using an RL-based framework derived from recent developments in learning from human preferences, we introduce *emulated fine-tuning (EFT)*, a principled and practical method for sampling from a distribution that approximates the result of pre-training and fine-tuning at different scales. Our experiments with EFT show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality. Further, we show that EFT enables test-time adjustment of competing behavioral factors like helpfulness and harmlessness without additional training. Finally, we find that a special case of emulated fine-tuning, which we call LM *up-scaling*, avoids resource-intensive fine-tuning of large pre-trained models by ensembling small fine-tuned models with large pre-trained models, essentially 'emulating' the result of fine-tuning the large pre-trained model. Up-scaling consistently improves helpfulness and factuality of widely used pre-trained models like Llama, Llama-2, and Falcon, without additional hyperparameters or training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=wsWGcw6qKD": {
    "title": "Toward Student-oriented Teacher Network Training for Knowledge Distillation",
    "volume": "review",
    "abstract": "How to conduct teacher training for knowledge distillation is still an open problem. It has been widely observed that a best-performing teacher does not necessarily yield the best-performing student, suggesting a fundamental discrepancy between the current teacher training practice and the ideal teacher training strategy. To fill this gap, we explore the feasibility of training a teacher that is oriented toward student performance with empirical risk minimization (ERM). Our analyses are inspired by the recent findings that the effectiveness of knowledge distillation hinges on the teacher's capability to approximate the true label distribution of training inputs. We theoretically establish that ERM minimizer can approximate the true label distribution of training data as long as the feature extractor of the learner network is Lipschitz continuous and is robust to feature transformations. In light of our theory, we propose a teacher training method SoTeacher which incorporates Lipschitz regularization and consistency regularization into ERM. Experiments on benchmark datasets using various knowledge distillation algorithms and teacher-student pairs confirm that SoTeacher can improve student accuracy consistently",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jE8xbmvFin": {
    "title": "Language Models Represent Space and Time",
    "volume": "review",
    "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process---a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=pAoqRlTBtY": {
    "title": "Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning",
    "volume": "review",
    "abstract": "Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=8OBuqbLb8h": {
    "title": "Fast-ELECTRA for Efficient Pre-training",
    "volume": "review",
    "abstract": "ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity to hyper-parameters and enhances the pre-training stability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNpSUN0uCc": {
    "title": "Maximum Entropy Model Correction in Reinforcement Learning",
    "volume": "review",
    "abstract": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model's next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=DDAtRS5Ngf": {
    "title": "Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings",
    "volume": "review",
    "abstract": "Multi-modal embeddings encode images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call ``adversarial illusions.'' Given an image or a sound, an adversary can perturb it so as to make its embedding close to an arbitrary, adversary-chosen input in another modality. This enables the adversary to align any image and any sound with any text. Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=iPtgY9cJaV": {
    "title": "Identifying Latent State Transition Processes for Individualized Reinforcement Learning",
    "volume": "review",
    "abstract": "In recent years, reinforcement learning (RL) has been increasingly applied to systems that interact with individuals in various domains, such as healthcare, education, and e-commerce. When an RL agent interacts with individuals, individual-specific factors, ranging from personal preferences to physiological nuances, may causally influence state transitions, such as health conditions, learning progress, or user selections. Consequently, different individuals may exhibit different state transition processes. Understanding these individualized state-transition processes is crucial for making individualized policies. In practice, however, identifying these state-transition processes is challenging, especially since individual-specific factors often remain latent. In this paper, we present a practical method that effectively learns these processes from observed state-action trajectories, backed by theoretical guarantees. To our knowledge, this is the first work to provide a theoretical guarantee for identifying the state-transition processes involving latent individual-specific factors. Our experiments on synthetic and real-world datasets demonstrate that our method can effectively identify the latent state-transition processes and help learn individualized RL policies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0Ce3c9l7G1": {
    "title": "Learning Multi-Agent Communication using Regularized Attention Messages",
    "volume": "review",
    "abstract": "Learning how to communicate in Multi-Agent Reinforcement Learning (MARL) can be key to solve complex cooperative tasks. Recent approaches have shown the advantages of using an efficient communication architecture, tackling problems such as what, when, or whom to communicate. However, these methods still fail to solve some complex scenarios, and some of them do not evaluate the implications of having limited communication channels. In this paper, we propose Attentive Regularized Communication (ARCOMM), a new method for communication in MARL. The proposed method uses an attention module to evaluate the weight of the messages generated by the agents, together with a message regularizer that facilitates learning more meaningful messages, improving the performance of the team. We further analyse how ARCOMM reacts to situations where the messages must be compressed before being sent to other agents. Our results show that the proposed method helps, through the power of communication, to improve the performances of the agents in complex domains when compared to other methods. Furthermore, we show that, although there is a decrease of performance, agents are still capable of learning even with lossy communication. The messages learned by the agents also support the motivations for our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=aN4Jf6Cx69": {
    "title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
    "volume": "review",
    "abstract": "Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 9.0,
    "authors": []
  },
  "https://openreview.net/forum?id=D9rJdtmIG6": {
    "title": "SpaCE: The Spatial Confounding Environment",
    "volume": "review",
    "abstract": "Spatial confounding poses a significant challenge in scientific studies involving spatial data, where unobserved spatial variables can influence both treatment and outcome, possibly leading to spurious associations. To address this problem, we introduce SpaCE: The Spatial Confounding Environment, the first toolkit to provide realistic benchmark datasets and tools for systematically evaluating causal inference methods designed to alleviate spatial confounding. Each dataset includes training data, true counterfactuals, a spatial graph with coordinates, and smoothness and confounding scores characterizing the effect of a missing spatial confounder. It also includes realistic semi-synthetic outcomes and counterfactuals, generated using state-of-the-art machine learning ensembles, following best practices for causal inference benchmarks. The datasets cover real treatment and covariates from diverse domains, including climate, health and social sciences. SpaCE facilitates an automated end-to-end pipeline, simplifying data loading, experimental setup, and evaluating machine learning and causal inference models. The SpaCE project provides several dozens of datasets of diverse sizes and spatial complexity. It is publicly available as a Python package, encouraging community feedback and contributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=TTEwosByrg": {
    "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as \"System Star is better than System Square.\" We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of \\textbf{40\\%} of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be \\textbf{49.6\\%}, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=OkHHJcMroY": {
    "title": "PILOT: An $\\mathcal{O}(1/T)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation",
    "volume": "review",
    "abstract": "Learning an accurate value function for a given policy is a critical step in solving reinforcement learning (RL) problems. So far, however, the convergence speed and sample complexity performances of most existing policy evaluation algorithms remain unsatisfactory, particularly with non-linear function approximation. This challenge motivates us to develop a new path-integrated primal-dual stochastic gradient (PILOT) method, that is able to achieve a fast convergence speed for RL policy evaluation with nonlinear function approximation. To further alleviate the periodic full gradient evaluation requirement, we further propose an enhanced method with an adaptive-batch adjustment called PILOT$^+$. The main advantages of our methods include: i) PILOT allows the use of {\\em{constant}} step sizes and achieves the $\\mathcal{O}(1/K)$ convergence rate to first-order stationary points of non-convex policy evaluation problems; ii) PILOT is a generic {\\em{single}}-timescale algorithm that is also applicable for solving a large class of non-convex strongly-concave minimax optimization problems; iii) By adaptively adjusting the batch size via historical stochastic gradient information, PILOT$^+$ is more sample-efficient empirically without loss of theoretical convergence rate. Our extensive numerical experiments verify our theoretical findings and showcase the high efficiency of the proposed PILOT and PILOT$^+$ algorithms compared with the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2ve0q6cIO": {
    "title": "Graph Neural Networks Gone Hogwild",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) constitute a dominant class of architectures for modelling graph-structured data. Message-passing GNNs in particular appear to be ideal for applications where distributed inference is desired, since node updates can be performed locally. Implementing distributed inference of GNNs on enormous graphs is a conspicuous example of such an application. In this work, we are particularly motivated by the view that GNNs can be interpreted as parametric communication policies between agents which collectively solve a distributed optimization problem (e.g., in robotic swarms or sensor networks). For these applications, node synchrony and central control are undesirable, since they result in communication bottlenecks and reduce fault tolerance and scalability. We examine GNN inference under asynchrony, and find that most GNNs generate arbitrarily incorrect predictions in this regime. A notable exception is GNNs which cast message passing as a fixed point iteration with contractive update functions. We propose a novel GNN architecture, energy GNNs, in which node embeddings are computed by minimizing a scalar-valued convex \"energy\" function. By framing message passing as convex optimization, we unlock a richer class of update functions which preserve robustness under asynchronous execution. We show that, empirically, we outperform other GNNs which are amenable to asynchronous execution on a multitude of tasks across both synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=u8L1zzGXRq": {
    "title": "Impact of Molecular Representations on Deep Learning Model Comparisons in Drug Response Predictions",
    "volume": "review",
    "abstract": "Deep learning (DL) plays a crucial role in tackling the complexity and heterogeneity of cancer, particularly in predicting drug response. However, the effectiveness of these models is often hindered by inconsistent benchmarks and disparate data sources. To address the gaps in comparisons, we introduce CoMParison workflow for Cross Validation (CMP-CV), an automated cross-validation framework that trains multiple models with user-specified parameters and evaluation metrics. The effectiveness of DL models in predicting drug responses is closely tied to the methods used to represent drugs at the molecular level. In this contribution, we benchmarked commonly leveraged drug representations (graph, molecular descriptors, molecular fingerprints, and SMILES) to lean and understand the predictive capabilities of the models. We compare the ability of different drug representations to encode different structural properties of the drugs by using prediction errors made by models in different drug descriptor domains. We find that, in terms of the average prediction error over the entire test set, molecular descriptor and encoded SMILES representations perform slightly better than the others. However, we also observe that the rankings of the model performance vary in different regions over the descriptor space studied in this work, emphasizing the importance of domain-based model comparison when selecting a model for a specific application. Our efforts are part of CANcer Distributed Learning Environment (CANDLE), enhancing the model comparison capabilities in cancer research and driving the development of more effective strategies for drug response prediction and optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMuPAOY8Oz": {
    "title": "Positional Description Matters for Transformers Arithmetic",
    "volume": "review",
    "abstract": "Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=q38SZkUmUh": {
    "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "volume": "review",
    "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future research, we will release FreshQA after blind review and commit to updating it at regular intervals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=PEuO8WTolW": {
    "title": "STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning",
    "volume": "review",
    "abstract": "Recently, multi-objective optimization (MOO) problems have received increasing attention due to their wide range of applications in various fields, such as machine learning (ML), operations research, and many engineering applications. However, MOO algorithm design remains in its infancy and many existing MOO methods suffer from unsatisfactory convergence performance. To address this challenge, in this paper, we propose an algorithm called STIMULUS (**ST**ochastic path-**I**ntegrated **MUL**ti-graident rec**U**rsive e**S**timator), a new and robust approach for solving MOO problems. Different from the traditional methods, STIMULUS introduces a simple yet powerful recursive framework for updating stochastic gradient estimates. This methodology improves convergence performance by reducing the variance in multi-gradient estimation, leading to more stable convergence paths. In addition, we introduce an enhanced version of STIMULUS, termed STIMULUS-M, which incorporates the momentum term to further expedite convergence. One of the key contributions of this paper is the theoretical analysis for both STIMULUS and STIMULUS-M, where we establish an $\\mathcal{O}(\\frac{1}{T})$ convergence rate for both methods, which implies a state-of-the-art sample complexity of $O\\left(n+\\sqrt{n}\\epsilon^{-1}\\right)$ under non-convexity settings. In the case where the objectives are strongly convex, we further establish a linear convergence rate of $\\mathcal{O}(e^{-\\mu T})$ of the proposed methods, which suggests an even stronger $\\mathcal{O}\\left(n+ \\sqrt{n} \\ln ({\\mu/\\epsilon})\\right)$ sample complexity. Moreover, to further alleviate the periodic full gradient evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced versions with adaptive batching called STIMULUS$^+$/STIMULUS-M$^+$ and provide their theoretical analysis. Our extensive experimental results verify the efficacy of our proposed algorithms and their superiority over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4eJDMjYZZG": {
    "title": "Language Model Detectors Are Easily Optimized Against",
    "volume": "review",
    "abstract": "The fluency and general applicability of large language models (LLMs) has motivated significant interest in detecting whether a piece of text was written by a language model. While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems. In this paper, we demonstrate a data-efficient attack that fine-tunes language models to confuse existing detectors, leveraging recent developments in reinforcement learning of language models. We use the 'human-ness' score (often just a log probability) of various open-source and commercial detectors as a reward function for reinforcement learning, subject to a KL-divergence constraint that the resulting model does not differ significantly from the original. For a 7B parameter Llama-2 model, fine-tuning for under a day reduces the AUROC of the OpenAI RoBERTa-Large detector from 0.84 to 0.62, while perplexity on OpenWebText increases from 8.7 to only 9.0; with a larger perplexity budget, we reduce AUROC to 0.30 (worse than random), with a perplexity increase to 9.9. Similar to traditional adversarial attacks, we find that this increase in 'detector evasion' generalizes to other detectors not used during training. In light of our empirical results, we advise against continued reliance on LLM-generated text detectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WnqD3EiylC": {
    "title": "The Representation Jensen-Shannon Divergence",
    "volume": "review",
    "abstract": "Statistical divergences quantify the difference between probability distributions, thereby allowing for multiple uses in machine-learning. However, a fundamental challenge of these quantities is their estimation from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose a divergence inspired by the Jensen-Shannon divergence which avoids the estimation of the probability density functions. Our approach embeds the data in an reproducing kernel Hilbert space (RKHS) where we associate data distributions with uncentered covariance operators in this representation space. Therefore, we name this measure the representation Jensen-Shannon divergence (RJSD). We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without an explicit mapping to the RKHS. We provide consistency convergence results for the proposed estimator. Moreover, we demonstrate that this quantity is a lower bound on the Jensen-Shannon divergence, leading to a variational approach to estimate it with theoretical guarantees. We leverage the proposed divergence to train generative networks, where our method mitigates mode collapse and encourages samples diversity. Additionally, RJSD surpasses other state-of-the-art techniques in multiple two-sample testing problems, demonstrating superior performance and reliability in discriminating between distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0chJTSbci": {
    "title": "Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models",
    "volume": "review",
    "abstract": "If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot's own training data. We propose SuSIE, a method that leverages an image editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller attains. Specifically, we fine-tune InstructPix2Pix on robot data such that it outputs a hypothetical future observation given the robot's current observation and a language command. We then use the same robot data to train a low-level goal-conditioned policy to reach a given image observation. We find that when these components are combined, the resulting system exhibits robust generalization capabilities. The high-level planner utilizes its Internet-scale pre-training and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization than conventional language-conditioned policies. We demonstrate that this approach solves real robot control tasks involving novel objects, distractors, and even environments, both in the real world and in simulation. The project website can be found at http://subgoal-image-editing.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=9jMoHuqjfg": {
    "title": "Learning to Reach Goals via Diffusion",
    "volume": "review",
    "abstract": "Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=VmnWoLbzCS": {
    "title": "LUMOS: Towards Language Agents that are Unified, Modular, and Open Source",
    "volume": "review",
    "abstract": "In this paper, we present LUMOS, **L**anguage agents with **U**nified formats, **M**odular design, and **O**pen **S**ource LLMs. LUMOS features a modular architecture consisting of planning, grounding, and execution modules built based on open-source LLMs such as LLAMA-2. The planning module decomposes a task into a sequence of high-level subgoals; the grounding module then grounds the generated subgoals to a series of low-level actions that can then be executed by the execution module. To obtain high-quality annotations for training these modules, we leverage LLMs to convert ground-truth intermediate reasoning steps in existing benchmarks into a unified format that can be used in the LUMOS framework. LUMOS achieves competitive or superior performance compared to the state of the art on a variety of complex interactive tasks. We observe: (1) LUMOS is competitive with the LLM agents that are 2 − 4× larger on maths tasks, and outperforms GPT-4/3.5-based agents on complex QA and web agent tasks; (2) LUMOS shows superior performance against open-source agent baseline formulations including chain-of-thoughts fine-tuning and unmodularized training; (3) LUMOS surpasses larger LLM-based agents on an unseen interactive task, WebShop, and achieves 5-10 reward improvement over domain-specific agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=TB5THwq1sq": {
    "title": "Physics Informed Neurally Constructed ODE Networks (PINeCONes)",
    "volume": "review",
    "abstract": "Recently, there has been a growing interest in using neural networks to approximate the solutions of partial differential equations (PDEs). Physics-informed neural networks (PINNs) have emerged as a promising framework for parameterizing PDE solutions using deep neural networks. However, PINNs often rely on memory-intensive optimizers to attain reasonable accuracy and can encounter training difficulties due to issues such as stiffness in the gradient flow of the loss. To address these challenges, we propose a novel network architecture that combines neural ordinary differential equations (ODEs) with physics-informed constraints in the loss function. In this approach, the dynamics within a neural ODE are expanded to include a system of ODEs whose solution provides the partial derivatives governing our PDE system. We call this architecture PINECONEs: physics-informed neurally constructed ODE networks. We evaluate the approach using simple but canonical PDEs from the literature to illustrate its potential. Our results show that training requires fewer iterations than previous approaches to achieve higher accuracy when using first-order optimization methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.6,
    "authors": []
  },
  "https://openreview.net/forum?id=MqEQbvPvkE": {
    "title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US",
    "volume": "review",
    "abstract": "In policy research, one of the most critical analytic tasks is to estimate the causal effect of a policy-relevant shift to the distribution of a continuous exposure/treatment on an outcome of interest. We call this problem *shift-response function* (SRF) estimation. Existing neural network methods involving robust causal-effect estimators lack theoretical guarantees and practical implementations for SRF estimation. Motivated by a key policy-relevant question in public health, we develop a neural network method and its theoretical underpinnings to estimate SRFs with robustness and efficiency guarantees. We then apply our method to data consisting of 68 million individuals and 27 million deaths across the U.S. to estimate the causal effect from revising the US National Ambient Air Quality Standards (NAAQS) for $\\text{PM}_{2.5}$ from 12 to 9 $\\mu g/m^3$ . This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate, for the first time, the reduction in deaths that would result from this anticipated revision using causal methods for SRFs. Our proposed method, called Targeted Regularization for Exposure Shifts with Neural Networks (TRESNET), contributes to the neural network literature for causal inference in two ways: first, it proposes a targeted regularization loss with theoretical properties that ensure double robustness and achieves asymptotic efficiency specific for SRF estimation; second, it enables loss functions from the exponential family of distributions to accommodate non-continuous outcome distributions (such as hospitalization or mortality counts). We complement our application with benchmark experiments that demonstrate TRESNET's broad applicability and competitiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FItPCl4uEc": {
    "title": "Efficient Transfer Learning from Arbitrary Pre-Trained Models",
    "volume": "review",
    "abstract": "Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LCQ7YTzgRQ": {
    "title": "On the Role of Edge Dependency in Graph Generative Models",
    "volume": "review",
    "abstract": "In this work, we introduce a novel evaluation framework for generative models of graphs, emphasizing the importance of model-generated graph *overlap* (Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We delineate a hierarchy of graph generative models categorized into three levels of complexity: edge independent, node independent, and fully dependent models. This hierarchy encapsulates a wide range of prevalent methods. We derive theoretical bounds on the number of triangles and other short-length cycles producible by each level of the hierarchy, contingent on the model overlap. We provide instances demonstrating the asymptotic optimality of our bounds. Furthermore, we introduce new generative models for each of the three hierarchical levels, leveraging dense subgraph discovery (Gionis & Tsourakakis, 2015). Our evaluation, conducted on real-world datasets, focuses on assessing the output quality and overlap of our proposed models in comparison to other popular models. Our results indicate that our simple, interpretable models provide competitive baselines to popular generative models. Through this investigation, we aim to propel the advancement of graph generative models by offering a structured framework and robust evaluation metrics, thereby facilitating the development of models capable of generating accurate and edge-diverse graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=1IaoWBqB6K": {
    "title": "DiffDock-Pocket: Diffusion for Pocket-Level Docking with Sidechain Flexibility",
    "volume": "review",
    "abstract": "When a small molecule binds to a protein, the 3D structure of the protein and its function change. Understanding this process, called molecular docking, can be crucial in areas such as drug design. Recent learning-based attempts have shown promising results at this task, yet lack features that traditional approaches support. In this work, we close this gap by proposing DiffDock-Pocket, a diffusion-based docking algorithm that is conditioned on a binding target to predict ligand poses only in a specific binding pocket. On top of this, our model supports receptor flexibility and predicts the position of sidechains close to the binding site. Empirically, we improve the state-of-the-art in site-specific-docking on the PDBBind benchmark. Especially when using in-silico generated structures, we achieve more than twice the performance of current methods while being more than 20 times faster than other flexible approaches. Although the model was not trained for cross-docking to different structures, it yields competitive results in this task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tHHzfZSP6T": {
    "title": "How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks",
    "volume": "review",
    "abstract": "Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing simple logical operations. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we aim to assess in this paper \"how capable can a transformer become?\". Specifically, we train Transformer models on a data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) Transformers can learn compositional structures from the training data and generalize to exponentially or even combinatorially many functions; (2) Composing functions by generating intermediate outputs is more effective at generalizing to unseen compositions, compared to generating no intermediate outputs; (3) The training data has a significant impact on the model's ability to compose unseen combinations of functions; (4) The attention layers in the latter half of the Transformer seem critical to compositionality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=QlFlo5533z": {
    "title": "Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation",
    "volume": "review",
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) has emerged as a popular method to protect personally identifiable information in deep learning (DL) applications. Unfortunately, DP-SGD's per-sample gradient clipping and uniform noise addition during training can significantly degrade model utility. To enhance the model's utility, researchers proposed various adaptive/dynamic DP-SGD methods by adapting the noise multiplier and clipping threshold. However, we examined and discovered that these established techniques result in greater privacy leakage or lower accuracy than the traditional DP-SGD method, or a lack of evaluation on a complex data set such as CIFAR100. To address these limitations, we propose an automatic DP-SGD (Auto DP-SGD). Our method automates clipping threshold estimation based on the DL model's total gradient norm and scales the gradients of each training sample instead of simply clipping them without losing gradient information or requiring an additional privacy budget. This helps to improve the algorithm's utility while using a less privacy budget. To further improve accuracy, we introduce automatic noise multiplier decay mechanisms to decrease the noise multiplier after every epoch. Finally, we develop closed-form mathematical expressions using the truncated concentrated differential privacy (tCDP) accountant, which offers a straightforward and tight privacy-bound analysis for automatic noise multiplier and automatic clipping estimation. Through extensive experimentation, we demonstrate that Auto DP-SGD outperforms existing state-of-the-art (SOTA) classification results in privacy and accuracy on various benchmark datasets. We also show that privacy can be improved by lowering the scale factor and using learning rate schedulers without significantly reducing privacy. Moreover, we also explain how to select the best Auto DP-SGD variant without additional privacy leakage. Specifically, Auto DP-SGD, when used with a step noise multiplier (Auto DP-SGD-S), improves accuracy by 3.20\\%, 1.57\\%, 6.73\\%, and 1.42\\% for the MNIST, CIFAR10, CIFAR100, and AG News Corpus datasets, respectively. Furthermore, we achieve a substantial reduction in the privacy budget ($\\epsilon$) of 94.9\\%, 79.16\\%, 67.36\\%, and 53.37\\% for the corresponding data sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cdng6X2Joq": {
    "title": "A New, Physics-Based Continuous-Time Reinforcement Learning Algorithm with Performance Guarantees",
    "volume": "review",
    "abstract": "We introduce a new, physics-based continuous-time reinforcement learning (CT-RL) algorithm for control of affine nonlinear systems, an area that enables a plethora of well-motivated applications. Based on fundamental input/output control mechanisms, our approach uses reference command input (RCI) as probing noise in learning. With known physical dynamics of the environment, and by leveraging on the Kleinman algorithm structure, our RCI-based CT-RL algorithm not only provides theoretical guarantees such as learning convergence, solution optimality, and closed-loop stability, but also well-behaved dynamic system responses with data efficiency during learning. Our results are therefore an advance from the two currently available classes of approaches to CT-RL. The first school of adaptive dynamic programming (ADP) methods features elegant theoretical results stemming from adaptive and optimal control. Yet, they have not been shown effectively synthesizing meaningful controllers. The second school of fitted value iteration (FVI) methods, also the state-of-the-art (SOTA) deep RL (DRL) design, has shown impressive learning solutions, yet theoretical guarantees are still to be developed. We provide several evaluations to demonstrate that our RCI-based design leads to new, SOTA CT-RL results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ogV88XPnK6": {
    "title": "Graph neural processes and their application to molecular functions",
    "volume": "review",
    "abstract": "Neural processes (NPs) are models for meta-learning which output uncertainty estimates. So far, most studies of NPs have focused on low-dimensional datasets of highly-correlated tasks. While these homogeneous datasets are useful for benchmarking, they may not be representative of realistic transfer-learning. In particular, applications in scientific research may prove especially challenging due to the potential novelty of meta-testing tasks. Drug discovery is one such research area that is characterized by sparse datasets of many functions on a shared molecular space. In this paper, we study the application of graph NPs to drug discovery with DOCKSTRING, a diverse dataset of docking scores. Graph NPs show competitive performance in few-shot learning tasks relative to supervised learning baselines common in chemoinformatics, as well as alternative techniques for transfer learning and meta-learning. In order to increase meta-generalization to divergent test functions, we propose fine-tuning strategies that adapt the parameters of NPs. We find that adaptation can substantially increase NPs' regression performance while maintaining good calibration of uncertainty estimates. Finally, we present a Bayesian optimization experiment which showcases the potential advantages of NPs over GPs in molecular applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kXHEBK9uAY": {
    "title": "Simple Hierarchical Planning with Diffusion",
    "volume": "review",
    "abstract": "Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a \"jumpy\" planning strategy at the high level, which allows it to have a larger receptive field but at a lower computational cost—a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of training and planning speed compared to the non-hierarchical Diffuser as well as other hierarchical planning methods. Moreover, we explore our model's generalization capability, particularly on how our method improves generalization capabilities on compositional out-of-distribution tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=fj2E5OcLFn": {
    "title": "Stochastic Gradient Descent for Gaussian Processes Done Right",
    "volume": "review",
    "abstract": "We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly on the problem or on a reduced-order version of it. However, stochastic gradient descent has recently gained traction in the Gaussian process literature, driven largely by its successes in deep learning. In this paper, we show that this approach when done right---by which we mean using specific insights from the optimisation and kernel communities---is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, conveniently implementable with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies. We then show that the new method is highly competitive: our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apart from conjugate gradients, variational Gaussian process approximations, and a prior version of stochastic gradient descent tailored for Gaussian processes. On a molecular binding affinity prediction task, our method places Gaussian process regression on par in terms of performance with graph neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=gkfUvn0fLU": {
    "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
    "volume": "review",
    "abstract": "Large language models are typically aligned with human preferences by optimizing reward models (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to *overoptimization*, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally given by the Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=c56TWtYp0W": {
    "title": "GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings",
    "volume": "review",
    "abstract": "Analyzing multivariate time series is important in many domains. However, it has been difficult to learn robust and generalizable representations within multivariate datasets due to complex inter-channel relationships and dynamic shifts. In this paper, we introduce a novel approach for learning spatiotemporal structure and using it to improve the application of transformers to timeseries datasets. Our framework learns a set of group tokens, and builds an instance-specific group embedding (GE) layer that assigns input tokens to a small number of group tokens to incorporate structure into learning. We then introduce a novel architecture, Group-Aware transFormer (GAFormer), which incorporates both spatial and temporal group embeddings to achieve state-of-the-art performance on a number of time-series classification and regression tasks. In evaluations on a number of diverse timeseries datasets, we show that GE on its own can provide a nice enhancement to a number of backbones, and that by coupling spatial and temporal group embeddings, the GAFormer can outperform the existing baselines. Finally, we show how our approach discerns latent structures in data even without information about the spatial ordering of channels, and yields a more interpretable decomposition of spatial and temporal structure underlying complex multivariate datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=90QOM1xB88": {
    "title": "Improved order analysis and design of exponential integrator for diffusion models sampling",
    "volume": "review",
    "abstract": "Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be attributed to the absence of essential order conditions. By reformulating the differential equations in DMs and capitalizing on the theory of exponential integrators, we propose refined EI solvers that fulfill all the order conditions, which we designate as Refined Exponential Solver (RES). Utilizing these improved solvers, RES exhibits more favorable error bounds theoretically and achieves superior sampling efficiency and stability in practical applications. For instance, a simple switch from the single-step DPM-Solver++ to our order-satisfied numerical scheme when NFE$=9$, results in a reduction of numerical defects by 25.2 and FID improvement of 25.4 (16.77 vs 12.51) on a pre-trained ImageNet diffusion model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OCx7dp58H1": {
    "title": "Setting the Record Straight on Transformer Oversmoothing",
    "volume": "review",
    "abstract": "Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that can oversmooth the input. This causes their performance to quickly saturate as model depth increases. A natural question is: How can Transformers achieve success given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Finally, we describe a simple way to reparameterize the weights of the Transformer update equations to ensure that oversmoothing does not occur. Compared to other solutions for oversmoothing, our approach does not require a new architecture, or any additional hyperparameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vrhrhGrdXm": {
    "title": "KBFormer: A Transformer-based Diffusion Model of Structured Entities with Heterogeneous Properties",
    "volume": "review",
    "abstract": "We present a generative attention-based architecture that models structured entities comprising different property types, such as numerical, categorical, string, and composite. This architecture handles such heterogeneous data through a mixed continuous-discrete diffusion process over the properties. This flexible framework is capable of modeling entities with arbitrary hierarchical properties, enabling applications to structured KB entities and tabular data. Experiments with a device KB and a nuclear physics dataset demonstrate the model's ability to learn representations useful for entity completion in diverse settings. This has many downstream use cases, including modeling numerical properties with high accuracy - critical for science applications. An additional benefit of the model is its inherent probabilistic nature, enabling predictions accompanied by uncertainties. These critical capabilities are leveraged in a nuclear physics dataset to make precise predictions on various properties of nuclei",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=DP4NkPZOpD": {
    "title": "Bridging Sequence and Structure: Latent Diffusion for Conditional Protein Generation",
    "volume": "review",
    "abstract": "Protein design encompasses a range of challenging tasks, including protein folding, inverse folding, and protein-protein docking. Despite significant progress in this domain, many existing methods address these tasks separately, failing to adequately leverage the joint relationship between protein sequence and three-dimensional structure. In this work, we propose a novel generative modeling technique to capture this joint distribution. Our approach is based on a diffusion model applied on a geometrically-structured latent space, obtained through an encoder that produces roto-translational invariant representations of the input protein complex. It can be used for any of the aforementioned tasks by using the diffusion model to sample the conditional distribution of interest. Our experiments show that our method outperforms competitors in protein docking and is competitive with state-of-the-art for protein inverse folding. Exhibiting a single model that excels on on both sequence-based and structure-based tasks represents a significant advancement in the field and paves the way for additional applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aZCPl3ZvR": {
    "title": "Why is SAM Robust to Label Noise?",
    "volume": "review",
    "abstract": "Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in ``flatter'' regions of the loss landscape. In particular, the peak performance occurs with early stopping, far before the loss converges. We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian. The first can be observed in linear logistic regression where SAM provably upweights the gradient contribution from clean examples. Although this explicit upweighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance. We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian. We theoretically derive the explicit regularization induced by this Jacobian effect in two layer linear networks. Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits even in deep networks trained on real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fAGEAEQvRr": {
    "title": "Gradient descent for matrix factorization: Understanding large initialization",
    "volume": "review",
    "abstract": "In deep learning practice, large random initialization is commonly used. Understanding the behavior of gradient descent (GD) with such initialization is both crucial and challenging. This paper focuses on a simplified matrix factorization problem, delving into the dynamics of GD when using large initialization. Leveraging a novel signal-to-noise ratio argument and an inductive argument, we offer a detailed trajectory analysis of GD from the initial point to the global minima. Our insights indicate that even with a large initialization, GD can exhibit incremental learning, which coincides with experimental observations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=xnUIMz5u2s": {
    "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?",
    "volume": "review",
    "abstract": "*Thinking is for Doing.*\" Humans can infer other people's mental states from observations–an ability called Theory-of-Mind (ToM)–and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Qz9BT4mpM": {
    "title": "Predicting the Performance of Foundation Models via Agreement-on-the-line",
    "volume": "review",
    "abstract": "Estimating out-of-distribution performance is critical to safely deploying machine learning models. Recently, Baek et al. showed that the phenomenon ``agreement-on-the-line'' can be a reliable method for predicting OOD accuracy of models in an ensemble consisting largely of CNNs trained from scratch. However, it is now increasingly common to lightly fine-tune foundation models, and it is unclear whether such fine-tuning is sufficient to produce enough diversity in models for such agreement-based methods to work properly. In this paper, we develop methods for reliably applying agreement-on-the-line-based performance estimation to fine-tuned foundation models. In particular, we first study the case of fine-tuning a single foundation model, where we extensively study how different types of randomness (linear head initialization, hyperparameter selection, data subsetting, and data shuffling) contribute to the agreement on the line of the resulting model sets; we find, somewhat surprisingly, that it is typically possible to obtain strong agreement via random initialization of the linear head alone. Next, we study how \\emph{multiple} foundation models, pretrained on different data sets but fine-tuned on the same task, may or may not produce agreement; we show, again rather surprisingly, that the diversity of such models is already sufficient and not too disparate for them to all lie on the same agreement lines. In total, these methods enable reliable and efficient estimation of OOD accuracy for fine-tuned foundation models, without leveraging any labeled OOD data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xxaEhwC1I4": {
    "title": "Revisiting the Last-Iterative Convergence of Stochastic Gradient Methods",
    "volume": "review",
    "abstract": "In the past several years, the convergence of the last iterate of the Stochastic Gradient Descent (SGD) algorithm has triggered people's great interest due to its good performance in practice but lack of theoretical understanding. For Lipschtiz and convex functions, different works have established the optimal $O(\\log(1/\\delta)\\log T/\\sqrt{T})$ or $O(\\sqrt{\\log(1/\\delta)/T})$ high-probability convergence rates for the final iterate, where $T$ is the time horizon and $\\delta$ is the failure probability. However, to prove these bounds, all the existing works are limited to compact domains, and almost all of them also require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. Besides this important question, there are still lots of theoretical problems lacking an answer. For example, compared with the last iterate convergence of SGD for non-smooth problems, only very few results for smooth optimization have yet been developed. Additionally, the existing results are all limited to a single objective and the standard Euclidean norm. It still remains unclear whether the last-iterative convergence can be provably extended to wider composite optimization and non-Euclidean norms. In this work, to address the issues mentioned above, we revisit the last-iterative convergence of stochastic gradient methods and provide the first unified way to prove the convergence rates both in expectation and in high probability to accommodate general domains, composite objectives, non-Euclidean norms, Lipschitz conditions, smoothness and (strong) convexity simultaneously",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ajG8vLTHh5": {
    "title": "Learning transferrable and interpretable representation for brain network",
    "volume": "review",
    "abstract": "The human brain is a complex, dynamic network, which is commonly studied using functional magnetic resonance imaging (fMRI) and modeled as network of Regions of interest (ROIs) for understanding various brain functions. Recent studies predominantly utilize Graph Neural Networks (GNNs) to learn the brain network representation based on the functional connectivity (FC) profile, typically falling into two main categories. The Fixed-FC approaches, utilize the FC profile which represents the linear temporal relation within the brain network, is limited by failing to capture the informative temporal dynamics of brain activity. On the other hand, the Dynamic-FC approaches, modeling the evolving FC profile over time, often exhibit less satisfactory performance due to challenges in handling the inherent noisy nature of fMRI data. In this study, to address these challenges, we propose Brain Masked Auto-Encoder (BrainMAE) for learning representations directly from fMRI time-series data. Our approach incorporates two essential components—an embedding-based graph attention mechanism and a self-supervised masked autoencoding framework. These components empower our model to capture the rich temporal dynamics of brain activity while maintaining resilience to the inherent noise in fMRI data. Our experiments demonstrate that BrainMAE consistently outperforms several established baseline models by a significant margin in three distinct downstream tasks. Finally, leveraging the model's inherent interpretability, our analysis of model-generated representations reveals intriguing findings that resonate with ongoing research in the field of neuroscience",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=O8ouVV8PjF": {
    "title": "CNN Kernels Can Be the Best Shapelets",
    "volume": "review",
    "abstract": "Shapelets and CNN are two typical approaches to model time series. Shapelets aim at finding a set of sub-sequences that extract feature-based interpretable shapes, but may suffer from accuracy and efficiency issues. CNN performs well by encoding sequences with a series of hidden representations, but lacks interpretability. In this paper, we demonstrate that shapelets are essentially equivalent to a specific type of CNN kernel with a squared norm and pooling. Based on this finding, we propose ShapeConv, an interpretable CNN layer with its kernel serving as shapelets to conduct time-series modeling tasks in both supervised and unsupervised settings. By incorporating shaping regularization, we enforce the similarity for maximum interpretability. We also find human knowledge can be easily injected to ShapeConv by adjusting its initialization and model performance is boosted with it. Experiments show that ShapeConv can achieve state-of-the-art performance on time-series benchmarks without sacrificing interpretability and controllability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ZlZakr4GYK": {
    "title": "COTIC: Embracing Non-uniformity in Event Sequence Data via Multilayer Continuous Convolution",
    "volume": "review",
    "abstract": "Massive samples of event sequences occur in various domains, including e-commerce, healthcare, and finance. There are two main challenges regarding modeling such data: methodological and computational. The methodological peculiarity for event sequences is their non-uniformity and sparsity. These requirements make time series models unsuitable. The computational challenge arises from a large amount of available data and the significant length of each sequence. Thus, the problem requires complex and efficient models. Existing solutions include large recurrent and transformer neural network architectures. On top of existing blocks, their authors introduce specific intensity functions defined at each moment. However, due to their parametric nature, these continuous-time-aware intensities represent only a limited class of event sequences. We propose the COTIC method based on an efficient continuous convolution neural network suitable for the non-uniform occurrence of events in time. In COTIC, dilations and multi-layer architecture efficiently handle long-term dependencies between events. Furthermore, the model provides intensity dynamics in continuous time --- including self-excitement encountered in practice. Being the first to introduce multiple continuous convolution layers that can handle arbitrary complex dependencies via MLP-modeled convolutions, we obtain these properties. When benchmarked against existing models, the COTIC consistently outperforms them, especially in predicting the next event time and type: it has the average rank of 2.125 vs. 3.688 of the primal competitor. Additionally, its ability to produce effective embeddings showcases its potential for a range of downstream tasks, as produced embeddings are sufficient to solve various downstream tasks, e.g., 0.459 vs. 0.452 baseline accuracy on a 4-label age bin prediction for transactions dataset. The code of the proposed method is available at https://anonymous.4open.science/r/COTIC-F47D/README.md",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YfZMfrpEnl": {
    "title": "Stochastic Vision Transformers with Wasserstein Distance-Aware Attention",
    "volume": "review",
    "abstract": "Self-supervised learning is one of the most promising approaches to acquiring knowledge from limited labeled data. Despite the substantial advancements made in recent years, self-supervised models have posed a challenge to practitioners, as they do not readily provide insight into the model's confidence and uncertainty. Tackling this issue is no simple feat, primarily due to the complexity involved in implementing techniques that can make use of the latent representations learned during pre-training without relying on explicit labels. Motivated by this, we introduce a new stochastic vision transformer that integrates uncertainty and distance awareness into self-supervised learning (SSL) pipelines. Instead of the conventional deterministic vector embedding, our novel stochastic vision transformer encodes image patches into elliptical Gaussian distributional embeddings. Notably, the attention matrices of these stochastic representational embeddings are computed using Wasserstein distance-based attention, effectively capitalizing on the distributional nature of these embeddings. Additionally, we propose a regularization term based on Wasserstein distance for both pre-training and fine-tuning processes, thereby incorporating distance awareness into latent representations. We perform extensive experiments across different tasks such as in-distribution generalization, out-of-distribution detection, dataset corruption, semi-supervised settings, and transfer learning to other datasets and tasks. Our proposed method achieves superior accuracy and calibration, surpassing the self-supervised baseline in a wide range of experiments on a variety of datasets. Our code is in the supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=RtOTTdWbZd": {
    "title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment",
    "volume": "review",
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also provide a theoretical justification supporting the design of our loss function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=YBSEwwveMr": {
    "title": "Score-Based Multimodal Autoencoders",
    "volume": "review",
    "abstract": "Multimodal Variational Autoencoders (VAEs) represent a promising group of generative models that facilitate the construction of a tractable posterior within the latent space, given multiple modalities. Daunhawer et al. (2022) demonstrate that as the number of modalities increases, the generative quality of each modality declines. In this study, we explore an alternative approach to enhance the generative performance of multimodal VAEs by jointly modeling the latent space of unimodal VAEs using score-based models (SBMs). The role of the SBM is to enforce multimodal coherence by learning the correlation among the latent variables. Consequently, our model combines the superior generative quality of unimodal VAEs with coherent integration across different modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WPZ2yPag4K": {
    "title": "Fine-Tuning Language Models for Factuality",
    "volume": "review",
    "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. However, language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations', which can harmfully perpetuate myths and misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we leverage two key recent innovations in NLP to fine-tune language models to be more factual without human labeling, targeting more open-ended generation settings than past work. First, several recent works have proposed methods for scoring the factuality of open-ended text derived from consistency with an external knowledge base or simply a large model's confidence scores. Second, the Direct Preference Optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from preference rankings generated by either automated criterion significantly improves the factuality of Llama-2 on held-out topics (percent of generated claims that are correct) compared with existing RLHF procedures or decoding strategies targeted at factuality, showing over 50% and 20-30% error reduction for biographies and medical questions respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4Hf5pbk74h": {
    "title": "Improving classifier decision boundaries using nearest neighbors",
    "volume": "review",
    "abstract": "In this paper, we show that neural networks are not learning optimal decision boundaries. Decision boundaries go through areas of low training data density. They are impacted by few training samples which can easily lead to overfitting. We show that performing a weighted average of the prediction of a sample and its nearest neighbors' (computed in latent space) leads to a variety of minor favorable outcomes. In our evaluation, we employ various self-trained and pre-trained convolutional neural networks to show that our approach improves (i) resistance to label noise, (ii) robustness against adversarial attacks, (iii) classification accuracy, and to some degree even (iv) interpretability. While improvements are not necessarily large in all four areas, our approach is conceptually simple, i.e., improvements come without any modification to network architecture, training procedure or dataset. Furthermore, they are in stark contrast to prior works that often require trade-offs among the four objectives or provides only non-actionable insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=f3g5XpL9Kb": {
    "title": "LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures",
    "volume": "review",
    "abstract": "Joint embedding (JE) architectures have emerged as a promising avenue for ac- quiring transferable data representations. A key obstacle to using JE methods, however, is the inherent challenge of evaluating learned representations without access to a downstream task, and an annotated dataset. Without efficient and re- liable evaluation, it is difficult to iterate on architectural and training choices for JE methods. In this paper, we introduce LiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the quality of representations within JE archi- tectures. Our metric addresses several shortcomings of recent approaches based on feature covariance rank by discriminating between informative and uninforma- tive features. In essence, LiDAR quantifies the rank of the Linear Discriminant Analysis (LDA) matrix associated with the surrogate SSL task—a measure that intuitively captures the information content as it pertains to solving the SSL task. We empirically demonstrate that LiDAR significantly surpasses naive rank based approaches in its predictive power of optimal hyperparameters. Our proposed cri- terion presents a more robust and intuitive means of assessing the quality of rep- resentations within JE architectures, which we hope facilitates broader adoption of these powerful techniques in various domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=t8D9JxEn0J": {
    "title": "Malcom-PSGD: Inexact Proximal Stochastic Gradient Descent for Communication Efficient Decentralized Machine Learning",
    "volume": "review",
    "abstract": "Recent research indicates that frequent model communication stands as a major bottleneck to the efficiency of decentralized machine learning (ML), particularly for large-scale and over-parameterized neural networks (NNs). In this paper, we introduce \\textsc{Malcom-PSGD}, a new decentralized ML algorithm that strategically integrates gradient compression techniques with model sparsification. \\textsc{Malcom-PSGD} leverages proximal stochastic gradient descent to handle the non-smoothness resulting from the $\\ell_1$ regularization in model sparsification. Furthermore, we adapt vector source coding and dithering-based quantization for compressed gradient communication of sparsified models. Our analysis shows that decentralized proximal stochastic gradient descent with compressed communication has a convergence rate of $\\mathcal{O}\\left(\\ln(t)/\\sqrt{t}\\right)$ assuming a diminishing learning rate and where $t$ denotes the number of iterations. Numerical results verify our theoretical findings and demonstrate that our method reduces communication costs by approximately $75$\\% when compared to the state-of-the-art method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=lOwkOIUJtx": {
    "title": "Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling",
    "volume": "review",
    "abstract": "High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kz5igjl04W": {
    "title": "Approaching an unknown communication system by latent space exploration and causal inference",
    "volume": "review",
    "abstract": "This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values with methods inspired by causal inference into an approach we call causal disentanglement with extreme values (CDEV) and show that this method yields insights for model interpretability. With this, we can test for what properties of unknown data the model encodes as meaningful, using it to glean insight into the communication system of sperm whales (Physeter macrocephalus), one of the most intriguing and understudied animal communication systems. The network architecture used has been shown to learn meaningful representations of speech; here, it is used as a learning mechanism to decipher the properties of another vocal communication system in which case we have no ground truth. The proposed methodology suggests that sperm whales encode information using the number of clicks in a sequence, the regularity of their timing, and audio properties such as the spectral mean and the acoustic regularity of the sequences. Some of these findings are consistent with existing hypotheses, while others are proposed for the first time. We also argue that our models uncover rules that govern the structure of units in the communication system and apply them while generating innovative data not shown during training. This paper suggests that an interpretation of the outputs of deep neural networks with causal inference methodology can be a viable strategy for approaching data about which little is known and presents another case of how deep learning can limit the hypothesis space. Finally, the proposed approach can be extended to other architectures and datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=FcxwXnYXWh": {
    "title": "Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate",
    "volume": "review",
    "abstract": "Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit local superlinear rate of $\\mathcal{O}((1/\\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is $\\mathcal{O}(\\tau d)$ where $\\tau \\le d$ is the size of the window and $d$ is the problem dimension reducing the $\\mathcal{O}(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for any limited-memory quasi-Newton method. In this work, we close this gap by presenting a Limited-memory Greedy BFGS (LG-BFGS) method that can achieve an explicit non-asymptotic superlinear rate. We incorporate displacement aggregation, i.e., decorrelating projection, in post-processing gradient variations, together with a basis vector selection scheme on variable variations, which $\\textit{greedily}$ maximizes a progress measure of the Hessian estimate to the true Hessian. Their combination allows past curvature information to remain in a sparse subspace while yielding a valid representation of the full history. Interestingly, our established $\\textit{non-asymptotic}$ superlinear convergence rate demonstrates an explicit trade-off between the convergence speed and memory requirement, which to our knowledge, is the first of its kind. Numerical results corroborate our theoretical findings and demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Tigr1kMDZy": {
    "title": "Overthinking the Truth: Understanding how Language Models Process False Demonstrations",
    "volume": "review",
    "abstract": "Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some \"critical layer\", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=VPl472SKaB": {
    "title": "Transforming Smallholder Farmers Support with an AI-Powered FAQbot: A Comparison of Techniques",
    "volume": "review",
    "abstract": "Access to sufficient information on desired agricultural practices, such as planting period, when to apply fertiliser, how to transport grains, etc. is of utmost importance in the agricultural industry as it directly affects farm yields. The responses to these questions are closed domain, therefore leading to the development of a question-answering conversational bot (FAQbot) that can provide the appropriate responses immediately. This study undertakes a comparative analysis of three distinct methodologies for constructing a FAQbot. These approaches encompass the development of a generative-based chatbot employing BERT and GPT-2, the creation of an intent classification model leveraging PyTorch and the Natural Language Toolkit (NLTK) libraries, and the implementation of an information retrieval-based model utilising pre-trained Large Language Models (LLMs) using Langchain. Our methodological framework includes the transformation of a FAQ dataset into formats suitable for chatbot training, specifically CSV and JSON. Notably, the retrieval-based method surpassed the generative-based and intent classification methods by consistently providing precise answers for every question in the database, irrespective of rephrasing or reframing. Keywords: Agriculture, FAQBot, LLMs, Natural Language Processing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=p6hIAEHwSp": {
    "title": "Efficient Subgraph Rule Induction via Tree Folding in Differentiable Logic Programming",
    "volume": "review",
    "abstract": "Differentiable inductive logic programming techniques have proven effective at learning logic rules from noisy datasets; however, existing algorithms incur pernicious trade-offs between rule expressivity and scalability to large problems. Forward-chaining ILP algorithms can learn arbitrary rules, but their memory requirements scale exponentially with problem size. Backwards-chaining ILP algorithms address this limitation but do so with loss of generality by imposing the restrictive constraint that rules must be expressible as ensembles of independent chain-like Horn clauses. In this paper we present FUSE-ILP, a technique that relaxes this chain-like constraint and enables the differentiable evaluation of a restricted class of subgraph-like rules. Our method extends TensorLog-inspired backwards-chaining ILP techniques with branch masking and leaf grouping, which enable tree-like rule evaluation and \"folding\" of these trees into subgraphs. We demonstrate that this formulation allows our algorithm to learn more expressive rules than previous backwards-chaining algorithms while retaining a similar computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=DWJr05rymY": {
    "title": "Estimating Unknown Population Sizes Using Hypergeometric Maximum Likelihood",
    "volume": "review",
    "abstract": "The multivariate hypergeometric distribution describes the fundamental process of sampling without replacement from a discrete population of elements divided into multiple categories. Despite the hypergeometric distribution's long history, the literature has not yet addressed the problem of maximum likelihood estimation when both the size of the total population and its constituent categories are unknown. Here, we show that this estimation challenge can be solved by maximizing the hypergeometric likelihood, even in the presence of severe under-sampling. We extend this approach to capture data generating processes where the ground-truth high-dimensional distribution is conditional on a continuous latent variable using the variational autoencoder framework, and validate the resulting model using simulated datasets. In a practical use case, we demonstrate that our method can recover the true number of gene transcripts present in a cell from sparse single-cell genomics data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=dEz3ge8QSo": {
    "title": "Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity",
    "volume": "review",
    "abstract": "Robust Markov Decision Processes (MDPs) and risk-sensitive MDPs are both powerful tools for making decisions in the presence of uncertainties. Previous efforts have aimed to establish their connections, revealing equivalences in specific formulations. This paper introduces a new formulation for risk-sensitive MDPs, which assesses risk in a slightly different manner compared to the classical Markov risk measure \\cite{ruszczynski2010risk}, and establishes its equivalence with a class of regularized robust MDP (RMDP) problems, including the standard RMDP as a special case. Leveraging this equivalence, we further derive the policy gradient theorem for both problems, proving gradient domination and global convergence of the exact policy gradient method under the tabular setting with direct parameterization. This forms a sharp contrast to the Markov risk measure, known to be potentially non-gradient-dominant \\cite{huang2021convergence}. We also propose a sample-based offline learning algorithm, namely the robust fitted-Z iteration (RFZI), for a specific regularized RMDP problem with a KL-divergence regularization term (or equivalently the risk-sensitive MDP with an entropy risk measure). We showcase its streamlined design and less stringent assumptions due to the equivalence and analyze its sample complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IpJIq3iwMH": {
    "title": "Federated Binary Matrix Factorization using Proximal Optimization",
    "volume": "review",
    "abstract": "Identifying informative components in binary data is an essential task in many research areas, including life sciences, social sciences, natural language processing, and recommendation systems. Boolean matrix factorization (BMF) is a family of methods that performs this task by efficiently factorizing the data into its constituent parts. In real-world settings, the data is often distributed across stakeholders and required to stay private, prohibiting the straightforward application of BMF. To adapt BMF to this context, we approach the problem from a federated-learning perspective, while building on a state-of-the-art continuous binary matrix factorization relaxation to BMF that enables efficient gradient-based optimization. We propose to only share the relaxed component matrices, which are aggregated centrally using a proximal operator that regularizes for binary outcomes. We show the convergence of our federated proximal gradient descent algorithm and provide differential privacy guarantees. Our extensive empirical evaluation demonstrates that our algorithm outperforms, in terms of quality and efficacy, federation schemes of state-of-the-art BMF methods on a diverse set of real-world and synthetic data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=17pVDnpwwl": {
    "title": "Feature Learning in Infinite Depth Neural Networks",
    "volume": "review",
    "abstract": "Empirical studies have consistently demonstrated that increasing the size of neural networks often yields superior performance in practical applications. However, there is a lack of consensus regarding the appropriate scaling strategy, particularly when it comes to increasing the depth of neural networks. In practice, excessively large depths can lead to model performance degradation. In this paper, we introduce Depth-$\\mu$P, a principled approach for depth scaling, allowing for the training of arbitrarily deep architectures while maximizing feature learning and diversity among nearby layers. Our method involves dividing the contribution of each residual block and the parameter update by the square root of the depth. Through the use of Tensor Programs, we rigorously establish the existence of a limit for infinitely deep neural networks under the proposed scaling scheme. This scaling strategy ensures more stable training for deep neural networks and guarantees the transferability of hyperparameters from shallow to deep models. To substantiate the efficacy of our scaling method, we conduct empirical validation on neural networks with depths up to $2^{10}$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wTtDgucL7h": {
    "title": "Two Facets of SDE Under an Information-Theoretic Lens: Generalization of SGD via Training Trajectories and via Terminal States",
    "volume": "review",
    "abstract": "Stochastic differential equations (SDEs) have been shown recently to well characterize the dynamics of training machine learning models with SGD. This provides two opportunities for better understanding the generalization behaviour of SGD through its SDE approximation. Firstly, viewing SGD as full-batch gradient descent with Gaussian gradient noise allows us to obtain trajectories-based generalization bound using the information-theoretic bound from Xu & Raginsky (2017). Secondly, assuming mild conditions, we estimate the steady-state weight distribution of SDE and use information-theoretic bounds from Xu & Raginsky (2017) and Negrea et al. (2019) to establish terminal-state-based generalization bounds. Our proposed bounds have some advantages, notably the trajectories-based bound outperforms results in Wang & Mao (2022), and the terminal-state-based bound exhibits a fast decay rate comparable to stability-based bounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=iGDWZFc7Ya": {
    "title": "Language Models Linearly Represent Sentiment",
    "volume": "review",
    "abstract": "Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank. We further uncover the mechanisms that involve this direction, highlighting the roles of a small subset of attention heads and neurons. Finally, we discover a phenomenon which we term the summarization motif: sentiment is not solely represented on emotionally charged words, but is additionally summarised at intermediate positions without inherent sentiment, such as punctuation and names. We show that in Stanford Sentiment Treebank zero-shot classification, 76\\% of above-chance classification accuracy is lost when ablating the sentiment direction, nearly half of which (36\\%) is due to ablating the summarized sentiment direction exclusively at comma positions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNzy9bRDvG": {
    "title": "Improved Techniques for Training Consistency Models",
    "volume": "review",
    "abstract": "Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models, and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we borrow Pseudo-Huber losses from robust statistics. Additionally, we introduce a new noise schedule for the consistency training objective, and propose a new curriculum for total discretization steps. Collectively, these modifications enable consistency models to achieve FID scores of 2.62 and 3.91 on CIFAR-10 and ImageNet $64\\times 64$ respectively in a single sampling step. These scores mark a 3.3$\\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.28 and 3.64, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and state-of-the-art generative models on both datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPHcEpGvF8": {
    "title": "Demystifying Poisoning Backdoor Attacks from a Statistical Perspective",
    "volume": "review",
    "abstract": "The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understanding applies to both discriminative and generative models. We also demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MQ4JJIYKkh": {
    "title": "Concept Alignment as a Prerequisite for Value Alignment",
    "volume": "review",
    "abstract": "Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values---and is even capable of valuing---depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment---agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RgELE1dQXx": {
    "title": "Learning to make adherence-aware advice",
    "volume": "review",
    "abstract": "As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7VPTUWkiDQ": {
    "title": "Provable Compositional Generalization for Object-Centric Learning",
    "volume": "review",
    "abstract": "Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=yzfi15eVI7": {
    "title": "iHyperTime: Interpretable Time Series Generation with Implicit Neural Representations",
    "volume": "review",
    "abstract": "Implicit neural representations (INRs) have emerged as a powerful tool that provides an accurate and resolution-independent encoding of data. Their robustness as general approximators has been shown across diverse data modalities, such as images, video, audio, and 3D scenes. However, little attention has been given to leveraging these architectures for time series data. Addressing this gap, we propose an approach for time series generation based on two novel architectures: TSNet, an INR network for interpretable trend-seasonality time series representation, and iHyperTime, a hypernetwork architecture that leverages TSNet for time series generalization and synthesis. Through evaluations of fidelity and usefulness metrics, we demonstrate that iHyperTime outperforms current state-of-the-art methods in challenging scenarios that involve long or irregularly sampled time series, while performing on par on regularly sampled data. Furthermore, we showcase iHyperTime fast training speed, comparable to the fastest existing methods for short sequences and significantly superior for longer ones. Finally, we empirically validate the quality of the model's unsupervised trend-seasonality decomposition by comparing against the established STL method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=07xuZw59uB": {
    "title": "Bridging the Fairness Divide: Achieving Group and Individual Fairness in Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have emerged as a powerful tool for analyzing and learning from complex data structured as graphs, demonstrating remarkable effectiveness in various applications, such as social network analysis, recommendation systems, and drug discovery. However, despite their impressive performance, the fairness problem has increasingly gained attention as a crucial aspect to consider. Existing research on fairness in graph learning primarily emphasizes either group fairness or individual fairness; however, to the best of our knowledge, none of these studies comprehensively address both individual and group fairness simultaneously. In this paper, we propose a new concept of individual fairness within groups and a novel framework named Fairness for Group and Individual (FairGI), which considers both group fairness and individual fairness within groups in the context of graph learning. FairGI employs the similarity matrix of individuals to achieve individual fairness within groups, while leveraging adversarial learning to address group fairness in terms of both Equal Opportunity and Statistical Parity. The experimental results demonstrate that our approach not only outperforms other state-of-the-art models in terms of group fairness and individual fairness within groups, but also exhibits excellent performance in population-level individual fairness, while maintaining comparable prediction accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvFhCUPjtI": {
    "title": "Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs",
    "volume": "review",
    "abstract": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with \\eft for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dxm7eil2HT": {
    "title": "RoCA: A Robust Method to Discover Causal or Anticausal Relation by Noise Injection",
    "volume": "review",
    "abstract": "Understanding whether the data generative process is causal or anticausal is important for algorithm design. It helps machine learning practitioners understand whether semi-supervised learning should be employed for real-world learning tasks. In many cases, existing causal discovery methods cannot be adaptable to this task, as they struggle with scalability and are ill-suited for high-dimensional perceptual data such as images. In this paper, we propose a method that detects whether the data generative process is causal or anticausal. Our method is robust to label errors and is designed to handle both large-scale and high-dimensional datasets effectively. Both theoretical analyses and empirical results on a variety of datasets demonstrate the effectiveness of our proposed method in determining the causal or anticausal direction of the data generative process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=f1xnBr4WD6": {
    "title": "Cycle Consistency Driven Object Discovery",
    "volume": "review",
    "abstract": "Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. While these approaches have shown promise in certain scenarios, they still exhibit certain limitations. First, they rely on architectural priors which can be unreliable and usually require meticulous engineering to identify the correct objects. Second, there has been a notable gap in investigating the practical utility of these representations in downstream tasks. To address the first limitation, we introduce a method that explicitly optimizes the constraint that each object in a scene should be associated with a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. By integrating these consistency objectives into various existing slot-based object-centric methods, we showcase substantial improvements in object-discovery performance. These enhancements consistently hold true across both synthetic and real-world scenes, underscoring the effectiveness and adaptability of the proposed approach. To tackle the second limitation, we apply the learned object-centric representations from the proposed method to two downstream reinforcement learning tasks, demonstrating considerable performance enhancements compared to conventional slot-based and monolithic representation learning methods. Our results suggest that the proposed approach not only improves object discovery, but also provides richer features for downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=wQCPHxtzGV": {
    "title": "RF-POLICY: Rectified Flows are Adaptive Decision Makers",
    "volume": "review",
    "abstract": "Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making but comes at the cost of significantly slower inference due to the recursion in the diffusion process. However, in real-world scenarios, states that require multi-modal decision-making are rare, and the huge consumption of diffusion models is not necessary for most cases. It inspires us to design efficient policy generators that can wisely allocate computation for different contexts. To address this challenge, we propose RF-POLICY (Rectified Flow-Policy), an imitation learning algorithm based on Rectified Flow, a recent advancement in flow-based generative modeling~\\citep{liu2022flow}. RF-POLICY adopts probability flow ordinary differential equations (ODEs) for diverse policy generation, with the learning principle of following straight trajectories as much as possible. We uncover and leverage a surprisingly intriguing advantage of these flow-based models over previous diffusion models: their training objective indicates the uncertainty of a certain state, and when the state is uni-modal, they automatically reduce to one-step generators since the probability flows admit straight lines. Therefore, RF-POLICY is naturally an adaptive decision maker, offering rapid inference without sacrificing diversity. Our comprehensive empirical evaluation shows that \\ours{}, to the best of our knowledge, is the first algorithm to achieve high performance across all dimensions, including success rate, behavioral diversity, and inference speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=1qzUPE5QDZ": {
    "title": "Rectifying Group Irregularities in Explanations for Distribution Shift",
    "volume": "review",
    "abstract": "It is well-known that real-world changes constituting distribution shift adversely affect model performance. How to characterize those changes in an interpretable manner is poorly understood. Existing techniques take the form of shift explanations that elucidate how samples map from the original distribution toward the shifted one by reducing the disparity between the two distributions. However, these methods can introduce group irregularities, leading to explanations that are less feasible and robust. To address these issues, we propose Group-aware Shift Explanations (GSE), an explanation method that leverages worst-group optimization to rectify group irregularities. We demonstrate that GSE not only maintains group structures, but can improve feasibility and robustness over a variety of domains by up to 20% and 25% respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=SzV37yefM4": {
    "title": "Contrastive Decoding Improves Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=RVrINT6MT7": {
    "title": "Sufficient conditions for offline reactivation in recurrent neural networks",
    "volume": "review",
    "abstract": "During periods of quiescence, such as sleep, neural activity in many brain circuits resembles that observed during periods of task engagement. However, the precise conditions under which task-optimized networks can autonomously reactivate the same network states responsible for online behavior is poorly understood. In this study, we develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. We demonstrate mathematically that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation based on angular velocity cues. Overall, our work provides theoretical support for modeling offline reactivation as an emergent consequence of task optimization in noisy neural circuits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=N2ggBozsss": {
    "title": "Centroid-Based Learning for Malware Detection and Novel Family Identification",
    "volume": "review",
    "abstract": "Detecting out-of-distribution (OOD) data categories while preserving the accuracy of existing classifications is a pressing challenge in many domains. Conventional methods often falter when tasked with generating or identifying new data classes, especially when dealing with graphical data and the problem of graph isomorphism. In this paper, we present a novel approach, the Graph Centroid Model (GCM), which combines Control Flow Graphs (CFGs) with a Graph Neural Network (GNN) to address this challenge effectively. The GCM assigns embeddings produced by a GNN to partitions that support the classification of both known and new classes, even those absent during training. Our approach quantifies the differences between samples in the embedding space, enabling the identification of multiple distinct representations of familiar classes during training while providing a straightforward mechanism for detecting new classes during testing. This not only improves classification accuracy but also offers intuitive visualizations that provide valuable insights.When applied to a benchmark malware dataset (BODMAS), our method reveals structural commonalities among samples from different malware families while effectively discerning new, previously unseen classes based on their distance from learned representatives in the embedding space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Abr7dU98ME": {
    "title": "Forward Learning of Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks. However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning neural networks. Examples of such constraints include the storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains neural networks by performing two forward passes over positive and negative data. Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for GNNs, which avoids the constraints imposed by BP via an effective layer-wise local forward training. ForwardGNN extends the original FF to deal with graph data and GNNs, and makes it possible to operate without generating negative inputs (hence no longer forward-forward). Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors. Extensive experiments involving five real-world datasets and three representative GNNs show the effectiveness and generality of the proposed forward graph learning framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Rd1pjx84rk": {
    "title": "Size Generalization of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective",
    "volume": "review",
    "abstract": "We investigate size-induced distribution shifts in graphs and assess their impact on the ability of graph neural networks (GNNs) to generalize to larger graphs relative to the training data. Existing literature presents conflicting conclusions on GNNs' size generalizability, primarily due to disparities in application domains and underlying assumptions concerning size-induced distribution shifts. Motivated by this, we take a data-driven approach: we focus on real biological datasets and seek to characterize the types of size-induced distribution shifts. Diverging from prior approaches, we adopt a spectral perspective and identify that spectrum differences induced by size are related to differences in subgraph patterns (e.g., average cycle lengths). We further find that common GNNs cannot capture these subgraph patterns, resulting in performance decline when testing on larger graphs. Based on these spectral insights, we introduce and compare three model-agnostic strategies aimed at making GNNs aware of important subgraph patterns to enhance their size generalizability: self-supervision, augmentation, and size-insensitive attention. Our empirical results reveal that all strategies enhance GNNs' size generalizability, with simple size-insensitive attention surprisingly emerging as the most effective method. Notably, this strategy substantially enhances graph classification performance on large test graphs, which are 2-10 times larger than the training graphs, resulting in an improvement in F1 scores by up to 8%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=P50qJuu4IY": {
    "title": "Self-Supervised Learning with the Matching Gap",
    "volume": "review",
    "abstract": "Contrastive learning (CL) is a fundamental paradigm in self-supervised learning. CL methods rely on a loss that nudges the features of various views from one image to stay closer, while pulling away those drawn from different images. Such a loss favors invariance: feature representations of the same perturbed image should collapse to the same vector, while remaining far enough from those of any other image. Although intuitive, CL leaves room for trivial solutions, and has a documented propensity to collapse representations for very different images. This is often mitigated by using a very large variety of augmentations. In this work, we address this tension by introducing a different loss, the matching gap. Given a set of $n$ images transformed in two different ways, the matching gap is the difference between the mean cost (e.g. a squared distance), in representation space, of the $n$ paired images, and the optimal matching cost obtained by running an optimal matching solver across these two families of $n$ images. The matching gap naturally mitigates the problem of data augmentation invariance, since it can be zero without requiring features from the same image to collapse. We implement the matching gap using the Sinkhorn algorithm and show that it can be easily differentiated using Danskin's theorem. In practice, we show that we can learn competitive features, even without extensive data augmentations: Using only cropping and flipping, we achieve 74.2% top-1 accuracy with a ViT-B/16 on ImageNet-1k, to be compared to 72.9% for I-JEPA (Assran et al., 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Rry1SeSOQL": {
    "title": "COMPARATOR: Reference-free machine translation evaluation by inter-system comparison",
    "volume": "review",
    "abstract": "Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem—producing an absolute translation-quality score. However, this approach has two limitations: i) the scores lack interpretability and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent. In practice, we often care about whether a new MT system is better or worse than some competitors. In addition, reference-free MT evaluation is increasingly practical and necessary. However, these two practical considerations have yet to be jointly explored. In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem. Given the source sentence and a pair of translations, our system predicts which translation is better. In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior performance by merely using indirect supervision from natural language inference and weak supervision from our synthetic data. In the context of reference-free evaluation, our system, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors our system marks state-of-the-art against reference-free as well as reference-based baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Rh4DmXaf8R": {
    "title": "Multi-timestep models for Model-based Reinforcement Learning",
    "volume": "review",
    "abstract": "In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered environment, highlighting the potential of our approach in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TgTJvwMEax": {
    "title": "Embedding Improves Neural Regularizers for Inverse Problems",
    "volume": "review",
    "abstract": "Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering. Recent machine learning techniques based on proximal and diffusion-based methods have shown some promising results. However, as we show in this work, they can also face challenges when applied to some exemplary problems. We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions. The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector. We demonstrate the merit of this approach on several exemplary and common inverse problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9wSWiavGwU": {
    "title": "SwapTransformer: Highway Overtaking Tactical Planner Model via Imitation Learning on OSHA Dataset",
    "volume": "review",
    "abstract": "This paper investigates the high-level decision-making problem in highway scenarios regarding lane changing and over-taking other slower vehicles. In particular, this paper aims to improve the Travel Assist feature for automatic overtaking and lane changes on highways. About 9 million samples including lane images and other dynamic objects are collected in simulation. This data; Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this challenge. To solve this problem, an architecture called SwapTransformer is designed and implemented as an imitation learning approach on the OSHA dataset. Moreover, auxiliary tasks such as future points and car distance network predictions are proposed to aid the model in better understanding the surrounding environment. The performance of the proposed solution is compared with a multi-layer perceptron (MLP) and multi-head self-attention networks as baselines in a simulation environment. We also demonstrate the performance of the model with and without auxiliary tasks. All models are evaluated based on different metrics such as time to finish each lap, number of overtakes, and speed difference with speed limit. The evaluation shows that the SwapTransformer model outperforms other models in different traffic densities in the inference phase",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=NV6rn7j5p5": {
    "title": "GEO: Generative Engine Optimization",
    "volume": "review",
    "abstract": "The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), has the potential to generate accurate and personalized responses, and is rapidly replacing traditional search engines like Google and Bing. Generative engines typically satisfy queries by synthesizing information from multiple sources and summarizing them with the help of LLMs. While this shift significantly improves user utility and generative search engine traffic, it results in a huge challenge for the third stakeholder - website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, the right tools should be provided to ensure that creator economy is not severely disadvantaged. To address this, we introduce generative engine optimization (GEO), a novel paradigm to aid content creators in improving their visibility. In this work, we propose several optimizations that can be applied to improve the visibility of content. To evaluate and compare different GEO methods, we propose a benchmark encompassing diverse user queries from multiple domains and settings, along with relevant sources needed to answer those queries. Through rigorous experiments on the proposed benchmark, we demonstrate different GEO methods involving well-designed textual enhancements, are capable of boosting source visibility by up to 40% in Generative engines responses. We find several insights that aid content creators -- for example, adding citations and quotations significantly improves visibility. We also discover that these optimizations are domain dependent, thus requiring a change in the nature of the optimization based on the source. Our work opens a new frontier in the field of information discovery systems, with profound implications for both developers of Generative enginess and content creators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=rINBD8jPoP": {
    "title": "Curriculum reinforcement learning for quantum architecture search under hardware errors",
    "volume": "review",
    "abstract": "The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a solution where a circuit architecture is first fixed, and then the individual gate parameters are optimized in an external loop to solve a task. However, the performance optimization can be intractable, and the overall performance, as well as the optimization, highly depends on the initially fixed circuit's architecture. Several quantum architecture search (QAS) algorithms have been developed to automatically select the best circuit architecture. In the case of parameter optimization, it has been observed that noise effects dramatically influence the optimizer performance and final outcomes, and this is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. In this work, we tackle this issue. To do so, we first significantly improve the computational time to simulate realistic quantum circuits by employing pauli transfer matrix formalism in the Pauli-Liouville basis by fusing gates with their respective noise models and values. Then, we devise a curriculum-based reinforcement learning QAS (CRLQAS) algorithm optimized to tackle the challenges of realistic VQA deployment by introducing (i) a 3-D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation algorithm as an optimizer for faster convergence. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across noiseless and noisy environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=tnBaiidobu": {
    "title": "Does CLIP's generalization performance mainly stem from high train-test similarity?",
    "volume": "review",
    "abstract": "Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's OOD performance, and other properties of the training data must drive CLIP to learn more generalizable representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION (¼ of its original size) on which CLIP can be trained to match its original OOD performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=AnuHbhwv9Q": {
    "title": "Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift",
    "volume": "review",
    "abstract": "Designing deep neural network classifiers that perform robustly on distributions differing from the available training data is an active area of machine learning research. However, out-of-distribution generalization for regression---the analogous problem for modeling continuous targets---remains relatively unexplored. To tackle this problem, we return to first principles and analyze how the closed-form solution for ordinary least squares (OLS) regression is sensitive to covariate shift. We characterize the out-of-distribution risk of the OLS model in terms of the eigenspectrum decomposition of the source and target data. We then use this insight to propose a method for adapting the weights of the last layer of a pre-trained neural regression model to perform better on input data originating from a different distribution. We demonstrate how this lightweight spectral adaptation procedure can improve out-of-distribution performance in a suite of both synthetic and real-world experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=H4A9e8HvIn": {
    "title": "A Unified Approach for Online Continuous DR-Submodular Maximization",
    "volume": "review",
    "abstract": "This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\\alpha$-regret bounds or have better $\\alpha$-regret bounds than the state of the art, where $\\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this optimization area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4Ya9RkEEW": {
    "title": "Fast Sampling via De-randomization for Discrete Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have emerged as powerful tools for high-quality data generation, such as image generation. Despite its success in continuous spaces, discrete diffusion models, which apply to domains such as texts and natural languages, remain under-studied and often suffer from slow generation speed. In this paper, we propose a novel de-randomized diffusion process, which leads to an accelerated algorithm for discrete diffusion models. Our technique significantly reduces the number of function evaluations (i.e., calls to the score network), making the sampling process much faster. Furthermore, we introduce a continuous-time (i.e., infinite-step) sampling algorithm that can provide even better sample qualities than its discrete-time (finite-step) counterpart. Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality over existing methods for discrete diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=UOdz9U4fxg": {
    "title": "A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions",
    "volume": "review",
    "abstract": "An important aspect of developing reliable deep learning systems is devising strategies that make these systems robust to adversarial attacks. There is a long line of work that focuses on developing defenses against these attacks, but recently, researchers have begun to study ways to reverse engineer the attack process. This allows us to not only defend against several attack models, but also classify the threat model. However, there is still a lack of theoretical guarantees for the reverse engineering process. Current approaches that give any guarantees are based on the assumption that the data lies in a union of linear subspaces, which is not a valid assumption for more complex datasets. In this paper, we propose a novel framework for reverse engineering of deceptions which supposes that the clean data lies in the range of a GAN. To classify the signal and attack, we jointly solve a GAN inversion problem and a block-sparse recovery problem. The core contribution of this paper is to provide for the first time deterministic linear convergence guarantees for this problem. We also empirically demonstrate the merits of the proposed approach on several nonlinear datasets as compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=8ZW3oLNE0c": {
    "title": "SEArch: A Self-Evolving Framework for Network Architecture Optimization",
    "volume": "review",
    "abstract": "This paper studies a fundamental network optimization problem that finds a network architecture with optimal performance (low losses) under given resource budgets (small parameter size and/or fast inference). Different from existing network optimization approaches such as network pruning, knowledge distillation (KD), and network architecture search (NAS), in this work we introduce a novel self-evolving pipeline to perform network optimization. In this framework, a simple network iteratively and adaptively modifies its structures by using the guidance from the teacher network, until it reaches the resource budget. An attention module is introduced to transfer the knowledge from teacher network to student network. The splitting edge scheme helps the student model find an optimal macro architecture. The proposed framework combines the advantages of pruning, KD, and NAS, and hence, can efficiently generate networks with flexible structure and desirable performance. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrated that our framework achieves state-of-the-art performance in this network architecture optimization task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=5HCnKDeTws": {
    "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
    "volume": "review",
    "abstract": "While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=CZ6XT5phWW": {
    "title": "Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance",
    "volume": "review",
    "abstract": "Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' \\cite{kojima2022large}). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. PRoMTd consistently outperforms traditional zero-shot approaches on all the datasets. Notably, we observe an absolute improvement of 10\\% on the complex MATHS dataset and 5\\% on the code generation task on HumanEval. In addition, we also showed that the rewritten prompt can provide better interpretability of how the LLM resolves each test instance, which can potentially be leveraged as a defense mechanism against adversarial prompting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xcMmebCT7s": {
    "title": "Learning to design protein-protein interactions with enhanced generalization",
    "volume": "review",
    "abstract": "Discovering mutations enhancing protein-protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct PPIRef, the largest and non-redundant dataset of 3D protein-protein interactions, enabling effective large-scale learning. Second, we leverage PPIRef to pre-train PPIformer, a new SE(3)-equivariant model, generalizing across diverse protein-binder variants. We fine-tune PPIformer to predict effects of mutations on protein-protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new PPIformer approach by outperforming other state-of-the-art methods on the new non-leaking splits of the standard labeled PPI mutational data and independent case studies optimizing a human antibody against SARS-CoV-2 and increasing staphylokinase thrombolytic activity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3pgJNIx3gc": {
    "title": "AlphaFold Distillation for Protein Design",
    "volume": "review",
    "abstract": "Inverse protein folding, the process of designing sequences that fold into a specific 3D structure, is crucial in bio-engineering and drug discovery. Traditional methods rely on experimentally resolved structures, but these cover only a small fraction of protein sequences. Forward folding models like AlphaFold offer a potential solution by accurately predicting structures from sequences. However, these models are too slow for integration into the optimization loop of inverse folding models during training. To address this, we propose using knowledge distillation on folding model confidence metrics, such as pTM or pLDDT scores, to create faster and end-to-end differentiable distilled model. This model can then be used as a structure consistency regularizer in training the inverse folding model. Our technique is versatile and can be applied to other design tasks, such as sequence-based protein infilling. Experimental results show that our method outperforms non-regularized baselines, yielding up to 3\\% improvement in sequence recovery and up to 45\\% improvement in protein diversity while maintaining structural consistency in generated sequences. Anonymized code for this work is available at https://anonymous.4open.science/r/AFDistill-28C3",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ayLov67GxD": {
    "title": "Video2Demo: Grounding Videos in State-Action Demonstrations",
    "volume": "review",
    "abstract": "Vision-language demonstrations provide a natural way for users to teach robots everyday tasks. However, for effective imitation learning, these demonstrations must be perceptually grounded in the robot's states and actions. While prior works train task-specific models to predict state-actions from images, these often require extensive manual annotation and fail to generalize to complex scenes. In this work, we leverage pre-trained instruction-following Vision-Language Models (VLMs) that have shown impressive zero-shot generalization for detailed caption generation. However, VLM captions, while descriptive, fail to maintain the structure and temporal consistency required to track object states over time. We propose a novel approach, Video2Demo, that uses GPT-4 to interactively query a generative VLM to construct temporally coherent state-action sequences. These sequences are in turn fed into a language model to generate robot task code that faithfully imitates the demonstration. We evaluate on a large-scale human activity dataset, EPIC-Kitchens, and show that Video2Demo outperforms pure VLM-based approaches, resulting in accurate robot task code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=77N93tc3o5": {
    "title": "Deep Independent Vector Analysis",
    "volume": "review",
    "abstract": "We introduce a deep multivariate latent variable model, Deep Independent Vector Analysis (DeepIVA), for learning linked and identifiable disentangled representations across multiple data modalities by unifying multidataset independent subspace analysis (MISA) and identifiable variational autoencoders (iVAE). DeepIVA aims to leverage hidden linkage information via the MISA loss to attain latent cross-modal alignment while leveraging the identifiability properties of the iVAE to ensure proper unimodal disentanglement. We propose a more strict set of performance measures, and demonstrate that DeepIVA can successfully recover nonlinearly mixed multimodal sources on multiple linked synthetic datasets compared with iVAE and MISA. We then apply DeepIVA on a large multimodal neuroimaging dataset, and show that DeepIVA can reveal linked nonlinear imaging sources associated with phenotype measures including age and sex",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LkQoiVp6XG": {
    "title": "G-Local Attention Graph Pooling for Graph Classification",
    "volume": "review",
    "abstract": "Graph pooling is an essential operation in Graph Neural Networks that reduces the size of an input graph while preserving its core structural properties. This compression operation improves the learned representation of the graph, yielding to a performance boost on downstream tasks. Existing pooling methods find a compressed representation considering the Global Topological Structures (e.g., cliques, stars, clusters) or Local information at node level (e.g., top-$k$ informative nodes). However, there is a lack of an effective graph pooling method that integrates both Global and Local properties of the graph. To this end, we propose a two-channel Global-Local Attention Pooling (GLA-Pool) layer that exploits the aforementioned graph properties, generating more robust graph representations. The GLA-Pool can be integrated into any GNN-based architectures. Further, we propose a smart data augmentation technique to enrich small-scale datasets. Exhaustive experiments on eight publicly available graph classification benchmarks, under standard metrics, show that GLA-Pool significantly outperforms thirteen state-of-the-art models on six datasets while being on par for the remaining two. The code will be available at this link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=CgkAGcp9lk": {
    "title": "Compositional Search of Stable Crystalline Structures in Multi-Component Alloys Using Generative Diffusion Models",
    "volume": "review",
    "abstract": "Exploring the vast composition space of multi-component alloys presents a challenging task for both ab initio (first principles) and experimental methods due to the time-consuming procedures involved. This ultimately impedes the discovery of novel, stable materials that may display exceptional properties. Here, the Crystal Diffusion Variational Autoencoder (CDVAE) model is adapted to characterize the stable compositions of a well studied multi-component alloy, NiFeCr, with two distinct crystalline phases known to be stable across its compositional space. To this end, novel extensions to CDVAE were proposed, enhancing the model's ability to reconstruct configurations from their latent space within the test set by approximately 30% . A fact that increases a model's probability of discovering new materials when dealing with various crystalline structures. Afterwards, the new model is applied for materials generation, demonstrating excellent agreement in identifying stable configurations within the ternary phase space when compared to first principles data. Finally, a computationally efficient framework for inverse design is proposed, employing Molecular Dynamics (MD) simulations of multi- component alloys with reliable interatomic potentials, enabling the optimization of materials property across the phase space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=EhrzQwsV4K": {
    "title": "L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation",
    "volume": "review",
    "abstract": "Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store. These components enable L2MAC to generate virtually unbounded code structures, bypassing the constraints of the finite context window while producing code that fulfills complex user-specified requirements. We empirically show that L2MAC succeeds in generating large code bases for system design tasks where other coding methods fall short in implementing user requirements and provide insight into the reasons for this performance gap",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ICuUgRLp4C": {
    "title": "Learning High-Order Relationships of Brain Regions",
    "volume": "review",
    "abstract": "Discovering reliable and informative interactions among brain regions from functional magnetic resonance imaging (fMRI) signals is essential in neuroscientific predictions of cognition. Most of the current methods fail to accurately characterize those interactions because they only focus on pairwise connections and overlook the high-order relationships of brain regions. We delve into this problem and argue that these high-order relationships should be maximally informative and minimally redundant (MIMR). However, identifying such high-order relationships is challenging and highly under-explored. Methods that can be tailored to our context are also non-existent. In response to this gap, we propose a novel method named HyBRiD that aims to extract MIMR high-order relationships from fMRI data. HyBRiD employs a Constructor to identify hyperedge structures, and a Weighter to compute a weight for each hyperedge. HyBRiD achieves the MIMR objective through an innovative information bottleneck framework named multi-head drop-bottleneck with theoretical guarantees. Our comprehensive experiments demonstrate the effectiveness of our model. In terms of the quality of hyperedges measured by the CPM metric, our model outperforms the state-of-the-art predictive model by an average of 12.1%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=NeWiiF6KLB": {
    "title": "Stabilized E(n)-Equivariant Graph Neural Networks-assisted Generative Models",
    "volume": "review",
    "abstract": "Due to its simplicity and computational efficiency, the E(n)-equivariant graph neural network (EGNN) [Satorras, et al., ICML, 2021] has been used as the backbone of equivariant normalizing flows (ENF), equivariant diffusion model (EDM), and beyond for Euclidean equivariant generative modeling. Nonetheless, it has been observed that ENF and EDM can be unstable; in this paper, we investigate the source of their instability by performing a sensitivity analysis of their backpropagation. Based on our theoretical analysis, we propose a regularization to stabilize and improve ENF and EDM. Experiments on benchmark datasets demonstrate that the regularized ENF outperforms the baseline model in terms of stability and computational efficiency by a remarkable margin. Furthermore, our results show that the proposed regularization can stabilize EDM and improve its performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=c93SBwz1Ma": {
    "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger is embedded in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0\\% across the six benchmark tasks on GPT-4. We also demonstrate the interpretability of BadChain by showing that the relationship between the trigger and the backdoor reasoning step can be well-explained based on the output of the backdoored model. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xNn2nq5kiy": {
    "title": "Plan-based Prompting Improves Literature Review Generation",
    "volume": "review",
    "abstract": "We explore the zero-shot abilities of recent large language models (LLMs) for the task of writing the literature review of a scientific research paper conditioned on its abstract and the content of related papers. We propose and examine a novel strategy for literature review generation with an LLM in which we first generate a plan for the review, and then use it to generate the actual text. While modern LLMs can easily be trained or prompted to condition on all abstracts of papers to be cited to generate a literature review without such intermediate plans, our empirical study shows that these intermediate plans improve the quality of generated literature reviews over vanilla zero-shot generation. Furthermore, we also create a new test corpus consisting of recent arXiv papers (with full content) posted after both open-sourced and closed-sourced LLMs that were used in our study were released. This allows us to ensure that our zero-shot experiments do not suffer from test set contamination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjJcJhIzYx": {
    "title": "Neural Rankers for Code Generation via Inter-Cluster Modeling",
    "volume": "review",
    "abstract": "Code Large Language Models (CodeLLMs) have ushered in a new era of code generation advancements. However, selecting the best solutions from among all possible CodeLLM solutions remains a challenge. Previous methods frequently overlooked the intricate functional similarities and interactions between clusters, resulting in suboptimal results. In this work, we introduce SRank, a novel rerank- ing strategy for selecting the best solution from code generation that focuses on modeling inter-cluster relationship. By quantifying the functional overlap between clusters, our approach provides a better ranking strategy of code solutions. Empir- ical results show that our method achieves a remarkable results on pass@1 score. For instance, on the Human-Eval benchmark, we achieve 69.66% in pass@1 with Codex002, 75.31% for WizardCoder, 53.99% for StarCoder and 60.55% for Code- Gen, which surpass the state-of-the-arts solution ranking methods, such as CodeT and Coder-Reviewer on the same CodeLLMs with significant margin (≈ 6.1% improvement on average). Comparing to the random sampling method, we can achieve an average improvement of ≈ 23.07% on Human-Eval. Even in scenar- ios with limited test inputs, our approach demonstrates robustness and superiority, marking a new benchmark in code generation reranking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=bLhqPxRy3G": {
    "title": "Linear programming using diagonal linear networks",
    "volume": "review",
    "abstract": "Linear programming has played a crucial role in shaping decision-making, resource allocation, and cost reduction in various domains. In this paper, we investigate the application of overparametrized neural networks and their implicit bias in solving linear programming problems. Specifically, our findings reveal that training diagonal linear networks with gradient descent, while optimizing the squared $L_2$-norm of the slack variable, leads to solutions for entropically regularized linear programming problems. Remarkably, the strength of this regularization depends on the initialization used in the gradient descent process. We analyze the convergence of both discrete-time and continuous-time dynamics and demonstrate that both exhibit a linear rate of convergence, requiring only mild assumptions on the constraint matrix. For the first time, we introduce a comprehensive framework for solving linear programming problems using diagonal neural networks. We underscore the significance of our discoveries by applying them to address challenges in basis pursuit and optimal transport problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=M0QHJI9OuF": {
    "title": "TROJFAIR: TROJAN FAIRNESS ATTACKS",
    "volume": "review",
    "abstract": "Deep learning models have been incorporated into high-stakes sectors, including healthcare diagnosis, loan approvals, and candidate recruitment, among others. Consequently, any bias or unfairness in these models can harm those who depend on such models. In response, many algorithms have emerged to ensure fairness in deep learning. However, while the potential for harm is substantial, the resilience of these fair deep learning models against malicious attacks has never been thoroughly explored, especially in the context of emerging Trojan attacks. Moving beyond prior research, we aim to fill this void by introducing \\textit{TrojFair}, a Trojan fairness attack. Unlike existing attacks, TrojFair is model-agnostic and crafts a Trojaned model that functions accurately and equitably for clean inputs. However, it displays discriminatory behaviors - producing both incorrect and unfair results - for specific groups with tainted inputs containing a trigger. TrojFair is a stealthy Fairness attack that is resilient to existing model fairness audition detectors since the model for clean inputs is fair. TrojFair achieves a target group attack success rate exceeding 88.77\\%, with an average accuracy loss less than 0.44\\%. It also maintains a high discriminative score between the target and untarget groups across various datasets and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=bKzX0m6TEZ": {
    "title": "An Inexact Conditional Gradient Method for Constrained Bilevel Optimization",
    "volume": "review",
    "abstract": "Bilevel optimization is an important class of optimization problems where one optimization problem is nested within another. This framework is widely used in machine learning problems, including meta-learning, data hyper-cleaning, and matrix completion with denoising. In this paper, we focus on a bilevel optimization problem with a strongly convex lower-level problem and a smooth upper-level objective function over a compact and convex constraint set. Several methods have been developed for tackling unconstrained bilevel optimization problems, but there is limited work on methods for the constrained setting. In fact, for those methods that can handle constrained problems, either the convergence rate is slow or the computational cost per iteration is expensive. To address this issue, in this paper, we introduce a novel single-loop projection-free method using a nested approximation technique. Our proposed method has an improved per-iteration complexity, surpassing existing methods, and achieves optimal convergence rate guarantees matching the best-known complexity of projection-free algorithms for solving convex constrained single-level optimization problems. In particular, when the upper-level objective function is convex, our method requires $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ iterations to find an $\\epsilon$-optimal solution. Moreover, when the upper-level objective function is non-convex the complexity of our method is $\\mathcal{O}(\\epsilon^{-2})$ to find an $\\epsilon$-stationary point. We also present numerical experiments to showcase the superior performance of our method compared with state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=samyfu6G93": {
    "title": "NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks",
    "volume": "review",
    "abstract": "Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To train NeuroBack, a new dataset called DataBack containing 120,286 data samples is created. Finally, NeuroBack is implemented as an enhancement to a state-of-the-art SAT solver called Kissat. As a result, it allowed Kissat to solve 5.2% more problems on the recent SAT competition problem set, SATCOMP-2022. NeuroBack therefore shows how machine learning can be harnessed to improve SAT solving in an effective and practical manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=DpFeMH4l8Q": {
    "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models",
    "volume": "review",
    "abstract": "Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=kVj2uyytyg": {
    "title": "Unsupervised Federated Graph Matching with Graphlet Feature Extraction and Separate Trust Region",
    "volume": "review",
    "abstract": "Graph matching in the setting of federated learning is still an open problem. This paper proposes an unsupervised federated graph matching algorithm, UFGM, for inferring matched node pairs on different graphs across clients while maintaining privacy requirement, by leveraging graphlet theory and trust region optimization. First, the nodes' graphlet features are captured to generate pseudo matched node pairs on different graphs across clients as pseudo training data for tackling the dilemma of unsupervised graph matching in federated setting and leveraging the strength of supervised graph matching. An approximate graphlet enumeration method is proposed to sample a small number of graphlets and capture nodes' graphlet features. Theoretical analysis is conducted to demonstrate that the approximate method is able to maintain the quality of graphlet estimation while reducing its expensive cost. Second, we propose a separate trust region algorithm for pseudo supervised federated graph matching while maintaining the privacy constraints. In order to avoid expensive cost of the second-order Hessian computation in the trust region algorithm, we propose two weak quasi-Newton conditions to construct a positive definite scalar matrix as the Hessian approximation with only first-order gradients. We theoretically derive the error introduced by the separate trust region due to the Hessian approximation and conduct the convergence analysis of the approximation method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yiyDi50xx6": {
    "title": "GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data",
    "volume": "review",
    "abstract": "Vertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. In this work, we train a graph neural network (GNN) through VFL, where each client owns a part of the node features and a different edge set. This data scenario incurs a significant communication overhead, not only because of the handling of distributed features but also due to neighborhood aggregation in a GNN. Moreover, the training analysis is faced with a challenge caused by the biased stochastic gradients. We propose a model-splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip communication in neighborhood aggregation and in model updates, respectively, greatly reducing communication while enjoying convergence guarantees. We conduct extensive numerical experiments on real-world datasets, showing that GLASU effectively trains a GNN that matches the accuracy of centralized training, while using only a fraction of the time due to communication saving",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJxhZnGU1q": {
    "title": "Strategic Recommendations for Improved Outcomes in Congestion Games",
    "volume": "review",
    "abstract": "Traffic on roads, packets on the Internet, and electricity on power grids share a structure abstracted in congestion games, where self-interested behaviour can lead to socially sub-optimal results. External recommendations may seek to alleviate these issues, but recommenders must take into account the effect that their recommendations have on the system. In this paper, we investigate the effects that dynamic recommendations have on $Q$-learners as they repeatedly play congestion games. To do so, we propose a novel model of recommendation whereby a $Q$-learner receives a recommendation as a state. Thus, the recommender strategically picks states during learning, which we call the Learning Dynamic Manipulation Problem. We define the \\textit{manipulative potential} of these recommenders in repeated congestion games and propose an algorithm for the Learning Dynamic Manipulation Problem designed to drive the actions of $Q$-learners toward a target action distribution. We simulate our algorithm and show that it can drive the system to convergence at the social optimum of a well-known congestion game. Our results show theoretically and empirically that increasing the recommendation space can increase the manipulative potential of the recommender",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=73lu1yw6At": {
    "title": "Complexity of Formal Explainability for Sequential Models",
    "volume": "review",
    "abstract": "This work contributes to formal explainability in AI (FXAI) for sequential models, including Recurrent Neural Networks (RNN), Transformers, and automata models from formal language theory (e.g. finite-state automata). We study two common notions of explainability in FXAI: (1) abductive explanations (a.k.a. minimum sufficient reasons), and (2) counterfactual (a.k.a. contrastive) explanations. To account for various forms of sequential data (e.g. texts, time series, and videos), our models take a sequence of rational numbers as input. We first observe that simple RNN and Transformers suffer from NP-hard complexity (or sometimes undecidability) for both types of explanations. The works on extraction of automata from RNN hinge on the assumption that automata are more interpretable than RNN. Interestingly, it turns out that generating abductive explanations for DFA is computationally intractable (PSPACE-complete), for features that are represented by regular languages. On the positive side, we show that deterministic finite automata (DFA) admit polynomial-time complexity for counterfactual explanations. However, DFA are a highly inexpressive model for classifying sequences of numbers. To address this limitation, we provide two expressive extensions of finite automata, while preserving PTIME explainability and admitting automata learning algorithms: (1) deterministic interval automata, and (2) deterministic register automata with a fixed number of registers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=UqY0SEe5pC": {
    "title": "Analyzing Neural Network Based Generative Diffusion Models via Convexification",
    "volume": "review",
    "abstract": "Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of the score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. We show that the global optimum of the score matching objective can be attained by solving a simple convex program. Specifically, for univariate training data, we establish that the Langevin diffusion process through the learned neural network model converges in the Kullback-Leibler (KL) divergence to either a Gaussian or a Gaussian-Laplace distribution when the weight decay parameter is set appropriately. Our convex programs alleviate issues in computing the Jacobian and also extends to multidimensional score matching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=wgmOXVTGdb": {
    "title": "LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer",
    "volume": "review",
    "abstract": "Graphic layout designs play an essential role in visual communication. Yet handcrafting layout designs is skill-demanding, time-consuming, and non-scalable to batch production. Generative models emerge to make design automation scalable but it remains non-trivial to produce designs that comply with designers' multimodal desires, i.e., constrained by background images and driven by foreground content. We propose LayoutDETR that inherits the high quality and realism from generative modeling, while reformulating content-aware requirements as a detection problem: we learn to detect in a background image the reasonable locations, scales, and spatial relations for multimodal foreground elements in a layout. Our solution sets a new state-of-the-art performance for layout generation on public benchmarks and on our newly-curated ad banner dataset. We integrate our solution into a graphical system that facilitates user studies, and show that users prefer our designs over baselines by significant margins",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=AcGUW5655J": {
    "title": "Constraining Non-Negative Matrix Factorization to Improve Signature Learning",
    "volume": "review",
    "abstract": "Collaborative filtering approaches are fundamental for learning meaningful low-dimensional representations when only association data is available. Among these methods, Non-negative Matrix Factorization (NMF) has gained prominence due to its capability to yield interpretable and meaningful low-dimensional representations. However, one significant challenge for NMF is the vast number of solutions for the same problem instance, making the selection of high-quality signatures a complex task. In response to this challenge, our work introduces a novel approach, Self-Matrix Factorization (SMF), which leverages NMF by incorporating constraints that preserve the relationships inherent in the original data. This is achieved by drawing inspiration from a distinct family of matrix decomposition methods, known as Self-Expressive Models (SEM). In our experimental analyses, conducted on two diverse benchmark datasets, our findings present a compelling narrative. SMF consistently delivers competitive or even superior performance when compared to NMF in predictive tasks. However, what truly sets SMF apart, as validated by our empirical results, is its remarkable ability to consistently generate significantly more meaningful object representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zSwH0Wo2wo": {
    "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch",
    "volume": "review",
    "abstract": "Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text. Prior work has introduced automated tools that elicit harmful outputs to identify these risks. While this is a valuable step toward securing models, these approaches rely on a pre-existing way to efficiently classify undesirable outputs. Using a pre-existing classifier does not allow for red-teaming to be tailored to the target model. Furthermore, when failures can be easily classified in advance, red-teaming has limited marginal value because problems can be avoided by simply filtering training data and/or model outputs. Here, we consider red-teaming \"from scratch\" in which the adversary does not begin with a way to classify failures. Our framework consists of three steps: 1) Exploring the model's range of behaviors in the desired context; 2) Establishing a measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) Exploiting the model's flaws using this measure to develop diverse adversarial prompts. We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements. In doing so, we construct the CommonClaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither. We are making code and data available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=k82MvVIbrC": {
    "title": "Learning Structured Sparse Neural Networks Using Group Envelope Regularization",
    "volume": "review",
    "abstract": "We propose an efficient method to learn both unstructured and structured sparse neural networks during training, utilizing a novel generalization of the sparse envelope function (SEF) used as a regularizer, termed {\\itshape{weighted group sparse envelope function}} (WGSEF). The WGSEF acts as a neuron group selector, which is leveraged to induce structured sparsity. The method ensures a hardware-friendly structured sparsity of a deep neural network (DNN) to efficiently accelerate the DNN's evaluation. Notably, the method is adaptable, letting any hardware specify group definitions, such as filters, channels, filter shapes, layer depths, a single parameter (unstructured), etc. Owing to the WGSEF's properties, the proposed method allows to a pre-define sparsity level that would be achieved at the training convergence, while maintaining negligible network accuracy degradation or even improvement in the case of redundant parameters. We introduce an efficient technique to calculate the exact value of the WGSEF along with its proximal operator in a worst-case complexity of $O(n)$, where $n$ is the total number of group variables. In addition, we propose a proximal-gradient-based optimization method to train the model, that is, the non-convex minimization of the sum of the neural network loss and the WGSEF. Finally, we conduct an experiment and illustrate the efficiency of our proposed technique in terms of the completion ratio, accuracy, and inference latency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=B4nhr6OJWI": {
    "title": "Instilling Inductive Biases with Subnetworks",
    "volume": "review",
    "abstract": "Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases — preferences for some solutions over others — into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to adopt a specific, generalizable solution to a modular arithmetic task. Second, we demonstrate that Subtask Induction successfully induces a human-like shape bias while increasing data efficiency for convolutional and transformer-based image classification models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=jenyYQzue1": {
    "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
    "volume": "review",
    "abstract": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our data instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=gyJpajLkX2": {
    "title": "ENHANCING MULTIVARIATE TIME SERIES FORECAST- ING WITH MUTUAL INFORMATION-DRIVEN CROSS- VARIABLE AND TEMPORAL MODELING",
    "volume": "review",
    "abstract": "Recent researches have showcased the significant effectiveness of deep learning techniques for multivariate time series forecasting (MTSF). Broadly speaking, these techniques are bifurcated into two categories: Channel-independence and Channel-mixing approaches. While Channel-independence models have generally demonstrated superior outcomes, Channel-mixing methods, especially when dealing with time series that display inter-variable correlations, theoretically promise enhanced performance by incorporating the correlation between variables. However, we contend that the unnecessary integration of information through Channel-mixing can curtail the potential enhancement in MTSF model performance. To substantiate this claim, we introduce the Cross-variable Decorrelation Aware feature Modeling (CDAM) for Channel-mixing approaches. This approach is geared toward reducing superfluous information by minimizing the mutual information between the latent representation of a single univariate sequence and its accompanying multivariate sequence input. Concurrently, it optimizes the joint mutual information shared between the latent representation, its univariate input, and the associated univariate forecast series. Notably, prevailing techniques directly project future series using a single-step forecaster, sidelining the temporal correlation that might exist across varying timesteps in the target series. Addressing this gap, we introduce the Temporal correlation Aware Modeling (TAM). This strategy maximizes the mutual information between adjacent sub-sequences of both the forecasted and target series. By synergizing CDAM and TAM, we sculpt a pioneering framework for MTSF, named as InfoTime. Comprehensive experimental analysis have demonstrated the capability of InfoTime to consistently outpace existing models, encompassing even those considered state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mWxcEm7jIv": {
    "title": "Training Diffusion Classifiers with Denoising Assistance",
    "volume": "review",
    "abstract": "Score-matching and diffusion models have emerged as state-of-the-art generative models for both conditional and unconditional generation. Classifier-guided diffusion models are created by training a classifier on samples obtained from the forward-diffusion process (i.e., from data to noise). In this paper, we propose denoising-assisted (DA) classifiers wherein the diffusion classifier is trained using both noisy and denoised examples as simultaneous inputs to the model. We differentiate between denoising-assisted (DA) classifiers and noisy classifiers, which are diffusion classifiers that are only trained on noisy examples. Our experiments on Cifar10 and Imagenet show that DA-classifiers improve over noisy classifiers both quantitatively in terms of generalization to test data and qualitatively in terms of perceptually-aligned classifier-gradients and generative modeling metrics. We theoretically characterize the gradients of DA-classifiers to explain improved perceptual alignment. Building upon the observed generalization benefits of DA-classifiers, we propose and evaluate a semi-supervised framework for training diffusion classifiers and demonstrate improved generalization of DA-classifiers over noisy classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=GKxmmAwxj1": {
    "title": "Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules",
    "volume": "review",
    "abstract": "The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strategies, such as maximum likelihood alone, fail while our novel architecture and multi-stage training strategy are able to model the conformational distributions of protein G and HP35",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FL1VmOgiO8": {
    "title": "Sentiment-Enhanced Stock Price Prediction: A Novel Ensemble Model Approach",
    "volume": "review",
    "abstract": "Stock price prediction remains a formidable challenge within the realm of financial markets, wherein a multitude of models and methodologies have been under exploration to prognosticate the dynamic behaviour of equities. This research endeavour encompasses an exhaustive examination of extant stock prediction systems, entailing a meticulous assessment of their merits and demerits, concurrently pinpointing discernible lacunae and avenues for enhancement. Subsequently, we harnessed the capabilities of BERT, an exemplar in the domain of natural language processing, to conduct sentiment analysis across a heterogeneous corpus of news articles pertinent to the subject stocks. Additionally, an ancillary sub-experiment was conducted to ascertain the relative impact of three distinct categories of news articles, namely headlines, summaries, and a composite amalgamation of the two, on the efficacy of stock price prediction. The outcome of this investigative pursuit was the generation of sentiment scores for each trading date, which were subsequently integrated as input features in the training of a neural network. Through a comparative analysis of various neural network models, including but not limited to RNN, LSTM, GAN, and WGAN-GP, we discerned that the WGAN-GP model exhibited the most favourable predictive performance. Building upon these findings, we introduced the FB-GAN model, an ensemble architecture comprising WGAN-GP, which capitalizes on the fusion of historical stock price data and market sentiment scores for enhanced stock price prediction. Subsequently, a comprehensive evaluation of our approach was undertaken vis-à-vis established models, gauging its performance against five prominent equities, namely Amazon, Apple, Microsoft, Nvidia, and Adobe. In summation, this research makes a compelling case for the integration of BERT-based sentiment analysis within the ambit of stock price prediction. Our initial hypothesis regarding the significant influence of market sentiment on stock price prediction was validated, and our proposed FB-GAN model outperformed all other models. Furthermore, incorporating both the headline and summary of the news article contributed to enhanced stock price prediction compared to utilizing either the headline or summary in isolation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=zFWKKYz2yn": {
    "title": "Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network",
    "volume": "review",
    "abstract": "This paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA (Deterministic Finite Automata) with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are trained completely. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with the number of states very close to ground truth DFA. Among RNN cells, O2RNN produces stable DFA consistently compared to other cells. For Dyck Languages, we observe that although GRU outperforms other RNNs in network performance, the DFA extracted by O2RNN has higher performance and better stability. The stability is computed as the standard deviation of accuracy on test sets on networks trained across $10$ seeds. On Dyck Languages, quantization methods outperformed $L^{*}$ with better stability in accuracy and the number of states. $L^{*}$ often showed instability in accuracy in the order of $16\\% - 22\\%$ for GRU and MIRNN while deviation for quantization methods varied in $5\\% - 15\\%$. In many instances with LSTM and GRU, DFA's extracted by $L^{*}$ even failed to beat chance accuracy ($50\\%$), while those extracted by quantization method had standard deviation in the $7\\%-17\\%$ range. For O2RNN, both rule extraction methods had a deviation in the $0.5\\% - 3\\%$ range",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=THJEa8adBn": {
    "title": "Harnessing Density Ratios for Online Reinforcement Learning",
    "volume": "review",
    "abstract": "The theories of offline and online reinforcement learning, despite having evolved in parallel, have recently started to see unification, and algorithms/concepts in one setting often have natural counterparts in the other. However, the notion of density ratio modeling, an emerging topic in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on a dataset with good coverage, but the core challenge in online RL is to collect such an exploratory dataset without having one to start. In this work we show—perhaps surprisingly—that density ratio-based algorithms have online counterparts. Assuming the mere existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give an algorithm (GLOW) which performs sample-efficient online exploration under value-function and density-ratio realizability. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HYGLOW, for the Hybrid RL setting (Song et al., 2023) in which online RL is augmented with additional offline data. HYGLOW is derived as a special case of a novel meta-algorithm, H2O, which provides a provable black-box reduction from hybrid RL to offline RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=w3YZ9MSlBu": {
    "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training",
    "volume": "review",
    "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic **M**usic und**ER**standing model with large-scale self-supervised **T**raining (**MERT**), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rDH7dIFn20": {
    "title": "Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits",
    "volume": "review",
    "abstract": "Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\\tilde O\\big(d\\sqrt{\\sum_{t=1}^T\\sigma_t^2} + d\\big)$, where $\\sigma_t$ is the variance of the pairwise comparison at round $t$, $d$ is the dimension of the context vectors, and $T$ is the time horizon. Our regret bound naturally aligns with the intuitive expectation — in scenarios where the comparison is deterministic, the algorithm only suffers from an $\\tilde O(d)$ regret. We perform empirical experiments on synthetic data to confirm the advantage of our method over previous variance-agnostic algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ueTdErd5Ib": {
    "title": "A Discretization Framework for Robust Contextual Stochastic Optimization",
    "volume": "review",
    "abstract": "We study contextual stochastic optimization problems. Optimization problems have uncertain parameters stemming from unknown, context-dependent, distributions. Due to the inherent uncertainty in these problems, one is often interested not only in minimizing expected cost, but also to be robust and protect against worst case scenarios. We propose a novel method that combines the learning stage with knowledge of the downstream optimization task. The method prescribes decisions which aim to maximize the likelihood that the cost is below a (user-controlled) threshold. The key idea is (1) to discretize the feasible region into subsets so that the uncertain objective function can be well approximated deterministically within each subset, and (2) devise a secondary optimization problem to prescribe decisions by integrating the individual approximations determined in step (1). We provide theoretical guarantees bounding the underlying regret of decisions proposed by our method. In addition, experimental results demonstrate that our approach is competitive in terms of average regret and yields more robust solutions than other methods proposed in the literature, including up to 20 times lower worst-case cost on a real-world electricity generation problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uCMxeZCp2T": {
    "title": "Nature-Inspired Local Propagation",
    "volume": "review",
    "abstract": "The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections. On the opposite, intelligent processes in nature arises without the need for such collections, but simply by online processing of the environmental information. In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality. This paper shows that such a feature arises from a pre-algorithmic view of learning that is inspired by related studies in Theoretical Physics. We show that the algorithmic interpretation of the derived \"laws of learning\", which takes the structure of Hamiltonian equations, reduces to Backpropagation when the the speed of propagation goes to infinity. This opens the doors to machine learning studies based on full on-line information processing that are based the replacement of Backpropagation with the proposed spatiotemporal local algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=npf3gREtf7": {
    "title": "Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3× more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2× fewer ICL examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=87XbxDnPqj": {
    "title": "Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction",
    "volume": "review",
    "abstract": "In computed tomography (CT), the forward model consists of a linear Radon transform followed by an exponential nonlinearity based on the attenuation of light according to the Beer–Lambert Law. Conventional reconstruction often involves inverting this nonlinearity as a preprocessing step and then solving a convex inverse problem. However, this nonlinear measurement preprocessing required to use the Radon transform is poorly conditioned in the vicinity of high-density materials, such as metal. This preprocessing makes CT reconstruction methods numerically sensitive and susceptible to artifacts near high-density regions. In this paper, we study a technique where the signal is directly reconstructed from raw measurements through the nonlinear forward model. Though this optimization is nonconvex, we show that gradient descent provably converges to the global optimum at a geometric rate, perfectly reconstructing the underlying signal with a near minimal number of random measurements. We also prove similar results in the under-determined setting where the number of measurements is significantly smaller than the dimension of the signal. This is achieved by enforcing prior structural information about the signal through constraints on the optimization variables. We illustrate the benefits of direct nonlinear CT reconstruction with cone-beam CT experiments on synthetic and real 3D volumes. We show that this approach reduces metal artifacts compared to a commercial reconstruction of a human skull with metal dental crowns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SmZD7yxpPC": {
    "title": "GlycoNMR: A Carbohydrate-Specific NMR Chemical Shift Dataset for Machine Learning Research",
    "volume": "review",
    "abstract": "Molecular representation learning (MRL) is a powerful contribution by machine learning to chemistry as it converts molecules into numerical representations, which serves as fundamental for diverse biochemical applications, such as property prediction and drug design. While MRL has had great success with proteins and general biomolecules, it has yet to be explored for carbohydrates in the growing fields of glycoscience and glycomaterials (the study and design of carbohydrates). This under-exploration can be primarily attributed to the limited availability of comprehensive and well-curated carbohydrate-specific datasets and a lack of machine learning (ML) techniques tailored to meet the unique problems presented by carbohydrate data. Interpreting and annotating carbohydrate data is generally more complicated than protein data, and requires substantial domain knowledge. In addition, existing MRL methods were predominately optimized for proteins and small biomolecules, and may not be effective for carbohydrate applications without special modifications. To address this challenge, accelerate progress in glycoscience and glycomaterials, and enrich the data resources of the ML community, we introduce GlycoNMR. GlycoNMR contains two laboriously curated datasets with 2,609 carbohydrate structures and 211,543 annotated nuclear magnetic resonance (NMR) atomic-level chemical shifts that can be used to train ML models for precise atomic-level prediction. NMR data is one of the most appealing starting points for developing ML techniques to facilitate glycoscience and glycomaterials research, as NMR is the preeminent technique in carbohydrate structure research, and biomolecule structure is among the foremost predictors of functions and properties. We tailored a set of carbohydrate-specific features and adapted existing MRL models to effectively tackle the problem of predicting NMR shifts. For illustration, we benchmark these modified MRL models on the GlycoNMR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=kKXIYUi8ff": {
    "title": "DynamicsDiffusion: Generating and Rare Event Sampling of Molecular Dynamic Trajectories Using Diffusion Models",
    "volume": "review",
    "abstract": "Molecular dynamics simulations are fundamental tools for quantitative molecular sciences. However, these simulations are computationally demanding and often struggle to sample rare events crucial for understanding spontaneous organization and reconfiguration in complex systems. To improve general speed and the ability to sample rare events in a directed fashion, we propose a method called $\\textit{DynamicsDiffusion}$ based on denoising diffusion probabilistic models (DDPM) to generate molecular dynamics trajectories from noise. The generative model can then serve as a surrogate to sample rare events. We leverage the properties of DDPMs, such as conditional generation, the ability to generate variations of trajectories, and those with certain conditions, such as crossing from one state to another, using the 'inpainting' property of DDPMs, which became only applicable when generating whole trajectories and not just individual conformations. To our knowledge, this is the first deep generative modeling for generating molecular dynamics trajectories. We hope this work will motivate a new generation of generative modeling for the study of molecular dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=gisAooH2TG": {
    "title": "RePLan: Robotic Replanning with Perception and Language Models",
    "volume": "review",
    "abstract": "Advancements in large language models (LLMs) have demonstrated their potential in facilitating high-level reasoning, logical reasoning and robotics planning. Recently, LLMs have also been able to generate reward functions for low-level robot actions, effectively bridging the interface between high-level planning and low-level robot control. However, the challenge remains that even with syntactically correct plans, robots can still fail to achieve their intended goals. This failure can be attributed to imperfect plans proposed by LLMs or to unforeseeable environmental circumstances that hinder the execution of planned subtasks due to erroneous assumptions about the state of objects. One way to prevent these challenges is to rely on human-provided step-by-step instructions, limiting the autonomy of robotic systems. Vision Language Models (VLMs) have shown remarkable success in tasks such as visual question answering and image captioning. Leveraging the capabilities of VLMs, we present a novel framework called RePLan that enables real-time replanning capabilities. This framework utilizes the physical grounding provided by a VLM's understanding of the world's state to adapt robot actions when the initial plan fails to achieve the desired goal. We test our approach within two long-horizon task domains, a wooden cabinet puzzle and a larger-scale kitchen environment. We find that RePLan enables a robot to successfully adapt to unforeseen obstacles while accomplishing open-ended, long-horizon goals, while baseline models cannot",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=agPpmEgf8C": {
    "title": "Predictive auxiliary objectives in deep RL mimic learning in the brain",
    "volume": "review",
    "abstract": "The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain-- that of an auxiliary learning system that benefits representation learning in other regions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=AP779Zy70y": {
    "title": "GATE: How to Keep Out Intrusive Neighbors",
    "volume": "review",
    "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neighborhood aggregation, as we show experimentally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over-smoothing by addressing its root cause of unnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non-)linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neighbors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to analyze a model's ability to utilize the appropriate amount of neighborhood aggregation, which could be of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=AcoXPIPh4A": {
    "title": "Risk Bounds of Accelerated SGD for Overparameterized Linear Regression",
    "volume": "review",
    "abstract": "Accelerated stochastic gradient descent (ASGD) is a workhorse in deep learning and often achieves better generalization performance than SGD. However, existing optimization theory can only explain the faster convergence of ASGD, but cannot explain its better generalization. In this paper, we study the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. We establish an instance-dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. Our analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD. Our result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when our analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=MY8SBpUece": {
    "title": "A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks",
    "volume": "review",
    "abstract": "Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component---spike---in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the loss, we demonstrate that these non-linear features can enhance learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=iARAKITHTH": {
    "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
    "volume": "review",
    "abstract": "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using pre-trained LLMs. The method, called *Binoculars*, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate *Binoculars* on a number of text sources and in varied situations. On news documents *Binoculars* detect 95\\% of synthetic samples at a false positive rate of 0.01%, given 512 tokens of text from either humans or ChatGPT, matching highly competitive commercial detectors tuned specifically to detect ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=q4cfN6PGY7": {
    "title": "Towards Deep Viticultural Representations: Joint Region and Grape Variety Embeddings",
    "volume": "review",
    "abstract": "The creation of embeddings, representations, or features for abstract or non-numeric variables is a prerequisite to utilize these variables in machine learning models; this is also the case for viticulture (growing grapes for wine). Viticultural regions and grape varieties are variables for which deep representations are currently not available. Regions are somewhat definable by their approximate longitude and latitude, average elevation, or averages of climate variables. Each of these 'raw' features contributes valuable information about the region but it does not easily define a metric for agro-ecological proximity between regions. Grape varieties have much fewer 'raw' features; one example may be their genetic markers, which, however, are still categorical in nature. Analysis of lineage is possible but does not necessarily provide useful features to the viticulturists as grape attributes are not necessarily inferable by their lineage such as dominant wine style or suitability for a particular region. Therefore, here we present a self-supervised approach to learning joint regional and varietal embeddings using joint variational autoencoder (VAE) networks. This is based on the assumption that regions that grow similar proportions of similar grape varieties are more similar to each other than those that do not, or that grape varieties that often occur together may have similar viticultural characteristics (e.g. climate requirements, aromas, disease resistance). We thereby overcome the lack of detailed data and create deep embeddings for 1557 grape varieties (e.g. Merlot, Riesling, Chardonnay etc.) and 595 viticulturally important regions (e.g. Piemonte, Bourgogne, Mosel etc.). We examine the embeddings, their usability for downstream tasks as well as whether the joint autoencoder network may be used as a varietal suitability ranking system. We show our embeddings to outperform 'raw' features on downstream tasks and results indicating potential of the autoencoder networks as data-based recommender systems. This is also, to our knowledge, the first work to apply joint VAEs to purely categorical data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9t8dQ30kU": {
    "title": "Task structure and nonlinearity jointly determine learned representational geometry",
    "volume": "review",
    "abstract": "The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and on the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric saturation of ReLU, which leads feature neurons to specialize for different regions of input space. Feature neurons in Tanh networks, by contrast, tend to inherit the task label structure. Consequently, when the target outputs are low dimensional, Tanh networks generate neural representations that are more disentangled than those obtained with a ReLU nonlinearity. Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RnYd44LR2v": {
    "title": "OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift",
    "volume": "review",
    "abstract": "Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear way, under many distribution shifts. The latter enables the prediction of OOD robustness from ID robustness. Based on this, we are able to predict the upper limit of OOD robustness for existing robust training schemes. The results suggest that achieving OOD robustness requires designing novel methods beyond the conventional ones. Last, we discover that extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness. Noticeably, the discovered training schemes, compared to the baseline, exhibit dramatically higher robustness under threat shift while keeping high ID robustness, demonstrating new promising solutions for robustness against both multi-attack and unforeseen attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=BSqVfAFJWz": {
    "title": "The Distributional Reward Critic Architecture for Reinforcement Learning Under Confusion Matrix Reward Perturbations",
    "volume": "review",
    "abstract": "We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed especially for that setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WnqRR915j": {
    "title": "Llemma: An Open Language Model for Mathematics",
    "volume": "review",
    "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known openly released models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=IWpLQfZ8Xg": {
    "title": "Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention",
    "volume": "review",
    "abstract": "Self-attention has been widely used in various machine learning models, such as vision transformers. The standard dot-product self-attention is arguably the most popular structure, and there is a growing interest in understanding the mathematical properties of such attention mechanisms. This paper presents a fine-grained local sensitivity analysis of the standard dot-product self-attention. Despite the well-known fact that dot-product self-attention is not (globally) Lipschitz, we develop new theoretical local bounds quantifying the effect of input feature perturbations on the attention output. Utilizing mathematical techniques from optimization and matrix theory, our analysis reveals that the local sensitivity of dot-product self-attention to $\\ell_2$ perturbations can actually be controlled by several key quantities associated with the attention weight matrices and the unperturbed input. We empirically validate our theoretical findings through several examples, offering new insights for achieving low sensitivity in dot-product self-attention against $\\ell_2$ input perturbations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vmSEVL19f": {
    "title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards",
    "volume": "review",
    "abstract": "We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of reward functions, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=NgaLU2fP5D": {
    "title": "Predictive, scalable and interpretable knowledge tracing on structured domains",
    "volume": "review",
    "abstract": "Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (\"knowledge tracing\"; KT), and the prerequisite structure of the learning domain (\"knowledge mapping\"). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and interaction data. Evaluated on three datasets from online learning platforms, PSI-KT achieves superior multi-step **p**redictive accuracy and **s**calable inference in continual-learning settings, all while providing **i**nterpretable representations of learner-specific traits and the prerequisite structure of knowledge that causally supports learning. In sum, predictive, scalable and interpretable knowledge tracing with solid knowledge mapping lays a key foundation for effective personalized learning to make education accessible to a broad, global audience",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=eoTCKKOgIs": {
    "title": "Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift",
    "volume": "review",
    "abstract": "A key challenge of modern machine learning systems is to achieve Out-of-Distribution (OOD) generalization --- generalizing to target data whose distribution differs from those of source data. Despite its significant importance, the fundamental question of ``what are the most effective algorithms for OOD generalization'' remains open even under the standard setting of covariate shift. This paper addresses this fundamental question by proving that, surprisingly, classical Maximum Likelihood Estimation (MLE) purely using source data (without any modification) achieves the *minimax* optimality for covariate shift under the *well-specified* setting. This result holds for a very large class of parametric models, including but not limited to linear regression, logistic regression, and phase retrieval, and does not require any boundedness condition on the density ratio. This paper further complement the study by proving that for the *misspecified setting*, MLE can perform poorly, and the Maximum Weighted Likelihood Estimator (MWLE) emerges as minimax optimal in specific scenarios, outperforming MLE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=UTLv72uDlS": {
    "title": "Scaling Safe Learning-based Control to Long-Horizon Temporal Tasks",
    "volume": "review",
    "abstract": "This paper introduces a model-based approach for training parameterized policies for an autonomous agent operating in a highly nonlinear (albeit deterministic) environment. We desire the trained policy to ensure that the agent satisfies specific task objectives and safety constraints, both expressed in Signal Temporal Logic. We show that this learning problem reduces to the problem of training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agent's task objectives. This poses a challenge: RNNs are susceptible to vanishing and exploding gradients, and naive gradient descent-based strategies to solve long-horizon task objectives thus suffer from the same problems. To tackle this challenge, we introduce a novel gradient approximation algorithm based on the idea of gradient sampling, and a smooth computation graph that provides a neurosymblic encoding of STL formulas. We show that these two methods combined improve the quality of the stochastic gradient, enabling scalable backpropagation over long time horizon trajectories. We demonstrate the efficacy of our approach on various motion planning applications requiring complex spatio-temporal and sequential tasks ranging over thousands of time steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pAVJKp3Dvn": {
    "title": "Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks",
    "volume": "review",
    "abstract": "This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. Finally, we introduce an effective initialization method for the proposed scheme. Our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=B4XM9nQ8Ns": {
    "title": "HyperSINDy: Deep Generative Modeling of Nonlinear Stochastic Governing Equations",
    "volume": "review",
    "abstract": "The discovery of governing differential equations from data is an open frontier in machine learning. The {\\em sparse identification of nonlinear dynamics} (SINDy) \\citep{brunton_discovering_2016} framework enables data-driven discovery of interpretable models in the form of sparse, deterministic governing laws. Recent works have sought to adapt this approach to the stochastic setting, though these adaptations are severely hampered by the curse of dimensionality. On the other hand, Bayesian-inspired deep learning methods have achieved widespread success in high-dimensional probabilistic modeling via computationally efficient approximate inference techniques, suggesting the use of these techniques for efficient stochastic equation discovery. Here, we introduce {\\em HyperSINDy}, a framework for modeling stochastic dynamics via a deep generative model of sparse, nonlinear governing equations whose parametric form is discovered from data. HyperSINDy employs a variational encoder to approximate the distribution of observed states and derivatives. A hypernetwork \\citep{ha_hypernetworks_2016} transforms samples from this distribution into the coefficients of a differential equation whose sparse form is learned simultaneously using a trainable binary mask \\citep{louizos_learning_2018}. Once trained, HyperSINDy generates stochastic dynamics via a differential equation whose coefficients are driven by a Wiener process. In experiments HyperSINDy accurately recovers ground truth stochastic governing equations, with stochasticity scaled to match that of the data. Finally, HyperSINDy provides uncertainty quantification that scales to high-dimensional systems, retaining computational efficiency and interpretability. Taken together, HyperSINDy offers a promising framework for model discovery and uncertainty quantification in real-world systems, integrating sparse equation discovery methods with advances in statistical machine learning and deep generative modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=IoDhZOgteu": {
    "title": "DEXR: A Unified Approach Towards Environment Agnostic Exploration",
    "volume": "review",
    "abstract": "The exploration-exploitation dilemma poses pivotal challenges in reinforcement learning (RL). While recent advances in curiosity-driven techniques have demonstrated capabilities in sparse reward scenarios, they necessitate extensive hyperparameter tuning on different types of environments and often fall short in dense reward settings. In response to these challenges, we introduce the novel \\textbf{D}elayed \\textbf{EX}ploration \\textbf{R}einforcement Learning (DEXR) framework. DEXR adeptly curbs over-exploration and optimization instabilities issues of curiosity-driven methods, and can efficiently adapt to both dense and sparse reward environments with minimal hyperparameter tuning. This is facilitated by an auxiliary exploitation-only policy that streamlines data collection, guiding the exploration policy towards high-value regions and minimizing unnecessary exploration. Additionally, this exploration policy yields diverse, in-distribution data, and bolsters training robustness with neural network structures. We verify the efficacy of DEXR with both theoretical validations and comprehensive empirical evaluations, demonstrating its superiority in a broad range of environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=W2tCmRrj7H": {
    "title": "A Flexible Generative Model for Heterogeneous Tabular EHR with Missing Modality",
    "volume": "review",
    "abstract": "Realistic synthetic electronic health records (EHRs) can be leveraged to acceler- ate methodological developments for research purposes while mitigating privacy concerns associated with data sharing. However, the training of Generative Ad- versarial Networks remains challenging, often resulting in issues like mode col- lapse. While diffusion models have demonstrated progress in generating qual- ity synthetic samples for tabular EHRs given ample denoising steps, their perfor- mance wanes when confronted with missing modalities in heterogeneous tabular EHRs data. For example, some EHRs contain solely static measurements, and some contain only contain temporal measurements, or a blend of both data types. To bridge this gap, we introduce FLEXGEN-EHR– a versatile diffusion model tai- lored for heterogeneous tabular EHRs, equipped with the capability of handling missing modalities in an integrative learning framework. We define an optimal transport module to align and accentuate the common feature space of hetero- geneity of EHRs. We empirically show that our model consistently outperforms existing state-of-the-art synthetic EHR generation methods both in fidelity by up to 3.10% and utility by up to 7.16%. Additionally, we show that our method can be successfully used in privacy-sensitive settings, where the original patient-level data cannot be shared",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=VyMW4YZfw7": {
    "title": "Simplifying GNN Performance with Low Rank Kernel Models",
    "volume": "review",
    "abstract": "We revisit recent spectral GNN approaches to semi-supervised node classification (SSNC). We posit that many of the current GNN architectures may be over-engineered. Instead, simpler, traditional methods from nonparametric estimation, applied in the spectral domain, could replace many deep-learning inspired GNN designs. These conventional techniques appear to be well suited for a variety of graph types reaching state-of-the-art performance on many the common SSNC benchmarks. Additionally, we show that recent performance improvements in GNN approaches may be partialy attributed to shifts in evaluation conventions. Lastly, an ablative study is conducted on the various hyperparameters associated with GNN spectral filtering techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=79rfgv3jw4": {
    "title": "Designing Skill-Compatible AI: Methodologies and Frameworks in Chess",
    "volume": "review",
    "abstract": "Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power. For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts. We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities. Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents. We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners. On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance. Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=d5DGVHMdsC": {
    "title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
    "volume": "review",
    "abstract": "Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time, beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory, centered on causal abstractions (rather than general ''helpful hints''), that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Jos5c7vJPP": {
    "title": "Exchangeable Dataset Amortization for Bayesian Posterior Inference",
    "volume": "review",
    "abstract": "Bayesian inference is a natural approach to reasoning about uncertainty. Unfortunately, in practice it generally requires expensive iterative methods like MCMC to approximate posterior distributions. Not only are these methods computationally expensive, they must be re-run when new observations are available, making them impractical or of limited use in many contexts. In this work, we amortize the posterior parameter inference for probabilistic models by leveraging permutation invariant, set-based network architectures which respect the inherent exchangeability of independent observations of a dataset. Such networks take a set of observations explicitly as input to predict the posterior with a single forward pass and allow the model to generalize to datasets of different cardinality and different orderings. Our experiments explore the effectiveness of this approach for both posterior estimation directly as well as model predictive performance. They show that our approach is comparable to dataset-specific procedures like Maximum Likelihood estimation and MCMC on a range of probabilistic models. Our proposed approach uses a reverse KL-based training objective which does not require the availability of ground truth parameter values during training. This allows us to train the amortization networks more generally. We compare this approach to existing forward KL-based training methods and show substantially improved generalization performance. Finally, we also compare various architectural elements, including different set-based architectures (DeepSets vs Transformers) and distributional parameterizations (Gaussian vs Normalizing Flows)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=1mOeklnLf4": {
    "title": "FroSSL: Frobenius Norm Minimization for Self-Supervised Learning",
    "volume": "review",
    "abstract": "Self-supervised learning (SSL) is an increasingly popular paradigm for representation learning. Recent methods can be classified as sample-contrastive, dimension-contrastive, or asymmetric network-based, with each family having its own approach to avoiding informational collapse. While dimension-contrastive methods converge to similar solutions as sample-contrastive methods, it can be empirically shown that some methods require more epochs of training to converge. Motivated by closing this divide, we present the objective function FroSSL which is both sample- and dimension-contrastive up to embedding normalization. FroSSL works by minimizing covariance Frobenius norms for avoiding collapse and minimizing mean-squared error for augmentation invariance. We show that FroSSL converges more quickly than a variety of other SSL methods and provide theoretical and empirical support that this faster convergence is due to how FroSSL affects the eigenvalues of the embedding covariance matrices. We also show that FroSSL learns competitive representations on linear probe evaluation when used to train a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RaqZX9LSGA": {
    "title": "Tree Search-Based Policy Optimization under Stochastic Execution Delay",
    "volume": "review",
    "abstract": "The conventional formulation of Markov decision processes (MDPs) assumes that the agent's decisions are promptly executed. However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay which value can even be stochastic. In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation. We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case. Armed with this insight, we devise Delayed EfficientZero, a model-based algorithm that optimizes over the class of Markov policies. Delayed EfficientZero leverages the Monte-Carlo tree search of its non-delayed variant EfficientZero to accurately infer future states from the action queue. Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero. Through empirical analysis, we demonstrate that our algorithm surpasses all benchmark methods in Atari games when dealing with both constant and stochastic delays",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=lJYAkDVnRU": {
    "title": "Context-Aware Meta-Learning",
    "volume": "review",
    "abstract": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach---without meta-training or fine-tuning---exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oWKPZ1Hcsm": {
    "title": "Efficient Offline Reinforcement Learning: The Critic is Critical",
    "volume": "review",
    "abstract": "Recent work has demonstrated both benefits and limitations from using supervised approaches (without temporal-difference learning) for offline reinforcement learning. While off-policy reinforcement learning provides a promising approach for improving performance beyond supervised approaches, we observe that training is often inefficient and unstable due to temporal difference bootstrapping. In this paper we propose a best-of-both approach by first learning the behaviour policy and critic with supervised learning, before improving with off-policy reinforcement learning. Crucially, we demonstrate that the critic can be learned by pre-training with a supervised Monte-Carlo value-error, making use of commonly neglected downstream information from the provided offline trajectories. This provides consistent initial values for efficient improvement with temporal difference learning. We further generalise our approach to entropy-regularised reinforcement learning and apply our proposed pre-training to state-of-the-art hard and soft off-policy algorithms. We find that we are able to more than halve the training time of the considered offline algorithms on standard benchmarks, and surprisingly also achieve greater stability. We further build on our insight into the importance of having consistent policy and value functions to propose novel hybrid algorithms that regularise both the actor and the critic towards the behaviour policy. This maintains the benefits of pre-training when learning from limited human demonstrations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ZUKLCxwBo": {
    "title": "A simple and interpretable model of grokking modular arithmetic tasks",
    "volume": "review",
    "abstract": "We present a simple neural network that can generalize on various modular arithmetic tasks such as modular addition or multiplication, and exhibits a sudden jump in generalization known as \\emph{grokking}. Concretely, we present (i) fully-connected two-layer networks that exhibit grokking on various modular arithmetic tasks under vanilla gradient descent with the MSE loss function in the absence of any regularization; (ii) evidence that grokking modular arithmetic corresponds to learning specific representations whose structure is determined by the task; (iii) \\emph{analytic} expressions for the weights -- and thus for the embedding -- that solve a large class of modular arithmetic tasks; and (iv) evidence that these representations are also found by gradient descent as well as AdamW, establishing complete (\"mechanistic\") interpretability of the representations learnt by the network",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOm1RYdHHu": {
    "title": "SAFHE: Defending Against Backdoor and Gradient Inversion Attacks in Federated Learning",
    "volume": "review",
    "abstract": "Federated learning (FL) is an increasingly popular approach in machine learning that enables a set of clients to jointly train a global model without ever sharing their private data, using a central server to aggregate clients' local weight updates. However, previous work has shown that the distributed nature of federated learning makes it susceptible to two major attacks: backdoor attacks, where malicious clients submit large weights that incorrectly change model behavior, and gradient inversion attacks, where a malicious eavesdropper is able to reconstruct the clients' training data by viewing the weight updates sent by clients to the central server. Although various solutions have been proposed in the literature that defend against these two attacks separately, present approaches remain largely incompatible, creating a trade-off between defending against the two types of attacks. This poses a major challenge in deploying FL in privacy-sensitive ML applications. We present SAFHE (Secure Aggregation with Fully Homomorphic Encryption), a novel scheme to defend against both backdoor attacks and gradient inversion attacks. Our secure aggregation method combines the use of fully homomorphic encryption (FHE) and the gradient norm clipping defense to defend against large malicious client updates, by pre-weighting client updates using a function that can be evaluated in the encrypted domain. This allows the server to reject large-magnitude updates without seeing their cleartext values. We demonstrate that Chebyshev approximations of a product of sigmoids work for this purpose, and perform simulations suggesting that such a scheme can defend against backdoor attacks without significantly impacting model accuracy. Additionally, we show that these approximations can be accurately and efficiently computed in the encrypted domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=lr806pdNZa": {
    "title": "LLM Censorship: The Problem and its Limitations",
    "volume": "review",
    "abstract": "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=o0C2v4xTdS": {
    "title": "CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation",
    "volume": "review",
    "abstract": "Molecular conformer generation (MCG) is an important task in cheminformatics and drug discovery. The ability to efficiently generate low-energy 3D structures can avoid expensive quantum mechanical simulations, leading to accelerated virtual screenings and enhanced structural exploration. Several generative models have been developed for MCG, but many struggle to consistently produce high-quality conformers. To address these issues, we introduce CoarsenConf, which coarse-grains molecular graphs based on torsional angles and integrates them into an SE(3)-equivariant hierarchical variational autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained atomic coordinates of subgraphs connected via rotatable bonds, creating a variable-length coarse-grained latent representation. Our model uses a novel aggregated attention mechanism to restore fine-grained coordinates from the coarse-grained latent representation, enabling efficient generation of accurate conformers. Furthermore, we evaluate the chemical and biochemical quality of our generated conformers on multiple downstream applications, including property prediction and oracle-based protein docking. Overall, CoarsenConf generates more accurate conformer ensembles compared to prior generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4QNyUqBIu": {
    "title": "Graph Neural Modeling of Network Flows",
    "volume": "review",
    "abstract": "Network flow problems, which involve distributing traffic such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the general Multi-Commodity Network Flow (MCNF) problem concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures whose global message function constrains the routing unnecessarily. We also find that an MLP is competitive with other standard architectures. Furthermore, we analyze the relationship between graph structure and predictive performance for data-driven routing of flows, an aspect that has not been considered by existing work in the area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9GviaQcGnx": {
    "title": "Constrained Parameter Regularization",
    "volume": "review",
    "abstract": "In this work, we present constrained parameter regularization (CPR), an alternative to traditional weight decay. Instead of applying a constant penalty uniformly to all parameters, we enforce an upper bound on a statistical measure (e.g., the L2-norm) of parameter groups. Consequently, learning becomes a constraint optimization problem, which we address by an adaptation of the augmented Lagrangian method. This formulation permits varying regularization strengths for each parameter group, eliminating the need for explicit penalty coefficients for regularization terms. CPR only requires two hyperparameters and incurs no measurable runtime overhead. Additionally, we propose a simple but efficient mechanism to adapt the upper bounds during the optimization. We provide empirical evidence of CPR's efficacy in experiments on the ``grokking'' phenomenon, computer vision, and language modeling tasks. Our results demonstrate that CPR counteracts the effects of grokking and consistently matches or outperforms traditional weight decay",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=KJYIgEteHX": {
    "title": "Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse Training Data",
    "volume": "review",
    "abstract": "Deep learning based methods for image reconstruction are state-of-the-art for a variety of imaging tasks. However, neural networks often perform worse if the training data differs significantly from the data they are applied to. For example, a network trained for accelerated magnetic resonance imaging (MRI) on one scanner performs worse on another scanner. In this work, we investigate the impact of the training data on the model's performance and robustness. We find that models trained on the combination of various data distributions, such as those obtained from different MRI scanners and anatomies, exhibit robustness equal or superior to models trained on the best single distribution for a specific distributions shift. Thus training on diverse data tends to improve robustness. Furthermore, training on diverse data does not compromise in-distribution performance, i.e., a model trained on diverse data yields in-distribution performance at least as good as models trained on the more narrow individual distributions. Our results suggest that training a model for imaging on a variety of distributions tends to yield a more effective and robust model than maintaining separate models for individual distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=p5SurcLh24": {
    "title": "Unifying Model-Based and Model-Free Reinforcement Learning with Equivalent Policy Sets",
    "volume": "review",
    "abstract": "Model-based and model-free reinforcement learning (RL) each possess relative strengths that prevent either algorithm from strictly dominating the other. Model-based RL often offers greater data efficiency, as it can use models to evaluate many possible behaviors before choosing one to enact. However, because models cannot perfectly represent complex environments, agents that rely too heavily on models may suffer from poor asymptotic performance. Model-free RL avoids this problem at the expense of data efficiency. In this work, we seek a unified approach to RL that combines the strengths of both algorithms. To this end, we propose *equivalent policy sets* (EPS), a novel tool for quantifying the limitations of models for the purposes of decision making. Based on this concept, we propose *Unified RL*, a novel RL algorithm that uses models to constrain model-free RL to the set of policies that are not provably suboptimal, according to model-based bounds on policy performance. We demonstrate across a range of benchmarks that Unified RL effectively combines the relative strengths of both model-based and model-free RL, in that it achieves comparable data efficiency to model-based RL and exceeds the data efficiency of model-free RL, while achieving asymptotic performance similar or superior to that of model-free RL. Additionally, we show that Unified RL outperforms a number of existing state-of-the-art model-based and model-free RL algorithms, and can learn effective policies in situations where either model-free or model-based RL alone fail",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=gDDW5zMKFe": {
    "title": "FIITED: Fine-grained embedding dimension optimization during training for recommender systems",
    "volume": "review",
    "abstract": "Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference. Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED). Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. A novel embedding storage system based on virtually hashed physically indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method. On public click-through rate prediction datasets, FIITED is able to prune up to 93.75%-99.75% embeddings without significant accuracy loss. Given the same embedding size reduction, FIITED is able to achieve better model quality than the baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=caW7LdAALh": {
    "title": "Beyond Accuracy: Evaluating Self-Consistency of Code LLMs",
    "volume": "review",
    "abstract": "Code Large Language Models (LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the general accuracy of Code LLMs on individual tasks has been substantially evaluated and improved, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating documentation for its own code and generating code for its own natural language specifications. Failure to preserve self-consistency reveals a model's lack of understanding of the shared semantics underlying natural language and programming language and therefore undermines its trustworthiness. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which can evaluate a model's self-consistency and general accuracy at the same time. We study eleven Code LLMs and show that their self-consistency is indeed a concerning aspect, distinct from general accuracy, which should be highlighted in the evaluation and improved in the training of Code LLMs in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=uNl1UsUUX2": {
    "title": "Improving Generalization for Small Datasets with Data-Aware Dynamic Reinitialization",
    "volume": "review",
    "abstract": "The efficacy of deep learning techniques is contingent upon copious volumes of data (labeled or unlabeled). Nevertheless, access to such data is frequently restricted in practical domains such as medical applications. This presents a formidable obstacle: How can we effectively train a deep neural network on a relatively small dataset while improving generalization? Recent works explored evolutionary or iterative training paradigms, which reinitialize a subset of the parameters to improve generalization performance for small datasets. While effective, these methods randomly select the subset of parameters and maintain a fixed mask throughout iterative training, which can be suboptimal. Motivated by the process of neurogenesis in the brain, we propose a novel iterative training framework, Selective Knowledge Evolution (SKE), that employs a data-aware dynamic masking scheme to eliminate redundant connections by estimating their significance, thereby increasing the model's capacity for further learning via random weight reinitialization. The experimental results demonstrate that our approach outperforms existing methods in accuracy and robustness, highlighting its potential for real-world applications where collecting data is challenging",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=fjpfCOV4ru": {
    "title": "Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting",
    "volume": "review",
    "abstract": "This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, the drift and diffusion coefficient in our chain can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove the bound in $\\mathcal{W}_2$-distance between the laws of our Ito chain and the corresponding differential equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=vngVydDWft": {
    "title": "From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication",
    "volume": "review",
    "abstract": "It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction tasks, observing consistent latent similarity and downstream performance improvements in a zero-shot stitching setting. The experimental analysis comprises three modalities (vision, text, and graphs), twelve pretrained foundational models, eight benchmarks, and several architectures trained from scratch",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFKIfhvdmZ": {
    "title": "Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning",
    "volume": "review",
    "abstract": "Training generally capable agents that thoroughly explore their environment and learn new and diverse skills is a long-term goal of robot learning. Quality Diversity Reinforcement Learning (QD-RL) is an emerging research area that blends the best aspects of both fields – Quality Diversity (QD) provides a principled form of exploration and produces collections of behaviorally diverse agents, while Reinforcement Learning (RL) provides a powerful performance improvement operator enabling generalization across tasks and dynamic environments. Existing QD-RL approaches have been constrained to sample efficient, deterministic off- policy RL algorithms and/or evolution strategies and struggle with highly stochastic environments. In this work, we, for the first time, adapt on-policy RL, specifically Proximal Policy Optimization (PPO), to the Differentiable Quality Diversity (DQD) framework and propose several changes that enable efficient optimization and discovery of novel skills on high-dimensional, stochastic robotics tasks. Our new algorithm, Proximal Policy Gradient Arborescence (PPGA), achieves state-of- the-art results, including a 4x improvement in best reward over baselines on the challenging humanoid domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgDDyoWQt3": {
    "title": "Feasibility with Language Models for Open-World Compositional Zero-Shot Learning",
    "volume": "review",
    "abstract": "Humans can easily tell if an attribute (also called state) is realistic, i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In Open-World Compositional Zero-Shot Learning, when all possible state-object combinations are considered as unseen classes, zero-shot predictors tend to perform poorly. Our work focuses on using external auxiliary knowledge to determine the feasibility of state-object combinations. Our Feasibility with Language Model (FLM) is a simple and effective approach that leverages Large Language Models (LLMs) to better comprehend the semantic relationships between states and objects. FLM involves querying an LLM about the feasibility of a given pair and retrieving the output logit for the positive answer. To mitigate potential misguidance of the LLM given that many of the state-object compositions are rare or completely infeasible, we observe that significant work needs to go into exploiting the in-context learning ability of LLMs. We present an extensive study on many prompt variants and involving six LLMs, including two LLMs with open access to the logit values, identifying Vicuna and ChatGPT as best performing, and we demonstrate that our FLM consistently improves OW-CZSL performance across all three benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=o2IEmeLL9r": {
    "title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning",
    "volume": "review",
    "abstract": "Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior regularization. PTGM involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent RL tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in RL, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate PTGM's superiority in sample efficiency and task performance compared to baselines. Moreover, PTGM exemplifies enhanced interpretability and generalization of the acquired low-level skills",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=MrR3rMxqqv": {
    "title": "Memorization Capacity of Multi-Head Attention in Transformers",
    "volume": "review",
    "abstract": "Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can memorize, as a function of the number of heads and sequence length. Motivated by experimental findings on vision transformers, we introduce novel assumptions about the linear independence of input data, distinct from the commonly used general-position assumption. Under these assumptions, we demonstrate that an attention layer with $H$ heads, dimension $d$, and context size $n < d,$ featuring $\\Theta(Hd^2)$ parameters, can memorize $\\Omega(Hn)$ examples. Our analysis sheds light on how different attention heads handle various example sequences, aided by the softmax operator's saturation property. We validate our findings through experiments on synthetic data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rjLgCkJH79": {
    "title": "A SIMILARITY-AGNOSTIC REINFORCEMENT LEARNING APPROACH FOR LEAD OPTIMIZATION",
    "volume": "review",
    "abstract": "Lead optimization in drug discovery is a pivotal phase in identifying promising drug candidates for further development. Traditionally, lead optimization in the machine learning community has been treated as a constraint optimization problem where methods like generative models and reinforcement learning(RL) have been widely employed. However, these methods often rely on molecular similarity metrics to define constraints, which poses significant challenges due to the inherently ambiguous nature of molecular similarity. In this work, we present a similarity-agnostic approach to lead optimization, which we term \"Lead Optimization using Goal-conditioned Reinforcement Learning\" or LOGRL. Contrary to conventional methods, LOGRL is uniquely trained on a distinct task: source-to-target path prediction. This allows LOGRL to produce molecules with significantly higher Tanimoto similarity to target molecules, even without direct exposure to this metric during training. Furthermore, we incorporate a beam search strategy during the molecule generation process. This strategy empowers us to generate a substantial number of candidate molecules, facilitating further curation to meet desired properties. Notably, our unique approach permits us to leverage the Euclidean distance between learned action representations as a surrogate for molecular similarity during beam search",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=W3VsHuga3j": {
    "title": "Modeling Boundedly Rational Agents with Latent Inference Budgets",
    "volume": "review",
    "abstract": "We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents' goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=rFZtyj5kBz": {
    "title": "Certifiably Byzantine-Robust Federated Conformal Prediction",
    "volume": "review",
    "abstract": "Conformal prediction has shown impressive capacity in constructing statistically rigorous prediction sets for machine learning models with exchangeable data samples. The burgeoning amount of large-scale data, coupled with the escalating privacy concerns related to local data sharing, has inspired recent innovations extending conformal prediction into federated environments with distributed data samples. However, this framework for distributed uncertainty quantification is susceptible to Byzantine failures. A minor subset of malicious clients can significantly compromise the practicality of coverage guarantees. To address this vulnerability, we introduce a novel algorithm Rob-FCP to execute robust federated conformal prediction, effectively countering malicious clients capable of reporting arbitrary statistics during the conformal calibration process. We theoretically provide the conformal coverage bound of Rob-FCP in the Byzantine setting and show that the coverage of Rob-FCP is asymptotically close to the desired coverage level under mild conditions in both IID and non-IID settings. We also propose a malicious client number estimator to tackle a more challenging setting where the number of malicious clients is unknown to the defender and theoretically show its effectiveness. We empirically demonstrate the robustness of Rob-FCP against diverse proportions of malicious clients under a variety of Byzantine attacks on five realistic benchmark and healthcare datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=zkE2js9qRe": {
    "title": "Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors",
    "volume": "review",
    "abstract": "For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include, Order embedding, Poincar\\'e embedding on hyperbolic space, and Box embedding. Each of the above approaches suffers from some significant weaknesses. Order embedding fails to capture full spectrum of logical operations (such as, inverse, union) on their embedding vectors, which are essential for deducing complementary or aggregated concepts. Box embedding overcomes this limitation by making the representation richer, but along the process it sacrifices simplicity requiring custom-made optimization scheme for learning the representation. Poincar\\'e embedding improves embedding quality by exploiting the ever-expanding property of hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not easy in the hyperbolic space. In this work, we propose BINDER, a novel approach for order-based representation. BINDER uses binary bits as representation vectors. BINDER uses a simple, yet efficient algorithm for learning representation vectors in a fraction of time in comparison to existing order-based representation learning methods. Our experimental results show that BINDER is very accurate, yielding better results than the existing state-of-the-art methods for both prediction and reconstruction tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.6,
    "authors": []
  },
  "https://openreview.net/forum?id=MEGQGNUfPx": {
    "title": "The Effectiveness of Random Forgetting for Robust Generalization",
    "volume": "review",
    "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \"Forget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between the standard and robust accuracy outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=gxhRR8vUQb": {
    "title": "Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction",
    "volume": "review",
    "abstract": "Mesh deformation plays a pivotal role in many 3D vision tasks including dynamic simulations, rendering, and reconstruction. However, defining an efficient discrepancy between predicted and target meshes remains an open problem. A prevalent approach in current deep learning is the set-based approach which measures the discrepancy between two surfaces by comparing two randomly sampled point-clouds from the two meshes with Chamfer pseudo-distance. Nevertheless, the set-based approach still has limitations such as lacking a theoretical guarantee for choosing the number of points in sampled point-clouds, and the pseudo-metricity and the quadratic complexity of the Chamfer divergence. To address these issues, we propose a novel metric for learning mesh deformation. The metric is defined by sliced Wasserstein distance on meshes represented as probability measures that generalize the set-based approach. By leveraging probability measure space, we gain flexibility in encoding meshes using diverse forms of probability measures, such as continuous, empirical, and discrete measures via \\textit{varifold} representation. After having encoded probability measures, we can compare meshes by using the sliced Wasserstein distance which is an effective optimal transport distance with linear computational complexity and can provide a fast statistical rate for approximating the surface of meshes. To the end, we employ a neural ordinary differential equation (ODE) to deform the input surface into the target shape by modeling the trajectories of the points on the surface. Our experiments on cortical surface reconstruction demonstrate that our approach surpasses other competing methods in multiple datasets and metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=p34fRKp8qA": {
    "title": "Lie Group Decompositions for Equivariant Neural Networks",
    "volume": "review",
    "abstract": "Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \\textnormal{GL}^{+}(n, \\mathbb{R})$ and $G = \\textnormal{SL}(n, \\mathbb{R})$, as well as their representation as affine transformations $\\mathbb{R}^{n} \\rtimes G$. Invariant integration as well as a global parametrization is realized by decomposing the \"larger\" groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ByW9j60mvV": {
    "title": "RL Algorithms are Information-State Policies in the Bayes-Adaptive MDP",
    "volume": "review",
    "abstract": "RL studies the challenge of maximizing reward in unknown environments; the Bayes-Adaptive MDP (BAMDP) provides a formal specification of this problem, albeit one that may be intractable to solve directly. In this paper, rather than trying to solve the BAMDP, we use it as a theoretical resource. In particular, we view RL algorithms as *hand-written information-state policies* for the BAMDP and derive a number of insights from this approach. For instance, one simple observation from bandit theory is that optimal policies for the BAMDP, i.e., ideal RL algorithms, do not necessarily converge to optimal policies for the underlying MDP---even though RL theory has typically regarded the latter property as essential. We also apply the theory of potential-based reward shaping in the BAMDP to analyze valid forms of intrinsic motivation. We then show that BAMDP Q-values can be decomposed into separate measures of the value gained from exploration and exploitation. We finally derive a direct relationship between an RL algorithm's shaping function in the MDP and its suboptimality in the BAMDP, and use these results to clarify the roles of many forms of reward shaping",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=QiJuMJl0QS": {
    "title": "Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation",
    "volume": "review",
    "abstract": "We tackle the problem of meta-learning across heterogenous tasks. This problem seeks to extract and generalize transferable meta-knowledge through streaming task sets from a multi-modal task distribution. The extracted meta-knowledge can be used to create predictors for new tasks using a small number of labeled samples. Most meta-learning methods assume a homogeneous task distribution, thus limiting their generalization capacity when handling multi-modal task distributions. Recent work has shown that the generalization of meta-learning depends on the similarity of tasks in the training distribution, and this has led to many clustering approaches that aim to detect homogeneous clusters of tasks. However, these methods suffer from a significant increase in parameter complexity. To overcome this weakness, we propose a new heterogeneous meta-learning strategy that efficiently captures the multi-modality of the task distribution via modulating the routing between convolution channels in the network, instead of directly modulating the network weights. This new mechanism can be cast as a permutation learning problem. We further introduce a novel neural permutation layer based on the classical Benes routing network, which has sub-quadratic parameter complexity in the total number of channels, as compared to the quadratic complexity of the state-of-the-art Gumbel-Sinkhorn layer. We demonstrate our approach on various multi-modal meta-learning benchmarks, showing that our framework outperforms previous methods in both generalization accuracy and convergence speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHjE5v5MB7": {
    "title": "To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets",
    "volume": "review",
    "abstract": "Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider two-layer neural networks trained on modular arithmetic tasks where ($\\\\xi \\\\cdot 100\\\\%$) of labels are corrupted (*i.e.* some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels *and* achieve $100\\\\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve $100\\\\%$ accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (\"mechanistically\") interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes the *grokking* dynamics reaching high train *and* test accuracy; second, it unlearns the memorizing representations, where train accuracy suddenly jumps from $100\\\\%$ to $100 (1-\\\\xi)\\\\%$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFVQaqkf8Z": {
    "title": "Revisiting Non-separable Binary Classification and its Applications in Anomaly Detection",
    "volume": "review",
    "abstract": "The inability to linearly classify $\\texttt{XOR}$ has motivated much of deep learning. We revisit this age-old problem and show that $\\textit{linear}$ classification of $\\texttt{XOR}$ is indeed possible. Instead of separating data between halfspaces, we propose a slightly different paradigm, $\\texttt{equality separation}$, that adapts the SVM objective to distinguish data within or outside the margin. Our classifier can then be integrated into neural network pipelines with a smooth approximation. From its properties, we intuit that equality separation is suitable for anomaly detection. To formalize this notion, we introduce $\\textit{closing numbers}$, a quantitative measure on the capacity for classifiers to form closed decision regions for anomaly detection. Springboarding from this theoretical connection between binary classification and anomaly detection, we test our hypothesis on supervised anomaly detection experiments, showing that equality separation can detect both seen and unseen anomalies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=HmKav4WZ9w": {
    "title": "Basis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy",
    "volume": "review",
    "abstract": "Factorization machine (FM) variants are widely used for large scale real-time content recommendation systems, since they offer an excellent balance between model accuracy and low computational costs for training and inference. These systems are trained on tabular data with both numerical and categorical columns. Incorporating numerical columns poses a challenge, and they are typically incorporated using a scalar transformation or binning, which can be either learned or chosen a-priori. In this work, we provide a systematic and theoretically-justified way to incorporate numerical features into FM variants by encoding them into a vector of function values for a set of functions of one's choice. We view factorization machines as approximators of *segmentized* functions, namely, functions from a field's value to the real numbers, assuming the remaining fields are assigned some given constants, which we refer to as the segment. From this perspective, we show that our technique yields a model that learns segmentized functions of the numerical feature spanned by the set of functions of one's choice, namely, the spanning coefficients vary between segments. Hence, to improve model accuracy we advocate the use of functions known to have strong approximation power, and offer the B-Spline basis due to its well-known approximation power, availability in software libraries, and efficiency. Our technique preserves fast training and inference, and requires only a small modification of the computational graph of an FM model. Therefore, it is easy to incorporate into an existing system to improve its performance. Finally, we back our claims with a set of experiments that include a synthetic experiment, performance evaluation on several data-sets, and an A/B test on a real online advertising system which shows improved performance. The results can be reproduced with the code in the supplemental material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=voLFfrWzFI": {
    "title": "Task Generalization in Decision-Focused Learning",
    "volume": "review",
    "abstract": "Real-world optimization problems often contain uncertain parameters that must be predicted prior to solving. For example, a delivery company must make its routing decisions when the traffic conditions, and thus the road traversal times, are uncertain. The models used to predict these uncertain quantities are commonly trained in a way that is agnostic of the optimization problem and that focuses solely on predictive accuracy. However, such a prediction-focused training procedure generally does not minimize the downstream task loss of interest (e.g., the suboptimality of the roads that are selected based on the predictions). This has led to the development of decision-focused learning (DFL) methods, which specifically train the predictive model to make predictions that lead to good decisions on the considered optimization task. However, as we show in this paper, such models often generalize poorly to altered optimization tasks. For example, in the context of a routing problem, their performance may deteriorate when the destination node changes. To improve on this, we first explore how the model can be trained to generalize implicitly, by simply training it on different tasks sampled at training time. We then propose a more sophisticated approach by adding the use of explicit task representations, to enable the model to adapt its predictions better to different tasks. To this end, we represent the optimization problems as bipartite variable-constraint graphs, and train graph neural networks (GNNs) to produce informative node embeddings that are then given to the predictive model. In our experiments, we start by showing that the state of the art in DFL tends to overfit to the specific task it is trained on, and generalizes poorly to changing tasks. We then show that both of our proposed strategies significantly improve on this, with the explicit task representations generally providing an additional improvement over the implicit strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=wrqAn3AJA1": {
    "title": "Everybody Needs a Little HELP: Explaining Graphs via Hierarchical Concepts",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ONhwvkaIe6": {
    "title": "Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy",
    "volume": "review",
    "abstract": "Text-to-image synthesis has recently attracted widespread attention of the community due to rapidly improving generation quality and numerous practical applications. However, little is known about the language understanding capabilities of text-to-image models, making it difficult to reason about prompt formulations that the model would understand well. In this work, we measure the capability of popular text-to-image models to understand *hypernymy*, or the ``is-a\" relation between words. To this end, we design two automatic metrics based on the WordNet semantic hierarchy and existing image classifiers pretrained on ImageNet. These metrics both enable quantitative comparison of linguistic capabilities for text-to-image models and offer a way of finding qualitative differences, such as words that are unknown to models and thus are difficult for them to draw. We comprehensively evaluate our metrics on various popular text-to-image generation models, including GLIDE, Latent Diffusion, and Stable Diffusion, which allows a better understanding of their shortcomings for downstream applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ntUmktUfZg": {
    "title": "Generate to Discriminate: Expert Routing for Continual Learning",
    "volume": "review",
    "abstract": "In many real-world settings, norms, regulations, or economic incentives permit the sharing of models but not data across environments. Prominent examples arise in healthcare due to regulatory concerns. In this scenario, the practitioner wishes to adapt the model to each new environment but faces the danger of losing performance on previous environments due to the well-known problem of catastrophic forgetting. In this paper, we propose Generate-to-Discriminate (G2D), a novel approach that leverages recent advancements in generative models to alleviate the catastrophic forgetting problem in continual learning. Unlike previous approaches based on generative models that primarily use synthetic data for training the label classifier, we use synthetic data to train a domain discriminator. Our method involves the following steps: For each domain, (i) fine-tune the classifier and adapt a generative model to the current domain data; (ii) train a domain discriminator to distinguish synthetic samples from past versus current domain data; and (iii) during inference, route samples to the respective classifier. We compare G2D to an alternative approach, where we simply replay the generated synthetic data, and, surprisingly, we find that training a domain discriminator is significantly more effective than augmenting the training data with the same synthetic samples. We consistently outperform previous state-of-the-art domain-incremental learning algorithms by up to $7.6$ and $6.2$ points across three standard domain incremental learning benchmarks in the vision and language modalities, respectively, and $10.0$ points on a challenging real-world dermatology medical imaging task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=bHOcs4PBgR": {
    "title": "Flatter, Faster: Scaling Momentum for Optimal Speedup of SGD",
    "volume": "review",
    "abstract": "Commonly used optimization algorithms often show a trade-off between good generalization and fast training times. For instance, stochastic gradient descent (SGD) tends to have good generalization; however, adaptive gradient methods have superior training times. Momentum can help accelerate training with SGD, but so far there has been no principled way to select the momentum hyperparameter. Here we study training dynamics arising from the interplay between SGD with label noise and momentum in the training of overparametrized neural networks. We find that scaling the momentum hyperparameter $1-\\beta$ with the learning rate to the power of $2/3$ maximally accelerates training, without sacrificing generalization. To analytically derive this result we develop an architecture-independent framework, where the main assumption is the existence of a degenerate manifold of global minimizers, as is natural in overparametrized models. Training dynamics display the emergence of two characteristic timescales that are well-separated for generic values of the hyperparameters. The maximum acceleration of training is reached when these two timescales meet, which in turn determines the scaling limit we propose. Our experiments in matrix-sensing, a 6-layer MLP on FashionMNIST and ResNet-18 on CIFAR10 validate this scaling for the time to convergence, and additionally for the momentum hyperparameter which maximizes generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=SUUrkC3STJ": {
    "title": "VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections",
    "volume": "review",
    "abstract": "Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from complex graph topological information and makes heavy feature engineering offline and independent, such that mini-batch training of graph transformers is possible by loading each node's token list in batches. We further prove this PPR tokenization is viable as a graph convolution network with a fixed polynomial filter and jumping knowledge. However, only using personalized PageRank may limit information carried by a token list, which could not support different graph inductive biases for model training. To this end, (2) we rewire graphs by introducing multiple types of virtual connections through structure- and content-based super nodes that enable PPR tokenization to encode local and global contexts, long-range interaction, and heterophilous information into each node's token list, and then formalize our Virtual Connection Ranking based Graph Transformer (VCR-Graphormer). Overall, VCR-Graphormer only needs $O(m+klogk)$ complexity for graph tokenization as compared to $O(n^{3})$ of previous works. We also show that VCR-Graphormer outperforms the state-of-the-arts on node classification in 12 datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=D4NJFfrqoq": {
    "title": "Optimistic Bayesian Optimization with Unknown Constraints",
    "volume": "review",
    "abstract": "Though some research efforts have been dedicated to constrained Bayesian optimization (BO), there remains a notable absence of a principled approach with a theoretical performance guarantee in the decoupled setting. Such a setting involves independent evaluations of the objective function and constraints at different inputs, and is hence a relaxation of the commonly-studied coupled setting where functions must be evaluated together. As a result, the decoupled setting requires an adaptive selection between evaluating either the objective function or a constraint, in addition to selecting an input (in the coupled setting). This paper presents a novel constrained BO algorithm with a provable performance guarantee that can address the above relaxed setting. Specifically, it considers the fundamental trade-off between exploration and exploitation in constrained BO, and, interestingly, affords a noteworthy connection to active learning. The performance of our proposed algorithms is also empirically evaluated using several synthetic and real-world optimization problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=m7aPLHwsLr": {
    "title": "DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness",
    "volume": "review",
    "abstract": "Machine Learning (ML) models have been utilized for malware detection for over two decades. Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks. However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and/or can defend against only specific attacks, which makes them less practical in real-world settings. In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the *de-randomized smoothing* technique for the domain of malware detection. Specifically, we propose a *window ablation* scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables. After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness. To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables. More surprisingly, through evaluating DRSM against $9$ empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model. In addition, we collected $15.5K$ recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pEGSdJu52I": {
    "title": "Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable",
    "volume": "review",
    "abstract": "Typical neural network trainings have substantial variance in test-set performance between repeated runs, impeding hyperparameter comparison and training reproducibility. We present the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have very little variance in their performance on the test-distributions from which their test-sets are sampled, suggesting that variance is less of a practical issue than previously thought. (2) We present a simplifying statistical assumption which closely approximates the structure of the test-set accuracy distribution. (3) We prove that test-set variance is unavoidable given the observation that ensembles of independently trained networks are well-calibrated. (4) We conduct preliminary studies of distribution-shift, fine-tuning, data augmentation and learning rate through the lens of variance between runs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=VaZa8zj0Yw": {
    "title": "Lyfe Agents: generative agents for low-cost real-time social interactions",
    "volume": "review",
    "abstract": "Highly autonomous generative agents powered by large language models promise to simulate intricate social behaviors in virtual societies. However, achieving real-time interactions with humans at a low computational cost remains challenging. Here, we introduce Lyfe Agents. They combine low-cost with real-time responsiveness, all while remaining intelligent and goal-oriented. Key innovations include: (1) an option-action framework, reducing the cost of high-level decisions; (2) asynchronous self-monitoring for better self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation and sociability across several multi-agent scenarios in our custom LyfeGame 3D virtual environment platform. When equipped with our brain-inspired techniques, Lyfe Agents can exhibit human-like self-motivated social reasoning. For example, the agents can solve a crime (a murder mystery) through autonomous collaboration and information exchange. Meanwhile, our techniques enabled Lyfe Agents to operate at a computational cost 10-100 times lower than existing alternatives. Our findings underscore the transformative potential of autonomous generative agents to enrich human social experiences in virtual worlds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=fpoAYV6Wsk": {
    "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
    "volume": "review",
    "abstract": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in (Wang, 2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to ‘repair' the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=7em7Jl0qMm": {
    "title": "Fourier Ordinary Differential Equations",
    "volume": "review",
    "abstract": "Continuous models such as Neural Ordinary Differential Equations (NODEs) are powerful approaches for modeling time series data, known for their ability to capture underlying dynamics and generalization. Current continuous models focus on learning mappings within finite-dimensional Euclidean spaces, raising two critical questions for enhancing their effectiveness. First, Is Euclidean space the optimal representation for capturing the underlying patterns and features in time series data? Second, how can we maintain granularity while benefiting from the generalization capabilities of continuous models? To address the first question, we propose a novel approach for learning dynamics in the Fourier domain. In contrast to Euclidean space, each point in Fourier space summarizes the original signal at a specific frequency, enabling more comprehensive data representations. Additionally, time differentiation in the Fourier domain simplifies the modeling of dynamics as it becomes a multiplication operation. To answer the second question, we introduce element-wise filtering, a method designed to compensate for the bias of continuous models when fitting discrete data points. These techniques culminate in the introduction of a new approach—Fourier Ordinary Differential Equations (FODEs). Our experiments provide compelling evidence of FODEs' superiority in terms of accuracy, efficiency, and generalization capabilities when compared to existing methods across various time series datasets. By offering a novel method for modeling time series data capable of capturing both short-term and long-term patterns, FODEs have the potential to significantly enhance the modeling and prediction of complex dynamic systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeoEFlF0E": {
    "title": "NEURAL ADDITIVE TENSOR DECOMPOSITION FOR SPARSE TENSORS",
    "volume": "review",
    "abstract": "Canonical Polyadic Decomposition (CPD) is a fundamental technique for tensor analysis, discovering underlying multi-linear structures represented as rank-one tensors (components). The simplicity of the rank-one tensors facilitates the interpretation of hidden structures within tensors compared to other types of conventional tensor decomposition models. However, CPD has limitations in modeling nonlinear structures present in real-world tensors. Recent tensor decomposition models combined with neural networks have shown superior performance in tensor completion tasks compared to multi-linear tensor models. Nevertheless, one drawback of those nonlinear tensor models is the lack of interpretability since their black-box approaches entangle all interactions between latent components, unlike CPD, which handles the components individually as rank-one tensors. To overcome this major limitation and bridge the gap between CPD and various state-of-the-art neural tensor models, we propose Neural Additive Tensor Decomposition (NeAT) to accurately capture non-linear interactions in sparse tensors while respecting the separation of distinct components in a similar vein as CPD. The main idea is to neuralize each component to model non-linear interactions within each component separately. This not only captures non-linear interactions but also makes the decomposition results easy to interpret by being as close to the CPD model as possible. Extensive experiments with six large-scale real-world datasets demonstrate that \\method{} is more accurate than the state-of-the-art neural tensor models and easy to interpret latent patterns. In the link prediction task, NeAT outperforms CPD by 10\\% and the second-best performing neural tensor model by 4\\%, in terms of AUC score. Finally, we demonstrate the interpretability of NeAT by visualizing and analyzing latent components from real data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=OOxotBmGol": {
    "title": "Large Language Models to Enhance Bayesian Optimization",
    "volume": "review",
    "abstract": "Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present $\\texttt{LLAMBO}$, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that $\\texttt{LLAMBO}$ is effective at zero-shot warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate $\\texttt{LLAMBO}$'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=sojpn00o8z": {
    "title": "Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps",
    "volume": "review",
    "abstract": "Cascaded models are multi-scale generative models with a marked capacity for producing perceptually impressive samples at high resolutions. In this work, we show that they can also be excellent likelihood models, so long as we overcome a fundamental difficulty with probabilistic multi-scale models: the intractability of the likelihood function. Chiefly, in cascaded models each intermediary scale introduces extraneous variables that cannot be tractably marginalized out for likelihood evaluation. This issue vanishes by modeling the diffusion process on latent spaces induced by a class of transformations we call hierarchical volume-preserving maps, which decompose spatially structured data in a hierarchical fashion without introducing local distortions in the latent space. Not only do such reparameterizations allow the model likelihood to be directly expressed as a joint likelihood over the scales, they also pave the way for significant improvements to the state-of-the-art on a selection of benchmarks, including density estimation, lossless compression, and out-of-distribution detection. Investigating the theoretical basis of our empirical gains we uncover deep connections to score matching under the Earth Mover's Distance (EMD), which is a well-known surrogate for perceptual similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=55uj7mU7Cv": {
    "title": "Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach",
    "volume": "review",
    "abstract": "Unsupervised domain translation (UDT) is often realized by generative adversarial network (GAN)-based probability distribution matching of the source and target domains. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in recent works that CycleGAN and variants could fail to identify the desired translation function and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions---referred to as ``measure-preserving automorphism\" (MPA)---in the solution space of the learning criteria. Despite the awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability challenge and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are aligned by the learning criterion. Our theory leads to a UDT learner using distribution matching over auxiliary variable-induced subsets of the domains---other than over the entire source/target domains as in the classical setting. The proposed framework is the first to rigorously establish identifiability of the desired translation function for UDT, to our best knowledge. Experiments corroborate with our theoretical claims",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=UhcXE3o1R3": {
    "title": "Apollo: Zero-shot MultiModal Reasoning with Multiple Experts",
    "volume": "review",
    "abstract": "We propose a modular framework that leverages the expertise of different foundation models over different modalities and domains in order to perform a single, complex, multi-modal task, without relying on prompt engineering or otherwise tailor-made multi-modal training. Our approach enables decentralized command execution and allows each model to both contribute and benefit from the expertise of the other models. Our method can be extended to a variety of foundation models (including audio and vision), above and beyond only language models, as it does not depend on prompts. We demonstrate our approach on two tasks. On the well-known task of stylized image captioning, our experiments show that our approach outperforms semi-supervised state-of-the-art models, while being zero-shot and avoiding costly training, data collection, and prompt engineering. We further demonstrate this method on a novel task, audio-aware image captioning, in which an image and audio are given and the task is to generate text that describes the image within the context of the provided audio",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=SqMVI1GFnp": {
    "title": "Lie Neurons: A General Adjoint-Equivariant Neural Network for Semisimple Lie Algebras",
    "volume": "review",
    "abstract": "In this paper, we propose an adjoint-equivariant neural network that takes Lie algebra data as input. Various types of equivariant neural networks have been proposed in the literature, which treat the input data as elements in a vector space carrying certain types of transformations. In comparison, we aim to process inputs that are transformations between vector spaces. The change of basis on transformation is described by conjugations, inducing the adjoint-equivariance relationship that our model is designed to capture. Leveraging the invariance property of the Killing form, the proposed network is a general framework that works for arbitrary semisimple Lie algebras. Our network possesses a simple structure that can be viewed as a Lie algebraic generalization of a multi-layer perceptron (MLP). This work extends the application of equivariant feature learning. As an example, we showcase its value in homography modeling using $\\mathfrak{sl}(3)$ Lie algebra",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RlbFGQYsJr": {
    "title": "Learning Dynamics on Manifolds with Neural Ordinary Differential Equations",
    "volume": "review",
    "abstract": "Neural ordinary differential equations (Neural ODEs) have garnered significant attention for their ability to efficiently learn dynamics from data. However, for high-dimensional systems, capturing dynamics remains to be a challenging task. Existing methods often rely on learning ODEs on low-dimensional manifolds but usually require the knowledge of the manifold. Nevertheless, such knowledge is usually unknown in many scenarios. Therefore, we propose a novel approach to jointly learn data dynamics and the underlying manifold. Specifically, we employ an encoder to project the original data into the manifold and leverage the Jacobian matrix of its corresponding decoder for recovery. Our experimental evaluations encompass multiple datasets, where we compare the accuracy, number of function evaluations (NFE), and convergence speed of our model against existing baselines. Our results demonstrate superior performance, underscoring the effectiveness of our approach in addressing the challenges of high-dimensional dynamic learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yuy6cGt3KL": {
    "title": "Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation",
    "volume": "review",
    "abstract": "We study the problem of model selection in causal inference, specifically for the case of conditional average treatment effect (CATE) estimation under binary treatments. Unlike model selection in machine learning, there is no perfect analogue of cross-validation as we do not observe the counterfactual potential outcome for any data point. Towards this, there have been a variety of proxy metrics proposed in the literature, that depend on auxiliary nuisance models estimated from the observed data (propensity score model, outcome regression model). However, the effectiveness of these metrics has only been studied on synthetic datasets as we can access the counterfactual data for them. We conduct an extensive empirical analysis to judge the performance of these metrics introduced in the literature, and novel ones introduced in this work, where we utilize the latest advances in generative modeling to incorporate multiple realistic datasets. Our analysis suggests novel model selection strategies based on careful hyperparameter tuning of CATE estimators and causal ensembling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=lLhEQWQYtb": {
    "title": "Parameter Estimation of Long Memory Stochastic Processes with Deep Neural Networks",
    "volume": "review",
    "abstract": "We present a pure deep neural network-based approach for estimating long memory parameters of time series models that incorporate the phenomenon of long range dependence. Long memory parameters such as the Hurst exponent are critical in characterizing the long-range dependence, roughness, and self-similarity of stochastic processes. The accurate and fast estimation of these parameters is of paramount importance in various scientific fields, including finance, physics, and engineering. We harnessed efficient process generators to provide high-quality synthetic training data to train 1D Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models. Our neural models outperform conventional statistical methods, even if the latter have neural network extensions. Precision, speed as well as consistency and robustness of the estimators are supported by experiments with fractional Brownian motion (fBm), the Autoregressive Fractionally Integrated Moving Average (ARFIMA) process, and the fractional Ornstein-Uhlenbeck process (fOU). We believe that our work will inspire further research in the application of deep learning techniques for stochastic process modeling and parameter estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=MOviNImhfq": {
    "title": "Effective Graph Representation Learning via Smoothed Contrastive Learning",
    "volume": "review",
    "abstract": "Graph contrastive learning (GCL) aligns node representations through the utilization of positive/negative node pairs, a selection process that typically relies on the correspondences and non-correspondences among nodes within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment of misclassified false negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to exploit proximity information associated with positive/negative pairs in contrastive loss. The proposed SGCL adjusts the significance of these pairs in contrastive loss by incorporating three distinct smoothing techniques that yield smoothed positive/negative pairs. To enhance scalability for large-scale graphs, the proposed framework incorporates a graph batch-generating strategy that partitions the given graphs into multiple subgraphs, facilitating efficient training in separate batches. Through extensive experimentation in an unsupervised setting on various benchmark datasets, particularly those of large scale, we demonstrate the superiority of our proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vMBNUCZ4cS": {
    "title": "Graph Neural Tangent Kernel and Graph Neural Network Gaussian Processes for Node Classification/ Regression",
    "volume": "review",
    "abstract": "This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph Structured Data, when their width, that is the number of nodes in each fully-connected layers is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of Datasets on the task of Transductive Node Regression and Classification. Extending the setting to Inductive Graph Learning tasks is straightforward and is briefly discussed in 7.5",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=LSYhE2hLWG": {
    "title": "SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations",
    "volume": "review",
    "abstract": "We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model's performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage. We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information. Our method is rigorously tested on multiple PDE datasets, including the Navier-Stokes equations and shallow water equations, showcasing the advantages of our proposed approach over conventional U-Nets with a comparable parameter budget. We further demonstrate that increasing the number of waves in SineNet while maintaining the same number of parameters leads to a monotonically improved performance. The results highlight the effectiveness of SineNet and the potential of our approach in advancing the state-of-the-art in neural PDE solver design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6bAfAcuuZD": {
    "title": "Emergence of Surprise and Predictive Signals from Local Contrastive Learning",
    "volume": "review",
    "abstract": "Hierarchical predictive models are often used to model cortical representations. These models exploit the local or global computation of predictive signals in the neural network, but their biological plausibility is limited as it is currently unknown whether cortical circuits perform such computations at all. This paper seeks to further investigate the inverted Forward-Forward Algorithm, a biologically plausible innovative approach to learning with only forward passes, in order to demonstrate that hierarchical predictive computations can emerge from a simpler contrastive constraint on the network's representation. Through the identification of compelling similarities between our model and hierarchical predictive coding, as well as the examination of the emergent properties of resulting representations, we advance the hypothesis that the computational properties that emerge in neocortical circuits, widely acknowledged as the basis of human intelligence, may be attributed to local learning principles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=WIzzXCVYiH": {
    "title": "GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries",
    "volume": "review",
    "abstract": "While Graph Neural Networks (GNNs) have achieved remarkable performance on various machine learning tasks on graph data, they also raised questions regarding their transparency and interpretability. Recently, there have been extensive research efforts in explaining the decision-making process of GNNs. These efforts often focus on explaining why a certain prediction is being made for a particular instance, or what discriminative features the GNNs try to detect for each class. However, to the best of our knowledge, there is no existing study on understanding the decision boundaries of GNNs, even though the decision-making process of GNNs is directly determined by the decision boundaries. To bridge this research gap, we propose a model-level explainability method called GNNBoundary, which attempts to gain deeper insights into the decision boundaries of graph classifiers. Specifically, we first develop an algorithm to identify the pairs of classes whose decision regions are adjacent. For an adjacent class pair, the near-boundary graphs between them are effectively generated by optimizing a novel objective function specifically designed for the purpose of boundary graph generation. Thus, by analyzing the near-boundary graphs, the important characteristics of decision boundaries can be uncovered. To evaluate the efficacy of GNNBoundary, we conduct experiments on both synthetic datasets and public real-world datasets. The results have demonstrated that, through the analysis of faithful near-boundary graphs generated by GNNBoundary, we can thoroughly assess the robustness and generalizability of the explained GNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=5mtwoRNzjm": {
    "title": "Optimization without retraction on the random generalized Stiefel manifold for canonical correlation analysis",
    "volume": "review",
    "abstract": "Optimization over the set of matrices that satisfy $X^\\top B X = I_p$, referred to as the generalized Stiefel manifold, appears in many applications such as canonical correlation analysis (CCA) and the generalized eigenvalue problem. Solving these problems for large-scale datasets is computationally expensive and is typically done by either computing the closed-form solution with subsampled data or by iterative methods such as Riemannian approaches. Building on the work of Ablin \\& Peyré (2022), we propose an inexpensive iterative method that does not enforce the constraint in every iteration exactly, but instead it produces iterations that converge to the generalized Stiefel manifold. We also tackle the random case, where the matrix $B$ is an expectation. Our method requires only efficient matrix multiplications, and has the same sublinear convergence rate as its Riemannian counterpart. Experiments demonstrate its effectiveness in various machine learning applications involving generalized orthogonality constraints, including CCA for measuring model representation similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=sNtDKdcI1f": {
    "title": "A Long Way To Go: Investigating Length Correlations in RLHF",
    "volume": "review",
    "abstract": "Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. As the growth of open-source preference datasets, reward models, and language models has enabled wider experimentation, RLHF's benefits have been demonstrated in settings beyond general chat agents, including web question answering, summarization, and multi-turn dialogue. However, RLHF has also been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements on helpfulness. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets. In these settings, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can achieve the same downstream improvements as RLHF without increasing length. While our interventions mitigate length increases, they aren't uniformly effective across settings. Furthermore, we find that even running RLHF with a reward based solely on length can reproduce most of the downstream improvements over the initial policy model, showing that reward models in these settings have a long way to go",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jsfesDZDq": {
    "title": "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN",
    "volume": "review",
    "abstract": "Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired machine learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task and, next, eliminating neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning an untrained (arbitrarily initialized) large model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from an untrained RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and time-series prediction. The experimental results show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=8LBS1nixTJ": {
    "title": "HashOrder: Accelerating Graph Processing Through Hashing-based Reordering",
    "volume": "review",
    "abstract": "Graph processing systems are a fundamental tool across various domains such as machine learning, and their efficiency has become increasingly crucial due to the rapid growth in data volume. A major bottleneck in graph processing systems is poor cache utilization. Graph reordering techniques can mitigate this bottleneck and significantly speed up graph workloads by improving the data locality of the graph memory layout. However, since existing approaches use greedy algorithms or simple heuristics to find good orderings, they suffer from either high computational overhead or suboptimal ordering quality. To this end, we propose HashOrder, a probabilistic algorithm for graph reordering based on randomized hashing. We theoretically show that hashing-based orderings have quality guarantees under reasonable assumptions. HashOrder produces high-quality orderings while being lightweight and parallelizable. We empirically show that HashOrder beats the efficiency-quality tradeoff curve of existing algorithms. Evaluations on various graph processing workloads and GNN data loaders reveal that HashOrder is competitive with or outperforms the existing best method while being 592$\\times$ more efficient in reordering, speeding up PageRank by up to 2.49$\\times$ and GNN data loaders by up to 2.33$\\times$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=UJkgGbLfWA": {
    "title": "Guiding Language Models Reasoning with Planning Tokens",
    "volume": "review",
    "abstract": "Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce 'planning tokens' at the start of each reasoning step, serving as a guide for the model. These tokens embeddings are then fine-tuned along with the rest of the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. plain chain-of-thought fine-tuning baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0wAim2F8A": {
    "title": "PrivilegedDreamer: Explicit Imagination of Privileged Information for Adaptation in Uncertain Environments",
    "volume": "review",
    "abstract": "In many real-world control problems, such as robotics, the system dynamics can be significantly affected by unobservable hidden parameters, like friction coefficients. To represent these kinds of domains, we use Hidden-parameter Markov Decision Processes (HIP-MDPs), which model sequential decision problems where hidden variables affect the transition and reward functions. Existing approaches, such as domain randomization, domain adaptation, and meta-learning, simply treat the effect of hidden parameters as additional variance in dynamics and often struggle to effectively handle HIP-MDP problems, especially when rewards are parameterized by hidden variables. To address this, we introduce PrivilegedDreamer, a model-based reinforcement learning framework that extends Dreamer, a powerful world-modeling approach, by incorporating an explicit parameter estimation module. We introduce a novel dual recurrent architecture that explicitly estimates hidden parameters from limited historical data and enables us to condition the model, actor, and critic networks on these estimated parameters. Our empirical analysis on five diverse HIP-MDP tasks demonstrates that it outperforms state-of-the-art model-based, model-free, and domain adaptation learning algorithms. Furthermore, we also conduct ablation studies to justify our design decisions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=GgEAdqYPNA": {
    "title": "Investigating the Benefits of Projection Head for Representation Learning",
    "volume": "review",
    "abstract": "Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP \\citep{radford2021learning}, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and uncover two mechanisms behind MMCL's robustness: \\emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \\emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP on MS COCO \\citep{lin2014microsoft} and evaluating the model on variations of shifted ImageNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=1YO4EE3SPB": {
    "title": "A Variational Perspective on Solving Inverse Problems with Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for image restoration tasks such as inpainting and superresolution demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=vqIH0ObdqL": {
    "title": "Can Large Language Models Infer Causation from Correlation?",
    "volume": "review",
    "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g. commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning ability and generalizability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pmrc0nEvxf": {
    "title": "MemStranding: Adversarial attacks on temporal graph neural networks",
    "volume": "review",
    "abstract": "Temporal graph neural networks (TGNN) have achieved significant momentum in many real-world dynamic graph tasks. While this trend raises an urgent to study their robustness against adversarial attacks, developing an attack on TGNN is challenging due to the dynamic nature of their input dynamic graphs. On the one hand, subsequent graph changes after the attacks may diminish the impact of attacks on seen nodes. On the other hand, targeting future nodes, which are unseen during the attack, poses significant challenges due to missing knowledge about them. To tackle these unique challenges in attacking TGNNs, we propose a practical and effective adversarial attack framework, MemStranding, that leverages node memories in TGNN models to yield long-lasting and spreading adversarial noises in dynamic graphs. The MemStranding allows the attacker to inject noises into nodes' memory by adding fake nodes/edges at arbitrary timestamps. During future updates, the noises in nodes will persist with the support from their neighbors and be propagated to the future nodes by molding their memories into similar noisy states. The experimental results demonstrate that MemStranding can significantly decrease the TGNN models' performances in various tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Of2nEDc4s7": {
    "title": "Anisotropy helps: improved statistical and computational complexity of the mean-field Langevin dynamics under structured data",
    "volume": "review",
    "abstract": "Recent works have shown that neural networks optimized by gradient-based methods can adapt to sparse or low-dimensional target functions through feature learning; an often studied target is the sparse parity function defined on the unit hypercube. However, such isotropic data setting does not capture the anisotropy and low intrinsic dimensionality exhibited in realistic datasets. In this work, we address this shortcoming by studying how gradient-based feature learning interacts with structured (anisotropic) input data: we consider the sparse parity problem on high-dimensional orthotope where the feature coordinates have varying magnitudes, and analyze the learning complexity of the mean-field Langevin dynamics (MFLD), which describes the noisy gradient descent update on two-layer neural network. We show that the statistical complexity (i.e. sample size) and computational complexity (i.e. width of the neural network) of MFLD can both be improved when prominent directions of the anisotropic input data aligns with the support of the target function. Moreover, by employing an anisotropic weight decay regularization determined by the gradient covariance, the problem can be efficiently learned by a constant-width neural network",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Zr96FfaUGR": {
    "title": "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews",
    "volume": "review",
    "abstract": "Revising scientific papers based on peer feedback is a challenging task that requires not only deep scientific knowledge and reasoning, but also the ability to recognize the implicit requests in high-level feedback and to choose the best of many possible ways to update the manuscript in response. We introduce this task for large language models and release ARIES, a dataset of review comments and their corresponding paper edits, to enable training and evaluating models. We study two versions of the task: comment-edit alignment and edit generation, and evaluate several baselines, including GPT-4. We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than the underlying intent, and includes fewer technical details than human-written edits. We hope that our formalization, dataset, and analysis will form a foundation for future work in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=wRkfniZIBl": {
    "title": "Splicing Up Your Predictions with RNA Contrastive Learning",
    "volume": "review",
    "abstract": "In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Recent self-supervised methods in other domains have demonstrated the ability to learn rules underlying the data-generating process such as sentence structure in language. Inspired by this, we extend contrastive learning techniques to genomic data by utilizing functional similarities between sequences generated through alternative splicing and gene duplication. Our novel dataset and contrastive objective enable the learning of generalized RNA isoform representations. We validate their utility on downstream tasks such as RNA half-life and mean ribosome load prediction. Our pre-training strategy yields competitive results using linear probing on both tasks, along with up to a two-fold increase in Pearson correlation in low-data conditions. Importantly, our exploration of the learned latent space reveals that our contrastive objective yields semantically meaningful representations, underscoring its potential as a valuable initialization technique for RNA property prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=fapHf9fmqp": {
    "title": "Unnormalized Density Estimation with Root Sobolev Norm Regularization",
    "volume": "review",
    "abstract": "We propose a new approach to non-parametric density estimation that is based on regularizing a Sobolev norm of the density. This method is consistent, different from Kernel Density Estimation, and makes the inductive bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=MCUvAc1GTg": {
    "title": "Network Alignment with Transferable Graph Autoencoders",
    "volume": "review",
    "abstract": "Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scalability of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Dtxc7mlKRg": {
    "title": "Class-Conditional Conformal Prediction for Imbalanced Data via Top-$k$ Classes",
    "volume": "review",
    "abstract": "Classification tasks where data contains skewed class proportions (aka {\\em imbalanced data}) arises in many real-world applications including medical diagnosis. Safe deployment of classifiers for imbalanced data settings require theoretically-sound uncertainty quantification. Conformal prediction (CP) is a promising framework for producing prediction sets from black-box classifiers with a user-specified coverage (i.e., true class is contained with high probability). Existing class-conditional CP (CCP) method employs a black-box classifier to find one threshold for each class during calibration and then includes every class label that meets the corresponding threshold for testing inputs, leading to large prediction sets. This paper studies the problem of how to develop provable CP methods with small prediction sets for the class-conditional coverage setting and makes several contributions. First, we theoretically show that marginal CP can perform arbitrarily poorly and cannot provide coverage guarantee for minority classes. Second, we propose a principled algorithm referred to as {\\em $k$-Class-conditional CP ($k$-CCP)}. The key idea behind $k$-CCP is to restrict the candidate labels for the prediction set of a testing input to only top-$k$ labels based on the classifier scores (in contrast to all labels in CCP). Third, we prove that $k$-CCP provides class-conditional coverage and produces smaller prediction sets over the CCP method. Our experiments on benchmark datasets demonstrate that $k$-CCP achieves class-conditional coverage and produces smaller prediction sets over baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=uU0Adp7Sfo": {
    "title": "Competitive-Collaborative GAN with Performance Guarantee",
    "volume": "review",
    "abstract": "Generative Adversarial Networks (GANs) generate data based on a competition game to minimize the distribution distance between existing and new data. However, such a competition game falls short when insights about data distributions beyond their authenticity are imperative, such as in multi-modal generation and image super resolution. In recognition of the limitations inherent to the pure-competitive mechanism, we introduce CCGAN, a Collaborative-Competitive Generative Adversarial Network scheme to enable data generation with additional knowledge beyond the provided dataset distribution. For theoretically preserving the equilibrium point and numerically avoiding training collapse issue, we show the need to convert regularization term into a divergence, so that the modified GAN is well-defined in game theory. By harmonizing the competition and collaboration losses in CCGAN, we effectively reduce the degree complexity of solving the optima, facilitating the establishment of a closed-form equilibrium point. This equilibrium point serves as a guidance for training and hyper-parameter tuning, resulting in consistently high-quality generated samples. Meanwhile, the regularization breaks the mutual dependency between the generator and discriminator. This newfound independence empowers the CCGAN to explore a broader parameter space, effectively mitigating the training collapse issue. To validate the capabilities of CCGAN, we design comprehensive experiments across four publicly available datasets and systematically compare CCGAN against a range of baseline models. The experiments demonstrate the efficacy of CCGAN on generating satisfactory samples tailored to specific requirements, particularly when applied to the generation of images featuring regularly shaped objects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=66e22qCU5i": {
    "title": "Certified Copy: A Resistant Backdoor Attack",
    "volume": "review",
    "abstract": "The robustness, security, and safety of artificial intelligence systems have become a major concern in recent studies. One of the most significant threats to deep learning models is the backdoor attack, which has been thoroughly investigated. Despite numerous backdoor detection mechanisms developed for computer vision systems, our research shows that even simple backdoor attacks can bypass these defenses if the backdoor planting process and poisoning data are carefully crafted. To evade existing backdoor detection systems, we propose a new backdoored model called Certified Copy, which is trained using a novel cost function. This cost function controls the activation of neurons in the model to ensure that the activation generated by clean inputs is similar to that produced by poisoned input data. The model copies the corresponding clean model during training in all situations except when fed with poisoned inputs. We tested our model against six state-of-the-art defense mechanisms, including Neural Cleanse, TAO, ABS, TABOR, NNoculation, and STRIP. The results showed that most of these methods cannot detect the backdoored model. We conclude that deep learning models have a vast hypothesis space, which can be exploited by malicious attackers to hide malicious activation of neurons using poisoned data, leading to undetected backdoored models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ffcHGwb4KF": {
    "title": "SPADE: Sparsity-Guided Debugging for Deep Neural Networks",
    "volume": "review",
    "abstract": "Interpretability, broadly defined as mechanisms for understanding why and how machine learning models reach their decisions, is one of the key open goals at the intersection of deep learning theory and practice. Towards this goal, multiple tools have been proposed to aid a human examiner in reasoning about a network's behavior in general or on a set of instances. However, the outputs of these tools---such as input saliency maps or neuron visualizations---are frequently difficult for a human to interpret, or even misleading, due, in particular, to the fact that neurons can be multifaceted, i.e., a single neuron can be associated with multiple distinct feature combinations. In this paper, we present a new general approach to address this problem, called SPADE, which, given a trained model and a target sample, uses sample-targeted pruning to provide a \"trace\" of the network's execution on the sample, reducing the network to the connections that are most relevant to the specific prediction. We demonstrate that preprocessing with SPADE significantly increases both the accuracy of image saliency maps across several interpretability methods and the usefulness of neuron visualizations, aiding humans in reasoning about network behavior. Our findings show that sample-specific pruning of connections can disentangle multifaceted neurons, leading to consistently improved interpretability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5CBxA1l5RO": {
    "title": "TimewarpVAE: Simultaneous Time-Warping and Representation Learning of Trajectories",
    "volume": "review",
    "abstract": "Human demonstrations of trajectories are an important source of training data for many machine learning problems. However, the difficulty of collecting human demonstration data for complex tasks makes learning efficient representations of those trajectories challenging. For many problems, such as for handwriting or for quasistatic dexterous manipulation, the exact timings of the trajectories should be factored from their spatial path characteristics. In this work, we propose TimewarpVAE, a fully differentiable manifold-learning algorithm that incorporates Dynamic Time Warping (DTW) to simultaneously learn both timing variations and latent factors of spatial variation. We show how the TimewarpVAE algorithm learns appropriate time alignments and meaningful representations of spatial variations in small handwriting and fork manipulation datasets. Our results have lower spatial reconstruction test error than baseline approaches and the learned low-dimensional representations can be used to efficiently generate semantically meaningful novel trajectories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tEAF9LBdgu": {
    "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
    "volume": "review",
    "abstract": "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=jX2DT7qDam": {
    "title": "Jointly-Learned Exit and Inference for a Dynamic Neural Network",
    "volume": "review",
    "abstract": "Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for $\\textit{every}$ inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs are decoupled during training, leading to a train-test mismatch; and 2) the thresholding gating mechanism introduces a positive bias into the predictive probabilities, making it difficult to readily extract uncertainty information. We propose a novel architecture that connects these two modules. This leads to significant performance improvements on classification datasets and enables better uncertainty characterization capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=PQY2v6VtGe": {
    "title": "Confidential-DPproof: Confidential Proof of Differentially Private Training",
    "volume": "review",
    "abstract": "Post hoc privacy auditing techniques can be used to test the privacy guarantees of a model, but come with several limitations: (i) they can only establish lower bounds on the privacy loss, (ii) the intermediate model updates and some data must be shared with the auditor to get a better approximation of the privacy loss, and (iii) the auditor typically faces a steep computational cost to run a large number of attacks. In this paper, we propose to proactively generate a cryptographic certificate of privacy during training to forego such auditing limitations. We introduce Confidential-DPproof , a framework for Confidential Proof of Differentially Private Training, which enhances training with a certificate of the $(\\varepsilon,\\delta)$-DP guarantee achieved. To obtain this certificate without revealing information about the training data or model, we design a customized zero-knowledge proof protocol tailored to the requirements introduced by differentially private training, including random noise addition and privacy amplification by subsampling. In experiments on CIFAR-10, Confidential-DPproof trains a model achieving state-of-the-art $91$% test accuracy with a certified privacy guarantee of $(\\varepsilon=0.55,\\delta=10^{-5})$-DP in approximately 100 hours",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=1XarNmzbgG": {
    "title": "Understanding of Server-Assisted Federated Learning with Incomplete Client Participation",
    "volume": "review",
    "abstract": "Existing works in federated learning (FL) often assumes an ideal system with either full client or uniformly distributed client participation. However, in practice, it has been observed that some clients may never participate in FL training (aka incomplete client participation) due to a myriad of system heterogeneity factors. To mitigate impacts of incomplete client participation, a popular approach is the server-assisted federated learning (SA-FL) framework, where the server is equipped with an auxiliary dataset. However, despite the fact that SA-FL has been empirically shown to be effective in addressing the incomplete client participation problem, there remains a lack of theoretical understanding for SA-FL. Meanwhile, the ramifications of incomplete client participation in conventional FL is also poorly understood. These theoretical gaps motivate us to rigorously investigate SA-FL. Toward this end, to fully understand the impact of incomplete client participation on conventional FL, we first show that conventional FL is {\\em not} PAC-learnable under incomplete client participation in the worst case. Then, we show that the PAC-learnability of FL with incomplete client participation can indeed be revived by SA-FL, which theoretically justifies the use of SA-FL for the first time. Lastly, to provide practical guidance for SA-FL training under {\\em incomplete client participation}, we propose the SAFARI (server-assisted federated averaging) algorithm that enjoys the same linear convergence speedup guarantees as classic FL with ideal client participation assumptions, offering the first SA-FL algorithm with convergence guarantee. Extensive experiments on different datasets show SAFARI significantly improve the performance under incomplete client participation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Ns8SXMJ2ic": {
    "title": "Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational Quantum Systems",
    "volume": "review",
    "abstract": "In the field of quantum information, classical optimizers play an important role. From experimentalists optimizing their physical devices to theorists exploring variational quantum algorithms, many aspects of quantum information require the use of a classical optimizer. For this reason, there are many papers that benchmark the effectiveness of different optimizers for specific quantum learning tasks and choices of parameterized algorithms. However, for researchers exploring new algorithms or physical devices, the insights from these studies don't necessarily translate. To address this concern, we compare the performance of a class optimizers across a series of partially-randomized tasks to more broadly sample the space of quantum learning problems. We focus on local zeroth-order optimizers due to their generally favorable performance and query-efficiency on quantum systems. We discuss insights from these experiments that can help motivate future works to improve these optimizers for use on quantum systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=GfXF04YYvu": {
    "title": "Enhancing Group Fairness in Federated Learning through Personalization",
    "volume": "review",
    "abstract": "Instead of producing a single global model for all participating clients, personalized Federated Learning (FL) algorithms aim to collaboratively train customized models for each client, enhancing their local accuracy. For example, clients could be clustered into different groups in which their models are similar, or clients could tune the global model locally to achieve better local accuracy. In this paper, we investigate the impact of personalization techniques in the FL paradigm on local (group) fairness of the learned models, and show that personalization techniques can also lead to improved fairness. We establish this effect through numerical experiments comparing two types of personalized FL algorithms against the baseline FedAvg algorithm and a baseline fair FL algorithm, and elaborate on the reasons behind improved fairness using personalized FL methods. We further provide analytical support under certain conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtl4XnJYBh": {
    "title": "Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift",
    "volume": "review",
    "abstract": "Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP \\citep{radford2021learning}, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and uncover two mechanisms behind MMCL's robustness: \\emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \\emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP on MS COCO \\citep{lin2014microsoft} and evaluating the model on variations of shifted ImageNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=tS3gexmfeT": {
    "title": "Fusion Token: Enhancing Compression and Efficiency in Language Model Tokenization",
    "volume": "review",
    "abstract": "In the realm of language models, data encoding is pivotal, influencing efficiency and effectiveness of model training. Byte Pair Encoding (BPE) is a well-established subword tokenization technique that balances computational efficiency and linguistic expressiveness by merging frequent byte or character pairs. As language model training requires substantial computational resources, we propose Fusion Token, a method that substantially enhances the conventional Byte Pair Encoding (BPE) approach in data encoding for language models. Fusion Token employs a more aggressive computational strategy compared to BPE, expanding the token groups from bi-grams to 10-grams. Remarkably, with the addition of 1024 tokens to the vocabulary, the compression rate significantly surpasses that of a regular BPE tokenizer with a vocabulary of one million. Overall, the Fusion Token method leads to noticeable performance improvements due to an increased data scope per compute unit. Additionally, higher compression results in faster inference times due to fewer tokens per given string. By devoting more compute resources to the tokenizer building process, Fusion Token maximizes the potential of language models as efficient data compression engines, enabling more effective language modeling systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=FJWT0692hw": {
    "title": "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking",
    "volume": "review",
    "abstract": "In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented without adversarial training or major architectural changes. We identify the SequenceMatch-χ2 divergence as a more suitable training objective for autoregressive models which are used for generation. We show that empirically, SequenceMatch training leads to improvements over MLE on text generation with language models and arithmetic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfmZh91tDI": {
    "title": "Layer-wise linear mode connectivity",
    "volume": "review",
    "abstract": "Averaging neural network parameters is an intuitive method for fusing the knowledge of two independent models. It is most prominently used in federated learning. If models are averaged at the end of training, this can only lead to a good performing model if the loss surface of interest is very particular, i.e., the loss in the exact middle between the two models needs to be sufficiently low. This is impossible to guarantee for the non-convex losses of state-of-the-art networks. For averaging models trained on vastly different datasets, it was proposed to average only the parameters of particular layers or combinations of layers, resulting in better performing models. To get a better understanding of the effect of layer-wise averaging, we analyse the performance of the models that result from averaging single layers, or groups of layers. Based on our empirical and theoretical investigation, we introduce a novel notion of the layer-wise linear connectivity, and show that deep networks do not have layer-wise barriers between them. We analyze additionally the layer-wise personalization averaging and conjecture that in particular problem setup all the partial aggregations result in the approximately same performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=h05eQniJsQ": {
    "title": "Understanding Certified Training with Interval Bound Propagation",
    "volume": "review",
    "abstract": "As robustness verification methods are becoming more precise, training certifiably robust neural networks is becoming ever more relevant. To this end, certified training methods compute and then optimize an upper bound on the worst-case loss over a robustness specification. Curiously, training methods based on the imprecise interval bound propagation (IBP) consistently outperform those leveraging more precise bounds. Still, we lack a theoretical understanding of the mechanisms making IBP so successful. In this work, we investigate these mechanisms by leveraging a novel metric measuring the tightness of IBP bounds. We first show theoretically that, for deep linear models (DLNs), tightness decreases with width and depth at initialization, but improves with IBP training. We, then, derive sufficient and necessary conditions on weight matrices for IBP bounds to become exact and demonstrate that these impose strong regularization, providing an explanation for the observed robustness-accuracy trade-off. Finally, we show how these results on DLNs transfer to ReLU networks, before conducting an extensive empirical study, (i) confirming this transferability and yielding state-of-the-art certified accuracy, (ii) finding that while all IBP-based training methods lead to high tightness, this increase is dominated by the size of the propagated input regions rather than the robustness specification, and finally (iii) observing that non-IBP-based methods do not increase tightness. Together, these results help explain the success of recent certified training methods and may guide the development of new ones",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=GnOLWS4Llt": {
    "title": "Offline RL with Observation Histories: Analyzing and Improving Sample Complexity",
    "volume": "review",
    "abstract": "Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by \"stitching\" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new behaviors where each individual state is in-distribution, but the overall returns are higher. However, in many interesting and complex applications, such as autonomous navigation and dialogue systems, the state is partially observed. Even worse, the state representation is unknown or not easy to define. In such cases, policies and value functions are often conditioned on observation histories instead of states. In these cases, it is not clear if the same kind of \"stitching\" is feasible at the level of observation histories, since two different trajectories would always have different histories, and thus \"similar states\" that might lead to effective stitching cannot be leveraged. Theoretically, we show that standard offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition. We then identify sufficient conditions under which offline RL can still be efficient -- intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection. We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity. Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WEQS3oUPs3": {
    "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations",
    "volume": "review",
    "abstract": "Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student's current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised fine-tuning or ``single-step'' RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. In this work, we explore a new method for adapting LLMs with RL for such goal-directed dialogue. Our key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. Our algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirically, we show that our proposed approach achieves state-of-the-art performance in various goal-directed dialogue tasks that include teaching and preference elicitation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=UvRjDCYIHw": {
    "title": "Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types",
    "volume": "review",
    "abstract": "The task of inductive link prediction in discrete attributed multigraphs (e.g., knowledge graphs, multilayer networks, heterogeneous networks, etc.) generally focuses on test predictions with solely new nodes but not both new nodes and new relation types. In this work, we formally define the task of predicting (completely) new nodes and new relation types in test as a doubly inductive link prediction task and introduce a theoretical framework for the solution. We start by defining the concept of double permutation-equivariant representations that are equivariant to permutations of both node identities and edge relation types. We then propose a general blueprint to design neural architectures that impose a structural representation of relations that can inductively generalize from training nodes and relations to arbitrarily new test nodes and relations without the need for adaptation, side information, or retraining. We also introduce the concept of distributionally double equivariant positional embeddings designed to perform the same task. Finally, we empirically demonstrate the capability of the two proposed models on a set of novel real-world benchmarks, showcasing relative performance gains of up to 41.40% on predicting new relations types compared to baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=R6AA1NZhLd": {
    "title": "Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting",
    "volume": "review",
    "abstract": "Spatial functional organization is a hallmark of biological brains: neurons are arranged topographically according to their response properties at different scales. In contrast, representations within most machine learning models lack spatial biases, and instead manifest as disorganized vector spaces that are difficult to visualize and interpret. Here, we propose a novel form of self-attention that turn Transformers into \"Topoformers\" with topographic organization. Our primary contribution is Spatial Querying, where keys and queries are arranged on 2D grids, and local pools of queries are associated with a given key. Our secondary contribution is Spatial Reweighting, where we convert the standard fully connected layer of self-attention into a locally connected layer. We first demonstrate the feasibility of our approach using by training a 1-layer Topoformer on a sentiment classification task. We show that training with Spatial Querying results in corresponding topographic organization between queries and keys, and Spatial Reweighting results in corresponding topographic organization between values and self-attention outputs. This emergent organization is \\textit{semantically interpretable}: the internal activation magnitudes show spatial biases for sentences with positive and negative sentiment. Moreover, generic topographic organization is seen in the low dimensional structure of activations revealed through principal component analysis. After establishing that we can indeed obtain interpretable topography, we apply the Topoformer motifs at scale. We train the widely used BERT architecture on larger corpora with a masked language modeling objective. We find that the topographic variant of this model performs on par with a non-topographic control architecture on downstream NLP benchmarks. Finally, we analyze an fMRI dataset of human brain responses to a large set of naturalistic sentences, demonstrating that the Topoformer yields similar forms of topographic organization for linguistic information as that present in the language network of individual subjects. Scaling up Topoformers holds promise for greater interpretability in NLP research, and for more accurate models of the organization of linguistic and semantic information in the human brain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAqdG2dy5s": {
    "title": "Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations",
    "volume": "review",
    "abstract": "Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting available spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. To tackle this problem, we design a novel graph deep learning framework operating on a nested graph structure, which is used to learn dependencies between variables as well as locations. The proposed architecture, named Graph-graph Network (GgNet), relies on propagating information over such nested graph structure. GgNet is extensively evaluated under different virtual sensing scenarios, demonstrating higher reconstruction accuracy compared to the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=0bMmZ3fkCk": {
    "title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning",
    "volume": "review",
    "abstract": "We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves $29.79$\\% on AlpacaEval, which rises to $64.69$\\% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a $10$\\% improvement, with ShareGPT an $8$\\% improvement, and with OpenPlatypus an $8$\\% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=lxlMFlzZO9": {
    "title": "DS-Prover: A Dynamic Sampling Based Approach for Neural Theorem Proving",
    "volume": "review",
    "abstract": "Theorem proving is a fundamental task in mathematics. With the advent of large language models (LLMs) and interactive theorem provers (ITPs) like Lean, there has been growing interest in integrating LLMs and ITPs to automate theorem proving. In this approach, the LLM generates proof steps (tactics), and the ITP checks the applicability of the tactics at the current goal. The two systems work together to complete the proof. In this paper, we introduce DS-Prover, a novel dynamic sampling method for theorem proving. This method dynamically determines the number of tactics to apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. This makes the proof search process more efficient by adjusting the balance between exploration and exploitation as time passes. We also study the effect of augmenting the training dataset by decomposing simplification and rewrite tactics with multiple premises into tactics with single premises. This gives the model more examples to learn from and helps it to predict the tactics with premises more accurately. We perform our experiments using the Mathlib dataset of the Lean theorem prover and report the performance on two standard datasets, MiniF2F and ProofNet. Our methods achieve significant performance gains on both datasets. We achieve a new state-of-the-art performance of 30.6% on MiniF2F using Lean, and a performance of 13.65% on ProofNet, which is comparable to the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WWlxFtR5sV": {
    "title": "An operator preconditioning perspective on training in physics-informed machine learning",
    "volume": "review",
    "abstract": "In this paper, we investigate the behavior of gradient descent algorithms in physics-informed machine learning methods like PINNs, which minimize residuals connected to partial differential equations (PDEs). Our key result is that the difficulty in training these models is closely related to the conditioning of a specific differential operator. This operator, in turn, is associated to the Hermitian square of the differential operator of the underlying PDE. If this operator is ill-conditioned, it results in slow or infeasible training. Therefore, preconditioning this operator is crucial. We employ both rigorous mathematical analysis and empirical evaluations to investigate various strategies, explaining how they better condition this critical operator, and consequently improve training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=VUR7STEajx": {
    "title": "M-BioBERTa: Modular RoBERTa-based Model for Biobank-scale Unified Representations",
    "volume": "review",
    "abstract": "Transformers provide a novel approach for unifying large-scale biobank data spread across different modalities and omic domains. We introduce M-BioBERTa, a modular architecture for multimodal data that offers a robust mechanism for managing missing information. We evaluate the model using genetic, demographic, laboratory, diagnostic, and drug prescription data from the UK Biobank, focusing on multimorbidity and polypharmacy related to major depressive disorder. We investigate the harmonized and modular representations in M-BioBERTa for patient stratification. Furthermore, leveraging the learned representations to forecast future disease and drug burdens outperforms traditional machine learning approaches applied directly to the raw data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=fjZMGKB2dU": {
    "title": "A neuro-symbolic framework for answering conjunctive queries",
    "volume": "review",
    "abstract": "The problem of answering logical queries over incomplete knowledge graphs is receiving significant attention in the machine learning community. Neuro-symbolic models are a promising recent approach, showing good performance and allowing for good interpretability properties. These models rely on trained architectures to execute atomic queries, combining them with modules that simulate the symbolic operators in queries. Unfortunately, most neuro-symbolic query processors are limited to the so-called _tree-like_ logical queries that admit a bottom-up execution, where the leaves are constant values or _anchors_, and the root is the target variable. Tree-like queries, while expressive, fail short to express properties in knowledge graphs that are important in practice, such as the existence of multiple edges between entities or the presence of triangles. We propose a framework for answering arbitrary conjunctive queries over incomplete knowledge graphs. The main idea of our method is to approximate a cyclic query by an infinite family of tree-like queries, and then leverage existing models for the latter. Our approximations achieve strong guarantees: they are _complete_, i.e. there are no false negatives, and _optimal_, i.e. they provide the best possible approximation using tree-like queries. Our method requires the approximations to be tree-like queries where the leaves are anchors or existentially quantified variables. Hence, we also show how some of the existing neuro-symbolic models can handle these queries, which is of independent interest. Experiments show that our approximation strategy achieves competitive results, and that including queries with existentially quantified variables tends to improve the general performance of these models, both on tree-like queries and on our approximation strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.6,
    "authors": []
  },
  "https://openreview.net/forum?id=pCEgna6Qco": {
    "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
    "volume": "review",
    "abstract": "Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization. ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization. Experiments also show that ProMoT can improve the generalization performance of multi-task training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=0bjIoHD45G": {
    "title": "Closing the gap on tabular data with Fourier and Implicit Categorical Features",
    "volume": "review",
    "abstract": "While Deep Learning has demonstrated impressive results in applications on various data types, it continues to lag behind tree-based methods when applied to tabular data, often referred to as the last \"unconquered castle\" for neural networks. We hypothesize that a significant advantage of tree-based methods lies in their intrinsic capability to model and exploit non-linear interactions induced by features with categorical characteristics. In contrast, neural-based methods exhibit biases toward a uniform numerical processing of features and smooth solutions, making it challenging for them to effectively leverage such patterns. We aim to address this performance gap by using simple, statistical-based feature processing techniques to identify and explicitly encode features that are strongly correlated with the target once discretized, as well as mitigate the bias of deep models for overly-smooth solutions, a bias that does not align with the inherent properties of the data, using Learned Fourier Features. Our proposed feature processing and method achieves a performance that closely matches or surpasses XGBoost on a comprehensive tabular data benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=SyuQKk7sX2": {
    "title": "(Dynamic) Prompting might be all you need to repair Compressed LLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs), while transformative for NLP, come with significant computational demands, underlining the need for efficient, training-free compression. Notably, the reliability of perplexity as a benchmark for compressed model efficacy is in question, as our tests using LLaMA-7B and OPT-6.7b reveal a significant performance drop in several realistic downstream tasks, underscoring the disparity between perplexity as a performance indicator and real-world performance. Investigation into the trade-off between resource-intensive post-compression re-training highlights the prospect of prompt-driven recovery as a lightweight adaption tool. However, existing studies, confined mainly to perplexity evaluations and simple tasks, fail to offer unequivocal confidence in the scalability and generalizability of prompting.We tackle this uncertainty in two key ways. First, we uncover the vulnerability of naive prompts in LLM compression as an over-reliance on a singular prompt per input. In response, we propose \\textit{inference-time dynamic prompting} (IDP), a mechanism that autonomously chooses from a set of curated prompts based on the context of each individual input. Second, we delve into a scientific understanding of why ``prompting might be all you need post-LLM compression\". Our findings suggest that compression doesn't irretrievably erase LLM model knowledge but displace it, necessitating a new inference path. IDP effectively redirects this path, enabling the model to tap into its inherent yet displaced knowledge and thereby recover performance. Empirical tests affirm the value of IDP, demonstrating an average performance improvement of 1.24\\% across nine varied tasks spanning multiple knowledge domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhLBhx5BVY": {
    "title": "Instance Segmentation with Supervoxel Based Topological Loss Function",
    "volume": "review",
    "abstract": "Reconstructing the intricate local morphology of neurons as well as their long-range projecting axons can address many connectivity related questions in neuroscience. While whole-brain imaging at single neuron resolution has recently become available with advances in light microscopy, segmenting multiple entangled neuronal arbors remains a challenging instance segmentation problem. Split and merge mistakes in automated tracings of neuronal branches can produce qualitatively different results and represent a bottleneck of reconstruction pipelines. Here, by extending the notion of simple points from digital topology to connected sets of voxels (i.e. supervoxels), we develop a topology-aware neural network based segmentation method with minimal overhead. We demonstrate the merit of our approach on a newly established public dataset that contains 3-d images of the mouse brain where multiple fluorescing neurons are visible as well as the DRIVE 2-d retinal fundus images benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=eJhgguibXu": {
    "title": "Using Approximate Models for Efficient Exploration in Reinforcement Learning",
    "volume": "review",
    "abstract": "In model-based reinforcement learning, an agent uses a learned model of environment dynamics to improve a policy. Using a learned model of the environment to select actions has many benefits. It can be used to generate experience for learning a policy or simulate potential outcomes in planning. It allows flexible adaptation to new tasks and goals without having to relearn the underlying fundamentals of the environment from scratch. These sample efficiency and generalisation gains from model use are restricted by the model's accuracy. An imperfect model can lead to failure if trusted by the agent in regions of the state space where predictions are inaccurate. It is well-documented in cognitive and developmental psychology that humans use approximate intuitive models of physics when navigating the world in everyday scenarios. These intuitive models, despite being imperfect, enable humans to reason flexibly about abstract physical concepts (for example, gravity, collisions and friction), and to apply these concepts to solve novel problems without having to relearn them from scratch. In other words, humans efficiently make use of imperfect models. In this paper, we learn dynamics models for intuitive physics tasks using graph neural networks that explicitly incorporate the abstract structure of objects, relations and events in their design. We demonstrate that these learned models can flexibly generalise to unseen tasks and, despite being imperfect, can improve the sample efficiency of policy learning through guiding exploration to useful regions of the state and action space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=wHgu98u8Sc": {
    "title": "$\\nu$-ensembles: Improving deep ensemble calibration in the small data regime",
    "volume": "review",
    "abstract": "We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=3j5bsiwRv6": {
    "title": "Sparse Refinement for Efficient High-Resolution Semantic Segmentation",
    "volume": "review",
    "abstract": "Semantic segmentation empowers numerous real-world applications, such as autonomous driving and augmented/mixed reality. These applications often operate on high-resolution images (e.g., 8 megapixels) to capture the fine details. However, this comes at the cost of considerable computational complexity, hindering the deployment in latency-sensitive scenarios. In this paper, we introduce SparseRefine, a novel approach that enhances dense low-resolution predictions with sparse high-resolution refinements. Based on coarse low-resolution outputs, SparseRefine first uses an entropy selector to identify a sparse set of pixels with the least confidence. It then employs a sparse feature extractor to efficiently generate the refinements for those pixels of interest. Finally, it leverages a gated ensembler to apply these sparse refinements to the initial coarse predictions. SparseRefine can be seamlessly integrated into any existing semantic segmentation model, regardless of CNN- or ViT-based. SparseRefine achieves significant speedup: 1.5 to 3.9 times when applied to HRNet-W48, SegFormer-B5, Mask2Former-T/L and SegNeXt-L on Cityscapes, with negligible to no loss of accuracy. We will release the code to reproduce our results. We hope that our \"dense+sparse\" paradigm could inspire future research on efficient high-resolution visual computing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=0fSNU64FV7": {
    "title": "Sorting Out Quantum Monte Carlo",
    "volume": "review",
    "abstract": "Molecular modeling at the quantum level requires choosing a parameterization of the wavefunction that both respects the required symmetries, and is scalable to systems of many particles. For the simulation of fermions, valid parameterizations must be antisymmetric with the transposition of particles. Typically, antisymmetry is enforced by leveraging the anti-symmetry of determinants with respect to exchange of matrix rows, but this involves computing a full determinant each time the wavefunction is evaluated. Instead, we introduce a new antisymmetrization layer derived from sorting, the $\\text{\\emph{sortlet}}$, which scales as $O(N \\log N )$ in the number of particles, in contrast to the $O(N^3)$ of the determinant. We show experimentally that applying this anti-symmeterization layer on top of an attention based neural-network backbone yields a flexible wavefunction parameterization capable of reaching chemical accuracy when approximating the ground state of first-row atoms and molecules",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=d2YjPbSpDZ": {
    "title": "Understanding the Theoretical Generalization Performance of Federated Learning",
    "volume": "review",
    "abstract": "Federated Learning (FL) has become widely popular because of its applicability in training ML on different sites without data sharing. However, the generalization performance of FL has remained relatively under-explored, primarily due to the intricate interplay between data heterogeneity and the local update procedures intrinsic to FL. This motivates us to answer a fundamental question in FL: How can we precisely quantify the impact of data heterogeneity and the local update process on the generalization performance for FL as the learning process evolves? To this end, we conduct a comprehensive theoretical study of FL's generalization performance using a linear model as the first step, where the data heterogeneity is considered for both the stationary and online/non-stationary cases. By providing closed-form expressions of the model error, we rigorously quantify the impact of local update steps (denoted as $K$) under three distinct settings ($K=1$, $K<\\infty$, and $K=\\infty$) and how the generalization performance evolves with the round number $t$. Our investigation also provides a comprehensive understanding of how different configurations (including the number of model parameters $p$ and the number of training samples $n$) contribute to the overall generalization performance, thus shedding new insights (such as benign overfitting) for the practical implementation of FL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzyZ4wzKlM": {
    "title": "Expressive Losses for Verified Robustness via Convex Combinations",
    "volume": "review",
    "abstract": "In order to train networks for verified adversarial robustness, it is common to over-approximate the worst-case loss over perturbation regions, resulting in networks that attain verifiability at the expense of standard performance. As shown in recent work, better trade-offs between accuracy and robustness can be obtained by carefully coupling adversarial training with over-approximations. We hypothesize that the expressivity of a loss function, which we formalize as the ability to span a range of trade-offs between lower and upper bounds to the worst-case loss through a single parameter (the over-approximation coefficient), is key to attaining state-of-the-art performance. To support our hypothesis, we show that trivial expressive losses, obtained via convex combinations between adversarial attacks and IBP bounds, yield state-of-the-art results across a variety of settings in spite of their conceptual simplicity. We provide a detailed analysis of the relationship between the over-approximation coefficient and performance profiles across different expressive losses, showing that, while expressivity is essential, better approximations of the worst-case loss are not necessarily linked to superior robustness-accuracy trade-offs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=0VZP2Dr9KX": {
    "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
    "volume": "review",
    "abstract": "As large language models (LLMs) quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. In particular, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=XWfjugkXzN": {
    "title": "On Sampling Information Sets to Learn from Imperfect Information",
    "volume": "review",
    "abstract": "In many real-world decision-making scenarios, agents are confronted with incomplete and imperfect information, requiring them to make choices based on limited knowledge. Imperfect-information games tackle this challenge by organising different potential situations into so-called information sets, i.e. sets of possible world states that are indistinguishable from one observer's perspective, but directly evaluating an information set is difficult. A common but often suboptimal strategy is to evaluate the individual states in the set with a perfect information evaluator and combine the results. This not only presents problems related to translating perfect information evaluations to imperfect information settings but is also immensely costly in situations with extensive hidden information. This work focuses on learning direct evaluators for information sets by assessing only a subset of the states in the information set, thereby reducing the overall cost of evaluation. Critically, we focus on one question: How many states should be sampled from a given information set? This involves a trade-off between the cost of computing a training signal and its accuracy. We present experimental results in three settings: an artificial MNIST variant with hidden information, Heads-Up Poker, and Reconnaissance Blind Chess. Our results show that the number of sampled states significantly influences the efficiency of training neural networks. However, there are diminishing returns when sampling a large number of states. Notably, in the three regarded domains, using one, two and two samples respectively leads to the best performance concerning the total number of evaluations required. This research contributes to the understanding of how to optimise the sampling of information sets in scenarios of incomplete information, thus offering practical insight into the balance between computational cost and accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.7,
    "authors": []
  },
  "https://openreview.net/forum?id=pUOesbrlw4": {
    "title": "Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting",
    "volume": "review",
    "abstract": "Machine {\\em unlearning} has emerged as a prominent and challenging area of interest, driven in large part by the rising regulatory demands for industries to delete user data upon request and the heightened awareness of privacy. Existing approaches either retrain models from scratch or use several finetuning steps for every deletion request, often constrained by computational resource limitations and restricted access to the original training data. In this work, we introduce a novel class unlearning algorithm designed to strategically eliminate an entire class or a group of classes from the learned model. To that end, our algorithm first estimates the Retain Space and the Forget Space, representing the feature or activation spaces for samples from classes to be retained and unlearned, respectively. To obtain these spaces, we propose a novel singular value decomposition-based technique that requires layer wise collection of network activations from a few forward passes through the network. We then compute the shared information between these spaces and remove it from the forget space to isolate class-discriminatory feature space for unlearning. Finally, we project the model weights in the orthogonal direction of the class-discriminatory space to obtain the unlearned model. We demonstrate our algorithm's efficacy on ImageNet using a Vision Transformer with only $\\sim 1.5$% drop in retain accuracy compared to the original model, while maintaining under $1$% accuracy on the unlearned class samples. Further our comprehensive analysis on a variety of image classification datasets and network architectures shows up to $4.07$% better retain accuracy with similar unlearning (forgetting) on the forget class samples while being $6.5\\times$ faster as compared to a strong baseline we propose. Additionally, we investigate the impact of unlearning on network decision boundaries and conduct saliency-based analysis to illustrate that the post-unlearning model struggles to identify class-discriminatory features from the forgotten classes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=vbebD7QRxP": {
    "title": "Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference",
    "volume": "review",
    "abstract": "Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a deep causal generative model, which utilizes the pre-trained model and can provably sample from identifiable interventional and counterfactual distributions. Our algorithm, called WhatIfGAN, uses adversarial training to learn the network weights, and to the best of our knowledge, is the first algorithm that can make use of pre-trained models and provably sample from any identifiable causal query in the presence of latent confounders with high-dimensional data. We demonstrate the utility of our algorithm using semi-synthetic and real-world datasets containing images as variables in the causal structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=LXVswInHOo": {
    "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries",
    "volume": "review",
    "abstract": "Language models are currently trained to predict tokens given document prefixes, enabling them to zero shot long form generation and prompting-style tasks which can be reduced to document completion. We instead present IN-CONTEXT PRETRAINING, a new approach where language models are trained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. Our approach builds on the fact that current pipelines train by concatenating random sets of shorter documents to create longer context windows; this improves efficiency even though the prior documents provide no signal for predicting the next document. Given this fact, we can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent batches with a graph cover algorithm. Our experiments show IN-CONTEXT PRETRAINING offers a scalable and simple approach to significantly enhance LM performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=hr4HTShC6l": {
    "title": "Detecting Shortcuts using Mutual Information",
    "volume": "review",
    "abstract": "The failure of deep neural networks to generalize to out-of-distribution (OOD) data is a well-known problem that raises concerns about the deployment of trained networks in safety-critical domains such as healthcare and autonomous vehicles. We study a particular kind of distribution shift — shortcuts or spurious correlations in the training data. These correlations are not present in real-world test data, so there is a performance drop due to distribution shift, also referred to as shortcut learning. Shortcut learning is often only exposed when models are evaluated in carefully controlled experimental settings, posing a serious dilemma for AI practitioners to properly assess the effectiveness of a trained model for real-world applications. In this work, we try to understand shortcut learning using information-theoretic tools and propose to use the mutual information (MI) between the learned representation and the input space as a domain-agnostic metric for detecting shortcuts in the training datasets. For studying the training dynamics of shortcut learning, we develop a Neural Tangent Kernel (NTK) based framework, which can be used to detect shortcuts and spurious correlations in the training data without requiring class labels of the test data. We empirically demonstrate on multiple datasets, such as MNIST, CelebA, NICO, Waterbirds, and BenchMD, that MI can effectively detect shortcuts. We benchmark against multiple OOD detection baselines to show that OOD detectors cannot detect shortcuts, and our method can be used in complementary with OOD detectors to identify all types of distribution shifts in the datasets, including shortcuts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=z62Xc88jgF": {
    "title": "Neural functional a posteriori error estimates",
    "volume": "review",
    "abstract": "We propose a new loss function for supervised and physics-informed training of neural networks and operators that incorporates a posteriori error estimate. More specifically, during the training stage, the neural network learns additional physical fields that lead to rigorous error majorants after a computationally cheap postprocessing stage. Theoretical results are based upon the theory of functional a posteriori error estimates, which allows for the systematic construction of such loss functions for a diverse class of practically relevant partial differential equations. From the numerical side, we demonstrate on a series of elliptic problems that for a variety of architectures and approaches (physics-informed neural networks, physics-informed neural operators, neural operators, and classical architectures in the regression and physics-informed settings), we can reach better or comparable accuracy and in addition to that cheaply recover high-quality upper bounds on the error after training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WroPkTLiAJ": {
    "title": "FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation",
    "volume": "review",
    "abstract": "Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in the practical non-IID scenario, we efficiently infer the posteriors of each layer in each local model using layer-wise Laplace approximation and aggregate them to train the global parameters. Extensive experimental results demonstrate that FedLPA significantly improves learning performance over state-of-the-art methods across several metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RvfPnOkPV4": {
    "title": "What's In My Big Data?",
    "volume": "review",
    "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of 16 high-level analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities---count and search---*at scale*, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to 10 different corpora used to train popular language models, including *C4*, *The Pile*, and *RedPajama*. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in *RedPajama* and *LAION-2B-en* are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0D6mUZTWoF": {
    "title": "A Topology-aware Graph Coarsening Framework for Continual Graph Learning",
    "volume": "review",
    "abstract": "Continual learning on graphs tackles the problem of training a graph neural network (GNN) where graph data arrive in a streaming fashion and the model tends to forget knowledge from previous tasks when updating with new data. Traditional continual learning strategies such as Experience Replay can be adapted to streaming graphs, however, these methods often face challenges such as inefficiency in preserving graph topology and incapability of capturing the correlation between old and new tasks. To address these challenges, we propose TA$\\mathbb{CO}$, a topology-aware graph coarsening and continual learning framework that stores information from previous tasks as a reduced graph. At each time period, this reduced graph expands by combining with a new graph and aligning shared nodes, and then it undergoes a ``zoom out'' process by reduction to maintain a stable size. We design a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph and preserve topological information. We empirically demonstrate the learning process on the reduced graph can approximate that of the original graph. Our experiments validate the effectiveness of the proposed framework on three real-world datasets using different backbone GNN models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=5RielfrDkP": {
    "title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network",
    "volume": "review",
    "abstract": "Graph Neural Networks are popular tools in graph representation learning that capture the graph structural properties. However, most GNNs employ single-resolution graph feature extraction, thereby failing to capture micro-level local patterns (high resolution) and macro-level graph cluster and community patterns (low resolution) simultaneously. Many multiresolution methods have been developed to capture graph patterns at multiple scales, but most of them depend on predefined and handcrafted multiresolution transforms that remain fixed throughout the training process once formulated. Due to variations in graph instances and distributions, fixed handcrafted transforms can not effectively tailor multiresolution representations to each graph instance. To acquire multiresolution representation suited to different graph instances and distributions, we introduce the Multiresolution Meta-Framelet-based Graph Convolutional Network (MM-FGCN), facilitating comprehensive and adaptive multiresolution analysis across diverse graphs. Extensive experiments demonstrate that our MM-FGCN achieves SOTA performance on various graph learning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HFG7LcCCwK": {
    "title": "Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand",
    "volume": "review",
    "abstract": "The ability to apply causal reasoning from observational data has made causal inference algorithms widely adopted in machine learning applications. While there exist sound and complete algorithms to compute causal effects, these algorithms require explicit access to conditional likelihoods over the observational distribution. In the high dimensional regime, conditional likelihoods are difficult to estimate. To alleviate this issue, researchers have approached the causal effect estimation problem by simulating causal relations with neural models. However, none of these existing approaches can be applied to generic scenarios such as causal graphs having latent confounders and obtaining conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph containing latent confounders can be computed through push-forward computations using trained conditional generative models. Based on this observation, we devise a diffusion-based approach to sample from any such interventional or conditional interventional distribution. To showcase our algorithm's performance, we conduct experiments on a semi-synthetic Colored MNIST dataset having both the intervention ($X$) and the target variable ($Y$) as images and present interventional image samples from $P(Y|do(X))$. We also perform a case study on a real-world COVIDx chest X-ray image dataset to demonstrate our algorithm's utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=f3NLRksLiZ": {
    "title": "Reservoir Transformer at Infinite Horizon: the Lyapunov Time and the Butterfly Effect",
    "volume": "review",
    "abstract": "We introduce Reservoir Transformer with non-linear readout, a novel neural network architecture, designed for long-context multi-variable time series prediction. Capable of efficiently modeling arbitrarily input length sequences, our model is powerful in predicting events in the distant future by retaining comprehensive historical data. Our design of a non-linear readout and group reservoirs overcomes the limitations inherent in conventional chaotic behavior prediction techniques, notably those impeded by challenges of prolonged Lyapunov times and the butterfly effect. Our architecture consistently outperforms state-of-the-art deep neural network (DNN) models, including NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an error reduction of up to -89.43% in various fields such as ETTh, ETTm, and air quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=fgKjiVrm6u": {
    "title": "REFACTOR: Learning to Extract Theorems from Proofs",
    "volume": "review",
    "abstract": "Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6\\% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=clU5xWyItb": {
    "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question-answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=1JR20YOE0H": {
    "title": "On Feature Diversity in Energy-based Models",
    "volume": "review",
    "abstract": "Energy-based learning is a powerful learning paradigm that encapsulates various discriminative and generative approaches. An energy-based model (EBM) is typically formed of inner-model(s) that learn a combination of the different features to generate an energy mapping for each input configuration. In this paper, we focus on the diversity of the produced feature set. We extend the probably approximately correct (PAC) theory of EBMs and analyze the effect of redundancy reduction on the performance of EBMs. We derive novel generalization bounds for various learning contexts, i.e., regression, classification, and implicit regression, with different energy functions and we show that indeed reducing redundancy of the feature set can consistently decrease the gap between the true and empirical expectation of the energy and boosts the performance of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Fj7Fzm5lWL": {
    "title": "Let's do the time-warp-attend: Learning topological invariants of dynamical systems",
    "volume": "review",
    "abstract": "Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior called \\textit{bifurcations} when their underlying parameters cross a threshold. Existing methods predict oncoming catastrophes from time-series in individual systems but struggle both to categorize qualitative dynamical regimes across diverse systems and to generalize to real data. To address this challenge, we propose a data-driven, physically-informed deep-learning framework for classifying dynamical regimes and characterizing bifurcation boundaries based on the extraction of topologically invariant features. We focus on the paradigmatic case of the supercritical Hopf bifurcation, which is used to model periodic dynamics across a wide range of applications. Our convolutional attention method is trained with data augmentations that encourage the learning of topological invariants which can be used to detect bifurcation boundaries in unseen systems and to design models of biological systems like oscillatory gene regulatory networks. We further demonstrate our method's use in analyzing real data, recovering distinct proliferation and differentiation dynamics along pancreatic endocrinogenesis trajectory in gene expression space based on single-cell data. Our method provides valuable insights into the qualitative, long-term behavior of a wide range of dynamical systems as well as detect bifurcations or catastrophic transitions in large-scale physical and biological systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LWEqTLCHrw": {
    "title": "Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning",
    "volume": "review",
    "abstract": "Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuring honest execution of defense mechanisms at the server by leveraging a zero-knowledge proof mechanism. We validate the superior performance of the proposed approach with extensive experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ySS7hH1smL": {
    "title": "Sparse MoE with Language Guided Routing for Multilingual Machine Translation",
    "volume": "review",
    "abstract": "Sparse Mixture-of-Experts (SMoE) has gained increasing popularity as a promising framework for scaling up multilingual machine translation (MMT) models with negligible extra computational overheads. However, current SMoE solutions neglect the intrinsic structures of the MMT problem: ($a$) $\\textit{Linguistics Hierarchy.}$ Languages are naturally grouped according to their lingual properties like genetic families, phonological characteristics, etc; ($b$) $\\textit{Language Complexity.}$ The learning difficulties are varied for diverse languages due to their grammar complexity, available resources, etc. Therefore, routing a fixed number of experts (e.g., $1$ or $2$ experts in usual) only at the word level leads to inferior performance. To fill in the missing puzzle, we propose $\\textbf{\\texttt{Lingual-SMoE}}$ by equipping the SMoE with adaptive and linguistic-guided routing policies. Specifically, it ($1$) extracts language representations to incorporate linguistic knowledge and uses them to allocate experts into different groups; ($2$) determines the number of activated experts for each target language in an adaptive and automatic manner, according to their translation difficulties, which aims to mitigate the potential over-/under-fitting issues of learning simple/challenges translations. Sufficient experimental studies on MMT benchmarks with {$16$, $50$, $100$} language pairs and various network architectures, consistently validate the superior performance of our proposals. For instance, $\\texttt{Lingual-SMoE}$ outperforms its dense counterpart by over $5\\%$ BLEU scores on $\\texttt{OPUS-100}$ dataset. Codes are included in the supplement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=UNv8RzIf5x": {
    "title": "Class-Wise Generalization Error: An Information-Theoretic Analysis",
    "volume": "review",
    "abstract": "Existing generalization theories of supervised learning typically take a holistic approach and provide bounds for the expected generalization over the whole data distribution, which implicitly assumes that the model generalizes uniformly for all the classes. In practice, however, there are significant variations in generalization performance among different classes, which cannot be captured by the existing generalization bounds. In this work, we tackle this problem by theoretically studying the class-generalization error, which quantifies the generalization performance of each individual class. We first derive a novel information-theoretic bound for class-generalization error using the KL divergence, and we further obtain several tighter bounds using the conditional mutual information (CMI), which are significantly easier to estimate in practice. We empirically validate our proposed bounds in different neural networks and show that they capture the class-generalization error behavior closely. Moreover, we show that the theoretical tools developed in this paper are useful beyond this context and can be applied in several other applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=zWqr3MQuNs": {
    "title": "Detecting Pretraining Data from Large Language Models",
    "volume": "review",
    "abstract": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method MIN-K PROB based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. MIN-K PROB can be applied without any knowledge about the pretrainig corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that MIN-K PROB achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply MIN-K PROB to two real-world scenarios, copyrighted book detection and contaminated downstream example detection, and find that it to be a consistently effective solution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=V5tdi14ple": {
    "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
    "volume": "review",
    "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code --- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting --- the previously best method to identify correct answers, by more than 12\\% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=lR3rk7ysXz": {
    "title": "On Diffusion Modeling for Anomaly Detection",
    "volume": "review",
    "abstract": "Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Estimation (DTE). DTE estimates the distribution over diffusion time for a given input and uses the mode or mean of this distribution as the anomaly score. We derive an analytical form for this density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods perform competitively for both semi-supervised and unsupervised settings. Notably, DTE achieves orders of magnitude faster inference time than DDPM, while outperforming it on this benchmark. These results establish diffusion-based anomaly detection as a scalable alternative to traditional methods and recent deep-learning techniques for standard unsupervised and semi-supervised anomaly detection settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tvwf4Vsi5F": {
    "title": "Defending Against Transfer Attacks From Public Models",
    "volume": "review",
    "abstract": "Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under this threat model, our defense, PubDef, outperforms the state-of-the-art white-box adversarial training by a large margin with almost no loss in the normal accuracy. For instance, on ImageNet, our defense achieves 62% accuracy under the strongest transfer attack vs only 36% of the best adversarially trained model. Its accuracy when not under attack is only 2% lower than that of an undefended model (78% vs 80%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FwdnG0xR02": {
    "title": "Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic Contrast Sets",
    "volume": "review",
    "abstract": "Vision-language models are growing in popularity and public visibility to generate, edit, and caption images at scale; but their outputs can perpetuate and amplify societal biases learned during pre-training on uncurated image-text pairs from the internet. Although debiasing methods have been proposed, we argue that these measurements of model bias lack validity due to dataset bias. We demonstrate there are spurious correlations in COCO Captions, the most commonly used dataset for evaluating bias, between background context and the gender of people in-situ. This is problematic because commonly-used bias metrics (such as Bias@K) rely on per-gender base rates. To address this issue, we propose a novel dataset debiasing pipeline to augment the COCO dataset with synthetic, gender-balanced contrast sets, where only the gender of the subject is edited and the background is fixed. As existing image editing methods have limitations and sometimes produce low-quality images; we introduce a method to automatically filter the generated images based on their similarity to real images. Using our balanced synthetic contrast sets, we benchmark bias in multiple CLIP-based models, demonstrating how metrics are skewed by imbalance in the original COCO images. Our results indicate that the proposed approach improves the validity of the evaluation, ultimately contributing to more realistic understanding of bias in CLIP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=tjn2YZSHUv": {
    "title": "Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community",
    "volume": "review",
    "abstract": "Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to actively engage and contribute with content to accumulate peers approval. In the realm of text-conditioned image syn- thesis, the recent surge in progress has ushered in a collaborative era where users and AI systems coalesce to refine visual creations. This co-creative process in the landscape of online social networks empowers users to craft original visual art- works seeking for community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Ex- isting evaluation methods predominantly center on limited size user studies guided by image quality and alignment with prompts. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from an anonymous online visual creation and editing platform (referred to as Platform A in this paper), yielding the first million-user- scale dataset of implicit human preferences for user-generated visual art named PA Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=KZZbdJ4wff": {
    "title": "PRO: Pseudo-label Regularized Optimization on Unlabeled Test Data",
    "volume": "review",
    "abstract": "Web-scale foundation models like CLIP have impressive zero-shot capabilities on many downstream classification tasks, but they still underperform target domain-specific supervised classifiers. This inspired researchers to investigate adaptation strategies that take advantage of unlabeled data, often via pseudolabeling. However, previous methods for adaptation can be difficult to train; poor hyperparameter choices can result in catastrophic collapses in accuracy, and absent target labels, there is little to guide the search with. In this paper, we propose Pseudo-label Regularized Optimization (PRO), which addresses the collapses in test-time adaptation without any label peeking for hyperparameter tuning. On the 18 datasets addressed in our experiments PRO improves the accuracy of ViT-B-32 by 2.5\\% on average and in the best case by 6.1\\% from tuning the textual encoder. Our code is available at \\url{https://github.com/anonWAEWA/PRO}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=bDcaz87WCZ": {
    "title": "Recent Link Classification on Temporal Graphs Using Profile Builder",
    "volume": "review",
    "abstract": "The performance of Temporal Graph Learning (TGL) methods are typically evaluated on the future link prediction task, i.e., whether two nodes will get connected and dynamic node classification task, i.e., whether a node's class will change. Comparatively, recent link classification is investigated much less even though it exists in many industrial settings. In this work, we first formalize recent link classification on temporal graphs as a benchmark downstream task and introduce corresponding benchmark datasets. Secondly, we evaluate the performance of state-of-the-art methods with a statistically meaningful metric Matthews Correlation Coefficient, which is more robust to imbalanced datasets, in addition to the commonly used average precision and area under the curve, and propose several design principles for tailoring models to specific requirements of the task and the dataset. We explore modifications on message aggregation schema, readout layer and time encoding strategy which obtain significant improvement on benchmark datasets. Finally, we propose an architecture that we call Graph Profiler, which is capable of encoding previous events' class information on source and destination nodes. The experiments show that our proposed model achieves an improved Matthews Correlation Coefficient on most cases under interest. We believe the introduction of recent link classification as a benchmark task for temporal graph learning will be useful for the evaluation of prospective methods within the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=v3K5TVP8kZ": {
    "title": "AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ",
    "volume": "review",
    "abstract": "Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of TikZ, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce DaTikZ the first large-scale TikZ dataset, consisting of 120k TikZ drawings aligned with captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which augments LLaMA with multimodal CLIP embeddings. In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures, with CLiMA additionally improving text-image alignment. Our detailed analysis shows that all models generalize well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend to generate more simplistic figures compared to both humans and our models. We make our framework, AutomaTikZ, along with model weights and datasets, publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=3xDaj4pRna": {
    "title": "Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning",
    "volume": "review",
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising alternative to stochastic gradient descent (SGD) for minimizing the loss objective in neural network training. While the motivation behind SAM is to bias models towards flatter minima that are believed to generalize better, recent studies have shown conflicting evidence on the relationship between flatness and (in-distribution) generalization, leaving the mechanism behind SAM's performance improvement unclear. In this work, we present a complementary effect that cannot be explained by in-distribution improvements alone: we argue that SAM can enhance the quality of features in datasets containing redundant or spurious features. We explain how SAM can induce feature diversity by investigating a controlled setting. Our results imply that one mechanism by which SAM improves the quality of features is by adaptively suppressing well-learned features which gives remaining features opportunity to be learned",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=9rzEPbs4Wg": {
    "title": "Improving Generalization and Safety of Deep Neural Networks with Masked Anchoring",
    "volume": "review",
    "abstract": "Anchoring is a recent architecture and task-agnostic technique that can produce state-of-the-art epistemic uncertainty estimates, and improve extrapolation capabilities. However, the differences between anchored models and non-anchored variants is not well studied -- as there is little insight into the kinds of functions anchoring induces and how they behave under distribution shifts. In this paper, we analyze and improve anchoring as a training protocol for deep neural networks, evaluating them on important tasks of out of distribution generalization, task adaptation, anomaly detection and calibration. We pinpoint the impact of anchoring on generalization as being inversely related to the sensitivity of the model to the distribution of residuals. We further improve this sensitivity using a new technique called Random Anchor Masking (RAM) that significantly improves the quality of anchored models. We build evidence for the superiority of RAM-training using a range of benchmarks of varying size, using neural networks of varying complexity and scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ccxD4mtkTU": {
    "title": "Can LLM-Generated Misinformation Be Detected?",
    "volume": "review",
    "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=z8q8kBxC5H": {
    "title": "Sharp results for NIEP and NMF",
    "volume": "review",
    "abstract": "The orthodox Non-negative Inverse Eigenvalue Problem (oNIEP) has challenged mathematicians for over $70$ years. Motivated by applications in non-negative matrix factorization (NMF) and network modeling, we consider an NIEP as follows. Consider a $K \\times K$ diagonal matrix $J_{K, m} = \\diag(1 + a_{K, m}, 1, \\ldots, 1, -1, \\ldots, -1)$, where exactly $m$ entries are $-1$ and $a_{K, m} = \\max\\{0, (2m-K)\\}$. We wish to determine for which $(K, m)$, there is a $K \\times K$ orthogonal matrix $Q$ such that $Q J_{K, m} Q'$ is doubly stochastic. Using several approaches (especially a combined Haar and Discrete Fourier Transform (DFT) approach) we developed, we show that in most of the cases, the NIEP is solvable. We show that these results are sharp. Also, since these are construction approaches, they automatically provide an explicit way for computing matrix $Q$. As a result, these approaches give rise to both a computable NMF algorithm and sharp results for NMF. We also discuss the implication of our results for social network modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bkdWThqE6q": {
    "title": "A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis",
    "volume": "review",
    "abstract": "We present a novel usage of Transformers to make image classification interpretable. Unlike mainstream classifiers that wait until the last fully-connected layer to incorporate class information to make predictions, we investigate a proactive approach, asking each class to search for itself in an image. We realize this idea via a Transformer encoder-decoder inspired by DEtection TRansformer (DETR). We learn \"class-specific'' queries (one for each class) as input to the decoder, enabling each class to localize its patterns in an image via cross-attention. We name our approach INterpretable TRansformer (INTR), which is fairly easy to implement and exhibits several compelling properties. We show that INTR intrinsically encourages each class to attend distinctively; the cross-attention weights thus provide a faithful interpretation of the prediction. Interestingly, via \"multi-head'' cross-attention, INTR could identify different \"attributes'' of a class, making it particularly suitable for fine-grained classification and analysis, which we demonstrate on eight datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=rwmWd2rjP1": {
    "title": "Molecule Relaxation by Reverse Diffusion with Time Step Prediction",
    "volume": "review",
    "abstract": "Molecule relaxation---finding the stable state of an unstable configuration---is an important subtask for exploring the chemical compound space, for instance, to identify novel drugs or catalysts. Existing methods rely on local energy minimization with the gradients (i.e., force field) estimated through computationally intensive ab initio methods or approximated by a neural network trained on large expensive datasets encompassing \\emph{labeled stable and unstable} molecules. In this work, we propose molecule relaxation by reverse diffusion (MoreRed), a novel purely statistical approach where unstable molecules are seen as \\emph{noisy} samples to be denoised by a diffusion model equipped with a time step predictor to handle arbitrarily noisy inputs. Notably, MoreRed learns a simpler pseudo energy surface instead of the complex physical energy surface and is trained on a significantly smaller dataset consisting of solely \\emph{unlabeled stable} molecules, which is considerably less expensive to generate. Nevertheless, our experiments demonstrate its competitive performance to the state-of-the-art baseline in terms of the quality of the relaxed molecules inferred. Furthermore, we identify the high potential that time step prediction has to enhance the performance of data generation, where our findings are promising both in molecular structure and image generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2VAi5F9BOJ": {
    "title": "PLPP: PROMPT LEARNING WITH PERPLEXITY FOR VISION-LANGUAGE MODELS",
    "volume": "review",
    "abstract": "Pre-trained vision-language (VL) models such as CLIP have demonstrated their excellent performance across numerous downstream tasks. A recent method, called Context Optimization (CoOp), further improves the performance of CLIP on downstream tasks by introducing prompt learning. CoOp optimizes a set of learnable vectors, aka prompt and freezes the whole CLIP model, instead of using manually crafted templates (e.g., a template ``a photo of a \\{category\\}'') to fine-tune the CLIP model. Nonetheless, we observed that the resulting prompts are always incomprehensible, which is counter-intuitive, and existing CoOp-based methods overlook this issue. As the first work aiming at learning comprehensible prompts, this paper proposes to use Perplexity to supervise the process of prompt learning in the CoOp framework. Perplexity is a metric to evaluate the quality of a language model (LM) in Natural Language Processing field, and we design a two-step operation to compute the perplexity for prompts. The first step is a calculation of cosine similarity to obtain the labels of vectors, and the second step is a training-free LM Head to output word probability distribution. Our proposed method, i.e., \\textbf{P}rompt \\textbf{L}earning with \\textbf{P}er\\textbf{P}lexity (PLPP), can be integrated in any CoOp-based method and the experiments show that the learned prompts are much more comprehensible compared with the original and an improved CoOp methods, without sacrificing model accuracy. Codes are available at \\href{https://github.com}{https://github.com}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Z91rwXnJsw": {
    "title": "Interactive Semantic Map Representation for Skill-based Visual Object Navigation",
    "volume": "review",
    "abstract": "Visual object navigation using learning methods is one of the key tasks in mobile robotics. This paper introduces a new representation of a scene semantic map formed during the embodied agent interaction with the indoor environment. It is based on a neural network method that adjusts the weights of the segmentation model with backpropagation of the predicted fusion loss values during inference on a regular (backward) or delayed (forward) image sequence. We have implemented this representation into a full-fledged navigation approach called SkillTron, which can select robot skills from end-to-end policies based on reinforcement learning and classic map-based planning methods. The proposed approach makes it possible to form both intermediate goals for robot exploration and the final goal for object navigation. We conducted intensive experiments with the proposed approach in the Habitat environment, which showed a significant superiority in navigation quality metrics compared to state-of-the-art approaches. The developed code and used custom datasets will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTEGyKf0dZ": {
    "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
    "volume": "review",
    "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs. (This paper contains red-teaming data and model-generated content that can be offensive in nature.)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRoBig6ov1": {
    "title": "High-Order Tensor Recovery with A Tensor $U_1$ Norm",
    "volume": "review",
    "abstract": "Recently, numerous tensor SVD (t-SVD)-based tensor recovery methods have emerged, showing promise in processing visual data. However, these methods often suffer from performance degradation when confronted with high-order tensor data exhibiting non-smooth changes (possibly caused by random slice permutation), commonly observed in real-world scenarios but ignored by the traditional t-SVD-based methods. Our objective in this study is to provide an effective tensor recovery technique for handling non-smooth changes in tensor data and efficiently exploring the correlations of high-order tensor data across its various dimensions. To this end, we introduce a new tensor decomposition and a new tensor norm called the Tensor U1 norm. An optimization algorithm is proposed to solve the resulting tensor completion model iteratively by combining the proximal algorithm with the Alternating Direction Method of Multipliers. Theoretical analysis showed the convergence of the algorithm to the Karush–Kuhn–Tucker (KKT) point of the optimization problem. Numerical experiments demonstrated the effectiveness of the proposed method in high-order tensor completion, especially for tensor data with non-smooth changes. This study fills a critical gap in the t-SVD-based tensor recovery by providing a practical and effective solution that enables the exploration of correlations in high-order tensor data across its different dimensions, even in the presence of non-smooth changes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=EDXkkUAIFW": {
    "title": "One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models",
    "volume": "review",
    "abstract": "Active learning (AL) for multiple target models aims to reduce labeled data querying while effectively training multiple models concurrently. Existing AL algorithms often rely on iterative model training, which can be computationally expensive, particularly for deep models. In this paper, we propose a one-shot AL method to address this challenge, which performs all label queries without repeated model training. Specifically, we extract different representations of the same dataset using distinct network backbones, and actively learn linear prediction layer on each representation via an $\\ell_p$-regression formulation. The regression problems are solved approximately by sampling and reweighting the unlabeled instances based on their maximum Lewis weights across the representations. An upper bound on the number of samples needed is provided with a rigorous analysis for $p\\in (0, +\\infty)$. Notably, in the case of $p=2$, our result substantially improves the bound of applying (Gajjar et al., 2023) to our setting. Experimental results on 8 benchmarks show that our one-shot approach achieves competitive performances with the state-of-the-art AL methods for multiple target models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=iI7hZSczxE": {
    "title": "Disentangling Time Series Representations via Contrastive based $l$-Variational Inference",
    "volume": "review",
    "abstract": "Learning disentangled representations is crucial for Time Series, offering benefits like feature derivation and improved interpretability, thereby enhancing task performance. We focus on disentangled representation learning for home appliance electricity usage, enabling users to understand and optimize their consumption for a reduced carbon footprint. Our approach frames the problem as disentangling each attribute's role in total consumption (e.g., dishwashers, fridges, \\dots). Unlike existing methods assuming attribute independence, we acknowledge real-world time series attribute correlations, like the operating of dishwashers and washing machines during the winter season. To tackle this, we employ weakly supervised contrastive disentanglement, facilitating representation generalization across diverse correlated scenarios and new households. Our method utilizes innovative $l$-variational inference layers with self-attention, effectively addressing temporal dependencies across bottom-up and top-down networks. We find that DisCo (Disentangling via Contrastive) can enhance the task of reconstructing electricity consumption for individual appliances. We introduce TDS (Time Disentangling Score) to gauge disentanglement quality. TDS reliably reflects disentanglement performance, making it a valuable metric for evaluating time series representations. Code available at https://anonymous.4open.science/r/DisCo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Aj1wftldeR": {
    "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "Offline reinforcement learning algorithms hold the promise of enabling data-driven RL methods that do not require costly or dangerous real-world exploration and benefit from large pre-collected datasets. This in turn can facilitate real-world applications, as well as a more standardized approach to RL research. Furthermore, offline RL methods can provide effective initializations for online finetuning, overcoming challenges with exploration. However, evaluating progress on offline RL algorithms requires effective and challenging benchmarks that capture properties of real-world tasks, provide a range of task difficulties, and cover a range of challenges both in terms of the parameters of the domain (e.g., length of the horizon, sparsity of rewards) and the parameters of the data (e.g., narrow demonstration data or broad exploratory data). While considerable progress in offline RL in recent years has been enabled by simpler benchmark tasks, the most widely used datasets are increasingly saturating in performance and might fail to reflect properties of realistic tasks. We propose a new benchmark for offline RL that focuses on realistic simulations of robotic manipulation and locomotion environments, based on models of real-world robotic systems, and comprising a variety of data sources, including scripted data, over 20 hours of demonstrations and play-style data collected by human teleoperators, and other data sources. Our proposed benchmark covers state-based and image-based domains, and aims to test a number of real-world robot training challenges such as long-horizon manipulation, fine-grained motor control, imperfect controllers, and representation learning. Our proposed tasks vary in complexity from single instance to diverse scenarios with multiple distribution shifts, which can require significant robustness and generalization. Moreover, we support both offline RL evaluation and evaluation with online finetuning, with some of the tasks specifically designed to require both pretraining and finetuning. We hope that our proposed benchmark will facilitate further progress on both offline RL algorithms and algorithms designed for online finetuning from offline initialization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=eP6ZSy5uRj": {
    "title": "Endowing Protein Language Models with Structural Knowledge",
    "volume": "review",
    "abstract": "Protein language models have shown strong performance in predicting function and structure across diverse tasks. These models undergo unsupervised pretraining on vast sequence databases to generate rich protein representations, followed by finetuning with labeled data on specific downstream tasks. The recent surge in computationally predicted protein structures opens new opportunities in protein representation learning. In our study, we introduce a novel framework to enhance transformer protein language models specifically on protein structures. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules. This refined model, termed the Protein Structure Transformer (PST), is further pretrained on a protein structure database such as AlphaFoldDB, using the same masked language modeling objective as traditional protein language models. Our empirical findings show superior performance on several benchmark datasets. Notably, PST consistently outperforms the foundation model for protein sequences, ESM-2, upon which it is built. Our code and pretrained models will be released upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=GlpawHh80l": {
    "title": "Improved algorithm and bounds for successive projection",
    "volume": "review",
    "abstract": "Consider a $K$-vertex simplex in a $d$-dimensional space. We measure $n$ points on the simplex, but due to the measurement noise, some of the observed points fall outside the simplex. The interest is vertex hunting (i.e., estimating the vertices of the simplex). The successive projection algorithm (SPA) is one of the most popular approaches to vertex hunting, but it is vulnerable to noise and outliers, and may perform unsatisfactorily. We propose pseudo-point SPA (pp-SPA) as a new approach to vertex hunting. The approach contains two novel ideas (a projection step and a denoise step) and generates roughly $n$ pseudo-points, which can be fed in to SPA for vertex hunting. For theory, we first derive an improved non-asymptotic bound for the orthodox SPA, and then use the result to derive the bounds for pp-SPA. Compared with the orthodox SPA, pp-SPA has a faster rate and more satisfactory numerical performance in a broad setting. The analysis is quite delicate: the non-asymptotic bound is hard to derive, and we need precise results on the extreme values of (possibly) high-dimensional random vectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=AfnsTnYphT": {
    "title": "Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs",
    "volume": "review",
    "abstract": "Vision-based tasks are known to exhibit the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts at quantifying the statistical benefits of these biases in CNNs over local convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either do not establish a gap between the performance of these architectures, or ignore optimization considerations, or consider stylized settings that are not reflective of image-like tasks, particularly translation invariance. We introduce the Dynamic Signal Distribution (DSD), a data model that is designed to capture properties of real-world images such as locality and translation invariance. In DSD, each image is modeled with $k$ patches, with each patch of dimension $d$, and the label is determined by a $d$-sparse signal vector that can freely appear in any one of the $k$ patches. Under this task, we show that CNNs trained using gradient descent require $\\tilde{O}(k+d)$ samples, whereas LCNs require $\\Omega(kd)$ samples for predicting the label, establishing the statistical advantages of weight sharing in translation invariant tasks. Additionally, LCNs need $\\tilde{O}(k(k+d))$ samples, compared to FCNs, which need $\\Omega(k^2d)$ samples, showcasing the benefits of locality in local tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=MtzHEqqUm0": {
    "title": "In-Depth Comparison of Regularization Methods For Long-Tailed Learning in Trajectory Prediction",
    "volume": "review",
    "abstract": "Autonomous robots have the biggest potential for risk because they operate in open-ended environments where humans interact in complex, diverse ways. To operate, such systems must predict this behaviour, especially if it's part of the unexpected and potentially dangerous long tail of the dataset. Previous works on long-tailed trajectory prediction use models which do not predict a distribution of trajectories with likelihoods associated with each prediction. Furthermore, they report metrics which are biased by the ground-truth. Therefore, we aim to examine regularization methods for long-tailed trajectory prediction by comparing them on the KDE metric, which is designed to compare distributions of trajectories. Moreover, we are the first to report the performance of these methods on both the pedestrian and vehicle classes of the NuScenes dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=OMwD6pGYB4": {
    "title": "A Distributional Analogue to the Successor Representation",
    "volume": "review",
    "abstract": "This paper contributes a new approach for distributional reinforcement learning which allows for a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences from behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We model the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Extending γ-models (Janner et al., 2020), we propose an algorithm that learns the distributional SM from samples by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable in the context of learning generative models of state. As an illustration of the practical usefulness of the distributional successor measure, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=zLwCT9srfo": {
    "title": "H-Rockmate: Hierarchical Approach for Efficient Re-materialization of Large Neural Networks",
    "volume": "review",
    "abstract": "Training modern neural networks poses a significant memory challenge, as storing intermediate results during the forward and backward passes demands substantial memory resources. To address this issue while maintaining model accuracy, re-materialization techniques have been introduced to recompute selected intermediate results rather than storing them, thereby adhering to peak memory constraints. The main algorithmic problem is to compute a re-materialization schedule that minimizes the computational overhead within a given memory budget. Our H-Rockmate framework builds upon an existing Rockmate solution and overcomes its limitation to work with sequential block structures by proposing a hierarchical approach. The framework performs an automatic decomposition of the data-flow graph into a hierarchy of small-scale subgraphs, and finds a re-materialization schedule for the whole graph by recursively solving optimization problems for each subgraph. H-Rockmate allows users to transform their PyTorch models into nn.Modules that execute forward and backward passes efficiently within the specified memory budget. This framework can handle neural networks with diverse data-flow graph structures, including U-Nets and encoder-decoder Transformers. H-Rockmate outperforms existing re-materialization approaches in terms of average training iteration time and peak memory trade-offs, demonstrating superior memory efficiency in training modern neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0TZs6WOs16": {
    "title": "Hyperbolic Embeddings in Sequential Self-Attention for Improved Next-Item Recommendations",
    "volume": "review",
    "abstract": "In recent years, self-attentive sequential learning models have surpassed conventional collaborative filtering techniques in next-item recommendation tasks. However, Euclidean geometry utilized in these models may not be optimal for capturing a complex structure of the behavioral data. Building on recent advances in the application of hyperbolic geometry to collaborative filtering tasks, we propose a novel approach that leverages hyperbolic geometry in the sequential learning setting. Our approach involves transitioning the learned parameters to a Poincar\\'e ball, which enables a linear predictor in a non-linear space. Our experimental results demonstrate that under certain conditions hyperbolic models may simultaneously improve recommendation quality and gain representational capacity. We identify several determining factors that affect the results, which include the ability of a loss function to preserve hyperbolic structure and the general compatibility of data with hyperbolic geometry. For the latter, we propose an empirical approach based on Gromov delta-hyperbolicity estimation that allows categorizing datasets as either compatible or not",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=0tWTxYYPnW": {
    "title": "Understanding Hidden Context in Preference Learning: Consequences for RLHF",
    "volume": "review",
    "abstract": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called *Borda count*. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called *distributional preference learning* (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=kvByNnMERu": {
    "title": "Estimating Shape Distances on Neural Representations with Limited Samples",
    "volume": "review",
    "abstract": "Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance—a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a novel method-of-moments estimator with a tunable bias-variance tradeoff parameterized by an upper bound on bias. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Our theoretical work and estimator thus respectively define and dramatically expand the scope of neural data for which geometric similarity can be accurately measured",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=OqlmgmS4Wr": {
    "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
    "volume": "review",
    "abstract": "Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentDataset, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentDataset with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLlama. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLlama-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentDataset and AgentLlama-7B, 13B, and 70B models at https://anonymous.4open.science/r/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=NTWtNjlThd": {
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "volume": "review",
    "abstract": "Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal that our approach achieves the desired disentanglement while also numerically improving baseline performance in most cases. In addition, we show that our method can generate novel textures for a specific object or transfer textures between objects with distinct shapes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=TwB6N055Ub": {
    "title": "Reduced-Rank Online Gaussian Process Modeling With Uncertain Inputs",
    "volume": "review",
    "abstract": "Gaussian Process (GP) is an increasingly popular modeling approach. In its classical formulation, the inputs are supposed to be perfectly known. However, in some use cases, this assumption is not true: the inputs as well as the outputs can be corrupted by noise. Some methods already insert these uncertainties in GP modeling but the only currently existing online algorithm (i.e. that incrementally updates the model each time a measure is acquired) still lacks in robustness and precision. In this article we propose a novel online Gaussian Process (GP) modeling approach for vector field mapping with uncertain inputs. They are included into the GP through a complete second-order Taylor approximation with a better estimation of variances. Our experiments prove that our algorithm is more accurate and robust than the previous online method for a shorter computing time. Moreover, for high input uncertainties, our method achieves better performance than both online and offline state of the art methods on simulated data. This algorithm can also be applied to diverse real scenarios which require precise estimation of unknown functions from a small set of corrupted datapoints, as we show in the challenging problem of indoor localization, mapping magnetic fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMv6zKYYUs": {
    "title": "Learning semilinear neural operators: A unified recursive framework for prediction and data assimilation",
    "volume": "review",
    "abstract": "Recent advances in the theory of Neural Operators (NOs) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations (PDEs). Despite their great success, current NO-based solutions face important challenges when dealing with spatio-temporal PDEs over long time scales. Specifically, the current theory of NOs does not present a systematic framework to perform data assimilation and efficiently correct the evolution of PDE solutions over time based on sparsely sampled noisy measurements. In this paper, we propose a learning-based state-space approach to compute the solution operators to infinite-dimensional semilinear PDEs. Exploiting the structure of semilinear PDEs and the theory of nonlinear observers in function spaces, we develop a flexible recursive method that allows for both prediction and data assimilation by combining prediction and correction operations. The proposed framework is capable of producing fast and accurate predictions over long time horizons, dealing with irregularly sampled noisy measurements to correct the solution, and benefits from the decoupling between the spatial and temporal dynamics of this class of PDEs. We show through experiments on the Kuramoto-Sivashinsky, Navier-Stokes and Korteweg-de Vries equations that the proposed model is robust to noise and can leverage arbitrary amounts of measurements to correct its prediction over a long time horizon with little computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Wsab3NhIwC": {
    "title": "Resource Efficient Self-Supervised Learning for Speech Embeddings",
    "volume": "review",
    "abstract": "Representation learning from sequential data using self-supervised learning (SSL) has proven to be a powerful technique and improved state-of-the-art (SOTA) results when fine-tuned for various downstream tasks. So far the success of SSL frameworks, e.g., Wav2Vec2 and Data2Vec2, for learning audio embeddings is primarily carried out by masking intermediate features and then solving a contrastive or non-contrastive task in an end-to-end manner, respectively. In comparison to contrastive SSL methods such as Wav2Vec2, non-contrastive techniques such as Data2Vec2 have emerged having better model quality and training time. However, Data2Vec2 is still quite demanding in terms of resources, namely infrastructure (more and better GPUs), which remains a significant barrier to further improving models for downstream tasks. In this work we show that non-contrastive learning, such as an extension of the Barlow--Twins methodology, when applied to a range of downstream tasks simultaneously decreases training time and resource requirements while maintaining or improving SOTA results in key benchmark datasets. From a computional point of view, our approach decreases Data2Vec2 training time by $2\\times$ and permits effective training with smaller sequence lengths and batch sizes without requiring gradient accumulation reducing GPU VRAM requirements from NVIDIA A100's to V100's",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vkgAaCI3F": {
    "title": "Balancing Stability and Plasticity in Continual Learning: the readout-decomposition of activation change (RDAC) framework",
    "volume": "review",
    "abstract": "Continual learning (CL) algorithms strive to equip neural networks with the ability to acquire new knowledge while preserving prior information. However, the stability-plasticity trade-off remains a central challenge in CL. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It presents the Readout-Decomposition of Activation Change (RDAC) framework that relates learning-induced activation changes in the range of prior readouts to the degree of stability, and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework was used to explain the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF) and replay based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay excelled in preserving both stability and plasticity, while SI, EWC, and LwF traded off plasticity for stability. The inability of the regularization algorithms to maintain plasticity was linked to them restricting the change of activations in the null space of the prior readout. For one-hidden-layer linear neural networks, we additionally derived a gradient decomposition algorithm to restrict activation change only in the range of the prior readouts, to maintain high stability while not further sacrificing plasticity. Results demonstrate that the algorithm maintains stability without significant plasticity loss. The RDAC framework not only informs the behavior of existing CL algorithms but also paves the way for novel CL approaches. Finally, it sheds light on the connection between learning-induced activation/representation changes and the stability-plasticity dilemma, also offering insights into representational drift in biological systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=HOf3K763zg": {
    "title": "Beyond Differentiability: Neurosymbolic Learning with Black-Box Programs",
    "volume": "review",
    "abstract": "Neurosymbolic learning has demonstrated promising potential as a paradigm to combine the worlds of classical algorithms and deep learning. However, existing general neurosymbolic frameworks require that programs be written in differentiable logic programming languages, restricting their applicability to a small fragment of algorithms. We introduce Infer-Sample-Estimate-Descend (ISED), a general algorithm for neurosymbolic learning with black-box programs. We evaluate ISED extensively on a set of 30 benchmark tasks that encompass rich data types and reasoning patterns. ISED achieves 30% higher accuracy than end-to-end neural baselines. Moreover, ISED's solutions often outperform those obtained using Scallop, a state-of-the-art neurosymbolic framework: the programs in 17 (61%) of the benchmarks cannot be specified using Scallop, and ISED on average achieves higher accuracy on those that can be specified using Scallop",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Kfpaq5CJPy": {
    "title": "Leveraging image representations for bounded adversarial attacks and robustness",
    "volume": "review",
    "abstract": "Both classical and learned image transformations such as the discrete wavelet transforms (DWTs) and flow-based generative models provide semantically meaningful representations of images. In this paper, we propose a general method for robustness exploiting the expressiveness of image representations by targeting substantially low-dimensional subspaces inside the $L^\\infty$ box. Experiments with DCT, DWTs and Glow produce adversarial examples that are significantly more similar to the original than those found considering the full $L^\\infty$ box. Further, through adversarial training we show that robustness under the introduced constraints transfers better to robustness against a broad class of common image perturbations compared to the standard $L^\\infty$ box, without a major sacrifice of natural accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=PH0L3ABwM2": {
    "title": "SEER: Towards Efficient Preference-based Reinforcement Learning via Aligned Experience Estimation",
    "volume": "review",
    "abstract": "One of challenge in reinforcement learning lies in the meticulous design of a reward function that quantifies the quality of each decision as a scalar value. Preference-based reinforcement learning (PbRL) provides an alternative approach, avoiding reward engineering by learning rewards based on human preferences among various trajectories. PbRL involves sampling informative trajectories, learning rewards from preferences, optimizing policy with learned rewards, and subsequently generating higher-quality trajectories for the next iteration, thereby creating a virtuous circle. Distinct problems lie in effective reward learning and aligning the policy with human preferences, both of which are essential for achieving efficient learning. Motivated by these considerations, we propose an efficient preference-based RL method, dubbed SEER. We leverage state-action pairs that are well-supported in the current replay memory to bootstrap an empirical Q function ($\\widehat{Q}$), which is aligned with human preference. The empirical Q function helps SEER to sample more informative pairs for effective querying, and regularizes the neural Q function ($Q_\\theta$) thus leading to a policy which is more consistent with human intent. Theoretically, we show that the empirical Q function is a lower-bound of the oracle Q under human preference. Our experimental results over several tasks demonstrate that the empirical Q function is beneficial for preference-based RL to learn a more aligned Q function, outperforming state-of-the-art methods by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6werMQy1uz": {
    "title": "Rethinking the Buyer's Inspection Paradox in Information Markets with Language Agents",
    "volume": "review",
    "abstract": "This work addresses the long-standing buyer's inspection paradox for information markets. The paradox is that buyers need to access information to determine its value, while sellers need to limit access to prevent theft. To study this, we introduce an open-source simulated digital marketplace where intelligent agents, powered by language models, buy and sell information on behalf of external participants. The central mechanism enabling this marketplace is the agents' dual capabilities: they not only have the capacity to assess the quality of privileged information but also come equipped with the ability to forget. This feature allows vendors to grant temporary access to proprietary information, significantly reducing the risk of unauthorized retention while enabling agents to accurately gauge the information's relevance to specific queries or tasks. To perform well, agents must make rational decisions, strategically explore the marketplace through generated sub-queries, and synthesize answers from purchased information. Concretely, our experiments (a) uncover biases in language models leading to irrational behavior and evaluate techniques to mitigate these biases, (b) investigate how price affects demand in the context of informational goods, and (c) show that inspection and higher budgets both lead to higher quality outcomes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9v5uZPWZoV": {
    "title": "Not Just Pretty Pictures: Toward Interventional Data Augmentation Using Text-to-Image Generators",
    "volume": "review",
    "abstract": "Neural image classifiers are known to undergo severe performance degradation when exposed to inputs that exhibit covariate shift with respect to the training distribution. A general interventional data augmentation (IDA) mechanism that simulates arbitrary interventions over spurious variables has often been conjectured as a theoretical solution to this problem and approximated to varying degrees of success. In this work, we study how well modern Text-to-Image (T2I) generators and associated image editing techniques can solve the problem of IDA. We experiment across a diverse collection of benchmarks in domain generalization, ablating across key dimensions of T2I generation, including interventional prompts, conditioning mechanisms, and post-hoc filtering, showing that it substantially outperforms previously state-of-the-art image augmentation techniques independently of how each dimension is configured. We discuss the comparative advantages of using T2I for image editing versus synthesis, also finding that a simple retrieval baseline presents a surprisingly effective alternative, which raises interesting questions about how generative models should be evaluated in the context of domain generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IrZTJ7t2GW": {
    "title": "Fair Adversarial Training: on the Adversarial Attack and Defense of Fairness",
    "volume": "review",
    "abstract": "While numerous work has been proposed to address fairness in machine learning, existing methods do not guarantee fair predictions under imperceptible adversarial feature perturbation, and a seemingly fair model can suffer from large group-wise disparities under such perturbation. Moreover, while adversarial training has been shown to be reliable in improving a model's robustness to defend against adversarial feature perturbation that deteriorates accuracy, it has not been properly studied in the context of adversarial perturbation against fairness. To tackle these challenges, in this paper, we study the problem of adversarial attack and adversarial robustness w.r.t. two terms: fairness and accuracy. From the adversarial attack perspective, we propose a unified structure for adversarial attacks against fairness which brings together common notions in group fairness, and we theoretically prove the equivalence of adversarial attacks against different fairness notions. Further, we derive the connections between adversarial attacks against fairness and those against accuracy. From the adversarial robustness perspective, we theoretically align robustness to adversarial attacks against fairness and accuracy, where robustness w.r.t. one term enhances robustness w.r.t. the other term. Our study suggests a novel way to unify adversarial training w.r.t. fairness and accuracy, and experiments show our proposed method achieves better robustness w.r.t. both terms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjYNFZEjc7": {
    "title": "Head Information Bottleneck: An Evaluation Method for Transformer Head Contributions in Speech Task",
    "volume": "review",
    "abstract": "Multi-head attention mechanisms have been widely applied in speech pre-training. However, their roles and effectiveness in various downstream tasks have not been fully studied. Different attention heads may exhibit varying degrees of importance in different downstream tasks. We noticed that the attention allocation in the attention mechanism is similar to the information bottleneck, aiming to highlight the parts important to the task. Therefore, we introduced the information bottleneck into multi-head attention to estimate the degree of mutual information contained in each attention head's output about the input and forced it to focus on useful information. Additionally, we proposed a method to measure the contribution of attention heads in tasks. We also pruned the model heads according to their contributions, providing an interpretable direction for model pruning. Notably, our method can maintain an accuracy of 83.36% on the KS task while pruning 40% of the heads",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=IEduRUO55F": {
    "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=s90VIdza2K": {
    "title": "f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization",
    "volume": "review",
    "abstract": "Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence. While numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these methods are not amenable to stochastic optimization due to the complex and nonlinear structure of constraints and regularizers. Here, the term \"stochastic\" refers to the ability of the algorithm to work with small mini-batches of data. Motivated by the limitation of existing literature, this paper presents a unified stochastic optimization framework for fair empirical risk minimization based on $f$-divergence measures ($f$-FERM). The proposed stochastic algorithm enjoys theoretical convergence guarantees. In addition, our experiments demonstrate the superiority of fairness-accuracy tradeoffs offered by $f$-FERM for almost all batch sizes (ranging from full-batch to batch size of one). Moreover, we show that our framework can be extended to the case where there is a distribution shift from training to the test data. Our extension is based on a distributionally robust optimization reformulation of $f$-FERM objective under $\\ell_p$ norms as uncertainty sets. Again in this distributionally robust setting, $f$-FERM not only enjoys theoretical convergence guarantees but also outperforms other baselines in the literature in the tasks involving distribution shifts. An efficient stochastic implementation of $f$-FERM is publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7suavRDxe8": {
    "title": "Plausibly Deniable Encryption with Large Language Models",
    "volume": "review",
    "abstract": "We present a novel approach for achieving plausible deniability in cryptography by harnessing the power of large language models (LLMs) in conjunction with conventional encryption algorithms. Leveraging the inherent statistical properties of LLMs, we design an encryption scheme that allows the same ciphertext to be decrypted with any key, while still yielding a plausible message. Unlike established methods, our approach neither relies on a fixed set of decoy keys or messages nor introduces redundancy. Our method is founded on the observation that language models can be used as encoders to compress a low-entropy signal (such as natural language) into a stream indistinguishable from noise, and similarly, that sampling from the model is equivalent to decoding a stream of noise. When such a stream is encrypted and subsequently decrypted with an incorrect key, it will lead to a sampling behavior and will thus generate a plausible message. Through a series of experiments, we substantiate the resilience of our approach against various statistical detection techniques. Finally, although we mainly focus on language models, we establish the applicability of our approach to a broader set of generative models and domains, including images and audio",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=IORAqe04sO": {
    "title": "The crossover strategy based on the cellular automata for genetic Algorithms with binary chromosomes population",
    "volume": "review",
    "abstract": "In this paper we propose a crossover operator for genetic algorithms with binary chromosomes population based on the cellular automata (CGACell). After presenting the fundamental elements regarding cellular automata with specific examples for one- and two- dimensional cases, the the most widely used crossover operators in applications with genetic algorithms are described and the crossover operator based on cellular automata is defined. Specific forms of the crossover operator based on the ECA and 2D CA cases are described and exemplified. The CGACell crossover operator is used in the genetic structure to improved the KNN algorithm in terms of the parameter represented by the number of nearest neighbors selected by the data classification method. Validity and practical performance testing is performed on image data classification problems by optimizing the nearest-neighbors-based algorithm. The experimental study on the proposed crossover operator, by comparing the algorithm based on CGACell with standard data classification algorithms such as PCA, Kmeans or KNN, attests good qualitative performance in terms of correctness percentages in the recognition of new images, in classification applications of facial image classes corresponding to several persons",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPvufoBAIs": {
    "title": "Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation",
    "volume": "review",
    "abstract": "We consider the problem of source-free unsupervised category-level 3D pose estimation from only RGB images to an non-annotated and unlabelled target domain without any access to source domain data or annotations during adaptation. Collecting and annotating real world 3D data and corresponding images is laborious, expensive yet unavoidable process since even 3D pose domain adaptation methods require 3D data in the target domain. We introduce a method which is capable of adapting to a nuisance ridden target domain without any 3D data or annotations. We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled as a von Mises Fisher distribution at each mesh vertex learnt using differential rendering. We focus on individual mesh vertex features and iteratively update them based on their proximity to corresponding features in the target domain. Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates. Our model is then trained in an EM fashion alternating between updating the vertex features and feature extractor. We show that our method simulates fine-tuning on a global-pseudo labelled dataset under mild assumptions which converges to the target domain asymptotically. Through extensive empirical validation, we demonstrate the potency of our simple approach in addressing the domain shift challenge and significantly enhancing pose estimation accuracy. By accentuating robust and less changed object subcomponents, our framework contributes to the evolution of UDA techniques in the context of 3D pose estimation using only images from the target domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBwxmTbY6Z": {
    "title": "Tensor Time-Series Forecasting and Anomaly Detection with Augmented Causality",
    "volume": "review",
    "abstract": "In time series, variables often exhibit high-dimensional characteristics, and relationships between variables tend to be intricate, encompassing aspects such as non-linearity and time-dependency. Understanding the interaction of variables and comprehending the distribution of their values can significantly enhance the effectiveness of time series data analysis tasks, such as forecasting and anomaly detection. Hence, in this paper, we start from the tensor time series, which can encode higher dimensional information than classic multivariate time series, and aim to discover and leverage their fine-grained time-dependent causal relations to contribute to a more accurate analysis. To this end, we first form an augmented Granger Causality model, named TBN-Granger Causality, which adds time-respecting Bayesian Networks to the time-lagged Neural Granger Causality through a bi-level optimization, such that the overlooking of instantaneous effects in typical causal time series analysis can be addressed. Then, we propose an end-to-end deep generative model, named TacSas, which takes the historical tensor time series, outputs the future tensor time series, and detects possible anomalies, by leveraging the TBN-Granger Causality in the history. Moreover, we show TacSas not only can capture the ground-truth causality but also can be applied when the ground-truth causal structures are hardly available, to help forecasting and anomaly detection. For evaluations, besides synthetic benchmark data, we have four datasets from the climate domain benchmark database ERA5 as the real-world tensor time series for forecasting. Moreover, we extend ERA5 with the extreme weather database NOAA for testing anomaly detection accuracy. We show the effectiveness of TacSas in different time series analysis tasks by comparing with causal baselines, forecasting baselines, and anomaly detection baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=IOp3Qgep9V": {
    "title": "Towards Adversarially Robust Condensed Dataset by Curvature Regularization",
    "volume": "review",
    "abstract": "Dataset condensation is a recent technique designed to mitigate the rising computational demands of training deep neural networks. It does so by generating a significantly smaller, synthetic dataset derived from a larger one. While an abundance of research has aimed at improving the accuracy of models trained on synthetic datasets and enhancing the efficiency of synthesizing these datasets, there has been a noticeable gap in research focusing on analyzing and enhancing the robustness of these datasets against adversarial attacks. This is surprising considering the appealing hypothesis that condensed datasets might inherently promote models that are robust to adversarial attacks. In this study, we first challenge this intuitive assumption by empirically demonstrating that dataset condensation methods are not inherently robust. This empirical evidence propels us to explore methods aimed at enhancing the adversarial robustness of condensed datasets. Our investigation is underpinned by the hypothesis that the observed lack of robustness originates from the high curvature of the loss landscape in the input space. Based on our theoretical analysis, we propose a new method that aims to enhance robustness by incorporating curvature regularization into the condensation process. Our empirical study suggests that the new method is capable of generating robust synthetic datasets that can withstand various adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=dONpC9GL1o": {
    "title": "Closing the Curious Case of Neural Text Degeneration",
    "volume": "review",
    "abstract": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bsKMPAFHO7": {
    "title": "Mediator Interpretation and Faster Learning Algorithms for Linear Correlated Equilibria in General Sequential Games",
    "volume": "review",
    "abstract": "A recent paper by Farina and Pipis (2023) established the existence of uncoupled no-linear-swap regret dynamics with polynomial-time iterations in extensive-form games. The equilibrium points reached by these dynamics, known as linear correlated equilibria, are currently the tightest known relaxation of correlated equilibrium that can be learned in polynomial time in any finite extensive-form game. However, their properties remain vastly unexplored, and their computation is onerous. In this paper, we provide several contributions shedding light on the fundamental nature of linear-swap regret. First, we show a connection between linear deviations and a generalization of communication deviations in which the player can make queries to a ``mediator'' who replies with action recommendations, and, critically, the player is not constrained to match the timing of the game as would be the case for communication deviations. We coin this latter set the untimed communication (UTC) deviations. We show that the UTC deviations coincide precisely with the linear deviations, and therefore that any player minimizing UTC regret also minimizes linear-swap regret. We then leverage this connection to develop state-of-the-art no-regret algorithms for computing linear correlated equilibria, both in theory and in practice. In theory, our algorithms achieve polynomially better per-iteration runtimes; in practice, our algorithms represent the state of the art by several orders of magnitude",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OinvjdvPjp": {
    "title": "xVal: A Continuous Number Encoding for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LokR2TTFMs": {
    "title": "3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining",
    "volume": "review",
    "abstract": "Masked autoencoders (MAE) have recently been introduced to 3D self-supervised pretraining for point clouds due to their great success in NLP and computer vision. Unlike MAEs used in the image domain, where the pretext task is to restore features at the masked pixels, such as colors, the existing 3D MAE works reconstruct the missing geometry only, i.e, the location of the masked points. In contrast to previous studies, we advocate that point location recovery is inessential and restoring intrinsic point features is much superior. To this end, we propose to ignore point position reconstruction and recover high-order features at masked points including surface normals and surface variations, through a novel attention-based decoder which is independent of the encoder design. We validate the effectiveness of our pretext task and decoder design using different encoder structures for 3D training and demonstrate the advantages of our pretrained networks on various point cloud analysis tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=VrHiF2hsrm": {
    "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
    "volume": "review",
    "abstract": "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities on our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, recover harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=JEYWfmz2TU": {
    "title": "Robot Learning from Demonstration: Enhancing Plan Execution with Failure Detection Model",
    "volume": "review",
    "abstract": "Learning plans from demonstrations has emerged as a valuable paradigm, in which a robot autonomously completes a task by executing a sequence of actions according to a learned plan. Nevertheless, the execution of an action may encounter failures in the real environment, such as failing to pick up a cup, resulting in plan execution failure. The execution of a broken plan may damage the environment, e.g., cooking coffee when a cup is not successfully placed. To avoid such risks, action failure detection is crucial. However, the action failure within the execution of task plans is often neglected in existing research. To address the problem, we propose a framework that learns an executable plan that checks failures of each action, called failure-aware plan. Our framework employs meta-learning to learn neural network-based failure-aware task plans. Initially, by using trajectory data collected from robot randomness execution, the framework pre-trains a model that discriminatively captures the state features of various actions at different stages. Utilizing user demonstration trajectories labeled as either success or failure, the pre-trained model undergoes fine-tuning, which is then employed to determine the success or failure of an action execution by means of the corresponding state features. We demonstrate the effectiveness of our approach through experiments on a robot in a simulation environment. Our approach outperforms the compared method when only limited demonstration data is available. This work contributes to enhancing the reliability of plan execution for robot by considering action failure detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=e4xS9ZarDr": {
    "title": "Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts",
    "volume": "review",
    "abstract": "Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It achieves results comparable to AdamW but with greater memory efficiency. As what we can expect from the result of the random search, Lion blends a number of elements from existing algorithms, including signed momentum, decoupled weight decay, Polayk and Nesterov momentum, but doesn't fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This absence of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy. This work aims to demystify Lion. Using both continuous-time and discrete-time analysis, we demonstrate that Lion is a novel and theoretically grounded approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $||x||_\\infty \\leq 1/\\lambda$. Lion achieves this through the incorporation of decoupled weight decay, where $\\lambda$ represents the weight decay coefficient. Our analysis is facilitated by the development of a new Lyapunov function for the Lion updates. It applies to a wide range of Lion-$\\phi$ algorithms, where the $sign(\\cdot)$ operator in Lion is replaced by the subgradient of a convex function $\\phi$, leading to the solution of the general composite optimization problem $\\min_x f(x) + \\phi^*(x)$. Our findings provide valuable insights into the dynamics of Lion and pave the way for further enhancements and extensions of Lion-related algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TTrzgEZt9s": {
    "title": "Distributionally Robust Optimization with Bias & Variance Reduced Gradients",
    "volume": "review",
    "abstract": "We consider the distributionally robust (DR) optimization problem with spectral risk-based uncertainty set and $f$-divergence penalty. This formulation includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate hyperparameter, and prove that it enjoys linear convergence for smooth regularized losses. This contrasts with previous algorithms that either require tuning multiple hyperparameters or potentially fail to converge due to biased gradient estimates or inadequate regularization. Empirically, we show that Prospect can converge 2-3x faster than baselines such as SGD and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTY1RtowlY": {
    "title": "Competition Priors for Object-Centric Learning",
    "volume": "review",
    "abstract": "Humans are very good at abstracting from data and constructing concepts that are then reused. This is missing in current learning systems. The field of object-centric learning tries to bridge this gap by learning abstract representations, often called slots, from data without human supervision. Different methods have been proposed to tackle this task for images, whereas most are overly complex, non-differentiable, or poorly scalable. In this paper, we introduce a conceptually simple, fully-differentiable, non-iterative, and scalable method called **COP** (**C**ompetition **O**ver **P**ixel features). It is implementable using only Convolution and MaxPool layers and an Attention layer. Our method encodes the input image with a convolutional neural network and then uses a branch of alternating convolution and MaxPool layers to create competition and extract primitive slots. These primitive slots are then used as queries for a variant of Cross-Attention over the encoded image. Despite its simplicity, our method is competitive or outperforms previous methods on standard benchmarks. The code is publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=1GUTzm2a4v": {
    "title": "Greedy PIG: Adaptive Integrated Gradients",
    "volume": "review",
    "abstract": "Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=OR4Jo158Dd": {
    "title": "Synthesizing Programmatic Policy for Domain Generalization",
    "volume": "review",
    "abstract": "Deep reinforcement learning has effectively addressed numerous complex control tasks. However, when the environment undergoes changes, such as increasing the number of discs from three to four in the `Tower of Hanoi', learned policies often struggle with generalization. We propose an algorithm for learning programmatic policies capable of capturing environment variations. In doing so, these policies gain the capability to generalize to instances where certain aspects of the domain exhibit variations, a property we term domain generalization. We design a Domain Specific Language to construct the structure of the policy. Through sampling tasks from a task distribution, we can train the policy with a meta-learning algorithm. Furthermore, our approach incorporates Recurrent Neural Network (RNN) into the structure of the programmatic policy to enhance agent-environment interactions. Experiment results demonstrate the efficiency of our approach across three environments with domain generalization. In addition, the learned policy shows its ability to generalize to tasks under different variations of environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=iWi2mL8qoc": {
    "title": "Multi-Scale Window based Transformer Network for High Quality Image Inpainting",
    "volume": "review",
    "abstract": "To achieve effective image inpainting, it is crucial for the model to understand contextual information. Previous studies using CNN-based algorithms have encountered limitations due to the absence of long-range dependencies, which resulted in the model's inability to capture contextual information. In this paper, we propose a Multi-Scale Window-based Transformer model for high-quality image inpainting. We introduce a transformer network with multi-scale windows to capture the influence of different window sizes and gather significant contextual information. To effectively integrate features processed through self-attention, we modified the polarized self-attention network to align with the dimensions of the multi-window scale. We also propose the Selective Mask Update method, which captures vital information from features processed by self-attention, enabling the generation of higher-quality results. Experiments show that it effectively fills in missing areas and demonstrates superior performance on the benchmark dataset compared to other models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=hDzjO41IOO": {
    "title": "Tweedie Moment Projected Diffusions for Inverse Problems",
    "volume": "review",
    "abstract": "Diffusion generative models unlock new possibilities for inverse problems as they allow for the incorporation of strong empirical priors into the process of scientific inference. Recently, diffusion models received significant attention for solving inverse problems by posterior sampling, but many challenges remain open due to the intractability of this sampling process. Prior work resorted to Gaussian approximations to conditional densities of the reverse process, leveraging Tweedie's formula to parameterise its mean, complemented with various heuristics. In this work, we leverage higher order information using Tweedie's formula and obtain a finer approximation with a principled covariance estimate. This novel approximation removes any time-dependent step-size hyperparameters required by earlier methods, and enables higher quality approximations of the posterior density which results in better samples. Specifically, we tackle noisy linear inverse problems and obtain a novel approximation to the gradient of the likelihood. We then plug this gradient estimate into various diffusion models and show that this method is optimal for a Gaussian data distribution. We illustrate the empirical effectiveness of our approach for general linear inverse problems on toy synthetic examples as well as image restoration using pretrained diffusion models as the prior. We show that our method improves the sample quality by providing statistically principled approximations to diffusion posterior sampling problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=grQ97sPU5T": {
    "title": "Spectral Highways: Injecting Homophily into Heterophilic Graphs",
    "volume": "review",
    "abstract": "The application of convolution in graphs is at the core of Graph Neural Network (GNN) algorithms that led to the emergence of Graph Representation Learning (GRL). Various algorithms have been proposed over the last few years to solve the classical GRL task of node classification in a transductive setting. It is widely assumed that standard GNNs perform better on graphs with high homophily, i.e., nodes belonging to the same class are highly connected to each other. This assumption has led to the designing of specialised algorithms in the last few years for datasets that do not contain the property of homophily, i.e., heterophilic datasets. In this work, we both challenge and leverage this assumption. We argue that it is not necessary to follow the common trend of designing new algorithms but instead focus on understanding and enriching the data. We present a new technique from the perspective of data engineering that enables better performance on heterophilic datasets by both heterophilic GNN algorithms and non-heterophilic GNN algorithms. Our proposed technique, Spectral Highways, enables better connectivity and information flow between nodes in a heterophilic graph. We also draw an analogy between the performance of Spectral Highways and a recently proposed network property, i.e., Adjusted homophily. We conduct experiments on 11 baselines and 8 heterophilic datasets and achieve significant improvements in results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=viNQSOadLg": {
    "title": "Biological Sequence Editing with Generative Flow Networks",
    "volume": "review",
    "abstract": "Editing biological sequences has extensive applications in synthetic biology and medicine, such as designing regulatory elements for nucleic-acid therapeutics and treating genetic disorders. The primary objective in biological-sequence editing is to determine the optimal modifications to a sequence which augment certain biological properties while adhering to a minimal number of alterations to ensure safety and predictability. In this paper, we propose GFNSeqEditor, a novel biological-sequence editing algorithm which builds on the recently proposed area of generative flow networks (GFlowNets). Our proposed GFNSeqEditor identifies elements within a starting seed sequence that may compromise a desired biological property. Then, using a learned stochastic policy, the algorithm makes edits at these identified locations, offering diverse modifications for each sequence in order to enhance the desired property. Notably, GFNSeqEditor prioritizes edits with a higher likelihood of substantially improving the desired property. Furthermore, the number of edits can be regulated through specific hyperparameters. We conducted extensive experiments on a range of real-world datasets and biological applications, and our results underscore the superior performance of our proposed algorithm compared to existing state-of-the-art sequence editing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=EsjoMaNeVo": {
    "title": "Steering No-Regret Learners to Optimal Equilibria",
    "volume": "review",
    "abstract": "We consider the problem of steering no-regret-learning agents to play desirable equilibria via nonnegative payments. We show that steering is impossible if the total budget (across iterations) is finite, both in normal- and extensive-form games. However, vanishing average payments are compatible with steering. When players' full strategies are observed at each timestep, constant per-iteration payments permit steering. When only trajectories through the game tree are observable, steering is impossible with constant per-iteration payments in general extensive-form games, but possible in normal-form games or if the maximum per-iteration payment may grow with time, maintaining vanishing average payments. We supplement our theoretical positive results with experiments highlighting the efficacy of steering in large games, and show how our framework relates to optimal mechanism design and information design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tzD9HVgeVx": {
    "title": "AgentMixer: Multi-Agent Correlated Policy Factorization",
    "volume": "review",
    "abstract": "Centralized training with decentralized execution (CTDE) has been popularly employed to stabilize the partially observable multi-agent reinforcement learning (MARL) by learning a centralized value function. However, existing methods typically assume that agents make decisions based on their local observation independently, which could hardly lead to a correlated joint policy with sufficient coordination. In this paper, we propose AgentMixer which fully takes advantage of CTDE to learn correlated decentralized policies. Specifically, AgentMixer first explicitly models the correlated joint policy by a module named \\textit{Policy Modifier} composing the partially observable individual policies conditioned on global state information. To overcome the mismatch problem caused by the asymmetric information when distilling the state-based joint policy into partially observable decentralized policies, we introduce \\textit{Individual-Global-Consistency} (IGC) to maintain the mode consistent between them. The incorporation of these two novel modules enables learning correlated decentralized policies with restricted partial observability. We further theoretically prove that AgentMixer converges to $\\epsilon$-approximate Correlated Equilibrium. The strong experimental performance on three MARL benchmarks also confirms the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=J7hbPeOZ39": {
    "title": "Dynamic Assortment Selection and Pricing with Learning",
    "volume": "review",
    "abstract": "We consider a dynamic assortment selection and pricing problem in which a seller has $n$ different items available for sale. In each round, the seller observes $d$-dimensional contextual preference information for the user and offers to the user an assortment of $K$ items at prices chosen by the seller. The user selects at most one of the products from the offered assortment according to a multinomial logit choice model whose parameters are unknown. The seller observes which, if any, item is chosen at the end of each round, with a goal of maximizing cumulative revenue over a selling horizon of length $T$. For this problem, we propose an algorithm that learns from user feedback and achieves $n$-independent revenue regret of order $\\widetilde{\\mathcal{O}}(d \\sqrt{T})$. We also show that this regret rate is optimal, up to logarithmic factors, by obtaining lower bounds for the regret achievable by any algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jla53ILAha": {
    "title": "Implicit regularization of multi-task learning and finetuning in overparameterized neural networks",
    "volume": "review",
    "abstract": "It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or \"lazy\") regime and the feature learning (\"rich\") regime identified in prior work. Moreover, PT+FT can exhibit a novel ``nested feature learning'' behavior not captured by either regime, which biases it to extract a sparse subset of the features learned during pretraining. In ReLU networks, we reproduce all of these qualitative behaviors. We also observe that PT+FT (but not MTL) is biased to learn features that are correlated with (but distinct from) those needed for the auxiliary task, while MTL is biased toward using identical features for both tasks. As a result, we find that in realistic settings, MTL generalizes better when comparatively little data is available for the task of interest, while PT+FT outperforms it with more data available. We show that our findings hold qualitatively for a deep architecture trained on image classification tasks. Our characterization of the nested feature learning regime also motivates a modification to PT+FT that we find empirically improves performance. Overall, our results shed light on the impact of auxiliary task learning and suggest ways to leverage it more effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=1zt8GWZ9sc": {
    "title": "Quack: Automatic Jailbreaking Large Language Models via Role-playing",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel in Natural Language Processing (NLP) with human-like text generation, but the misuse of them has raised public concern and prompted the need for safety measures. Proactive testing with jailbreaks, meticulously crafted prompts that bypass model constraints and policies, has become mainstream to ensure security and reliability upon model release. While researchers have made substantial efforts to explore jailbreaks against LLMs, existing methods still face the following disadvantages: (1) require human labor and expertise to design question prompts; (2) non-determination regarding reproducing jailbreak; (3) exhibit limited effectiveness on updated model versions and lack the ability for iterative reuse when invalid. To address these challenges, we introduce Quack, an automated testing framework based on role-playing of LLMs. Quack translates testing guidelines into question prompts, instead of human expertise and labor. It systematically analyzes and consolidates successful jailbreaks into a paradigm featuring eight distinct characteristics. Based on it, we reconstruct and maintain existing jailbreaks through knowledge graphs, which serve as Quack's repository of playing scenarios. It assigns four distinct roles to LLMs, for automatically organizing, evaluating, and further updating jailbreaks. We empirically demonstrate the effectiveness of our method on three state-of-the-art open-sourced LLMs (Vicuna-13B, LongChat-7B, and LLaMa-7B), as well as one widely-used commercial LLM (ChatGPT). Our work addresses the pressing need for LLM security and contributes valuable insights for creating safer LLM-empowered applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=hqUznsPMLn": {
    "title": "ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors",
    "volume": "review",
    "abstract": "Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a family of autotelic generation methods that leverage semantic descriptors evaluated by a large language model (LLM) to directly optimize for interesting diversity. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to explore that abstract semantic space, slowly discovering a diversity of solvable programming puzzles in any given run. Across a set of experiments, we show that ACES discovers a richer diversity of puzzles than existing diversity-maximizing algorithms as measured across a range of diversity metrics. We further study whether and in which conditions this diversity can translate into the successful training of puzzle solving models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=xRiZddh5Pb": {
    "title": "Learning from A Single Graph is All You Need for Near-Shortest Path Routing",
    "volume": "review",
    "abstract": "We propose a simple algorithm that needs only a few data samples from a single graph for learning local routing policies that generalize across classes of geometric random graphs in Euclidean and hyperbolic metric spaces. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that let each graph node efficiently and scalably route (i.e., forward) packets by considering only the node's state and the state of the neighboring nodes. Our algorithm design exploits network domain knowledge in the selection of input features and in the selection of a \"seed graph\" and its data samples. The leverage of domain knowledge provides theoretical assurance that the seed graph and node subsampling suffice for learning that is generalizable, scalable, and efficient. Remarkably, one of these DNNs we train —using distance as the only input feature— learns a policy that exactly matches the well-known Greedy Forwarding policy, which forwards packets to the neighbor with the shortest distance to the destination. We also learn a new policy, which we call Greedy Tensile routing —using both distance and stretch factor as the input features— that almost always outperforms greedy forwarding. We demonstrate the explainability and ultra-low latency runtime operation of Greedy Tensile routing by symbolically interpreting its DNN in terms as a low-complexity linear actions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=gBLEHzKOfF": {
    "title": "Generative Entropic Neural Optimal Transport To Map Within and Across Space",
    "volume": "review",
    "abstract": "Learning measure-to-measure mappings is a crucial task in machine learning, fea- tured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as Neural OT use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple chal- lenges, while the mass conservation constraint inherent to OT can provide too much credit to outliers. While each of these mismatches between practice and theory has been addressed independently in various works, we propose in this work an elegant framework to unify them, called generative entropic neural op- timal transport (GENOT). GENOT can accommodate any cost function; handles randomness using conditional generative models; can map points across incompa- rable spaces, and can be used as an unbalanced solver. We evaluate our approach through experiments conducted on various synthetic datasets and demonstrate its practicality in single-cell biology. In this domain, GENOT proves to be valu- able for tasks such as modeling cell development, predicting cellular responses to drugs, and translating between different data modalities of cells",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UZS6D7GfP1": {
    "title": "Human-in-the-loop Detection of AI-generated Text via Grammatical Patterns",
    "volume": "review",
    "abstract": "The increasing proliferation of large language models (LLMs) has raised significant concerns about the detection of AI-written text. Ideally, the detection method should be accurate (in particular, it should not falsely accuse humans of using AI-generated text), and interpretable (it should provide a decision as to why the text was detected as either human or AI-generated). Existing methods tend to fall short of one or both of these requirements, and recent work has even shown that detection is impossible in the full generality. In this work, we focus on the problem of detecting AI-generated text in a domain where a training dataset of human-written samples is readily available. Our key insight is to learn interpretable grammatical patterns that are highly indicative of human or AI written text. The most useful of these patterns can then be given to humans as part of a human-in-the-loop approach. In our experimental evaluation, we show that the approach can effectively detect AI-written text in a variety of domains and generalize to different language models. Our results in a human trial show an improvement in the detection accuracy from $43$% to $86$%, demonstrating the effectiveness of the human-in-the-loop approach. We also show that the method is robust to different ways of prompting LLM to generate human-like patterns. Overall, our study demonstrates that AI text can be accurately and interpretably detected using a human-in-the-loop approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=fM1ETm3ssl": {
    "title": "Towards Meta-Models for Automated Interpretability",
    "volume": "review",
    "abstract": "Mechanistic interpretability aims to open the black box of neural networks. Previous work has demonstrated that the mechanisms implemented by small neural networks can be fully reverse-engineered. Since these efforts rely on human labor that does not scale to models with billions of parameters, there is growing interest in automating interpretability methods. We propose to use \\emph{meta-models}, neural networks that take another network's parameters as input, to scale interpretability efforts. To this end, we present a scalable meta-model architecture and successfully apply it to a variety of problems, including mapping neural network parameters to human-legible code and detecting backdoors in networks. Our results aim to provide a proof-of-concept for automating mechanistic interpretability methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=gppLqZLQeY": {
    "title": "Efficient Subgraph GNNs by Learning Effective Selection Policies",
    "volume": "review",
    "abstract": "Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called _Policy-Learn_, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Our experimental results demonstrate that _Policy-Learn_ outperforms existing baselines across a wide range of datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=YIls9HEa52": {
    "title": "Parsing neural dynamics with infinite recurrent switching linear dynamical systems",
    "volume": "review",
    "abstract": "Unsupervised methods for dimensionality reduction of neural activity and behavior have provided unprecedented insights into the underpinnings of neural information processing. One popular approach involves the recurrent switching linear dynamical system (rSLDS) model, which describes the latent dynamics of neural spike train data using discrete switches between a finite number of low-dimensional linear dynamical systems. However, a few properties of rSLDS model limit its deployability on trial-varying data, such as a fixed number of states over trials, and no latent structure or organization of states. Here we overcome these limitations by endowing the rSLDS model with a semi-Markov discrete state process, with latent geometry, that captures key properties of stochastic processes over partitions with flexible state cardinality. We leverage partial differential equations (PDE) theory to derive an efficient, semi-parametric formulation for dynamical sufficient statistics to the discrete states. This process, combined with switching dynamics, defines our infinite recurrent switching linear dynamical system (irSLDS) model class. We first validate and demonstrate the capabilities of our model on synthetic data. Next, we turn to the analysis of mice electrophysiological data during decision-making, and uncover strong non-stationary processes underlying both within-trial and trial-averaged neural activity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.6,
    "authors": []
  },
  "https://openreview.net/forum?id=h7DGnWGeos": {
    "title": "Active Retrosynthetic Planning Aware of Route Quality",
    "volume": "review",
    "abstract": "Retrosynthetic planning is a sequential decision-making process of identifying synthetic routes from the available building block materials to reach a desired target molecule. Though existing planning approaches show promisingly high solving rates and low costs, the trivial route cost evaluation via pre-trained forward reaction prediction models certainly falls short of real-world chemical practice. An alternative option is to annotate the actual cost of a route, such as yield, through chemical experiments or input from chemists, while this often leads to substantial query costs. In order to strike the balance between query costs and route quality evaluation, we propose an Active Retrosynthetic Planning (ARP) framework that remains compatible with the established retrosynthetic planners. On one hand, the proposed ARP trains an actor that decides whether to query the cost of a reaction; on the other hand, it resorts to a critic to estimate the value of a molecule with its preceding reaction cost as input. Those molecules with low reaction costs are preferred to expand first. We apply our framework to different existing approaches on both the benchmark and an expert dataset and demonstrate that it outperforms the existing state-of-the-art approach by 6.2\\% in route quality while reducing the query cost by 12.8\\%. In addition, ARP consistently plans high-quality routes with either abundant or sparse annotations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=mBzsKsrXf9": {
    "title": "ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations",
    "volume": "review",
    "abstract": "As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate similar images. Interestingly, prompt diversity does not decrease as users find better prompts. We further propose a new metric to quantify the steerability of AI using our dataset. We define steerability as the expected number of interactions required to adequately complete a task. We estimate this value by fitting a Markov chain for each target task and calculating the expected time to reach an adequate score in the Markov chain. We quantify and compare AI steerability across different types of target images and two different models, finding that images of cities and natural world images are more steerable than artistic and fantasy images. These findings provide insights into human-AI interaction behavior, present a concrete method of assessing AI steerability, and demonstrate the general utility of the ArtWhisperer dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=zb3b6oKO77": {
    "title": "How do Language Models Bind Entities in Context?",
    "volume": "review",
    "abstract": "Language models (LMs) can recall facts mentioned in context, as shown by their performance on reading comprehension tasks. When the context describes facts about more than one entity, the LM has to correctly bind attributes to their corresponding entity. We show, via causal experiments, that LMs' internal activations represent binding information by exhibiting appropriate binding ID vectors at the entity and attribute positions. We further show that binding ID vectors form a subspace and often transfer across tasks. Our results demonstrate that LMs learn interpretable strategies for representing symbolic knowledge in context, and that studying context activations is a fruitful direction for understanding LM cognition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=L9G9nR8fMF": {
    "title": "LayerAct: Advancing CNNs with BatchNorm through Layer-direction Normalization",
    "volume": "review",
    "abstract": "In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions for CNNs with BatchNorm. These functions are designed to be more noise-robust compared to existing element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve this noise-robustness independent of the activation's saturation state, which limits the activation output space and complicates efficient training. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results with three benchmark datasets for image classification tasks show that LayerAct functions excel in handling noisy datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=tbVWug9f2h": {
    "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
    "volume": "review",
    "abstract": "Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang—a language with less than 200 speakers and therefore virtually no presence on the web—using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 language learning than L1 language acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=HfXDrAzFvG": {
    "title": "Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations",
    "volume": "review",
    "abstract": "Recently, semidefinite programming (SDP) techniques have shown great promise in providing accurate Lipschitz bounds for neural networks. Specifically, the LipSDP approach (Fazlyab et al., 2019) has received much attention and provides the least conservative Lipschitz upper bounds that can be computed with polynomial time guarantees. However, one main restriction of LipSDP is that its formulation requires the activation functions to be slope-restricted on $[0,1]$, preventing its further use for more general activation functions such as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for example as residual ReLU networks. However, a direct application of LipSDP to the resultant residual ReLU networks is conservative and even fails in recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our paper bridges this gap and extends LipSDP beyond slope-restricted activation functions. To this end, we provide novel quadratic constraints for GroupSort, MaxMin, and Householder activations via leveraging their underlying properties such as sum preservation. Our proposed analysis is general and provides a unified approach for estimating $\\ell_2$ and $\\ell_\\infty$ Lipschitz bounds for a rich class of neural network architectures, including non-residual and residual neural networks and implicit models, with GroupSort, MaxMin, and HouseHolder activations. Finally, we illustrate the utility of our approach with a variety of experiments and show that our proposed SDPs generate less conservative Lipschitz bounds in comparison to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEAVTKUEpJ": {
    "title": "SARI: SIMPLISTIC AVERAGE AND ROBUST IDENTIFICATION BASED NOISY PARTIAL LABEL LEARNING",
    "volume": "review",
    "abstract": "Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough experiments on four datasets and compare SARI against nine NPLL and PLL methods from the prior art. SARI achieves state-of-the-art results in all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2UnCj3jeao": {
    "title": "Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation",
    "volume": "review",
    "abstract": "In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in single-cell biology and computer vision. However, the classic OT framework enforces mass conservation, which makes it prone to outliers and limits its applicability in real-world scenarios. The latter can be particularly harmful in OT domain translation tasks, where the relative position of a sample within a distribution is explicitly taken into account. While unbalanced OT tackles this challenge in the discrete setting, its integration into neural Monge map estimators has received limited attention. We propose a theoretically grounded method to incorporate unbalancedness into any Monge map estimator. We improve existing estimators to model cell trajectories over time and to predict cellular responses to perturbations. Moreover, our approach seamlessly integrates with the OT flow matching (OT-FM) framework. While we show that OT-FM performs competitively in image translation, we further improve performance by incorporating unbalancedness (UOT-FM), which better preserves relevant features. We hence establish UOT-FM as a principled method for unpaired image translation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzo7N2XkJ2": {
    "title": "Corrupting Unbounded Unlearnable Datasets with Pixel-based Image Transformations",
    "volume": "review",
    "abstract": "Unlearnable datasets (UDs) lead to a drastic drop in the generalization performance of models trained on them by introducing elaborate and imperceptible perturbations into clean training sets. Many existing defenses, e.g., JPEG compression and adversarial training, effectively counter UDs based on norm constraints (i.e., bounded UDs). However, the recent emergence of unbounded UDs renders existing defense measures completely ineffective, presenting a greater challenge to defenders. To address this, we express the unbounded unlearnable sample as the result of multiplying a matrix by a clean sample in a simplified scenario. Meanwhile, we note in existing unbounded UDs that the consistency of intra-class and inter-class noise significantly affects unlearnable effect, which motivates us to formalize the intra-class matrix inconsistency as $\\Theta_{imi}$ and inter-class matrix consistency as $\\Theta_{imc}$ and conjecture that increasing both of these metrics enhances the test accuracy. Through validation experiments that commendably support our hypothesis, we further design a random matrix to boost both $\\Theta_{imi}$ and $\\Theta_{imc}$, achieving a notable degree of defense effect. Hence, by building upon and extending these facts, we first propose a brand-new image COrruption that employs randomly multiplicative transformation via INterpolation operation (COIN) to successfully defend against existing unbounded UDs. Our approach leverages global pixel random interpolations, effectively suppressing the impact of multiplicative noise in unbounded UDs. Extensive experiments demonstrate that our defense approach outperforms state-of-the-art defenses, achieving an improvement of 23.55\\%-48.11\\% in average test accuracy on the CIFAR-10 dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQALhPTAfj": {
    "title": "Navigating Scaling Laws: Accelerating Vision Transformer's Training via Adaptive Strategies",
    "volume": "review",
    "abstract": "In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: Investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a \"compute-optimal\" model, i.e. a model that allocates a given level of compute during training optimally to maximise performance. In this work, we extend the concept of optimality by allowing for an \"adaptive\" model, i.e. a model that can change its shape during the course of training. By allowing the shape to adapt, we can optimally traverse between the underlying scaling laws, leading to a significant reduction in required compute to reach a given target performance. We focus on vision tasks and the family of Vision Transformers, where the patch size as well as the width naturally serve as adaptive shape parameters. We demonstrate that, guided by scaling laws, we can design compute-optimal adaptive models that beat their \"static\" counterparts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=aM7US5jKCd": {
    "title": "Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation Models",
    "volume": "review",
    "abstract": "Adversarial robustness has been studied extensively in image classification, especially for the $\\ell_\\infty$-threat model, but significantly less so for related tasks such as object detection and semantic segmentation. Attacks on semantic segmentation models turn out to be harder than for image classification. We propose novel attacks and motivated by their complementary properties, we put them into an attack ensemble called SEA. We use SEA to show that existing attacks can severely overestimate the robustness of semantic segmentation models. Perhaps surprisingly, existing attempts of adversarial training for semantic segmentation turn out to yield only weakly robust models or are even completely non-robust. We investigate why previous adaptations of adversarial training to semantic segmentation failed and identify insufficient training time and number of attack steps as key elements. In turn we show how recently proposed robust ImageNet backbones can be used to obtain adversarially robust semantic segmentation models with up to six times less training time for Pascal-VOC and the more challenging ADE-20k",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=IU4L7wiwxw": {
    "title": "Pushing Gradient towards Zero: A Novel Pruning Method for Large Language Models",
    "volume": "review",
    "abstract": "Recently, large language models (LLMs) have attracted widespread attention due to their dominating performance on some complex language modelling tasks. However, because of their massive size, LLMs require huge amounts of GPU resources in inference which limits their usability. In this paper, we propose an effective pruning method termed PGZ(Pushing Gradient towards Zero), which prunes LLMs in one-shot, without any retraining. The method consists of a new gradual pruning method and a novel fine-tuning method where gradient is pushed towards zero. More precisely, we construct a loss function based on gradient information and optimize it leveraging second-order information implicitly. In addition, the inherently nature of PGZ makes it suitable for parallelization. Notably, we conduct a thorough evaluation of PGZ on LLaMA-7B,13B,30B,65B across various language benchmarks. Experimental results demonstrate that PGZ consistently outperforms the existing pruning methods for LLMs in unstructured pattern and semi-structured (2:4 and 4:8) pattern. PGZ is also competitive in terms of zero-shot tasks and is compatible with weight quantization approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=S24zdyiWDT": {
    "title": "Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning?",
    "volume": "review",
    "abstract": "Inverse Reinforcement Learning (IRL)---the problem of learning reward functions from demonstrations of an \\emph{expert policy}---plays a critical role in developing intelligent systems, such as those that understand and imitate human behavior. While widely used in applications, theoretical understandings of IRL admit unique challenges and remain less developed compared with standard RL theory. For example, it remains open how to do IRL efficiently in standard \\emph{offline} settings with pre-collected data, where states are obtained from a \\emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy. This paper provides the first line of results for efficient IRL in vanilla offline and online settings using polynomial samples and runtime. We first design a new IRL algorithm for the offline setting, Reward Learning with Pessimism (RLP), and show that it achieves polynomial sample complexity in terms of the size of the MDP, a concentrability coefficient between the behavior policy and the expert policy, and the desired accuracy. Building on RLP, we further design an algorithm Reward Learning with Exploration (RLE), which operates in a natural online setting where the learner can both actively explore the environment and query the expert policy, and obtain a stronger notion of IRL guarantee from polynomial samples. We establish sample complexity lower bounds for both settings showing that RLP and RLE are nearly optimal. Finally, as an application, we show that the learned reward functions can \\emph{transfer} to another target MDP with suitable guarantees when the target MDP satisfies certain similarity assumptions with the original (source) MDP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkL8djXrMM": {
    "title": "Neural Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image generation benchmarks, including CIFAR-10, downsampled versions of ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms of likelihood and produce high-quality samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=riYNe4jnKV": {
    "title": "Calibration-then-Calculation: A Variance Reduced Metric Framework",
    "volume": "review",
    "abstract": "Deep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and extensive experimental validations in the context of Deep Click-Through Rate Prediction Models and Image Classification Models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=wdEHqQWTG4": {
    "title": "Robust Reinforcement Learning for Portfolio Management via Competition and Cooperation Strategies",
    "volume": "review",
    "abstract": "In this study, we propose an intelligent system for portfolio management that applies robust reinforcement learning within a multi-agent framework. The proposed system incorporates both competition and cooperation strategies to enhance decision-making performance and adaptability. By formulating the portfolio management problem as a cooperative multi-agent environment, agents collaborate and jointly strive to achieve a common goal. On the other hand, the inclusion of competition strategies enables agents to dynamically compete for limited resources and advantages in the market. Specifically, the proposed cooperative strategies employ the absolute value of the reward, prioritizing accelerated model convergence. Meanwhile, the competitive strategies utilize previous rewards to guide action selection, aiming to seek gains and avoid losses. To assess the performance of our model, we evaluate it on a set of real-world financial data. The results obtained demonstrate that the proposed game strategies outperform traditional reinforcement learning approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=uZfjFyPAvn": {
    "title": "Implicit Neural Representations and the Algebra of Complex Wavelets",
    "volume": "review",
    "abstract": "Implicit neural representations (INRs) have arisen as useful methods for representing signals on Euclidean domains. By parameterizing an image as a multilayer perceptron (MLP) on Euclidean space, INRs effectively represent signals in a way that couples spatial and spectral features of the signal that is not obvious in the usual discrete representation, paving the way for continuous signal processing and machine learning approaches that were not previously possible. Although INRs using sinusoidal activation functions have been studied in terms of Fourier theory, recent works have shown the advantage of using wavelets instead of sinusoids as activation functions, due to their ability to simultaneously localize in both frequency and space. In this work, we approach such INRs and demonstrate how they resolve high-frequency features of signals from coarse approximations done in the first layer of the MLP. This leads to multiple prescriptions for the design of INR architectures, including the use of complex wavelets, decoupling of low and band-pass approximations, and initialization schemes based on the singularities of the desired signal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=sP1tCl2QBk": {
    "title": "Fiber Monte Carlo",
    "volume": "review",
    "abstract": "Integrals with discontinuous integrands are ubiquitous, arising from discrete structure in applications like topology optimization, graphics, and computational geometry. These integrals are often part of a forward model in an inverse problem where it is necessary to reason backwards about the parameters, ideally using gradient-based optimization. Monte Carlo methods are widely used to estimate the value of integrals, but this results in a non-differentiable approximation that is amenable to neither conventional automatic differentiation nor reparameterization-based gradient methods. This significantly disrupts efforts to integrate machine learning methods in areas that exhibit these discontinuities: physical simulation and robotics, design, graphics, and computational geometry. Although bespoke domain-specific techniques can handle special cases, a general methodology to wield automatic differentiation in these discrete contexts is wanting. We introduce a differentiable variant of the simple Monte Carlo estimator which samples line segments rather than points from the domain. We justify our estimator analytically as conditional Monte Carlo and demonstrate the diverse functionality of the method as applied to image stylization, topology optimization, and computational geometry",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=gMsZBhwiM4": {
    "title": "ICA model estimation using an optimized version of genetic algorithms",
    "volume": "review",
    "abstract": "This paper presents a method of estimating the independent component analysis model based on the use of a training algorithm based on an optimized version of genetic algorithms with a neural network algorithm. The mixed training algorithm is applied to optimize the objective function negentropy used to estimate the ICA model. The proposed estimation algorithm improves the training scheme based on genetic algorithms by using for crossover the most suitable chromosomes evaluated by the objective function with the parameters calculated calculated accordingly by a multilayer neural network algorithm. The performances of the proposed algorithm for estimating the independent components were evaluated through a comparative analysis with the versions of FastICA algorithms based on the standard Newton method, as well as on the secant method of derivation of the training scheme at the level of the optimization stage of the approximate objective function. The experimental results for the proposed algorithm for estimating the independent components are established in specific blind source separation applications using unidimensional and bidimensional signals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.3,
    "authors": []
  },
  "https://openreview.net/forum?id=KQe9tHd0k8": {
    "title": "Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation",
    "volume": "review",
    "abstract": "Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps again by using the second step's embeddings as new covariates for the next iteration. In the final iteration, a classifier is trained using the pseudo labels. Our algorithm displays strong gains against several SOTA baselines (upto **15%**) for the LLP Binary Classification problem on various dataset types - tabular and Image. We achieve these improvements with minimal computational overhead above standard supervised learning due to Belief Propagation, for large bag sizes, even for a million samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=20KYsQ8Q4Z": {
    "title": "High-dimensional Bayesian Optimization with Group Testing",
    "volume": "review",
    "abstract": "Bayesian optimization is an effective method for optimizing expensive-to-evaluate black-box functions. High-dimensional problems are particularly challenging as the surrogate model of the objective suffers from the curse of dimensionality, which makes accurate modeling difficult. We propose a group testing approach to identify active variables to facilitate efficient optimization in these domains. The proposed algorithm, Group Testing Bayesian Optimization (GTBO), first runs a testing phase where groups of variables are systematically selected and tested on whether they influence the objective. To that end, we extend the well-established theory of group testing to functions of continuous ranges. In the second phase, GTBO guides optimization by placing more importance on the active dimensions. By exploiting the axis-aligned subspace assumption, GTBO is competitive against state-of-the-art methods on several synthetic and real-world high-dimensional optimization tasks. Furthermore, GTBO aids in the discovery of active parameters in applications, thereby enhancing practitioners' understanding of the problem at hand",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=WQwV7Y8qwa": {
    "title": "Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems",
    "volume": "review",
    "abstract": "Understanding how multiple brain regions interact to produce behavior is a major challenge in systems neuroscience, with many regions causally implicated in common tasks such as sensory processing and decision making. A precise description of interactions between regions remains an open problem. Moreover, neural dynamics are nonlinear and non-stationary. Here, we propose MR-SDS, a multiregion, switching nonlinear state space model that decomposes global dynamics into local and cross-communication components in the latent space. MR-SDS includes directed interactions between brain regions, allowing for estimation of state-dependent communication signals, and accounts for sensory inputs effects, history effects, and heterogeneity across days and animals. We show that our model accurately recovers latent trajectories, vector fields underlying switching nonlinear dynamics, and cross-region communication profiles in three simulations. We then apply our method to two large-scale, multi-region neural datasets involving mouse decision making. The first includes hundreds of neurons per region, recorded simultaneously at single-cell-resolution across 3 distant cortical regions. The second is a mesoscale widefield dataset of 8 adjacent cortical regions imaged across both hemispheres. On these multi-region datasets, our model outperforms existing piece-wise linear multi-region models and reveals multiple distinct dynamical states and a rich set of cross-region communication profiles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=7GCRhebJEr": {
    "title": "Robustness via learned Bregman divergence",
    "volume": "review",
    "abstract": "We exploit the Bregman divergence to generate functions that are trained to measure the semantic similarity between images under corruptions and use these functions as alternatives to the $L^p$ norms to define robustness threat models. Then we replace the projected gradient descent (PGD) by semantic attacks, which are instantiations of the mirror descent, the optimization framework associated with the Bregman divergence. Adversarial training under these settings yield classification models that are more robust to common image corruptions. Particularly, for the contrast corruption that was found problematic in prior work we achieve an accuracy that exceeds the $L^p$- and the LPIPS-based adversarially trained neural networks by a margin of 29\\% on the CIFAR-10-C corruption dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=SEPaEuPwpr": {
    "title": "SOI: Scaling down computational complexity by estimating partial states of the model",
    "volume": "review",
    "abstract": "Consumer electronics used to follow the miniaturization trend described by Moore's Law. Despite the continuous growth in the processing power of Microcontroller Units (MCUs), the MCUs used in the smallest appliances are still not capable of running even moderately big, state-of-the-art artificial neural networks (ANNs). Deploying ANNs on this class of devices becomes even more challenging when they are required to operate in a time-sensitive manner, as the model's inference cannot be distributed over time. In this work, we present a novel method called Scattered Online Inference (SOI) that aims to reduce the computational complexity of ANNs. SOI is developed based on the premise that time-series data is continuous and/or seasonal, and so are the model's predictions. This applied extrapolation leads to processing speed improvements, especially in the deeper layers of the model. The application of strides forces the ANN to produce more general inner partial states of the model, as they are based on a higher number of input samples that lie further apart from each other. As a result, SOI allows skipping full model recalculation at each inference by performing only the strictly necessary operations. We present two possible patterns of inference achievable with SOI - Partially Predictive (PP) and Fully Predictive (FP). For the audio separation task, we achieved a 64.4% reduction in computational complexity at the cost of 9.8% of SI-SNRi for the PP variant, and a 41.9% reduction at the cost of 7.70% SI-SNRi with the FP variant. Moreover, the latter variant reduces inference time by an additional 28.7%. Similar results are also presented for the acoustic scene classification task with a model based on the GhostNet architecture",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=lwtaEhDx9x": {
    "title": "Elephants Never Forget: Testing Language Models for Memorization of Tabular Data",
    "volume": "review",
    "abstract": "While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Starting with simple qualitative tests for whether an LLM knows the names and values of features, we introduce a variety of different techniques to assess the degrees of contamination, including statistical tests for conditional distribution modeling and four tests that identify memorization. Our investigation reveals that LLMs are pre-trained on many popular tabular datasets. This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to the test set. Interestingly, we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim. On these datasets, although seen during training, good performance on downstream tasks might not be due to overfitting. Our findings underscore the need for ensuring data integrity in machine learning tasks with LLMs. To facilitate future research, we release an open-source tool that can perform various tests for memorization https://github.com/tabmem/tool",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=C42FkKhAUC": {
    "title": "IMPROVING ADVERSARIAL TRAINING WITH MARGIN- WEIGHTED PERTURBATION BUDGET",
    "volume": "review",
    "abstract": "Adversarial Training effectively improves the robustness of Deep Neural Networks (DNNs) to adversarial attacks. Generally, Adversarial Training involves training DNN models with adversarial examples obtained within a pre-defined, fixed perturbation bound. Notably, individual natural examples from which these adversarial examples are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting adversarial examples with fixed perturbation radius for all instances may not sufficiently unleash the potency of adversarial training. Motivated by this observation, we propose a simple, computationally cheap reweighting function for assigning perturbation bounds to adversarial examples used for Adversarial Training. We name our approach \\textit{Margin-Weighted Perturbation Budget (MWPB)}. The proposed method assigns perturbation radii to individual adversarial samples based on the vulnerability of their corresponding individual natural examples. Experimental results show that the proposed method yields a genuine improvement in the robustness of existing AT algorithms against various adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=HYyRwm367m": {
    "title": "Object-Centric Semantic Vector Quantization",
    "volume": "review",
    "abstract": "Neural discrete representations are crucial components of modern neural networks. However, their main limitation is that the primary strategies such as VQ-VAE can only provide representations at the patch level. Therefore, one of the main goals of representation learning, acquiring conceptual, semantic, and compositional abstractions such as the color and shape of an object, remains elusive. In this paper, we present the first approach to semantic neural discrete representation learning. The proposed model, called Semantic Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in unsupervised object-centric learning to address this limitation. Specifically, we observe that a simple approach quantizing at the object level poses a significant challenge and propose constructing scene representations hierarchically, from low-level discrete concept schemas to object representations. Additionally, we suggest a novel method for training a prior over these semantic representations, enabling the ability to generate images following the underlying data distribution, which is lacking in most object-centric models. In experiments on various 2D and 3D object-centric datasets, we find that our model achieves superior generation performance compared to non-semantic vector quantization methods such as VQ-VAE and previous object-centric generative models. Furthermore, we find that the semantic discrete representations can solve downstream scene understanding tasks that require reasoning about the properties of different objects in the scene",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=5LhYYajlqV": {
    "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
    "volume": "review",
    "abstract": "Machine unlearning has garnered increased attention within regulatory contexts, driven by the need to comply with the \"Right to be Forgotten''. However, achieving precise unlearning is computationally infeasible for large models, particularly when dealing with large language models (LLMs). To this end, several algorithms which approximate the removal of training data without retraining the model have been proposed which rely on gradient ascent based model updates. In this work, we propose a new class of unlearning methods called \"In-Context Unlearning'' suitable for LLMs by providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the instance alongside a different label and additional correctly labelled instances as inputs to the LLM at inference time. Our experimental results across various text classification tasks demonstrate that these contexts effectively remove specific information from the training set while maintaining performance levels that are competitive with state-of-the-art unlearning methods that require access to the LLM parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=0t1O8ziRZp": {
    "title": "Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization",
    "volume": "review",
    "abstract": "Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (``synthesis recipe\"), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of hardware design complexities — from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) — requires a nuanced 'synthesis recipe' guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We present RGLS, a meticulously tuned $\\alpha$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process. Computed based on similarity scores through nearest neighbor retrieval from the training dataset, RGLS yields superior synthesis recipes tailored for a wide array of hardware designs. Our findings showcase substantial enhancements in the Quality of Result (QoR) of synthesized circuits, boasting improvements of up to 24.8\\% compared to state-of-the-art techniques. Furthermore, RGLS achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=9k27IITeAZ": {
    "title": "ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching",
    "volume": "review",
    "abstract": "Self-attention is an essential component of GPT-style models and a significant cause of LLM inference latency for long sequences. In multi-tenant LLM inference servers, the compute and memory operation cost of self-attention can be amortized by making use of the probability that sequences from users may share long prompt prefixes. This paper introduces ChunkAttention, a unique self-attention kernel built on chunking, sharing the KV cache, and batching the attention computation. ChunkAttention recognizes matching prompt prefixes across several sequences and shares their KV cache in memory by chunking the KV cache and structuring it into the auxiliary prefix tree. To significantly improve the memory reuse of KV cache and consequently the speed of self-attention for long shared prompts, we design an efficient computation kernel on this new storage structure, where two-phased partitioning is implemented to reduce memory operations on shared KV cache during self-attention. Experiments show that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, with lengths ranging from 1024 to 8192",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=H5XZLeXWPS": {
    "title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
    "volume": "review",
    "abstract": "Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue — the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=04UvXg4CvW": {
    "title": "EPIC: Compressing Deep GNNs via Expressive Power Gap-Induced Knowledge Distillation",
    "volume": "review",
    "abstract": "The teacher-student paradigm-based knowledge distillation (KD) has recently emerged as a promising technique for compressing graph neural networks (GNNs). Despite the great success in compressing moderate-sized GNNs, distilling deep GNNs (e.g., with over 100 layers) remains a tough challenge. A widely recognized reason is the *teacher-student expressive power gap*, i.e., the embeddings of a deep teacher may be extremely hard for a shallow student to approximate. Besides, the theoretical analysis and measurement of this gap are currently missing, resulting in a difficult trade-off between the needs of being \"lightweight'' and being \"expressive'' when selecting a student for the deep teacher. To bridge the theoretical gap and address the challenge of distilling deep GNNs, we propose the *first* GNN KD framework that quantitatively analyzes the teacher-student expressive power gap, namely **E**xpressive **P**ower gap-**I**ndu**C**ed knowledge distillation (**EPIC**). Our key idea is to formulate the estimation of the expressive power gap as an embedding regression problem based on the theory of polynomial approximation. Then, we show that the minimum approximation error has an upper bound, which decreases rapidly with respect to the number of student layers. Furthermore, we empirically demonstrate that the upper bound exponentially converges to zero as the number of student layers increases. Moreover, we propose to select an appropriate value for the number of student layers based on the upper bound, and propose an expressive power gap-induced loss term to further encourage the student to generate embeddings similar to those of the teacher. Experiments on large-scale benchmarks demonstrate that EPIC can effectively reduce the numbers of layers of deep GNNs, while achieving comparable or superior performance. Specifically, for the 1,001-layer RevGNN-Deep, we reduce the number of layers by 94\\% and accelerate inference by roughly eight times, while achieving comparable performance in terms of ROC-AUC on the large-scale benchmark ogbn-proteins",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=9qtswuW5ux": {
    "title": "Unsupervised graph neural networks with recurrent features for solving combinatorial optimization problems",
    "volume": "review",
    "abstract": "In recent years, graph neural networks (GNNs) have gained considerable attention as a promising approach to tackle combinatorial optimization problems. We introduce a novel algorithm, dubbed QRF-GNN in the following, that leverages the power of GNNs to efficiently solve combinatorial problems which have quadratic unconstrained binary optimization (QUBO) formulation. It relies on unsupervised learning and minimizes the loss function derived from QUBO relaxation. The key components of the architecture are the recurrent use of intermediate GNN predictions, parallel convolutional layers and combination of artificial node features as input. The performance of the algorithm was evaluated on benchmark datasets for maximum cut and graph coloring problems. Results of experiments show that QRF-GNN surpasses existing graph neural network based approaches and is comparable to the state-of-the-art conventional heuristics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RwwM7pKGWv": {
    "title": "Towards Dynamic EHR Phenotyping: A Generative Clustering Model",
    "volume": "review",
    "abstract": "In healthcare, identifying clinical phenotypes—subgroups defined by specific clinical traits—is essential for optimizing patient care. The wealth of Electronic Health Record (EHR) information has fueled data-driven approaches to tackle this challenge. Unfortunately, the heterogeneity, multi-modality, and dynamic nature of EHR data pose significant hurdles. We propose DeepGC, a novel generative, clustering, outcome-sensitive end-to-end deep learning (DL) model for uncovering dynamic phenotypes within temporal EHR data. DeepGC leverages patient trajectories and outcomes to identify clinically meaningful phenotypes that evolve over time. Our generative model employs a dynamic sequential approach based on a Markovian Dirichlet distribution and Variational Auto-Encoders (VAEs), which is capable of providing insights into the evolution of patient phenotypes and health status. Preliminary evaluation indicates that DeepGC shows promise in identifying distinct and interpretable phenotypes, and outperforming existing benchmarks, particularly with regard to outcome sensitivity (3 % increase in F1). We also showcase the model's potential to yield valuable insights into the future evolution of patients' health status",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FH7lfTfjcm": {
    "title": "ADELT: Transpilation Between Deep Learning Frameworks",
    "volume": "review",
    "abstract": "We propose the Adversarial DEep Learning Transpiler (ADELT), a novel approach to source-to-source transpilation between deep learning frameworks. ADELT uniquely decouples code skeleton transpilation and API keyword mapping. For code transpilation, it uses few-shot prompting on large language models, while for API keyword mapping, it employs contextual embeddings from a code-specific BERT. These embeddings are trained in a domain-adversarial setup to generate a keyword translation dictionary. ADELT is trained on an unlabeled web-crawled deep learning corpus, eschewing hand-crafted rules and parallel data. It outperforms state-of-the-art transpilers, improving exact match scores by 15.9 pts and 12.0 pts for PyTorch-Keras and PyTorch-MXNet transpilation pairs respectively. We provide open access to our code, corpus, and evaluation benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LWwYyxF3w9": {
    "title": "Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation",
    "volume": "review",
    "abstract": "Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes *without additional training*. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of *training-free generalization*. Experiments validate that TabPTM achieves promising performance in new datasets, even under few-shot scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=IBACinPJG5": {
    "title": "MIRAGE: Modelling Interpretable Multivariate Time Series Forecasts with Actionable Ground Explanations",
    "volume": "review",
    "abstract": "Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Trans- formers. However, in critical situations like predicting a death in an ICU or sudden gaming overindulgence; an accurate prediction without a contributing evidence is irrelevant. It is important to have model driven Interpretability, allowing proactive comprehension of trajectory to an extremity; and an associated Explainability, al- lowing for preventive steps; e.g., controlling BP to avoid death, or nudging players to take breaks to prevent overplay. We introduce a novel deep neural network, MI- RAGE, which overcomes the inter-dependent challenges of—(a) temporally non- smooth data trajectories for interpretability; (b) highly multi-dimensional tempo- ral space for explainability; and (c) improving forecasting accuracy—all at once. MIRAGE: (i) achieves over 85% improvement on the MSE of the forecasts on the most relevant SOM-VAE based SOTA networks; and (ii) unravels the intricate multi-variate relationships and temporal trajectories contributing to any sudden movement to criticalities on temporally chaotic datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ztpy1gsUpT": {
    "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
    "volume": "review",
    "abstract": "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=VbR6K7TQV4": {
    "title": "Learning the Latent Noisy Data Generative Process for Label-Noise Learning",
    "volume": "review",
    "abstract": "In learning with noisy labels, the noise transition reveals how an instance relates from its clean label to its noisy one. Accurately inferring an instance's noise transition is crucial for inferring its clean label. However, when only a noisy dataset is available, noise transitions can typically be inferred only for a ``special'' group of instances. To use these learned transitions to assist in inferring others, it is essential to understand the connections among different transitions across different instances. Existing work usually addresses this by introducing assumptions that explicitly define the similarity of noise transitions across various instances. However, these similarity-based assumptions often lack empirical validation and may not be aligned with real-world data. The misalignment can lead to misinterpretations of both noise transitions and clean labels. In this work, instead of directly defining similarity, we propose modeling the generative process of noisy data. Intuitively, to understand the connections among noise transitions across different instances, we represent the causal generative process of noisy data using a learnable graphical model. Relying solely on noisy data, our method can effectively discern the underlying causal generative process, subsequently inferring the noise transitions of instances and their clean labels. Experiments on various datasets with different types of label noise further demonstrate our method's effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=hkjcdmz8Ro": {
    "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
    "volume": "review",
    "abstract": "There is growing research interest in ensuring that large language models align with human safety and ethical guidelines. Adversarial attacks known as 'jailbreaks' pose a significant threat as they coax models into overriding alignment safeguards. Identifying these vulnerabilities through attacking a language model (red teaming) is instrumental in understanding inherent weaknesses and preventing misuse. We present Prompt Automatic Iterative Refinement (PAIR), which generates semantic jailbreaks with only black-box access to a language model. Empirically, PAIR often requires fewer than 20 queries, orders of magnitude fewer than prior jailbreak attacks. PAIR draws inspiration from the human process of social engineering, and employs an attacker language model to automatically generate adversarial prompts in place of a human. The attacker model uses the target model's response as additional context to iteratively refine the adversarial prompt. PAIR achieves competitive jailbreaking success rates and transferability on open and closed-source language models, including GPT-3.5/4, Vicuna, and PaLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Jpu1Gd3F1r": {
    "title": "Data Imputation by Pursuing Better Classification: A Supervised Learning Approach",
    "volume": "review",
    "abstract": "Data imputation, the process of filling in missing feature elements for incomplete data sets, plays a crucial role in data-driven learning. A fundamental belief is that data imputation is helpful for learning performance, and it follows that the pursuit of better classification can guide the data imputation process. While some works consider using label information to assist in this task, their simplistic utilization of labels lacks flexibility and may rely on strict assumptions. In this paper, we propose a new framework that effectively leverages supervision information to complete missing data in a manner conducive to classification. Specifically, this framework operates in two stages. Firstly, it leverages labels to supervise the optimization of similarity relationships among data, represented by the kernel matrix, with the goal of enhancing classification accuracy. To mitigate overfitting that may occur during this process, a perturbation variable is introduced to improve the robustness of the framework. Secondly, the learned kernel matrix serves as additional supervision information to guide data imputation through regression, utilizing the block coordinate descent method. The superiority of the proposed method is evaluated on four real-world data sets by comparing it with state-of-the-art imputation methods. Remarkably, our algorithm significantly outperforms other methods when the data is missing more than 60\\% of the features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RE0aibEQ1J": {
    "title": "IG-Net: Image-Goal Network for Offline Visual Navigation on A Large-Scale Game Map",
    "volume": "review",
    "abstract": "Navigating vast and visually intricate gaming environments poses unique challenges, especially when agents are deprived of absolute positions and orientations during testing. This paper addresses the challenge of training agents in such environments using a limited set of offline navigation data and a more substantial set of offline position data. We introduce the \\textit{Image-Goal Network} (IG-Net), an innovative solution tailored for these challenges. IG-Net is designed as an image-goal-conditioned navigation agent, which is trained end-to-end, directly outputting actions based on inputs without intermediary mapping steps. Furthermore, IG-Net harnesses position prediction, path prediction and distance prediction to bolster representation learning to encode spatial map information implicitly, an aspect overlooked in prior works. Our experiments and results demonstrate IG-Net's potential in navigating large-scale gaming environments, providing both advancements in the field and tools for the broader research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ajRRisV1n1": {
    "title": "Learning the Hidden Set Locally",
    "volume": "review",
    "abstract": "Learning elements of the hidden set(s), also known as group testing (GT), is a well-established area in which one party tries to discover elements hidden by the other party by asking queries and analyzing feedback. The feedback is a function of the intersection of the query with the hidden set -- in our case, it is a classical double-threshold function, which returns $i$ if the intersection is a singleton $i\\in [n]$ and \"null\" otherwise (i.e., when the intersection is empty or of size at least $2$). In this work, we introduce a local framework to this problem: each hidden element is an \"autonomous\" element and can analyze feedback itself, but only for the queries which this element is a part of. The goal is to design a deterministic non-adaptive sequence of queries that allows each non-hidden element to learn about all other hidden agents. We show that, surprisingly, this task requires substantially more queries than the classic group testing -- by proving a super-qubic (in terms of the number of hidden elements) lower bound and constructing a specific sequence of slightly longer length. We also extend the results to the model, where agents belong to various clusters and selection must be done in queries avoiding elements from ``interfering'' clusters. Our algorithms could be generalized to other feedback functions, to adversarial/stochastic fault-prone scenarios and applied to codes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=jsvvPVVzwf": {
    "title": "What Makes a Good Prune? Optimal Unstructured Pruning for Maximal Cosine Similarity",
    "volume": "review",
    "abstract": "Pruning is an effective method to reduce the size of deep neural network models, maintain accuracy, and, in some cases, improve the network's overall performance. However, the mechanisms underpinning pruning remain unclear. Why can different methods prune by different percentages yet achieve similar performance? Why can we not prune at the start of training? Why are some models more amenable to being pruned than others? Given a model, what is the maximum amount it can be pruned before significantly affecting the performance? This paper explores and answers these questions from the global unstructured magnitude pruning perspective with one epoch of fine-tuning. We develop the idea that cosine similarity is an effective proxy measure for functional similarity between the parent and the pruned network. We prove that the L1 pruning method is optimal when pruning by cosine similarity. We show that the higher the kurtosis of a model's parameter distribution, the more it can be pruned while maintaining performance. Finally, we present a simple method to determine the optimal amount by which a network can be L1-pruned based on its parameter distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kjZlzuVJF0": {
    "title": "Boosting Multi-Agent Reinforcement Learning via Transition-Informed Representations",
    "volume": "review",
    "abstract": "Effective coordination among agents in a multi-agent system necessitates an understanding of the underlying dynamics of the environment. However, in the context of multi-agent reinforcement learning (MARL), agent partially observed information leads to a lack of consideration for agent interactions and coordination from an ego perspective under the world model, which becomes the main obstacle to improving the data efficiency of MARL methods. To address this, motivated by the success of learning a world model in RL and cognitive science, we devise a world-model-driven learning paradigm enabling agents to gain a more holistic representation of individual observation of the environment. Specifically, we present the Transition-Informed Multi-Agent Representations (TIMAR) framework, which leverages the joint transition model, i.e., the surrogate world model, to learn effective representations among agents through a self-supervised learning objective. TIMAR incorporates an auxiliary module to predict future transitions based on sequential observations and actions, allowing agents to infer the latent state of the system and consider the influences of others. Experimental evaluation of TIMAR in various MARL environments demonstrates its significantly improved performance and data efficiency compared to strong baselines such as MAPPO, HAPPO, finetuned QMIX, MAT, and MA2CL. In addition, we found TIMAR can also improve the robustness and generalization of the Transformer-based MARL algorithm such as MAT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=BqEvdOS1Hs": {
    "title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain",
    "volume": "review",
    "abstract": "Existing game AI research mainly focuses on enhancing agents' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a \"self-centered\" manner. In this paper, we propose a \"human-centered\" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a \"baseline\", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=unxTEvHOW7": {
    "title": "EXPLEME: A Study in Meme Interpretability, Diving Beyond Input Attribution",
    "volume": "review",
    "abstract": "Memes, originally created for humor and social commentary, have evolved into vehicles for offensive and harmful content online. Detecting such content is crucial for upholding the integrity of digital spaces. However, binary classification of memes as offensive or not often falls short in practical applications. Ensuring the reliability of these classifiers and addressing inadvertent biases during training are essential tasks. While numerous input-attribution based interpretability methods exist to shed light on the model's decision-making process, they frequently yield insufficient and semantically irrelevant keywords extracted from input memes. In response, we propose a novel, theoretically grounded approach that extracts meaningful ``tokens\" from a global vocabulary, yielding both relevant and exhaustive set of interpretable keywords. This method provides valuable insights into the model's behavior and uncovers hidden meanings within memes, significantly enhancing transparency and fostering user trust. Through comprehensive quantitative and qualitative evaluations, we demonstrate the superior effectiveness of our approach compared to conventional baselines. Our research contributes to a deeper understanding of meme content analysis and the development of more robust and interpretable multimodal systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=vY9nzQmQBw": {
    "title": "Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis",
    "volume": "review",
    "abstract": "Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. The source code and model weights have been open-sourced",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFS9Pm7bsM": {
    "title": "Adversarial Latent Feature Augmentation for Fairness",
    "volume": "review",
    "abstract": "As fairness in machine learning has been increasingly important to mitigate bias in models, various methods to enhance fairness have been proposed. Among them, the data augmentation approach has shown promising results in improving fairness. However, existing data augmentation methods on either input or latent features provide limited evidence of how they discover bias and rectify it. In this paper, we propose the Adversarial Latent Feature Augmentation (ALFA) for fairness, which effectively merges adversarial attacks against fairness and data augmentation in the latent space to promote fairness. Though the adversarial perturbation against fairness has been discussed in existing literature, the effect of such adversarial perturbations has been inadequately studied only as a means to depreciate fairness. In contrast, in this paper, we point out that such perturbation can in fact be used to augment fairness. Drawing from a covariance-based fairness constraint, our method unveils a counter-intuitive relationship between adversarial attacks against fairness and enhanced model fairness upon training with the resultant perturbed latent features by hyperplane rotation. We theoretically prove that our adversarial fairness objective assuredly generates biased feature perturbation, and we validate with extensive experiments that training with adversarial features significantly improve fairness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=YrTI2Zu0dd": {
    "title": "An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression",
    "volume": "review",
    "abstract": "We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an ``agnostic'' view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (cf. Mallinar et al. 2022)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=T9w5ttdqLV": {
    "title": "Towards Complete Expressiveness Capacity of Mixed Multi-Agent Q Value Function",
    "volume": "review",
    "abstract": "Value decomposition is an efficient approach to achieving centralized training with decentralized execution in fully cooperative Multi-Agent Reinforcement Learning (MARL) problems. Recently, Strictly Monotonic Mixing Function (SMMF) has gained widespread application in value decomposition methods, but SMMF could suffer from convergence difficulties for the representational limitation. This paper investigates the circumstances under which the representational limitation occurs and presents approaches to overcome it. We begin our investigation with Linear Mixing Function (LMF), a simple case of SMMF. Firstly, we prove that LMF is free from representational limitation only in a rare case of MARL problems. Secondly, we propose a two-stage mixing framework, which includes a difference rescaling stage after SMMF to complete the representational capability. However, the capacity could remain unrealized for the cross interference between the representation of different action-values. Finally, we introduce gradient shaping to address this problem. The experimental results validate the expressiveness of LMF and demonstrate the effectiveness of our proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=IjJU2BRSCV": {
    "title": "Differentiable Tree Search in Latent State Space",
    "volume": "review",
    "abstract": "In decision-making problems with limited training data, policy functions approximated using deep neural networks often exhibit suboptimal performance. An alternative approach involves learning a world model from the limited data and determining actions through online search. However, the performance is adversely affected by compounding errors arising from inaccuracies in the learnt world model. While methods like TreeQN have attempted to address these inaccuracies by incorporating algorithmic structural biases into their architectures, the biases they introduce are often weak and insufficient for complex decision-making tasks. In this work, we introduce Differentiable Tree Search (DTS), a novel neural network architecture that significantly strengthens the inductive bias by embedding the algorithmic structure of a best-first online search algorithm. DTS employs a learnt world model to conduct a fully differentiable online search in latent state space. The world model is jointly optimised with the search algorithm, enabling the learning of a robust world model and mitigating the effect of model inaccuracies. We address potential Q-function discontinuities arising from naive incorporation of best-first search by adopting a stochastic tree expansion policy, formulating search tree expansion as a decision-making task, and introducing an effective variance reduction technique for the gradient computation. We evaluate DTS in an offline-RL setting with a limited training data scenario on Procgen games and grid navigation task, and demonstrate that DTS outperforms popular model-free and model-based baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=YH9tnuUYds": {
    "title": "Model-based Reinforcement Learning for Parameterized Action Spaces",
    "volume": "review",
    "abstract": "We propose a novel model-based reinforcement learning algorithm---Dynamics Learning and predictive control with Parameterized Actions (DLPA)---for Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control. Our empirical results on several standard benchmarks show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=PdaPky8MUn": {
    "title": "Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors",
    "volume": "review",
    "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, *using only the downstream task data*, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAGtjl7HOw": {
    "title": "Explaining Kernel Clustering via Decision Trees",
    "volume": "review",
    "abstract": "Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCl0TLboP1": {
    "title": "Improving Offline RL by Blending Heuristics",
    "volume": "review",
    "abstract": "We propose **H**e**u**ristic **Bl**ending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies the Bellman operators used in these algorithms, partially replacing the bootstrapped values with heuristic ones that are estimated with Monte-Carlo returns. For trajectories with higher returns, HUBL relies more on the heuristic values and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. HUBL is very easy to combine with many existing offline RL implementations by relabeling the offline datasets with adjusted rewards and discount factors. We derive a theory that explains HUBL's effect on offline RL as reducing offline RL's complexity and thus increasing its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-World benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ctXZJLBbyb": {
    "title": "Understanding Heterophily for Graph Neural Networks",
    "volume": "review",
    "abstract": "Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\\sqrt{\\mathbb{E}\\left[\\operatorname{deg}\\right]}$, where $\\mathbb{E}\\left[\\operatorname{deg}\\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimental impact on separability, which is equivalent to degrading $\\mathbb{E}\\left[\\operatorname{deg}\\right]$. Finally, when applying multiple GC operations, we show that the separability gains are determined by the normalized distance of the $l$-powered neighborhood distributions. It indicates that the nodes still possess separability as $l$ goes to infinity in a wide range of regimes. Extensive experiments on both synthetic and real-world data verify the effectiveness of our theory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=K4fd38VWHt": {
    "title": "Assessing Robustness via Score-based Adversarial Image Generation",
    "volume": "review",
    "abstract": "Most adversarial attacks and defenses focus on perturbations within small $\\ell_p$-norm constraints. However, $\\ell_p$ threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond $\\ell_p$-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than $\\ell_p$-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=TS8PXBN6B6": {
    "title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
    "volume": "review",
    "abstract": "Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes it particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 pts in Bug Fixing and 3 pts in Java-C# Transpilation. Our code and model are publicly available at https://anonymized",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ZiHI6raor0": {
    "title": "CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "Before taking actions in an environment with more than one intelligent agent, an autonomous agent may benefit from reasoning about the other agents and utilizing a notion of a guarantee or confidence about the behavior of the system. In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAMMARL, which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability. We then use these estimates to inform an agent's decision-making. For estimating such sets, we use the concept of conformal predictions, by means of which, we not only obtain an estimate of the most probable outcome but get to quantify the operable uncertainty as well. For instance, we can predict a set that provably covers the true predictions with high probabilities (e.g., 95%). Through several experiments in two fully cooperative multi-agent tasks, we show that CAMMARL elevates the capabilities of an autonomous agent in MARL by modeling conformal prediction sets over the behavior of other agents in the environment and utilizing such estimates to enhance its policy learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=yQuF0jslCc": {
    "title": "Online Fractional Knapsack With Predictions",
    "volume": "review",
    "abstract": "The well-known classical version of the online knapsack problem decides which of the arriving items of different weights and values to accept into a capacity-limited knapsack. In this paper, we consider the online fractional knapsack problem where items can be fractionally accepted. We present the first online algorithms for this problem which incorporate prediction about the input in several forms, including predictions of the smallest value chosen in the optimal offline solution, and interval predictions which give upper and lower bounds on this smallest value. We present algorithms for both of these prediction models, prove their competitive ratios, and give a matching worst-case lower bound. Furthermore, we present a learning-augmented meta-algorithm that combines our prediction techniques with a robust baseline algorithm to simultaneously achieve consistency and robustness. Finally, we conduct numerical experiments that show that our prediction algorithms significantly outperform a simple greedy prediction algorithm for the problem and the robust baseline algorithm, which does not use predictions. Furthermore, we show that our learning-augmented algorithms can leverage imperfect predictions (e.g., from a machine learning model) to greatly improve average-case performance without sacrificing worst-case guarantees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=eqz5aXtQv1": {
    "title": "STUPD: A Synthetic Dataset for Spatial and Temporal Relation Reasoning",
    "volume": "review",
    "abstract": "Understanding relations between objects is crucial for understanding the semantics of a visual scene. It is also an essential step in order to bridge visual and language models. However, current state-of-the-art computer vision models still lack the ability to perform spatial reasoning well. Existing datasets mostly cover a relatively small number of spatial relations, all of which are static relations that do not intrinsically involve motion. In this paper, we propose the Spatial and Temporal Understanding of Prepositions Dataset (STUPD) – a large scale video dataset for understanding static and dynamic spatial relationships derived from prepositions of the English language. The dataset contains 150K visual depictions (videos and images), consisting of 30 distinct spatial prepositional senses, in the form of object interaction simulations generated synthetically using Unity3D. In addition to spatial relations, we also propose 50K visual depictions across 10 temporal relations, consisting of videos depicting event/time-point interactions. To our knowledge, no dataset exists that represents temporal relations through visual settings. In this dataset, we also provide 3D information about object interactions such as frame-wise coordinates, and descriptions of the objects used. The goal of this synthetic dataset is to help models perform better in visual relationship detection in real-world settings. We demonstrate an increase in the performance of various models over 2 real-world datasets (ImageNet VidVRD and Spatial Senses) when pretrained on the STUPD dataset, in comparison to other pretraining datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=T0FuEDnODP": {
    "title": "Cooperative Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either listen, broadcast, listen and broadcast, or to isolate. The standard message propagation scheme can then be viewed as a special case of this framework where every node `listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic dataset and on real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=NI0RsRuFsW": {
    "title": "How Hard is Trojan Detection in DNNs? Fooling Detectors With Evasive Trojans",
    "volume": "review",
    "abstract": "Trojan attacks can pose serious risks by injecting deep neural networks with hidden, adversarial functionality. Recent methods for detecting whether a model is trojaned appear highly successful. However, a concerning and relatively unexplored possibility is that trojaned networks could be made harder to detect. To better understand the scope of this risk, we develop a general method for making trojans more evasive based on several novel techniques and observations. In experiments, we find that our evasive trojans reduce the efficacy of a wide range of detectors across numerous evaluation settings while maintaining high attack success rates. Surprisingly, we also find that our evasive trojans are substantially harder to reverse-engineer despite not being explicitly designed with this attribute in mind. These findings underscore the importance of developing more robust monitoring mechanisms for hidden functionality and clarifying the offense-defense balance of trojan detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5TlHjMVrNG": {
    "title": "Evaluating Robustness to Unforeseen Adversarial Attacks",
    "volume": "review",
    "abstract": "When considering real-world adversarial settings, defenders are unlikely to have access to the full range of deployment-time adversaries during training, and adversaries are likely to use realistic adversarial distortions that will not be limited to small $L_p$-constrained perturbations. To narrow in on this discrepancy between research and reality we introduce eighteen novel adversarial attacks, which we use to create ImageNet-UA, a new benchmark for evaluating model robustness against a wide range of unforeseen adversaries. We make use of our benchmark to identify a range of defense strategies which can help overcome this generalization gap, finding a rich space of techniques which can improve unforeseen robustness. We hope the greater variety and realism of ImageNetUA will make it a useful tool for those working on real-world worst-case robustness, enabling development of more robust defenses which can generalize beyond attacks seen during training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=U6Qulbv2qT": {
    "title": "Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes",
    "volume": "review",
    "abstract": "In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems, such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency. To this end, we posit a {\\em joint model class} for tasks and use the notion of $\\eta$-bracketing number to quantify its complexity; this number also serves as a general metric to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study upstream multi-task learning over PSRs, in which all tasks share the same observation and action spaces. We propose a provably efficient algorithm UMT-PSR for finding near-optimal policies for all PSRs, and demonstrate that the advantage of multi-task learning manifests if the joint model class of PSRs has a smaller $\\eta$-bracketing number compared to that of individual single-task learning. We also provide several example multi-task PSRs with small $\\eta$-bracketing numbers, which reap the benefits of multi-task learning. We further investigate downstream learning, in which the agent needs to learn a new target task that shares some commonalities with the upstream tasks via a similarity constraint. By exploiting the learned PSRs from the upstream, we develop a sample-efficient algorithm that provably finds a near-optimal policy. Upon specialization to the examples used to elucidate the $\\eta$-bracketing numbers, our downstream results further highlight the benefit compared to directly learning the target PSR without upstream information. Ours is the first theoretical study that quantifies the benefits of multi-task RL with PSRs over its single-task counterpart",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4r2ybzJnmN": {
    "title": "Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings",
    "volume": "review",
    "abstract": "Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights – one per synapse – whose positions correspond to the delays. These positions are learned together with the weights using the recently proposed Dilated Convolution with Learnable Spacings (DCLS). We evaluated our method on three datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and its non spiking version Google Speech Commands v0.02 (GSC) benchmarks, which require detecting temporal patterns. We used feedforward SNNs with two or three hidden fully connected layers, and vanilla leaky integrate-and-fire neurons. We showed that fixed random delays help and that learning them helps even more. Furthermore, our method outperformed the state-of-the-art in the three datasets without using recurrent connections and with substantially fewer parameters. Our work demonstrates the potential of delay learning in developing accurate and precise models for temporal data processing. Our code is based on PyTorch / SpikingJelly and available at: https://anonymous.4open.science/r/SNN-delays-6DD1/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HmL2Buf0Ur": {
    "title": "Can Copyright be Reduced to Privacy?",
    "volume": "review",
    "abstract": "There is a growing concern that generative AI models may generate outputs that closely resemble the copyrighted input content used for their training. This worry has intensified as the quality and complexity of generative models have immensely improved, and the availability of extensive datasets containing copyrighted material has expanded. Researchers are actively exploring strategies to mitigate the risk of producing infringing samples, and a recent line of work suggests employing techniques such as differential privacy and other forms of algorithmic stability to safeguard copyrighted content. In this work, we examine whether algorithmic stability techniques such as differential privacy are suitable to ensure the responsible use of generative models without inadvertently violating copyright laws. We argue that there are fundamental differences between privacy and copyright that should not be overlooked. In particular, we highlight that although algorithmic stability may be perceived as a practical tool to detect copying, it does not necessarily equate to copyright protection. Therefore, if it is adopted as a standard for copyright infringement, it may undermine the intended purposes of copyright law",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=GPKTIktA0k": {
    "title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A",
    "volume": "review",
    "abstract": "We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form \"*A is B*\", it will not automatically generalize to the reverse direction \"*B is A*\". This is the **Reversal Curse**. For instance, if a model is trained on \"Olaf Scholz was the ninth Chancellor of Germany\", it will not automatically be able to answer the question, \"Who was the ninth Chancellor of Germany?\". Moreover, the likelihood of the correct answer (\"Olaf Scholz\") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if \"*A is B*\" occurs, \"*B is A*\" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah Hawthorne is the composer of *Abyssal Melodies*\" and showing that they fail to correctly answer \"Who composed *Abyssal Melodies?*\". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter. This shows a failure of logical deduction that we hypothesize is caused by the Reversal Curse",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=O0vy7hHqyU": {
    "title": "Fake News Detection via an Adaptive Feature Matching Optimization Framework",
    "volume": "review",
    "abstract": "The rampant proliferation of fake news across online platforms has become a significant cause for concern, necessitating the creation of robust detection techniques. Within the confines of this investigation, we present an optimization methodology built upon salient attributes tailored for the identification of fake news, spanning both unimodal and multimodal data sources. By harnessing the capabilities inherent in a diverse array of modalities, ranging from textual to visual elements, we are able to comprehensively apprehend the multifaceted nature of falsified news stories. Primarily, our methodology introduces an unprecedented array of features, encompassing word-level, sentence-level, and contextual features. This infusion bestows upon it a robust capacity to adeptly accommodate a wide spectrum of textual content. Subsequently, we integrate a feature-centric optimization technique grounded in the principles of simulated annealing. This approach enables us to ascertain the most optimal fusion of features, thereby mitigating potential conflicts and interferences arising from the coexistence of textual and visual components. Empirical insights garnered from exhaustive dataset experimentation decisively underscore the efficacy of our proposed methodology. Our approach outperforms standalone modalities as well as traditional single-classifier models, as evidenced by its superior detection capabilities. This research underscores the indispensable role played by the integration of multimodal data sources and the meticulous optimization of feature amalgamations. These factors collectively contribute to the creation of a resilient framework tailored for the identification of fake news within the intricate landscape of our contemporary, data-rich environment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LKx4rubqkO": {
    "title": "Metric Learning for Detection of Large Language Model Generated Texts",
    "volume": "review",
    "abstract": "More efforts are being put into improving Large Language Models' (LLM) capabilities than into dealing with their implications. Current LLMs are able to generate texts that are seemingly indistinguishable from those written by human experts. While offering great quality of life, such breakthroughs also pose new challenges in education, science, and a multitude of other areas. To add up, current approaches in LLM text detection are either computationally expensive or need accesses to the LLMs' internal computations, both of which hinder their public accessibility. With such motivation, this paper presents a new paradigm of metric-based detection for LLM-generated texts that is able to balance among computational costs, accessibility, and performances. Specifically, the detection is performed through evaluating the similarity between a given text to an equivalent example generated by LLMs and through that determining the former's origination. In terms of architecture, the detection framework includes a text embedding model and a metric model. Currently, the embedding component is a pretrained language model. We focus on designing the metric component which is trained with triplets of same-context instances to signify distances between human responses and LLM ones while reducing that among LLM texts. Additionally, we develop and publish four datasets totaling over 85,000 prompts and triplets of responses in which one from human and two from GPT-3.5 TURBO for benchmarking and uses by the public. Experiment studies show that our best architectures maintain F1 scores in between 0.87 to 0.95 across the tested corpora in both same-corpus and out-of-corpus settings, either with or without paraphrasing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=pOBvr1PxFd": {
    "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge due to their colossal model size when it comes to practical deployment. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters can be pruned without hurting performance. Building upon insights gained from pre-LLM models, particularly BERT-level language models, prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity levels, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields substantially improved results. To elucidate the underlying reasons for this disparity, we conduct a comprehensive analysis of the distribution of token features within LLMs. In doing so, we discover a strong correlation with the emergence of outliers, defined as features exhibiting significantly greater magnitudes compared to their counterparts in feature dimensions. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of **non-uniform layerwise sparsity ratios** specifically designed for LLM pruning, termed as Outlier Weighed Layerwise sparsity (**OWL**). The sparsity ratio of OWL is directly proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, our approach exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by **61.22** and **6.80** perplexity at a high sparsity level of 70%, respectively. Code is submitted",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qRbkTbe8JT": {
    "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning",
    "volume": "review",
    "abstract": "Continual learning (CL) remains one of the long-standing challenges for deep neural networks due to catastrophic forgetting of previously acquired knowledge. Although rehearsal-based approaches have been fairly successful in mitigating catastrophic forgetting, they suffer from overfitting on buffered samples and prior information loss, hindering generalization under low-buffer regimes. Inspired by how humans learn using strong inductive biases, we propose IMEX-Reg to improve the generalization performance of experience rehearsal in CL under low buffer regimes. Specifically, we employ a two-pronged implicit-explicit regular- ization approach using contrastive representation learning (CRL) and consistency regularization. To further leverage the global relationship between representations learned using CRL, we propose a novel regularization strategy to guide the clas- sifier toward the activation correlations in the unit hypersphere of the CRL. Our results show that IMEX-Reg significantly improves generalization performance and outperforms rehearsal-based approaches in several CL scenarios. It is also robust to natural and adversarial corruptions with less task-recency bias. Additionally, we provide theoretical insights to support our design decisions further",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=z3mPLBLfGY": {
    "title": "Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning",
    "volume": "review",
    "abstract": "Many processes in biology and drug discovery involve various 3D interactions between molecules, such as protein and protein, protein and small molecule, etc. Given that different molecules are usually represented in different granularity, existing methods usually encode each type of molecules independently with different models, leaving it defective to learn the universal underlying interaction physics. In this paper, we first propose to universally represent an arbitrary 3D complex as a geometric graph of sets, shedding light on encoding all types of molecules with one model. We then propose a Generalist Equivariant Transformer (GET) to effectively capture both domain-specific hierarchies and domain-agnostic interaction physics. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where each module is E(3) equivariant and specialized for handling sets of variable sizes. Notably, in contrast to conventional pooling-based hierarchical models, our GET is able to retain fine-grained information of all levels. Extensive experiments on the interactions between proteins, small molecules and RNA/DNAs verify the effectiveness and generalization capability of our proposed method across different domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzkpLkd1S8": {
    "title": "Improving Robustness in Vision Transformers with Nullspace Noise Augmented Finetuning",
    "volume": "review",
    "abstract": "Enhancing the robustness of deep learning models, particularly in the realm of vision transformers (ViTs), is crucial for their real-world deployment. In this work, we explore the robustness of vision transformer models through the lens of nullspace, a fundamental concept in linear algebra, to propose a fine-tuning method that improves model robustness under various input perturbations. Our investigation centers on whether a vision transformer can exhibit resilience to input variations akin to the nullspace property in linear mappings, implying that perturbations sampled from this nullspace do not influence the model's output when added to the input. We confirm this by demonstrating the existence of a non-trivial nullspace in vision transformers, primarily attributed to the patch embedding layer. Moreover, we extend this idea beyond the linear layers, showcasing the feasibility of learning a non-linear counterpart (approximate nullspace) to the traditional nullspace for vision transformers through optimization techniques. Based on these insights, we propose a fine-tuning approach employing approximate nullspace noise to bolster the robustness of ViT models. Remarkably, within just a single epoch of fine-tuning, our method effectively mitigates the adverse effects of distribution shifts and adversarial perturbations across a wide spectrum of scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=WTh6EnJXWQ": {
    "title": "DeepROCK: Error-controlled interaction detection in deep neural networks",
    "volume": "review",
    "abstract": "The complexity of deep neural networks (DNNs) makes them powerful but also makes them challenging to interpret, hindering their applicability in error-intolerant domains. Existing methods attempt to reason about the internal mechanism of DNNs by identifying feature interactions that influence prediction outcomes. However, such methods typically lack a systematic strategy to prioritize interactions while controlling confidence levels, making them difficult to apply in practice for scientific discovery and hypothesis validation. In this paper, we introduce a method, called DeepROCK, to address this limitation by using knockoffs, which are dummy variables that are designed to mimic the dependence structure of a given set of features while being conditionally independent of the response. Together with a novel DNN architecture involving a pairwise-coupling layer, DeepROCK jointly controls the false discovery rate (FDR) and maximizes statistical power. In addition, we identify a challenge in correctly controlling FDR using off-the-shelf feature interaction importance measures. DeepROCK overcomes this challenge by proposing a calibration procedure applied to existing interaction importance measures to make the FDR under control at a target level. Finally, we validate the effectiveness of DeepROCK through extensive experiments on simulated and real datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8S7eGD15b6": {
    "title": "Subspace Grid-sweep: ML Defense Evaluation via Constrained Brute-force Search",
    "volume": "review",
    "abstract": "It is becoming increasingly imperative to design robust ML defenses. However, recent work has found that many defenses that initially resist state-of-the-art attacks can be broken by an adaptive adversary. Attacks can initially make defenses look strong by not finding potential adversarial examples due to obfuscated gradients, limited compute, unlucky initialization, etc. In this work, we make steps towards more reliable defense evaluation by introducing a new defense evaluation tool, Subspace Grid-sweep, that leverages deterministic inference to more simply evaluate adversarial robustness. We use Subspace Grid-sweep to show that a previously published, but now broken, defense could have been known to be broken without performing a fully adaptive attack. In order to make Subspace Grid-sweep applicable to random defenses, we show how to make deterministic variants of random defenses while retaining similar empirical effectiveness. As a result, we show that randomness may not be necessary for these defense's robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2l7g7zwC4z": {
    "title": "Embedding File Structure for Tabular File Preparation",
    "volume": "review",
    "abstract": "We introduce the notion of file structure, the set of characters within a file's content that do not belong to data values. Data preparation can be considered as a pipeline of heterogeneous steps with the common theme of wrangling the structure of a file to access its payload in a downstream task. We claim that solving typical data preparation tasks benefits from an explicit representation of file structure. We propose a novel approach for learning such a representation, which we call a structural embedding, using the raw file content as input. Our approach is based on a novel neural network architecture, composed of a transformer module and a convolutional module, trained in a self-supervised fashion on almost 1M public data files to learn structural embeddings. We demonstrate the usefulness of structural embeddings in several steps of a data preparation pipeline: data loading, row classification, and column type annotation. For these tasks, we show that our approach obtains performances comparable with state-of-the-art baselines on six real-world datasets, and, more importantly, we improve upon such baselines by combining them with the structural embeddings provided by our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=7Jwpw4qKkb": {
    "title": "Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
    "volume": "review",
    "abstract": "The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=af2c8EaKl8": {
    "title": "Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making",
    "volume": "review",
    "abstract": "The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that DC better understands the underlying meaning in data and exhibits enhanced generalization capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=PXXuLvIH5r": {
    "title": "From Matching to Mixing: A Graph Interpolation Approach for SAT Instance Generation",
    "volume": "review",
    "abstract": "The Boolean satisfiability problem (SAT) stands as a canonical NP-complete combinatorial optimization (CO) problem, with wide impact on both theoretical and industrial scenarios. In particular, the scarcity of real-world SAT instances and their usefulness for tuning SAT solvers underscore the necessity for effective and efficient ways of hard instance generation, whereas existing methods either struggle to maintain plausible hardness or suffer from limited applicability. Different from the typical construction-based methods, this paper introduces an adaptive and efficient graph interpolation approach that in place modifies the raw structure of graph-represented SAT instance by replacing it with a counterpart from another instance. Specifically, our method involves a two-stage matching and mixing pipeline. The matching aims to find a correspondence map of literal nodes from two instance graphs via learned features from a matching network; while the mixing stage involves iteratively exchanging clause pairs with the highest correspondence scores until a specified replacement ratio is achieved. We further show that under our matching-mixing framework, moderate randomness can avoid hardness degradation of SAT instances by introducing Gumbel noise. Experimental results show the superiority of the proposed method with both resemblance in structure and hardness, as well as general applicability in an efficient way. Source code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YIWe2amtrV": {
    "title": "Are LLMs Aware that Some Questions are not Open-ended?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown the impressive capability of answering questions in a wide range of scenarios. However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and have to respond more deterministically but some do not. We refer to the ability as question awareness that LLMs know to adjust the determinacy of the answers according to the questions. The lack of question awareness leads to two contradictory issues: (1) Too casual to answer non-open-ended questions. (2) Too boring to answer open-ended questions. In this paper, we first evaluate the question awareness ability of LLMs. The experimental results show that LLMs have the above issues of lacking the awareness of questions in certain domains, e.g. factual knowledge. To mitigate these issues, we propose a method called Question Awareness Temperature (QAT) sampling. This method enhances the question awareness ability of LLMs by dynamically adjusting the answer distributions based on question features. The automatic adjustment in QAT eliminates the need for manual temperature tuning in text generation. These findings underscore the potential of QAT sampling to enhance LLMs' question-awareness capabilities, thereby advancing their performance in various LLM benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rUH2EDpToF": {
    "title": "Generative Marginalization Models",
    "volume": "review",
    "abstract": "We introduce *marginalization models* (MAMs), a new family of generative models for high-dimensional discrete data. They offer scalable and flexible generative modeling with tractable likelihoods by explicitly modeling all induced marginal distributions. Marginalization models enable fast evaluation of arbitrary marginal probabilities with a single forward pass of the neural network, which overcomes a major limitation of methods with exact marginal inference, such as autoregressive models (ARMs). We propose scalable methods for learning the marginals, grounded in the concept of \"*marginalization self-consistency*\". Unlike previous methods, MAMs support scalable training of any-order generative models for high-dimensional problems under the setting of *energy-based training*, where the goal is to match the learned distribution to a given desired probability (specified by an unnormalized (log) probability function such as energy function or reward function). We demonstrate the effectiveness of the proposed model on a variety of discrete data distributions, including binary images, language, physical systems, and molecules, for *maximum likelihood* and *energy-based training* settings. MAMs achieve orders of magnitude speedup in evaluating the marginal probabilities on both settings. For energy-based training tasks, MAMs enable any-order generative modeling of high-dimensional problems beyond the capability of previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3i7iNGxw6r": {
    "title": "Where Does In-context Machine Translation Happen in Large Language Models?",
    "volume": "review",
    "abstract": "Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs MT with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region in layer-wise attention heads where GPT models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on GPTNeo2.7B and Bloom3B, we demonstrate evidence of a \"task recognition\" point where the translation task is encoded into the input representations and attention to context is no longer necessary. Our layer-wise fine-tuning experiments indicate that the most effective layers for MT fine-tuning are the layers critical to task recognition. Next, we examine redundancy in layers following task recognition, observing that masking these later layers does not hurt performance significantly. Finally, we train discrete attention head gates with $L_0$ regularisation and find evidence that the most pruneable heads occur after task recognition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=j5EbZEyK9I": {
    "title": "Mo' Data Mo' Problems: How Data Composition Compromises Scaling Properties",
    "volume": "review",
    "abstract": "The accumulation of data in the machine learning setting is often presented as a panacea to address its many modeling problems---including issues with correctness, robustness, and bias. But when does adding more data help, and when does it hinder progress on desired model outcomes? We model data accumulation from multiple sources and present analysis of two practical strategies that result the addition of more data degrading overall model performance. We then demonstrate empirically on three real-world datasets that adding training data can result in reduced overall accuracy and reduced worst-subgroup performance while introducing further accuracy disparities between subgroups. We use a simple heuristic for determining when the accumulation of more data may worsen the issues the additional data is meant to solve. We conclude with a discussion on considerations for data collection and suggestions for studying data composition in the age of increasingly large models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=vSh5ePa0ph": {
    "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?",
    "volume": "review",
    "abstract": "Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RyUvzda8GH": {
    "title": "A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks",
    "volume": "review",
    "abstract": "Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly outperforms the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jo36Mzwuvf": {
    "title": "Gaussian Process-Based Corruption-resilience Forecasting Models",
    "volume": "review",
    "abstract": "Time series forecasting is challenging due to complex temporal dependencies and unobserved external factors, which can lead to incorrect predictions by even the best forecasting models. Using more training data is one way to improve the accuracy, but this source is often limited. In contrast, we are building on successful denoising approaches for image generation. When a time series is corrupted by the common isotropic Gaussian noise, it yields unnaturally behaving time series. To avoid generating unnaturally behaving time series that do not represent the true error mode in modern forecasting models, we propose to employ Gaussian Processes to generate smoothly-correlated corrupted time series. However, instead of directly corrupting the training data, we propose a joint forecast-corrupt-denoise model to encourage the forecasting model to focus on accurately predicting coarse-grained behavior, while the denoising model focuses on capturing fine-grained behavior. All three parts are interacting via a corruption model which enforces the model to be resilient. Our extensive experiments demonstrate that our proposed corruption-resilient forecasting approach is able to improve the forecasting accuracy of several state-of-the-art forecasting models as well as several other denoising approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=DjIsNDEOYX": {
    "title": "Scalable Monotonic Neural Networks",
    "volume": "review",
    "abstract": "In this research, we focus on the problem of learning monotonic neural networks, as preserving the monotonicity of a model with respect to a subset of inputs is crucial for practical applications across various domains. Although several methods have recently been proposed to address this problem, they have limitations such as not guaranteeing monotonicity in certain cases, requiring additional inference time, lacking scalability with increasing network size and number of monotonic inputs, and manipulating network weights during training. To overcome these limitations, we introduce a simple but novel architecture of the partially connected network which incorporates a 'scalable monotonic hidden layer' comprising three units: the exponentiated unit, ReLU unit, and confluence unit. This allows for the repetitive integration of the scalable monotonic hidden layers without other structural constraints. Consequently, our method offers ease of implementation and rapid training through the conventional error-backpropagation algorithm. We accordingly term this method as Scalable Monotonic Neural Networks (SMNN). Numerical experiments demonstrated that our method achieved comparable prediction accuracy to the state-of-the-art approaches while effectively addressing the aforementioned weaknesses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=s2NjWfaYdZ": {
    "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
    "volume": "review",
    "abstract": "Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose KPrune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models. KPrune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, KPrune shows significant accuracy improvements up to 58.02%p higher F1 score compared to existing retraining-free pruning algorithms under a high compression rate of 80% on the SQuAD benchmark without any retraining process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=bwZ9xh178a": {
    "title": "Exploiting Negative Samples: A Catalyst for Cohort Discovery in Healthcare Analytics",
    "volume": "review",
    "abstract": "In healthcare analytics, particularly when dealing with binary diagnosis or prognosis tasks, unique challenges arise from the inherent asymmetry between positive and negative samples. Positive samples, denoting patients who develop a disease, are defined based on stringent medical criteria. In contrast, negative samples are defined in an open-ended manner, leading to a vast potential set. Despite this fundamental asymmetry, the role of negative samples remains underexplored in prior research, possibly due to the enormous challenge of investigating an infinitely large negative sample space. To bridge this gap, we propose an innovative approach to facilitate cohort discovery within negative samples, leveraging a Shapley-based exploration of interrelationships between these samples, which holds promise for uncovering valuable insights concerning the studied disease, and related comorbidity and complications. We quantify each sample's contribution using data Shapley values, subsequently constructing the Negative Sample Shapley Field to model the distribution of all negative samples. Next, we transform this field through manifold learning, preserving the essential data structure information while imposing an isotropy constraint in data Shapley values. Within this transformed space, we pinpoint cohorts of medical interest via density-based clustering. We empirically evaluate the effectiveness of our approach on our hospital's electronic medical records. The medical insights derived from the discovered cohorts are validated by clinicians, which affirms the medical value of our proposal in unveiling meaningful insights aligning with existing domain knowledge, thereby bolstering medical research and well-informed clinical decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=N1gmpVd4iE": {
    "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
    "volume": "review",
    "abstract": "Agents built with large language models (LLMs) have recently achieved great advancements. However, most of the efforts focus on single-agent or cooperative settings, leaving more general multi-agent environments underexplored. We propose a new framework powered by reinforcement learning (RL) to develop strategic language agents, i.e., LLM-based agents with strategic thinking ability, for a popular language game, Werewolf. Werewolf is a social deduction game with hidden roles that involves both cooperation and competition and emphasizes deceptive communication and diverse gameplay. Our agent tackles this game by first using LLMs to reason about potential deceptions and generate a set of strategically diverse actions. Then an RL policy, which selects an action from the candidates, is learned by population-based training to enhance the agents' decision-making ability. By combining LLMs with the RL policy, our agent produces a variety of emergent strategies, achieves the highest win rate against other LLM-based agents, and stays robust against adversarial human players in the Werewolf game",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=DxM73sxtna": {
    "title": "Private Overparameterized Linear Regression without Suffering in High Dimensions",
    "volume": "review",
    "abstract": "This study focuses on differentially private linear regression in the over-parameterized regime. We propose a new variant of the differentially private Follow-The-Regularized-Leader (DP-FTRL) algorithm that uses a random noise with a general covariance matrix for differential privacy. This leads to improved privacy and utility (excess risk) trade-offs. Firstly, even when reduced to an existing DP-FTRL algorithm that uses an isotropic noise, our excess risk bound is sharper as a function of the eigenspectrum of the data covariance matrix and the ground truth model parameter. Furthermore, when unlabeled public data is available, we can design a better noise covariance matrix structure to improve the utility. For example, when the ground truth has a bounded $\\ell_2$-norm, and the eigenspectrum decays polynomially (i.e., $\\lambda_i=i^{-r}$ for $r>1$), our method achieves $\\mathcal{\\tilde O}(N^{-\\frac{r}{1+2r}})$ and $\\mathcal{\\tilde O}(N^{-\\frac{r}{3+r}\\wedge\\frac{2r}{1+3r}})$ excess error for identity and specially designed covariance matrices, respectively. Notably, our method with a specially designed covariance matrix outperforms the one with an identity matrix when the eigenspectrum decays at least quadratically fast, i.e., $r\\geq 2$. Our proposed method significantly improves upon existing differentially private methods for linear regression, which tend to scale with the problem dimension, leading to a vacuous guarantee in the over-parameterized regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=YKK1jXEWja": {
    "title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown the ability to solve complex decision-making tasks beyond the natural language processing tasks. Current LLM agents such as ReAct can solve interactive decision-making tasks by imitating the few-shot demonstrations given in the prompt. The LLM agents based on few-shot in-context learning (ICL) achieve surprisingly high performance without training. Despite the simplicity and generalizability, the ICL-based approaches lack optimizing trajectories based on the reward from an environment. In this paper, we introduce Prospector, a reflective LLM agent that features Self-Asking and Trajectory Ranking. To elicit the LLM agent to generate more proper actions that contribute to following a given instruction, we introduce additional Self-Asking steps in the few-shot demonstrations. Furthermore, to take advantages of the stochastic generation of LLMs, we provide Trajectory Ranking in which the LLM agent generates diverse (creative) trajectories and the most rewarding trajectory is selected by using the reward prediction models. On the representative decision-making benchmark environments such as ALFWorld and WebShop, we empirically demonstrate that Prospector can considerably increase the success rate of given tasks, while outperforming recent advancements such as ReAct and Reflexion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=vueANsev2R": {
    "title": "Investigating the chaotic dynamics produced by deep reinforcement learning controllers",
    "volume": "review",
    "abstract": "In recent years, deep Reinforcement Learning (RL) has demonstrated remarkable performance in simulated control tasks however there have been significantly fewer applications to real-world problems. While there are several reasons for this dichotomy, one key limitation is a need for theoretical stability guarantees in real-world applications, a property which cannot be provided by Deep Neural Network controllers. In this work, we investigate the stability of trained RL policies for continuous control tasks and identify the types of dynamics produced by the Markov Decision Process (MDP). We find the solutions produced by this interaction are deterministically chaotic with small initial inaccuracies in sensor readings or actuator movements compounding over time producing significantly different long-term outcomes, despite intervention in intermediate steps. The presence of these chaotic dynamics in the MDP provides evidence that RL controllers produce unstable solutions, limiting their application to real-world problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=x5LvBK43wg": {
    "title": "PROGRAM: PROtotype GRAph Model based Pseudo-Label Learning for Test-Time Adaptation",
    "volume": "review",
    "abstract": "Test-time adaptation (TTA) aims to adapt a pre-trained model from a source domain to a target domain only using online unlabeled target data during testing, without accessing to the source data or modifying the original training process. Among the various TTA methods, pseudo-labeling has gained popularity. However, the presence of incorrect pseudo-labels can hinder the effectiveness of target domain adaptation. To overcome this challenge, we propose a novel TTA method, called PROtotype GRAph Model based pseudo-label learning (PROGRAM). PROGRAM consists of two key components: (1) Prototype Graph Model (PGM) for reliable pseudo-label generation; (2) Robust Self-Training (RST) for test-time adaptation with noisy pseudo-labels. PGM constructs the graph using prototypes and test samples, facilitating effective message passing among them to generate more reliable pseudo-labels. RST combines the advantages of consistency regularization and pseudo-labeling to achieve robust target domain adaptation in the presence of noisy pseudo-labels. Our proposed PROGRAM can be easily integrated into existing baselines, resulting in consistent improvement. Extensive experiments show that our PROGRAM outperforms the existing TTA methods on multiple domain generalization and image corruption benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xC8xh2RSs2": {
    "title": "Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on HuggingFace",
    "volume": "review",
    "abstract": "Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face - one of the largest platforms for sharing and collaborating on ML models and datasets - as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity: While 86.0\\% of the top 100 downloaded dataset cards fill out all sections suggested by Hugging Face community, only 7.9\\% of dataset cards with no downloads complete all these sections. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, accounting for 36.2\\% and 33.6\\% of the total card length, respectively, for the most downloaded datasets. In contrast, the Considerations for Using the Data section receives the lowest proportion of content, accounting for just 2.1\\% of the text. (3) By analyzing the subsections within each section and utilizing topic modeling to identify key topics, we uncover what is discussed in each section, and underscore significant themes encompassing both technical and social impacts, as well as limitations within the Considerations for Using the Data section. (4) Our findings also highlight the need for improved accessibility and reproducibility of datasets in the Usage sections. (5) In addition, our human annotation evaluation emphasizes the pivotal role of comprehensive dataset content in shaping individuals' perceptions of a dataset card's overall quality. Overall, our study offers a unique perspective on analyzing dataset documentation through large-scale data science analysis and underlines the need for more thorough dataset documentation in machine learning research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=XCMbagV0No": {
    "title": "A Language-Agent Approach to Formal Theorem-Proving",
    "volume": "review",
    "abstract": "Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have emerged as a promising approach to control tasks. We present a language-agent approach that offers state-of-the-art performance in formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries. We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-art models fine-tuned on proof data, at finding correct proofs quickly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6klub5OXr": {
    "title": "An Extensive Analysis on the Underlying Premises Behind Deep Reinforcement Learning Algorithm Design",
    "volume": "review",
    "abstract": "The progress in reinforcement learning algorithm development is at one of its highest points starting from the initial study that enabled sequential decision making from high-dimensional observations. Currently, deep reinforcement learning research has had quite recent breakthroughs from learning without the presence of rewards to learning functioning policies without even knowing the rules of the game. In our paper we focus on the underlying premises that are actively used in deep reinforcement learning algorithm development. We theoretically demonstrate that the performance profiles of the algorithms developed for the data-abundant regime do not transfer to the data-limited regime monotonically. We conduct large-scale experiments in the Arcade Learning Environment and our results demonstrate that the baseline algorithms perform significantly better in the data-limited regime compared to the set of algorithms that were initially designed and compared in the data-abundant regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=hkSjjs4o5d": {
    "title": "A Differentially Private Clustering Algorithm for Well-Clustered Graphs",
    "volume": "review",
    "abstract": "We study differentially private (DP) algorithms for recovering clusters in well-clustered graphs, which are graphs whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such graphs have widespread application as a benchmark in the theoretical analysis of spectral clustering. We provide an efficient ($\\epsilon$,$\\delta$)-DP algorithm tailored specifically for such graphs. Our algorithm draws inspiration from the recent work of Chen et al., who developed DP algorithms for recovery of stochastic block models in cases where the graph comprises exactly two nearly-balanced clusters. Our algorithm works for well-clustered graphs with $k$ nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms. We conduct experimental evaluations on datasets with known ground truth clusters to substantiate the prowess of our algorithm. We also show that any (pure) $\\epsilon$-DP algorithm would result in substantial error",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=jeMZi2Z9xe": {
    "title": "Follow-the-Perturbed-Leader for Adversarial Bandits: Heavy Tails, Robustness, and Privacy",
    "volume": "review",
    "abstract": "We study adversarial bandit problems with potentially heavy-tailed losses. Unlike standard settings with non-negative and bounded losses, managing negative and unbounded losses introduces a unique challenge in controlling the ``stability'' of the algorithm and hence the regret. To tackle this challenge, we propose a Follow-the-Perturbed-Leader (FTPL) based learning algorithm. Notably, our method achieves (nearly) optimal worst-case regret, eliminating the need for an undesired assumption inherent in the Follow-the-Regularized-Leader (FTRL) based approach. Thanks to this distinctive advantage, our algorithmic framework finds novel applications in two important scenarios with unbounded heavy-tailed losses. For adversarial bandits with heavy-tailed losses and Huber contamination, which we call the robust setting, our algorithm is the first to match the lower bound (up to a $\\polylog(K)$ factor, where $K$ is the number of actions). In the private setting, where true losses are in a bounded range (e.g., $[0,1]$) but with additional Local Differential Privacy (LDP) guarantees, our algorithm achieves an improvement of a $\\polylog(T)$ factor in the regret bound compared to the best-known results, where $T$ is the total number of rounds. Furthermore, when compared to state-of-the-art FTRL-based algorithms, our FTPL-based algorithm has a more streamlined design. It eliminates the need for additional explicit exploration and solely maintains the absolute value of loss estimates below a predetermined threshold",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Ge0GEOvifh": {
    "title": "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks",
    "volume": "review",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets has achieved remarkable success in zero-shot classification and enabled transferability to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning and backdoor attacks, compared to supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks successful. This is four orders of magnitude smaller than what is required to poison supervised models. Despite this vulnerability, existing methods are very limited in defending CLIP models during pre-training. In this work, we propose a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately. Then, it carefully divides the data into safe and risky subsets. SAFECLIP trains on the risky data by applying unimodal CL to image and text modalities separately, and trains on the safe data using the CLIP loss. By gradually increasing the size of the safe subset during the training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks without harming the CLIP performance. Our extensive experiments show that SAFECLIP decrease the attack success rate of targeted data poisoning attacks from 93.75% to 0% and that of the backdoor attacks from 100% to 0%, without harming the CLIP performance on various datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=d94x0gWTUX": {
    "title": "Tool-Augmented Reward Modeling",
    "volume": "review",
    "abstract": "Reward modeling (a.k.a. preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We anticipate that this publicly available dataset will facilitate and inspire further research advancements in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=rpwES4pe9W": {
    "title": "Refined Tensorial Radiance Field: Harnessing coordinate based networks for novel view synthesis from sparse inputs",
    "volume": "review",
    "abstract": "The multi-plane encoding approach has been highlighted for its ability to serve as static and dynamic neural radiance fields without sacrificing generality. This approach constructs related features through projection onto learnable planes and interpolating adjacent vertices. This mechanism allows the model to learn fine-grained details rapidly and achieves outstanding performance. However, it has limitations in representing the global context of the scene, such as object shapes and dynamic motion over times when available training poses are sparse. In this work, we propose refined tensorial radiance fields that harness coordinate-based networks known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing global context, while the multi-plane network focuses on capturing fine-grained details. We demonstrate that using residual connections effectively preserves their inherent properties. Additionally, the proposed curriculum training scheme accelerates the disentanglement of these two features. We empirically show that the proposed method achieves comparable results to multi-plane encoding with high denoising penalties in static NeRFs. Meanwhile, it outperforms others for the task with dynamic NeRFs using sparse inputs. In particular, we prove that excessively increasing denoising regularization for multi-plane encoding effectively eliminates artifacts; however, it can lead to artificial details that appear authentic but are not present in the data. On the other hand, we note that the proposed method does not suffer from this issue",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=EW8ZExRZkJ": {
    "title": "Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods",
    "volume": "review",
    "abstract": "Recent deep learning applications, exemplified by text-to-image tasks, often involve high-dimensional inputs and outputs. While several studies have investigated the function estimation capabilities of deep learning, research on dilated convolutional neural networks (CNNs) has mainly focused on cases where input dimensions are infinite but output dimensions are one-dimensional, similar to many other studies. However, many practical deep learning tasks involve high-dimensional (or even infinite dimensional) inputs and outputs. In this paper, we investigate the optimality of dilated CNNs for estimating a map between infinite-dimensional input and output spaces by analyzing their approximation and estimation abilities. For that purpose, we first show that approximation and estimation errors depend only on the smoothness and decay rate with respect to the infinity norm of the output, and their estimation accuracy actually achieve the {\\it minimax optimal} rate of convergence. Second, we demonstrate that the dilated CNNs outperform {\\it any} linear estimators including kernel ridge regression and $k$-NN estimators in a minimax error sense, highlighting the usefulness of feature learning realized by deep neural networks. Our theoretical analysis particularly explains the success of deep learning in recent high-dimensional input-output tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=SXTr9hIvJ1": {
    "title": "Reweighted Solutions for Weighted Low Rank Approximation",
    "volume": "review",
    "abstract": "The weighted low rank approximation problem is an important yet computationally challenging primitive with applications ranging from statistical analysis, model compression, and signal processing. To cope with the NP-hardness of this problem, prior work either considers heuristics or bicriteria algorithms to solve this problem. In this work, we introduce a new relaxed solution to the weighted low rank approximation which outputs a matrix that is not necessarily low rank, but can be stored using very few parameters and gives provable approximation guarantees for this problem when the rank matrix has low rank. Our central idea is to use the weight matrix itself to reweight the low rank solution. Our algorithm is extremely simple to implement and achieves remarkable empirical performance in applications to model compression. Our algorithm also gives nearly optimal communication complexity bounds for a natural distributed algorithm associated with the low rank approximation problem, for which we show matching communication lower bounds. Together, our communication complexity bounds show that the rank of the weight matrix provably parameterizes the communication complexity of weighted low rank approximation. We also obtain the first feature selection guarantees for weighted low rank approximation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=XOnya9gSdF": {
    "title": "Consistent algorithms for multi-label classification with macro-at-$k$ metrics",
    "volume": "review",
    "abstract": "We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These ``macro-at-$k$'' metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices. Empirical results provide evidence for the competitive performance of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=50vyPuz0iv": {
    "title": "Iteratively Refined Behavior Regularization for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "One of the fundamental challenges for offline reinforcement learning (RL) is ensuring robustness to data distribution. Whether the data originates from a near-optimal policy or not, we anticipate that an algorithm should demonstrate its ability to learn an effective control policy that seamlessly aligns with the inherent distribution of offline data. Unfortunately, behavior regularization, a simple yet effective offline RL algorithm, tends to struggle in this regard. In this paper, we propose a new algorithm that substantially enhances behavior-regularization based on conservative policy iteration. Our key observation is that by iteratively refining the reference policy used for behavior regularization, conservative policy update guarantees gradual improvement, while also implicitly avoiding querying out-of-sample actions to prevent catastrophic learning failures. We prove that in the tabular setting this algorithm is capable of learning the optimal policy covered by the offline dataset, commonly referred to as the in-sample optimal policy. We then explore several implementation details of the algorithm when function approximations are applied. The resulting algorithm is easy to implement, requiring only a few lines of code modification to existing methods. Experimental results on the D4RL benchmark indicate that our method outperforms previous state-of-the-art baselines in most tasks, clearly demonstrate its superiority over behavior regularization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ul1cjLB98Y": {
    "title": "A Theory of Unimodal Bias in Multimodal Learning",
    "volume": "review",
    "abstract": "Using multiple input streams simultaneously in training multimodal neural networks is intuitively advantageous, but practically challenging. A key challenge is unimodal bias, where a network overly relies on one modality and ignores others during joint training. While unimodal bias is well-documented empirically, our theoretical understanding of how architecture and data statistics influence this bias remains incomplete. Here we develop a theory of unimodal bias with deep multimodal linear networks. We calculate the duration of the unimodal phase in learning, as a function of the depth at which modalities are fused within the network, dataset statistics, and initialization. We find that the deeper the layer at which fusion occurs, the longer the unimodal phase. In addition, our theory reveals the modality learned first is not necessarily the modality that contributes more to the output. Our results, derived for multimodal linear networks, extend to ReLU networks in certain settings. Taken together, this work illuminates pathologies of multimodal learning under joint training, showing that late and intermediate fusion architectures can give rise to long unimodal phases and even prioritize learning a less helpful modality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=FAY6ORIvn5": {
    "title": "How well does Persistent Homology generalize on graphs?",
    "volume": "review",
    "abstract": "Persistent Homology (PH) is one of the pillars of topological data analysis that leverages multiscale topological descriptors to extract meaningful features from data. More recently, the combination of PH and neural networks has been successfully used to tackle predictive tasks on graphs. However, the generalization capabilities of PH on graphs remain largely unexplored. We derive a PAC-Bayesian perturbation analysis to bridge this gap. Specifically, we introduce the first data-dependent generalization guarantees for neural network-based persistence layers (PersLay). Notably, PersLay consists of a general framework that subsumes various vectorization methods of persistence diagrams in the literature. We substantiate our theoretical analysis with experimental studies and provide insights about the generalization of PH on real-world graph classification benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=d4uL2MSe0z": {
    "title": "Dynamic Layer Tying for Parameter-Efficient Transformers",
    "volume": "review",
    "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=TLBPjECC5D": {
    "title": "Unlearning via Sparse Representations",
    "volume": "review",
    "abstract": "Machine unlearning, which involves erasing knowledge about a forget set from a trained model, can prove to be costly and infeasible by existing techniques. We propose a nearly compute-free zero-shot unlearning technique based on a discrete representational bottleneck. We show that the proposed technique efficiently unlearns the forget set and incurs negligible damage to the model's performance on the rest of the data set. We evaluate the proposed technique on the problem of \\textit{class unlearning} using three datasets: CIFAR-10, CIFAR-100, and LACUNA-100. We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning. Across all three datasets, the proposed technique performs as well as if not better than SCRUB while incurring almost no computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=GSBHKiw19c": {
    "title": "Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "Learning a precise dynamics model can be crucial for offline reinforcement learning, which, unfortunately, has been found to be quite challenging. Dynamics models that are learned by fitting historical transitions often struggle to generalize to unseen transitions. In this study, we identify a hidden but pivotal factor termed \\emph{dynamics reward} that remains consistent across transitions, offering a pathway to better generalization. Therefore, we propose the idea of reward-consistent dynamics models: any trajectory generated by the dynamics model should maximize the dynamics reward derived from the data. We implement this idea as the MOREC (Model-based Offline reinforcement learning with Reward Consistency) method, which can be seamlessly integrated into previous offline model-based reinforcement learning (MBRL) methods. MOREC learns a generalizable dynamics reward function from offline data, which is subsequently employed as a transition filter in any offline MBRL method: when generating transitions, the dynamics model generates a batch of transitions and selects the one with the highest dynamics reward value. On a synthetic task, we visualize that MOREC has a strong generalization ability and can surprisingly recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL benchmarks, MOREC improves the previous state-of-the-art performance by a significant margin, i.e., 4.6\\% on D4RL tasks and 25.9\\% on NeoRL tasks. Notably, MOREC is the first method that can achieve above 95\\% online RL performance in 6 out of 12 D4RL tasks and 3 out of 9 NeoRL tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=N581Nje6fH": {
    "title": "Long Horizon Episodic Decision Making for Cognitively Inspired Robots",
    "volume": "review",
    "abstract": "The Human decision-making process works by recollecting past sequences of observations and using them to decide the best possible action in the present. These past sequences of observations are stored in a derived form which only includes important information the brain thinks might be useful in the future, while forgetting the rest. Transformers have shown great results in multi-modal robotic navigation and human-robot collaboration tasks but lack the ability to scale to large memory sizes and learn long horizon tasks efficiently as the computational requirements needed to run these models scale non-linearly with memory length. Our model for tries to mimic the human brain and improve the memory efficiency of transformers by using a modified TransformerXL architecture which uses Automatic Chunking that chunks the past memories and only attends to the relevant chunks in the transformer block. On top of this, we use ForgetSpan which is technique to remove memories that do not contribute to learning. We also theorize the technique of Similarity based forgetting where the current observations are compared with the elements in the memory and only the new observations are stored, similar to how humans do not store repetitive memories. We test our model in various visual and audio-visual tasks that demand long horizon recollection, audio-visual instruction deciphering and robotic navigation. These tasks test the abilities of the robot that would be required in a human-robot collaboration scenario. We demonstrate that Automatic Chunking with ForgetSpan can improve the memory efficiency and help models to memorize important information and also achieve better performance than the baseline TransformerXL in the tasks previously mentioned. We also show that our model generalizes well by testing the trained models in modified versions of the tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.5,
    "authors": []
  },
  "https://openreview.net/forum?id=NgtEafc8NZ": {
    "title": "Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation",
    "volume": "review",
    "abstract": "Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy setting. This approach not only simplifies the implementation of off-policy policy gradient algorithms but also leads to consistent and robust performance across various benchmark tasks. Specifically, by removing the need for a state-action-value function Vlearn simplifies the learning process and allows for more efficient exploration and exploitation in complex environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kO8AxyGBxG": {
    "title": "UNITE:Universally Trustworthy GNN Via Subgraph Identification",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) have become instrumental in modeling graph-structured data, with applications spanning diverse fields. Despite their prowess, challenges such as susceptibility to adversarial attacks, inherent biases, and opacity in decision-making processes have emerged. While efforts exist to address individual trustworthiness facets like robustness, interpretability, and fairness, a comprehensive solution remains elusive. This study introduces \\Algname(\\unite), a novel end-to-end framework uniquely designed to holistically integrate these dimensions. Unlike traditional approaches, \\Algname leverages the intricate relationships between these aspects in graph data, presenting optimization goals grounded in information-theoretic principles. Preliminary experiments on real-world datasets indicate that \\Algname outperforms existing methods, achieving a harmonious blend of interpretability, robustness, and fairness. This work addresses the pressing challenges in GNNs for trustworthy graph neural networks, paving the way for their broader adoption in critical domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=ymjI8feDTD": {
    "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
    "volume": "review",
    "abstract": "Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.13). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IlNVkYUSfF": {
    "title": "Resonator-Gated RNNs",
    "volume": "review",
    "abstract": "Sequence learning tasks frequently involve data with repetitive and periodic temporal patterns. Detecting these patterns is essential for accurate predictions and informed decision-making in various domains. There is, however, still huge potential in augmenting sequence learning algorithms in this regard. In RNN-based sequence learning, gated RNNs, such as long short-term memory networks (LSTMs) and gated recurrent units (GRUs), are the de facto standard. While adept at capturing longer-term dependencies, gated RNNs still sometimes struggle with periodic data components, because their gating mechanism is designed to prioritize retaining static relevant information. As a result, these networks often challenged by periodicity in the data. We present a novel memory unit that incorporates a simple resonator circuit. The resonator facilitates the recognition of periodic data patterns, focusing on data-specific time scales and respective frequencies. Moreover, it enables the forward propagation of information through resonating dynamics while stably channeling the gradient backwards. We show that our resonator-gated RNN (RG-RNN) accelerates the training convergence on multiple sequence classifications tasks. Moreover, it significantly outperforms vanilla LSTMs on three out of four benchmark tasks in terms of accuracy. We conclude that resonator-based gating offers a new inductive bias to gated RNNs, focusing learning on the detection and processing of periodic data patterns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEhTnQZB3D": {
    "title": "Learning with Language Inference and Tips for Continual Reinforcement Learning",
    "volume": "review",
    "abstract": "Acquiring a generalizable policy by continually learning a sequence of tasks is a natural human skill yet challenging for current reinforcement learning algorithms. This is largely due to the dilemma that the agent is expected to quickly adapt to new tasks (plasticity) while retaining the common knowledge from previous tasks (stability). In this work, we present a scheme referred to as \"Learning with Language Inference and Tips (LLIT)\", which introduces a rewarding mechanism to parse and ground human knowledge in natural language form to the task space and produces an interpretable policy for each task in task-agnostic setting. LLIT trains a shared policy for each task by inferring and embedding the tips and content of the task. The language instructions inferred by the large language model (LLM) are then used to pre-train an auxiliary reward model with observations' embedding, thereby extracting the semantic representations in tasks. Simultaneously, the instructions and tips embedding will be collected and organized as a prompt pool to capture the correlation among tasks. Hence, closely related tasks exhibit greater neuron overlap in the policy network, stemming from shared semantics, which effectively curbs cross-task interference and forgetfulness. Given the auxiliary reward model trained on previous tasks that interprets human knowledge in natural language, new task adaptation reduces to highly efficient tips aggregation and sub-network finetuning. In experimental studies, LLIT achieves a desirable plasticity-stability trade-off without any task-specfic information. It also outperforms existing continual RL methods in terms of overall performance, forgetting reduction, and adaptation to unseen tasks. Our code is available at https://github.com/llm4crl/LLIT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAMoxm86KV": {
    "title": "Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients",
    "volume": "review",
    "abstract": "Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication round inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimation, and (b) develop the technique of adaptive gradient correction using these gradient surrogates to mitigate the aforementioned disparity. Based on these, we propose the federated zeroth-order optimization using trajectory-informed surrogate gradients (FZooS) algorithm for query- and communication round-efficient federated ZOO. FZooS achieves theoretical improvements over the existing approaches, which is supported by our real-world experiments on federated black-box adversarial attack and non-differentiable metric optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rUx0zQFwD1": {
    "title": "Quantum Speedups in Linear Programming via Sublinear Multi-Gibbs Sampling",
    "volume": "review",
    "abstract": "As a basic optimization technique, linear programming has found wide applications in many areas. In this paper, we propose an improved quantum algorithm for solving a linear programming problem with $m$ constraints and $n$ variables in time $\\widetilde{O} (\\sqrt{m+n}\\gamma^{2.25})$, where $\\gamma = Rr/\\varepsilon$ is the additive error $\\varepsilon$ scaled down with bounds $R$ and $r$ on the size of the primal and dual optimal solutions, improving the prior best $\\widetilde O(\\sqrt{m+n}\\gamma^{2.5})$ by Bouland, Getachew, Jin, Sidford, and Tian (ICML 2023) and Gao, Ji, Li, and Wang (NeurIPS 2023). Our algorithm solves linear programming via a zero-sum game formulation, under the framework of the sample-based optimistic multiplicative weight update. At the heart of our construction, is an improved quantum multi-Gibbs sampler for diagonal Hamiltonians with time complexity \\emph{sublinear} in inverse temperature $\\beta$, breaking the general $O(\\beta)$-barrier",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=icTZCUbtD6": {
    "title": "Dissecting sample hardness: Fine-grained analysis of Hardness Characterization Methods",
    "volume": "review",
    "abstract": "Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify ''hard'' samples. However, there is a lack of consensus regarding the definition and evaluation of ''hardness''. Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of various types of hardness. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers various strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=tPjVRmHqCg": {
    "title": "Curiosity Driven Protein Sequence Generation via Reinforcement Learning",
    "volume": "review",
    "abstract": "Protein sequence design is a critical problem in the field of bio-engineering and biotechnology. However, the search space for protein sequence design is incredibly vast and sparsely populated, which poses significant challenges. On the other hand, generative models struggle to adapt to different usage scenarios and objectives, leading to limited adaptability and generalization. To address these challenges, we explore a reinforcement learning algorithm based on latent space that enables protein sequence generation and mutation for different scenarios. Our approach has several advantages: (1) The reinforcement learning algorithm allows us to adjust the reward function according to different tasks and scenarios, enabling the model to generate and mutate protein sequences in a targeted manner. (2) The latent space mapped by ESM-2 is continuous, unlike the initial sparse and discrete space, and the curiosity mechanism further improves search efficiency. We evaluate our method in completely different scenarios, including different protein functions and sequences, and our experimental results demonstrate significant performance improvement over existing methods. We conduct multiple ablation studies to validate the rationality of our design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=D9SA02esgh": {
    "title": "MorphOcc: An Implicit Generative Model of Neuronal Morphologies",
    "volume": "review",
    "abstract": "Understanding the diversity and complexity of the morphology of different types of neurons is important for understanding neural circuits. We need quantitative, unbiased methods to capture the structural and morphological features of neurons. With the advent of large-scale structural datasets, this analysis becomes feasible using data-drive approaches. Existing generative models are limited to modeling dendritic and axonal skeleton graphs, without considering the actual 3D shape. In this work, we propose MORPHOCC, a model that represents the diversity of neu- rons in mouse primary visual cortex (V1) in a single neural network by encoding each neuron's morphology into a low-dimensional embedding. From this embed- ding the 3d shape can be reconstructed. We train our model on 797 dendritic shapes of V1 neurons. The learned embedding captures morphological features well and enables cell type classification into known cell types. Interpolating be- tween samples in embedding space generates new instances of neurons without supervision. MORPHOCC has the potential to improve our understanding of neu- rons in the brain by facilitating large-scale analysis and providing a model for representing neuronal morphologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OfAO2mes1": {
    "title": "Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency",
    "volume": "review",
    "abstract": "Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, *i.e.*, without the need for additional clean data or manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exposes the prediction invariance of poisoned data to an input scaling factor. Based on this, we resolve the backdoor data identification problem as a hierarchical data splitting optimization problem, leveraging a novel SPC-based loss function as the primary optimization objective. Our innovation unfolds in several key aspects. First, we revisit the vanilla SPC method, unveiling its limitations in addressing the proposed backdoor identification problem. Subsequently, we develop a bi-level optimization-based approach to precisely identify backdoor data by minimizing the advanced SPC loss. Finally, we demonstrate the efficacy of our proposal against a spectrum of backdoor attacks, encompassing basic label-corrupted attacks as well as more sophisticated clean-label attacks, evaluated across various benchmark datasets. Experiment results show that our approach often surpasses the performance of current baselines in identifying backdoor data points, resulting in about an average 4\\%-20\\% improvement in AUROC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7fxzVTSgZC": {
    "title": "Offline Imitation Learning without Auxiliary High-quality Behavior Data",
    "volume": "review",
    "abstract": "In this work, we study the problem of Offline Imitation Learning (OIL), where an agent aims to learn from the demonstrations composed of expert behaviors and sub-optimal behaviors without additional online environment interactions. Previous studies typically assume that there is high-quality behavioral data mixed in the auxiliary offline data and seriously degrades when only low-quality data from an off-policy distribution is available. In this work, we break through the bottleneck of OIL relying on auxiliary high-quality behavior data and make the first attempt to demonstrate that low-quality data is also helpful for OIL. Specifically, we utilize the transition information from offline data to maximize the policy transition probability towards expert-observed states. This guidance can improve long-term returns on states that are not observed by experts when reward signals are not available, ultimately enabling imitation learning to benefit from low-quality data. We instantiate our proposition in a simple but effective algorithm, Behavioral Cloning with Dynamic Programming (BCDP), which involves executing behavioral cloning on the expert data and dynamic programming on the unlabeled offline data respectively. In the experiments on benchmark tasks, unlike most existing offline imitation learning methods that do not utilize low-quality data sufficiently, our BCDP algorithm can still achieve an average performance gain of more than 40\\% even when the offline data is purely random exploration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=NLRo4qhg6t": {
    "title": "HIWE: Scene Importance Weighted Encoding For Fast Neural Radiance Field Training",
    "volume": "review",
    "abstract": "Neural radiance fields (NeRFs) have emerged as a powerful scene representa- tion technique to implicitly encode radiance information in space. Recent works demonstrated that using a grid-based positional encoding to encode 3D radiance information in space achieves fast training speeds, often requiring only a few min- utes of training on small-scale synthetic datasets. However, training a NeRF model that uses a grid encoding on large outdoor scenes requires several hours of train- ing. In many scenarios, large scenes may have different amounts of detailing at different regions, with reconstruction/representation quality more important for some detailing compared to others. Different regions of the scene are however given equal importance and thus typically no regions of the scene are prioritized in allocating parameters in the learned model. In this work, we propose a new grid-based positional encoding technique that integrates scene importance infor- mation in large scenes to accelerate training. Our encoding flexibly allocates more model parameters to learn the radiance information in regions of the scene that are deemed more important. This ensures that the more detailed scene regions are represented with a larger number of parameters, allowing more detailed radiance information to be encoded. With our approach, we demonstrate higher quality representation for the important parts of the scene compared to state-of-art tech- niques for instant NeRF training, while enabling on-par or faster training times as state-of-art NeRF models and small model sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8uFGTNXIF": {
    "title": "Simplifying Referred Visual Search with Conditional Contrastive Learning",
    "volume": "review",
    "abstract": "This paper introduces a new challenge for image similarity search in the context of fashion, addressing the inherent ambiguity in this domain stemming from complex images. We present Referred Visual Search (RVS), a task allowing users to define more precisely the desired similarity, following recent interest in the industry. We release a new large public dataset, LAION-RVS-Fashion, consisting of 272k fashion products with 842k images extracted from LAION, designed explicitly for this task. However, unlike traditional visual search methods in the industry, we demonstrate that superior performance can be achieved by bypassing explicit object detection and adopting weakly-supervised conditional contrastive learning on image tuples. Our method is lightweight and demonstrates robustness, reaching Recall at one superior to strong detection-based baselines against 2M distractors. Code, data, and models will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=hz2zhaZPXm": {
    "title": "Towards Foundation Models for Learning on Tabular Data",
    "volume": "review",
    "abstract": "Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference, but it also showcases performance that approaches, and in instances, even transcends, the renowned yet mysterious closedsource LLMs like GPT-4. Furthermore, when fine-tuning with scarce data, our model achieves remarkable efficiency and maintains competitive performance with abundant training data. Finally, while our results are promising, we also delve into TabFM's limitations and potential opportunities, aiming to stimulate and expedite future research on developing more potent TabFMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=8zJevzvk64": {
    "title": "Schrodinger Bridge to Bridge Generative Diffusion Method to Off-Policy Evaluation",
    "volume": "review",
    "abstract": "The problem of off-policy evaluation (OPE) in reinforcement learning (RL), which evaluates a given policy using data collected from a different behavior policy, plays an important role in many real-world applications. The OPE under the model of episodic non-stationary finite-horizon Markov decision process (MDP) has been widely studied. However, the general model-free importance sampling (IS) methods suffer from the curse of horizon and dimensionality, while the improved marginal importance sampling (MIS) can only be restrained to the case where the state space $\\mathcal{S}$ is sufficiently small. The model-based methods often have limited scope of application. To find a widely-applicable OPE algorithm when $\\mathcal{S}$ is continuous and high-dimensional that avoids the curse of horizon and dimensionality, which means the error of the estimator grows exponentially with the number of horizon $H$ and the dimension $d$ of the state space $\\mathcal{S}$, we apply the diffusion Schr\"odinger bridge generative model to construct a model-based estimator (CDSB estimator). Moreover, we established the statistical rate of the estimation error of the value function with a polynomial rate of $O(H^2\\sqrt{d})$, which, to the best of our knowledge, is one of the first theoretical rate results on applying Schr\"odinger bridge to reinforcement learning. This breaks the restraint of the complexity of the state space for OPE under MDP with large horizon and can be applied to various real-life decision problems with continuous setting, which is shown in our simulation using our method in continuous, high-dimensional and long-horizon RL environments and its comparison with other existing algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rO62BY3dYc": {
    "title": "Pruning via Ranking (PvR): A unified structured pruning approach",
    "volume": "review",
    "abstract": "The increase in width and depth has facilitated neural networks to learn from large amounts of data leading to state-of-the-art results in both vision and NLP tasks. In order to democratize such massive networks, it is important to deploy them on resource-limited devices through model compression techniques such as structured pruning. Unfortunately, most pruning methods are tailored towards compressing specific models due to widely differing network architectures for distinct tasks. At the same time, it is desirable for pruning algorithms to generate optimal subnetworks according to user-specified parameter budgets. In this work, we propose Pruning via Ranking (PvR), a novel, global structured pruning approach which generates dense sub-networks that comply with any user-supplied parameter budget. PvR consists of a grouping module and a ranking module that are used to generate smaller networks in terms of both function composition as well as network width for a given dataset. The smaller networks are then trained from scratch instead of being fine-tuned as we empirically demonstrate using a recently proposed model complexity measure that re-initialization after pruning followed by re-training results in better performance. We compare our method against multiple pruning approaches on benchmark datasets, namely, CIFAR10, Tiny ImageNet and IMDB 50K movie reviews, with standard models, namely, VGG16, ResNet34 and Bert-base-uncased. We use both accuracy and model inference latency metrics to evaluate the performance of each approach. The smaller networks proposed by PvR for a range of parameter budgets when trained from scratch outperform all other methods across all datasets and models. In fact, our recommended sub-networks with fewer layers achieve less than $1$\\% test accuracy drop even after pruning $90$\\% of the original model across all networks and datasets while enjoying lower inference latency due to reduced depth",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=5e0yWSNGIc": {
    "title": "Exposing the Silent Hidden Impact of Certified Training in Reinforcement Learning",
    "volume": "review",
    "abstract": "Deep reinforcement learning research has enabled reaching significant performance levels for sequential decision making in MDPs with highly complex observations and state dynamics with the aid of deep neural networks. However, this aid came with a cost that is inherent to deep neural networks which have increased volatilities towards indistinguishable peculiarly crafted non-robust directions. To alleviate these volatilities several studies suggested techniques to cope with this problem via explicitly regulating the temporal difference loss for the worst-case sensitivity. In our study, we show that these certified training techniques come with a cost that intriguingly causes inconsistencies and overestimations in the value functions. Furthermore, our results essentially demonstrate that vanilla trained deep reinforcement learning policies have more accurate and consistent estimates for the state-action values. We believe our results reveal foundational intrinsic properties of the certified Lipschitz training techniques and demonstrate the need to rethink the approach to resilience in deep reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=vlQ56aWJhl": {
    "title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks",
    "volume": "review",
    "abstract": "Spiking Neural Networks (SNNs) are biologically plausible models that have been identified as potentially apt for deploying energy-efficient intelligence at the edge, particularly for sequential learning tasks. However, training of SNNs poses significant challenges due to the necessity for precise temporal and spatial credit assignments. Back-propagation through time (BPTT) algorithm, whilst the most widely used method for addressing these issues, incurs a high computational cost due to its temporal dependency. In this work, we propose S-TLLR, a novel three-factor temporal local learning rule inspired by the Spike-Timing Dependent Plasticity (STDP) mechanism, aimed at training deep SNNs on event-based learning tasks. Furthermore, S-TLLR is designed to have low memory and time complexities, which are independent of the number of time steps, rendering it suitable for online learning on low-power edge devices. To demonstrate the scalability of our proposed method, we have conducted extensive evaluations on event-based datasets spanning a wide range of applications, such as image and gesture recognition, audio classification, and optical flow estimation. In all the experiments, S-TLLR achieved high accuracy, comparable to BPTT, with reduction in the number of computations between $1.1-10\\times$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5VD7dS3cZX": {
    "title": "Rethinking the Solution to Curse of Dimensionality on Randomized Smoothing",
    "volume": "review",
    "abstract": "Randomized Smoothing (RS) is currently a scalable certified defense method providing robustness certification against adversarial examples. Although significant progress has been achieved in providing defenses against $\\ell_p$ adversaries, early investigations found that RS suffers from the curse of dimensionality, indicating that the robustness guarantee offered by RS decays significantly with increasing input data dimension. Double Sampling Randomized Smoothing (DSRS) is the state-of-the-art method that provides a theoretical solution to the curse of dimensionality under concentration assumptions on the base classifier. However, we speculate the solution to the curse of dimensionality can be deepened from the perspective of the smoothing distribution. In this work, we further address the curse of dimensionality by theoretically showing that some Exponential General Gaussian (EGG) distributions with the exponent $\\eta$ can provide $\\Omega(\\sqrt{d})$ lower bounds for the $\\ell_2$ certified radius with tighter constant factors than DSRS. Our theoretical analysis shows that the lower bound improves with monotonically decreasing $\\eta \\in (0,2)$. Intriguingly, we observe a contrary phenomenon that EGG provides greater certified radii at larger $\\eta$, on real-world tasks. Further investigations show these discoveries are not contradictory, which are in essence dependent on whether the assumption in DSRS absolutely holds. Our experiments on real-world datasets demonstrate that EGG distributions bring significant improvements for point-to-point certified accuracy, up to 4\\%-6\\% on ImageNet. Furthermore, we also report the performance of Exponential Standard Gaussian (ESG) distributions on DSRS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mssRRt6OPE": {
    "title": "Relevance-based embeddings for efficient relevance retrieval",
    "volume": "review",
    "abstract": "In many machine learning applications, the most relevant items for a particular query should be efficiently extracted. The relevance function is typically an expensive neural similarity model making the exhaustive search infeasible. A typical solution to this problem is to train another model that separately embeds queries and items to a vector space, where similarity is defined via the dot product or cosine similarity. This allows one to search the most relevant objects through fast approximate nearest neighbors search at the cost of some reduction in quality. To compensate for this reduction, the found candidates are then re-ranked by the expensive similarity model. In this paper, we propose an alternative approach that utilizes the relevances of the expensive model to make relevance-based embeddings. We show both theoretically and empirically that describing each query by its relevance for a set of support items creates a powerful query representation. Additionally, we investigate several strategies for selecting these support items and show that additional significant improvements can be obtained. Our experiments on diverse datasets show improved performance over existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Ch7WqGcGmb": {
    "title": "Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants",
    "volume": "review",
    "abstract": "Error feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as Top-k. While EF was proposed almost a decade ago (Seide et al, 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtárik et al, 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the {\\em quadratic mean} of certain smoothness parameters, we improve this dependence to their {\\em arithmetic mean}, which is always smaller, and can be substantially smaller, especially in heterogeneous data regimes. We take the reader on a journey of our discovery process. Starting with the idea of applying EF21 to an equivalent reformulation of the underlying problem which (unfortunately) requires (often impractical) machine cloning, we continue to the discovery of a new {\\em weighted} version of EF21 which can (fortunately) be executed without any cloning, and finally circle back to an improved analysis of the original EF21 method. While this development applies to the simplest form of EF21, our approach naturally extends to more elaborate variants involving stochastic gradients and partial participation. Further, our technique improves the best-known theory of EF21 in the ``rare features'' regime (Richtárik et al, 2023). Finally, we validate our theoretical findings with suitable experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOPJantEd": {
    "title": "Wigner kernels: body-ordered equivariant machine learning without a basis",
    "volume": "review",
    "abstract": "Machine-learning models based on a point-cloud representation of a physical object are ubiquitous in scientific applications and particularly well-suited to the atomic-scale description of molecules and materials. Among the many different approaches that have been pursued, the description of local atomic environments in terms of their discretized neighbor densities has been used widely and very successfully. We propose a novel density-based method which involves computing \"Wigner kernels''. These are fully equivariant and body-ordered kernels that can be computed iteratively with a cost that is independent of the basis used to discretize the density and grows only linearly with the maximum body-order considered. This is in marked contrast to feature-space models, whose number of terms and computational cost scale exponentially with increasing order of correlations. We present several examples of the accuracy of models based on Wigner kernels in chemical applications, for both scalar and tensorial targets, reaching state-of-the-art accuracy on the popular QM9 benchmark dataset. We discuss the broader relevance of these findings to equivariant geometric machine-learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=X0fDR10B7c": {
    "title": "Predictive Coding beyond Correlations",
    "volume": "review",
    "abstract": "Bayesian and causal inference are fundamental processes for intelligence. Bayesian inference models observations: what can be inferred about $y$ if we observe a related variable $x$? Causal inference models interventions: if we directly change $x$, how will $y$ change? Predictive coding is a neuroscience-inspired method for performing Bayesian inference on continuous state variables using local information only. In this work, we show how a simple change in the inference process of predictive coding enables interventional and counterfactual inference in scenarios where the causal graph is known. We then extend our results, and show how predictive coding can be used in cases where the graph is unknown, and has to be inferred from observational data. This allows us to perform structure learning and causal query answering on predictive coding-based structural causal models. Empirically, we test our method on a large number of benchmarks, as well as presenting experiments that show potential applications in machine learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2inBuwTyL2": {
    "title": "Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks",
    "volume": "review",
    "abstract": "Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for instance, hanging a mug on a rack. In such cases, the solution should be equivariant to the initial position of the objects as well as the agent, and invariant to the pose of the camera. This poses a challenge for learning systems which attempt to solve this task by learning directly from high-dimensional demonstrations: the agent must learn to be both equivariant as well as precise, which can be challenging without any inductive biases about the problem. In this work, we propose a method for precise relative pose prediction which is provably SE(3)-equivariant, can be learned from only a few demonstrations, and can generalize across variations in a class of objects. We accomplish this by factoring the problem into learning an SE(3) invariant task-specific representation of the scene and then interpreting this representation with novel geometric reasoning layers which are provably SE(3) equivariant. We demonstrate that our method can yield substantially more precise placement predictions in simulated placement tasks than previous methods trained with the same amount of data, and can accurately represent relative placement relationships data collected from real-world demonstrations. Supplementary information and videos can be found at https://sites.google.com/view/reldist-iclr-2023",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=reBq1gmlhS": {
    "title": "Learning Differentially Private Rewards from Human Feedback",
    "volume": "review",
    "abstract": "We study the privacy of reinforcement learning from human feedback. In particular, we focus on solving the problem of reinforcement learning from preference rankings, subject to the constraint of differential privacy, in MDPs where true rewards are given by linear functions. To achieve this, we analyze $(\\epsilon,\\delta)$-differential privacy (DP) for both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. We provide a differentially private algorithm for learning rewards from human rankings. We further show that the privately learned rewards can be used to train policies achieving statistical performance guarantees that asymptotically match the best known algorithms in the non-private setting, which are in some cases minimax optimal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3YaCghZNt": {
    "title": "Lemur: Integrating Large Language Models in Automated Program Verification",
    "volume": "review",
    "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that typically demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kRdcwzEL5J": {
    "title": "CUS3D: A New Comprehensive Urban-Scale Semantic Segmentation 3D Benchmark Dataset",
    "volume": "review",
    "abstract": "With the continuous advancement of smart city construction, the availability of large-scale and semantically enriched datasets is essential for enhancing the machine's ability to understand urban scene. When dealing with large-scale scene, mesh data has a distinct advantage over point cloud data, as it can provide inherent geometric topology information and consume low memory space. However, existing publicly available large-scale scene mesh datasets have limitations in scale and semantic richness, and cannot cover a wider range of urban semantic information. Moreover, the prevailing large-scale 3D datasets mainly consist of a single data type, which restricts the wide applicability of benchmark applications and hinders the further development of 3D semantic segmentation techniques in urban scene. To address these issues, we propose a comprehensive urban-scale semantic segmentation benchmark dataset. This dataset provides finely annotated point cloud and mesh data types for 3D, as well as high-resolution original 2D images with detailed 2D semantic annotations. It is well suited for various research pursuits on semantic segmentation methodologies. The dataset covers a vast area of approximately 2.85 square kilometers, containing 10 semantic labels that span both urban and rural scenes. Each 3D point or triangular mesh in the dataset is meticulously labeled with one of ten semantic categories. We evaluate the performance of this novel benchmark dataset using 6 widely adopted deep learning baselines. The dataset will be publicly available upon the publish of the paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=liuqDwmbQJ": {
    "title": "ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models",
    "volume": "review",
    "abstract": "With the ever-increasing popularity of pretrained Video-Language Models (VidLMs), there is a pressing need to develop robust evaluation methodologies that delve deeper into their visio-linguistic capabilities. To address this challenge, we present ViLMA (Video Language Model Assessment), a task-agnostic benchmark that places the assessment of fine-grained capabilities of these models on a firm footing. Task-based evaluations, while valuable, fail to capture the complexities and specific temporal aspects of moving images that VidLMs need to process. Through carefully curated counterfactuals, ViLMA offers a controlled evaluation suite that sheds light on the true potential of these models, as well as their performance gaps compared to human-level understanding. ViLMA also includes proficiency tests, which assess basic capabilities deemed essential to solving the main counterfactual tests. We show that current VidLMs' grounding abilities are no better than those of vision-language models which use static images. This is especially striking once the performance on proficiency tests is factored in. Our benchmark serves as a catalyst for future research on VidLMs, helping to highlight areas that still need to be explored",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5Tp4WwZl8": {
    "title": "Error Feedback Shines when Features are Rare",
    "volume": "review",
    "abstract": "We provide the first proof that gradient descent $\\left({\\color{green}\\sf GD}\\right)$ with greedy sparsification $\\left({\\color{green}\\sf TopK}\\right)$ and error feedback $\\left({\\color{green}\\sf EF}\\right)$ can obtain better communication complexity than vanilla ${\\color{green}\\sf GD}$ when solving the distributed optimization problem $\\min_{x\\in \\mathbb{R}^d} {f(x)=\\frac{1}{n}\\sum_{i=1}^n f_i(x)}$, where $n$ = # of clients, $d$ = # of features, and $f_1,\\dots,f_n$ are smooth nonconvex functions. Despite intensive research since 2014 when ${\\color{green}\\sf EF}$ was first proposed by Seide et al., this problem remained open until now. Perhaps surprisingly, we show that ${\\color{green}\\sf EF}$ shines in the regime when features are rare, i.e., when each feature is present in the data owned by a small number of clients only. To illustrate our main result, we show that in order to find a random vector $\\hat{x}$ such that $\\lVert {\\nabla f(\\hat{x})} \\rVert^2 \\leq \\varepsilon$ in expectation, ${\\color{green}\\sf GD}$ with the ${\\color{green}\\sf Top1}$ sparsifier and ${\\color{green}\\sf EF}$ requires ${\\cal O} \\left( \\left( L + {\\color{blue}r} \\sqrt{ \\frac{{\\color{red}c}}{n} \\min \\left( \\frac{{\\color{red}c}}{n} \\max_i L_i^2, \\frac{1}{n}\\sum_{i=1}^n L_i^2 \\right) } \\right) \\frac{1}{\\varepsilon} \\right)$ bits to be communicated by each worker to the server only, where $L$ is the smoothness constant of $f$, $L_i$ is the smoothness constant of $f_i$, ${\\color{red}c}$ is the maximal number of clients owning any feature ($1\\leq {\\color{red}c} \\leq n$), and ${\\color{blue}r}$ is the maximal number of features owned by any client ($1\\leq {\\color{blue}r} \\leq d$). Clearly, the communication complexity improves as ${\\color{red}c}$ decreases (i.e., as features become more rare), and can be much better than the ${\\cal O}({\\color{blue}r} L \\frac{1}{\\varepsilon})$ communication complexity of ${\\color{green}\\sf GD}$ in the same regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=6yv8UHVJn4": {
    "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback",
    "volume": "review",
    "abstract": "We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, achieves a regret of $\\widetilde{O}(\\sqrt{K})$ without relying on simulators, where $K$ is the number of episodes. This is the first rate-optimal result in the considered setting. The second algorithm is computationally efficient and achieves a regret of $\\widetilde{O}(K^{\\frac{3}{4}})$ . These results significantly improve over the prior state-of-the-art: a computationally inefficient algorithm by Kong et al. (2023) with $\\widetilde{O}(K^{\\frac{4}{5}}+1/\\lambda_{\\min})$ regret, and a computationally efficient algorithm by Sherman et al. (2023b) with $\\widetilde{O}(K^{\\frac{6}{7}})$ regret",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=UMOlFJzLfL": {
    "title": "A Precise Characterization of SGD Stability Using Loss Surface Geometry",
    "volume": "review",
    "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=D1w3huGGpu": {
    "title": "Compositional Interfaces for Compositional Generalization",
    "volume": "review",
    "abstract": "With recent work such as GATO (Reed et al., 2022) we see the development of agents that can accomplish a variety of tasks, and are able to perceive the world and act in multiple observation and action spaces. We would want such agents to exhibit compositional generalization to unseen combinations of observation and action spaces, and adapt quickly to novel observation spaces by transfering knowledge. In this work, we demonstrate how these abilities can be achieved through the use of end-to-end modular architectures: the encoding of observations and the prediction of actions are handled by differentiable modules specialized to that space, with a single shared controller between them. To study the properties of such modular architectures in a controlled manner, we construct an environment with compositional structure, where each instance of the environment is created by combining an observation, action, and instruction space from a large set of options. We demonstrate that through the use of modularity, agents can generalize to unseen combinations of observation, action and instruction spaces; even when the unseen combinations are more challenging. Moreover, we demonstrate that modularity enables quick integration of novel observation modalities, requiring only adaptation of the modules encoding the new observation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WpQbM1kBuy": {
    "title": "Prodigy: An Expeditiously Adaptive Parameter-Free Learner",
    "volume": "review",
    "abstract": "We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $\\mathcal{O}(\\sqrt{\\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xt9Bu66rqv": {
    "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning",
    "volume": "review",
    "abstract": "The goal of reinforcement learning (RL) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as *dual RL*, is unconstrained and easier to optimize. In this work, we first cast several state-of-the-art offline RL and offline imitation learning (IL) algorithms as instances of dual RL approaches with shared structures. Such unification allows us to identify the root cause of the shortcomings of prior methods. For offline IL, our analysis shows that prior methods are based on a restrictive coverage assumption that greatly limits their performance in practice. To fix this limitation, we propose a new discriminator-free method ReCOIL that learns to imitate from arbitrary off-policy data to obtain near-expert performance. For offline RL, our analysis frames a recent offline RL method XQL in the dual framework, and we further propose a new method $f$-DVL that provides alternative choices to the Gumbel regression loss that fixes the known training instability issue of XQL. The performance improvements by both of our proposed methods, ReCOIL and $f$-DVL, in IL and RL are validated on an extensive suite of simulated robot locomotion and manipulation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=86NGO8qeWs": {
    "title": "CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models",
    "volume": "review",
    "abstract": "A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose \\textbf{CompA}, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. An ALM is evaluated on how well it matches the right audio to the right caption. Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning. Next, we propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to improve its compositional reasoning abilities. To train CompA-CLAP, we first propose improvements to contrastive training with composition-aware hard negatives, allowing for more focused training. Next, we propose a novel modular contrastive loss that helps the model learn fine-grained compositional understanding and overcomes the acute scarcity of openly available compositional audios. CompA-CLAP significantly improves over all our baseline models on the CompA benchmark, indicating its superior compositional reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Ro3EBZiKhT": {
    "title": "HiLoRL: A Hierarchical Logical Model for Learning Composite Tasks",
    "volume": "review",
    "abstract": "We propose HiLoRL, a hierarchical model to learn policies for composite tasks. Recent studies mostly focus on using human-specified logical specifications, which is laborious and produces models that perform poorly when facing tasks not entirely human-predictable. HiLoRL is composed of a high-level logical planner and low-level action policies. It initially learns a rough rule at its upper level with the help of low-level policies and then uses joint training with surrogate rewards to refine the rough rule and low-level policies. Furthermore, HiLoRL can incorporate specialized predicates derived from expert knowledge, thereby enhancing its training speed and performance. We also design a synthesis algorithm to illustrate our high-level planner's logical structure as an automaton, demonstrating our model's interpretability. HiLoRL outperforms state-of-the-art baselines in several benchmarks with continuous state and action spaces. Additionally, HiLoRL does not require human to hard-code logical structures, so it can solve logically uncertain tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LYS3RhIYCq": {
    "title": "Scaling Laws for Imitation Learning in Single-Agent Games",
    "volume": "review",
    "abstract": "Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, many works find it is often unable to fully recover the underlying expert behavior, even in constrained environments like single-agent games. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting for single-agent games. We first demonstrate our findings on a variety of Atari games, and thereafter focus on the extremely challenging game of NetHack. In all games, we find that IL *loss* and *mean return* scale smoothly with the compute budget (FLOPs) and are strongly correlated, resulting in power laws for training compute-optimal IL agents. Finally, we forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by 2x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a variety of single-agent games, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=F0q880yOgY": {
    "title": "Can Language Agents Approach the Performance of RL? An Empirical Study On OpenAI Gym",
    "volume": "review",
    "abstract": "The formidable capacity for zero- or few-shot decision-making in language agents encourages us to pose a compelling question: *Can language agents approach the performance of reinforcement learning (RL) in traditional sequential decision-making tasks and exhibit greater efficacy?* To investigate this, we first develop a $\\texttt{TextGym}$ simulator by grounding OpenAI Gym in a textual environment. This allows for straightforward comparisons between RL agents and language agents, given the widespread adoption of OpenAI Gym. To ensure a fair and effective benchmarking, we introduce $5$ levels of scenario for accurate domain-knowledge controlling and a unified RL-inspired framework for language agents. Additionally, we propose an innovative explore-exploit-guided language ($\\texttt{EXE}$) agent to solve the severely partially observable and sparse reward tasks within $\\texttt{TextGym}$. Through numerical experiments and ablation studies, we extract valuable insights into the decision-making capabilities of language agents and evaluate their potential to compete with RL in classical sequential decision-making problems. This paper sheds light on the performance of language agents and paves the way for future research in this exciting domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=LSrDaGWTnv": {
    "title": "Contrastive Representations Make Planning Easy",
    "volume": "review",
    "abstract": "Probabilistic inference over time series data is challenging when observations are high-dimensional. In this paper, we show how inference questions relating to prediction and planning can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By first extending this analysis to show that the marginal distribution over representations is Gaussian, we can then prove that conditional distribution of future representations is also Gaussian. Taken together, these results show that a variant of temporal contrastive learning results in representations distributed according to a Gaussian Markov chain, a graphical model where inference (e.g., filtering, smoothing) has closed form solutions. For example, in one special case the problem of trajectory inference simply corresponds to linear interpolation of the initial and final state representations. We provide brief empirical results validating our theory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7gVX2LxE7A": {
    "title": "SpecAR-Net: Spectrogram Analysis and Representation Network for Time Series",
    "volume": "review",
    "abstract": "Time series analysis involves modeling time series to extract valuable information, which finds broad applications in domains such as device malfunction diagnosis, human activity recognition, and medical-assisted diagnosis. Representing temporal-structured samples is crucial for time series analysis tasks. Recently, several advanced deep learning models, i.e., recurrent neural networks, convolutional neural networks, and transformer-style models, have been successively applied in the field of temporal data representation, yielding notable results. Those existing methods primarily model and represent the variation patterns within time series solely in time domain. However, as a highly abstracted information entity, time series data is formed by the coupling of various patterns such as trends, seasonality, and dramatic changes (instantaneous high dynamic), it is difficult to exploit these highly coupled properties only by means of analysis in the time domain. Consequently, it would be insufficient for time-domain dependent only methods to overcome the semantic representation bottleneck or construct comprehensive feature representations of 1D time series. To this end, we present Spectrum Analysis and Representation Network (SpecAR-Net). SpecAR-Net aims at learning more comprehensive representations by modeling raw time series in time-frequency domain, where an efficient joint extraction of time-frequency features is achieved through a group of learnable 2D multi-scale parallel complex convolution blocks. Experimental results show that the SpecAR-Net achieves excellent performance in five major downstream tasks of time series analysis i.e., classification, anomaly detection, imputation, long- and short-term series forecasting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=fnO5h1CFyh": {
    "title": "Learning Successor Representations with Distributed Hebbian Temporal Memory",
    "volume": "review",
    "abstract": "This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare the SRs produced by DHTM to another biologically inspired HMM-like algorithm, CSCG. Our findings suggest that DHTM is a promising approach for addressing the challenges of online hidden representation learning in dynamic environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7LrHqcOyh": {
    "title": "DNCs require more planning steps",
    "volume": "review",
    "abstract": "A Differentiable Neural Computer (DNC) is a memory-augmented neural computation model capable of learning to solve complex algorithmic tasks, from simple sorting algorithms, through graph problems, to text question answering. In previous works, it was always given a constant number of planning steps to complete its task. In this work, we argue that the number of planning steps the model is allowed to take, which we call \"planning budget\", is a constraint that can cause the model to generalize poorly and hurt its ability to fully utilize its external memory. By introducing an adaptive planning budget that scales with input size during training, the model is better able to utilize its memory space, and achieves substantially better accuracy on input sizes not seen during training. We experiment with Graph Shortest Path search, which has been used as a benchmark to measure these models in the past, and with the Graph MinCut problem. In both problems, our proposed approach improves performance and generalizes better compared to the standard planning budget",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ulaUJFd96G": {
    "title": "Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs) have established new standards in various natural language processing tasks. However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process. To relax the constraint, previous works have explored architectural changes and modifications in positional encoding, but they often require expensive training or do not address the computational demands of self-attention. In this paper, we present Hierarchical cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations. HOMER harnesses a divide-and-conquer methodology, segmenting extensive inputs into manageable units. The segments are then processed collectively, employing a hierarchical strategy that fuses adjacent chunks at progressive Transformer layers. A token reduction technique precedes each fusion, ensuring memory usage efficiency. We also propose an optimized computational order reducing the memory requirement to logarithmically scale with respect to input length, making it especially favorable for environments with tight memory restrictions. Our experimental results demonstrate the superior performance and memory efficiency of the proposed method, opening doors for broader applications of LLMs in scenarios with extended context requirements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=uw5U7FfTRf": {
    "title": "BaDLoss: Backdoor Detection via Loss Dynamics",
    "volume": "review",
    "abstract": "Backdoor attacks often inject synthetic features into a training dataset. Images classified with these synthetic features often demonstrate starkly different training dynamics when compared to natural images. Previous work has identified this phenomenon, claiming that backdoors are outliers (Hayase et al. 2021) or particularly strong features (Khaddaj et al. 2023), consequently being harder or easier to learn compared to regular examples. We instead identify backdoors as having \\textit{different}, anomalous training dynamics. With this insight, we present BaDLoss, a robust backdoor detection method. BaDLoss injects specially chosen probes that model anomalous training dynamics and tracks the loss trajectory for each example in the dataset, enabling the identification of unknown backdoors in the training set. Our method effectively transfers zero-shot to novel backdoor attacks without prior knowledge. Additionally, BaDLoss can detect multiple concurrent attacks, setting it apart from most existing approaches. By removing identified examples and retraining, BaDLoss eliminates the model's vulnerability to most attacks, far more effectively than previous defenses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ePOjNlOjLC": {
    "title": "Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation",
    "volume": "review",
    "abstract": "Originating from the diffusion phenomenon in physics that describes particle movement, the diffusion generative models inherit the characteristics of stochastic random walk in the data space along the denoising trajectory. However, the intrinsic mutual interference among image regions contradicts the need for practical downstream application scenarios where the preservation of low-level pixel information from given conditioning is desired (e.g., customization tasks like personalized generation and inpainting based on a user-provided single image). In this work, we investigate the diffusion (physics) in diffusion (machine learning) properties and propose our Cyclic One-Way Diffusion (COW) method to control the direction of diffusion phenomenon given a pre-trained frozen diffusion model for versatile customization application scenarios, where the low-level pixel information from the conditioning needs to be preserved. Notably, unlike most current methods that incorporate additional conditions by fine-tuning the base text-to-image diffusion model or learning auxiliary networks, our method provides a novel perspective to understand the task needs and is applicable to a wider range of customization scenarios in a learning-free manner. Extensive experiment results show that our proposed COW can achieve more flexible customization based on strict visual conditions in different application settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=QbXo9pbJpp": {
    "title": "Improved Invariant Learning for Node-level Out-of-distribution Generalization on Graphs",
    "volume": "review",
    "abstract": "Enhancing OOD generalization on graph data is a recent hot research topic. Among this, node-level OOD generalization remains an underexplored and challenging subject. The difficulty of node-level OOD tasks lies in the fact that representations between nodes are coupled through edges, making it difficult go characterize distribution shifts and capture invariant features. Furthermore, environment labels for nodes is typically expensive to obtain in practice, rendering invariant learning strategies based on environment partitioning infeasible. By establishing a theoretical model, we highlight that even with ground-truth environment partitioning, classical invariant learning methods like IRM and VREx designed for independently distributed training data will still capture spurious features when the depth of the GNN exceeds the width of a node's causal pattern (i.e., the invariant and predictive neighboring subgraph). Intriguingly, however, we theoretically and empirically find that by enforcing Cross-environment Intra-class Alignment (CIA) of node representations, we can remove the reliance on these spurious features. To harness the advantages of CIA and adapt it on graphs, we further propose Localized Reweighting CIA (LoRe-CIA), which does not require environment labels or intricate environment partitioning processes. Leveraging the neighbouring structural information of graphs, LoRe-CIA adaptively select node pairs that exhibit large differences in spurious features but minimal differences in causal features for alignment, enabling more efficient elimination of spurious features. The experiments on GOOD benchmark shows that LoRe-CIA achieves optimal OOD generalization performance on average",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=NPViqdhTIi": {
    "title": "Parameter-Free Molecular Classification and Regression with Gzip",
    "volume": "review",
    "abstract": "In recent years, natural language processing approaches to machine learning, most prominently deep neural network-based transformers, have been extensively applied to molecular classification and regression tasks, including the prediction of pharmacokinetic and quantum-chemical properties. However, models based on deep neural networks generally require extensive training, large training data sets, and resource-consuming hyperparameter tuning. Recently, a low-resource and universal alternative to deep learning approaches based on Gzip compression for text classification has been proposed, which reportedly performs surprisingly well compared to large language models such as BERT, given its conceptually simplistic nature. Here, we adapt the proposed method to support multiprocessing, multi-class classification, class-weighing, and regression and apply it to classification and regression tasks on various data sets of molecules from the organic chemistry, biochemistry, drug discovery, and material science domains. We further propose converting numerical descriptors into string representations, enabling the integration of language input with domain-informed descriptors. Our results show that the method can be used to classify and predict a variety of properties of molecules, can reach the performance of large-scale chemical language models in a subset of tasks, and has the potential for application in information retrieval from large chemical databases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=DfPtC8uSot": {
    "title": "Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthogonality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthogonal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=fcSDt7H8kI": {
    "title": "Boosting Reinforcement Learning with Extremum Experiences",
    "volume": "review",
    "abstract": "Reinforcement learning research has achieved high acceleration in its progress starting from the initial installation of deep neural networks as function approximators to learn policies that make sequential decisions in high-dimensional state representation MDPs. While several consecutive barriers have been broken in deep reinforcement learning research (i.e. learning from high-dimensional states, learning purely via self-play), several others still stand. On this line, in our paper we focus on experience collection in high-dimensional complex MDPs and we propose a unique technique based on experiences obtained through extremum actions. Our method provides theoretical basis for efficient experience collection, and further comes with zero additional computational cost while leading to significant sample efficiency gains in deep reinforcement learning training. We conduct extensive experiments in the Arcade Learning Environment with high-dimensional state representation MDPs. We demonstrate that our technique improves the human normalized median scores of Arcade Learning Environment by 248% in the low-data regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFLZFxoLnE": {
    "title": "Modify Training Direction in Function Space to Reduce Generalization Error",
    "volume": "review",
    "abstract": "To improve generalization performance by modifying the training dynamics, we present theoretical analyses of a modified natural gradient descent method in the neural network function space, leveraging the neural tangent kernel theory. Firstly, we provide an analytical expression for the function acquired through this modified natural gradient descent under the assumptions of an infinite network width limit and a Gaussian conditional output distribution. Subsequently, we explicitly derive the generalization error associated with the learned neural network function. By interpreting the generalization error as stemming from the distribution discrepancy between the training data and the true data, we propose a criterion for modification in the eigenspaces of the Fisher information matrix to reduce the generalization error bound. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in generalization error. These theoretical results are also illustrated through numerical experiments. Additionally, we demonstrate the connections between this theoretical framework and existing results of generalization-enhancing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oXjnwQLcTA": {
    "title": "Score Models for Offline Goal-Conditioned Reinforcement Learning",
    "volume": "review",
    "abstract": "Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns *scores* or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark comprised of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTU2X9mUBe": {
    "title": "LaDe: The First Comprehensive Last-mile Express Dataset from Industry",
    "volume": "review",
    "abstract": "Real-world last-mile delivery datasets are crucial for research in logistics, supply chain management, and spatio-temporal data mining. Despite a plethora of algorithms developed to date, no widely accepted, publicly available last-mile delivery dataset exists to support research in this field. In this paper, we introduce LaDe, the first publicly available last-mile delivery dataset with millions of packages from the industry. LaDe has three unique characteristics: (1) Large-scale. It involves 10,677k packages of 21k couriers over 6 months of real-world operation. (2) Comprehensive information. It offers original package information, such as its location and time requirements, as well as task-event information, which records when and where the courier is while events such as task-accept and task-finish events happen. (3) Diversity. The dataset includes data from various scenarios, including package pick-up and delivery, and from multiple cities, each with its unique spatio-temporal patterns due to their distinct characteristics such as populations. We verify LaDe on three tasks by running several classical baseline models per task. We believe that the large-scale, comprehensive, diverse feature of LaDe can offer unparalleled opportunities to researchers in the supply chain community, data mining community, and beyond. The dataset homepage is publicly available at https://anonymous.4open.science/r/Anonymous-64B3/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=krx55l2A6G": {
    "title": "Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning",
    "volume": "review",
    "abstract": "Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding the client-side detectability of MS attacks were raised, questioning their practicality. In this work, for the first time, we thoroughly study client-side detectability. We first demonstrate that all prior MS attacks are detectable by principled checks, and formulate a necessary set of requirements that a practical MS attack must satisfy. Next, we propose SEER, a novel attack framework that satisfies these requirements. The key insight of SEER is the use of a secret decoder, jointly trained with the shared model. We show that SEER can steal user data from gradients of realistic networks, even for large batch sizes of up to 512 and under secure aggregation. Our work is a promising step towards assessing the true vulnerability of federated learning in real-world settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=igfDXfMvm5": {
    "title": "USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields",
    "volume": "review",
    "abstract": "Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera. Experimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image synthesis as well as camera motion estimation. Furthermore, our algorithm can also be used to recover high-fidelity high frame-rate global shutter video from a sequence of RS images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk0ykeYCfP": {
    "title": "Analyzing the Effects of Emulating on the Reinforcement Learning Manifold",
    "volume": "review",
    "abstract": "Reinforcement learning has become a prominent research direction with the utilization of deep neural networks as state-action value function approximators enabling exploration and construction of functioning neural policies in MDPs with state representations in high dimensions. While reinforcement learning is currently being deployed in many different settings from medical to finance, the fact that reinforcement learning requires a reward signal from the MDP to learn a functioning policy can be restrictive for tasks in which the construction of the reward function is more or equally complex than learning it. In this line of research several studies proposed algorithms to learn a reward function or an optimal policy from observed optimal trajectories. In this paper, we focus on non-robustness of the state-of-the-art algorithms that accomplish learning without rewards in high dimensional state representation MDPs, and we demonstrate that the vanilla trained deep reinforcement learning policies are more resilient and value aligned than learning without rewards in MDPs with complex state representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=4y3GDTFv70": {
    "title": "A Latent Space Theory for Emergent Abilities in Large Language Models",
    "volume": "review",
    "abstract": "Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=S4wo3MnlTr": {
    "title": "A trainable manifold for accurate approximation with ReLU Networks",
    "volume": "review",
    "abstract": "We present a novel technique for exercising greater control of the weights of ReLU activated neural networks to produce more accurate function approximations. Many theoretical works encode complex operations into ReLU networks using smaller base components. In these works, a common base component is a constant width approximation to $x^2$, which has exponentially decaying error with respect to depth. We extend this block to represent a greater range of convex one-dimensional functions. We derive a manifold of weights such that the output of these new networks utilizes exponentially many piecewise-linear segments. This manifold guides their training process to overcome drawbacks associated with random initialization and unassisted gradient descent. We train these networks to approximate functions which do not necessarily lie on the manifold, showing a significant reduction of error values over conventional approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jkhVrIllKg": {
    "title": "Federated Learning Under Second-Order Data Heterogeneity",
    "volume": "review",
    "abstract": "We consider the problem of Federated Learning over clients with heterogeneous data. We propose an algorithm called SABER that samples a subset of clients and tasks each client with its own local subproblem. SABER provably reduces client drift by incorporating an estimate of the global update direction and regularization into each client's subproblem. Under second-order data heterogeneity with parameter $\\delta$, we prove that the method's communication complexity for nonconvex problems is $O\\left(\\delta\\varepsilon^2\\sqrt{M}\\right)$. In addition, for problems satisfying $\\mu$-Polyak-Lojasiewicz condition, the method converges linearly with communication complexity of $O\\left(\\left(\\frac{\\delta}{\\mu}\\sqrt{M} + M\\right)\\log\\frac{1}{\\varepsilon}\\right)$. To showcase the empirical performance of our method, we compare it to standard baselines such as FedAvg on a few empirical problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=bAMPOUF227": {
    "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=XN6ZPINdSg": {
    "title": "COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits",
    "volume": "review",
    "abstract": "Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning. To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component. Theoretically, we provide end-to-end certification of prediction coverage for COLEP in the presence of $\\ell_2$ bounded adversarial perturbations. We also provide certified coverage considering the finite size of the calibration set. Furthermore, we prove that COLEP achieves higher prediction coverage and accuracy over a single model as long as the utilities of knowledge models are non-trivial. Empirically, we show the validity and tightness of our certified coverage, demonstrating the robust conformal prediction of COLEP on various datasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to 12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% on AwA2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=f3UIvWeAKs": {
    "title": "Learning Node Selection via Tripartite Graph Representation in Mixed Integer Linear Programming",
    "volume": "review",
    "abstract": "Branch-and-bound methods are pivotal in solving Mixed Integer Linear Programs (MILPs), where the challenge of node selection arises, necessitating the prioritization of different regions of the space for subsequent exploration. While machine learning techniques have been proposed to address this, our paper resolves two crucial and open questions concerning \\textbf{(P1)} the representation of the MILP solving process and \\textbf{(P2)} the qualification of nodes in node selection. We present a novel tripartite graph representation for the branch-and-bound search tree, which, through theoretical validation, proves to effectively encapsulate the essential information of the search tree for node selection. To further this, we introduce three innovative metrics for node selection and formulate a GNN-based model, DQN-GNN, utilizing reinforcement learning to derive node selection policies. Empirical evaluations illustrate that DQN-GNN markedly enhances the efficiency of solving MILPs, surpassing the existing human-designed and learning-based models. compared to other AI methods, our experiments substantiate that DQN-GNN exhibits commendable generalization to MILPs that are substantially larger than those encountered during training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=0akLDTFR9x": {
    "title": "Contrastive Difference Predictive Coding",
    "volume": "review",
    "abstract": "Predicting and reasoning about the future lies at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitching together pieces of different time series data to decrease the amount of data required to learn to predict future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves higher success rates with less data, and can better cope with stochastic environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpiOUOaqh3": {
    "title": "PARAMETER OPTIMIZATION FOR EPIDEMIOLOGICAL MODEL WITH GENETIC ALGORITHM",
    "volume": "review",
    "abstract": "In this study, we propose a variation of the SEIR epidemiological model, called SEPAI3R3O, and apply genetic algorithms to analyze and optimize the associated parameters. This model was developed based on the analysis of sociodemographic and behavioral data from anomalous ICDs (International Classification of Disease) and ICPCs (International Classification of Primary Care) collected from units specialized in SARS (Severe Acute Respiratory Syndrome)(i.e., specifically flu and COVID-19) in the city of Recife, located in northeast Brazil, from April $26, 2020$, to March $7, 2021$. The main objective was to understand the dynamics of disease spread and identify critical factors that influence their spread. One of these factors is the underreporting rate, estimated at around $50\\%,$ which significantly increases cases due to inadequate testing. We could precisely adjust the model parameters using a genetic optimization approach, resulting in more accurate disease dynamics predictions and a more realistic view of the number of people infected by SARS. The results indicate that the SEPAI3R3O model, when optimized with genetic algorithms, could predict the spread of the disease with an effective reproduction rate $R_0$ of $3 (95\\%$ CI $2.8–3.2)$ and a growth rate of $0.014 (95\\%$ CI $0.013–0.015)$ for the period analyzed. With realistic data, this approach offers a valuable tool for researchers and healthcare professionals in making decisions and formulating more effective intervention strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ytGU2iit80": {
    "title": "From Fourier to Neural ODEs: Flow matching for modeling complex systems",
    "volume": "review",
    "abstract": "Modeling complex systems using standard neural ordinary differential equations (NODEs) often faces some essential challenges, including high computational costs and susceptibility to local optima. To address these challenges, we propose a simulation-free framework, called Fourier NODEs (FNODEs), that effectively trains NODEs by directly matching the target vector field based on Fourier analysis. Specifically, we employ the Fourier analysis to estimate temporal and potential high-order spatial gradients from noisy observational data. We then incorporate the estimated spatial gradients as additional inputs to a neural network. Furthermore, we utilize the estimated temporal gradient as the optimization objective for the output of the neural network. Later, the trained neural network generates more data points through an ODE solver without participating in the computational graph, facilitating more accurate estimations of gradients based on Fourier analysis. These two steps form a positive feedback loop, enabling accurate dynamics modeling in our framework. Consequently, our approach outperforms state-of-the-art methods in terms of training time, dynamics prediction, and robustness. Finally, we demonstrate the superior performance of our framework using a number of representative complex systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ISfY3YMXxU": {
    "title": "INRet: A General Framework for Accurate Retrieval of INRs for Shapes",
    "volume": "review",
    "abstract": "Implicit neural representations (INRs) have become an important representation for encoding various data types, such as 3D objects/scenes, videos, and audio. They have proven to be particularly effective at generating 3D content, e.g., 3D scene reconstruction from 2D images, novel content creation, as well as the representation, interpolation and completion of 3D shapes. With the widespread generation of 3D data in an INR format, there is a need to support effective organization and retrieval of INRs saved in a data store. A key aspect of retrieval and clustering of INRs in a data store is the formulation of similarity between INRs that would, for example, enable retrieval of similar INRs using a query INR. In this work, we propose INRet (INR Retrieve), a method for determining similarity between INRs that represent shapes, thus enabling accurate retrieval of similar shape INRs from an INR data store. INRet flexibly supports different INR architectures such as INRs with octree grids and hash grids, as well as different implicit functions including signed/unsigned distance function and occupancy field. We demonstrate that our method is more general and accurate than the existing INR retrieval method, which only supports simple MLP INRs and requires the same architecture between the query and stored INRs. Compared to 3D shape retrieval by converting INRs to other representations like point clouds or multi-view images, INRet achieves higher retrieval accuracy while avoiding the overhead of conversion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=pogJXugbN8": {
    "title": "BAFFLE: A Baseline of Backpropagation-Free Federated Learning",
    "volume": "review",
    "abstract": "Federated learning (FL) is a general principle for decentralized clients to train a server model collectively without sharing local data. FL is a promising framework with practical applications, but its standard training paradigm requires the clients to backpropagate through the model to compute gradients. Since these clients are typically edge devices and not fully trusted, executing backpropagation on them incurs computational and storage overhead as well as white-box vulnerability. In light of this, we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning; and 3) well-suited to trusted execution environments, because the clients in BAFFLE only execute forward propagation and return a set of scalars to the server. Empirically we use BAFFLE to train deep models from scratch or to finetune pretrained models, achieving acceptable results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=NiefAhgJqH": {
    "title": "Bayesian Exploration Networks",
    "volume": "review",
    "abstract": "Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian exploration network (BEN) which uses normalising flows to model both the aleatoric uncertainty (via density estimation) and epistemic uncertainty (via variational inference) in the Bellman operator. In the limit of complete optimisation, BEN learns true Bayes-optimal policies, but like in variational expectation-maximisation, partial optimisation renders our approach tractable. Empirical results demonstrate that BEN can learn true Bayes-optimal policies in tasks where existing model-free approaches fail",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=dtFN6T4aMU": {
    "title": "MAST: A Sparse Training Framework for Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "Deep Multi-agent Reinforcement Learning (MARL) is often confronted with large state and action spaces, necessitating the utilization of neural networks with extensive parameters and incurring substantial computational overhead. Consequently, there arises a pronounced need for methods that expedite training and enable model compression in MARL. Nevertheless, existing training acceleration techniques are primarily tailored for single-agent scenarios, as the task of compressing MARL agents within sparse models presents unique and intricate challenges. In this paper, we introduce an innovative Multi-Agent Sparse Training (MAST) framework. MAST capitalizes on gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. This is then combined with a novel hybrid TD-($\\lambda$) schema, coupled with the Soft Mellowmax Operator, to establish dependable learning targets, particularly in sparse scenarios. Additionally, we employ a dual replay buffer mechanism to enhance policy stability within sparse networks. Remarkably, our comprehensive experimental investigation on the SMAC benchmarks, for the first time, that deep multi-agent Q learning algorithms manifest significant redundancy in terms of Floating Point Operations (FLOPs). This redundancy translates into up to $20$-fold reduction in FLOPs for both training and inference, accompanied by a commensurate level of model compression, all achieved with less than 3\\% performance degradation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=u7LFI98JI6": {
    "title": "GraphDeepONet: Learning to simulate time-dependent partial differential equations using graph neural network and deep operator network",
    "volume": "review",
    "abstract": "Scientific computing using deep learning has seen significant advancements in recent years. There has been growing interest in models that learn the operator from the parameters of a partial differential equation (PDE) to the corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural operator, among other models, have been designed with structures suitable for handling functions as inputs and outputs, enabling real-time predictions as surrogate models for solution operators. There has also been significant progress in the research on surrogate models based on graph neural networks (GNNs), specifically targeting the dynamics in time-dependent PDEs. In this paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to effectively adapt DeepONet, which is well-known for successful operator learning. GraphDeepONet outperforms existing GNN-based PDE solver models by accurately predicting solutions, even on irregular grids, while inheriting the advantages of DeepONet, allowing predictions on arbitrary grids. Additionally, unlike traditional DeepONet and its variants, GraphDeepONet enables time extrapolation for time-dependent PDE solutions. We also provide theoretical analysis of the universal approximation capability of GraphDeepONet in approximating continuous operators across arbitrary time intervals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNLjU5C5dK": {
    "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
    "volume": "review",
    "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which can't fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named $\\textbf{FIGA}$. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quailty signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWzUA9zeAg": {
    "title": "Effective Data Augmentation With Diffusion Models",
    "volume": "review",
    "abstract": "Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5T46w5X3Go": {
    "title": "Theoretical Analysis on the Generalization Power of Overfitted Transfer Learning",
    "volume": "review",
    "abstract": "Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the number of features/parameters in both underparameterized and overparameterized regimes. Furthermore, we provide practical guidelines for determining the number of features in the common and task-specific parts for improved generalization performance. For example, when the total number of features in the source task's learning model is fixed, we show that it is more advantageous to allocate a greater number of redundant features to the task-specific part rather than the common part. Moreover, in specific scenarios, particularly those characterized by high noise levels and small true parameters, sacrificing certain true features in the common part in favor of employing more redundant features in the task-specific part can yield notable benefits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Bo6GpQ3B9a": {
    "title": "Out-Of-Domain Unlabeled Data Improves Generalization",
    "volume": "review",
    "abstract": "We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training. As a result, we also leverage efficient polynomial-time algorithms for the training stage. From a theoretical standpoint, we apply our framework on the classification problem of a mixture of two Gaussians in $\\mathbb{R}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\\gg m$) out of domain and unlabeled samples are gievn as well. Using only the labeled data, it is known that the generalization error can be bounded by $\\propto\\left(d/m\\right)^{1/2}$. However, using our method on both isotropic and non-isotropic Gaussian mixture models, one can derive a new set of analytically explicit and non-asymptotic bounds which show substantial improvement on the generalization error compared ERM. Our results underscore two significant insights: 1) out-of-domain samples, even when unlabeled, can be harnessed to narrow the generalization gap, provided that the true data distribution adheres to a form of the \"cluster assumption\", and 2) the semi-supervised learning paradigm can be regarded as a special case of our framework when there are no distributional shifts. We validate our claims through experiments conducted on a variety of synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYXtXLYHpR": {
    "title": "Towards Transparent Time Series Forecasting",
    "volume": "review",
    "abstract": "Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in high-stakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification and regression, time series forecasting presents some unique challenges for ensuring transparency. In particular, currently used bottom-up approaches that focus on the values of the time series at specific time points (usually regularly spaced) do not provide a holistic understanding of the entire time series. This limits the applicability of ML in many critical areas. To open up these domains for ML, we propose a top-down framework of bi-level transparency, which involves understanding the higher-level trends and the lower-level properties of the predicted time series. Applying this framework, we develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. Through a series of experiments, we demonstrate the efficacy and interpretability of our approach, paving the way for more transparent and reliable applications of ML in various domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=TCSoDjtSZL": {
    "title": "Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors",
    "volume": "review",
    "abstract": "Reconstructing 3D objects from a single image guided by pretrained diffusion models has demonstrated promising outcomes. However, due to utilizing the case-agnostic rigid strategy, their generalization ability to arbitrary cases and the 3D consistency of reconstruction are still poor. In this work, we propose Consistent123, a case-aware two-stage method for highly consistent 3D asset reconstruction from one image with both 2D and 3D diffusion priors. In the first stage, Consistent123 utilizes only 3D structural priors for sufficient geometry exploitation, with a CLIP-based case-aware adaptive detection mechanism embedded within this process. In the second stage, 2D texture priors are introduced and progressively take on a dominant guiding role, delicately sculpting the details of the 3D model. Consistent123 aligns more closely with the evolving trends in guidance requirements, adaptively providing adequate 3D geometric initialization and suitable 2D texture refinement for different objects. Consistent123 can obtain highly 3D-consistent reconstruction and exhibits strong generalization ability across various objects. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art image-to-3D methods. See https://Consistent123.github.io for a more comprehensive exploration of our generated 3D assets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=61mnwO4Mzp": {
    "title": "Denoising Diffusion Variational Inference",
    "volume": "review",
    "abstract": "Latent variable methods are a powerful tool for representation learning that greatly benefit from expressive variational posteriors, including generative models based on normalizing flows or adversarial networks. In this work, we propose denoising diffusion variational inference, which relies on diffusion models---recent generative algorithms with state-of-the-art sample quality---to fit a complex posterior by performing diffusion in latent space. Our method augments a variational posterior with auxiliary latent variables via a user-specified noising process that transforms a complex latent into a simple auxiliary latent. The approximate posterior then reverses this noising process by optimizing a lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method can be used to fit deep latent variable models, which yields the DiffVAE algorithm. This algorithm is especially effective at dimensionality reduction and representation learning, where it outperforms methods based on adversarial training or invertible flow-based posteriors. We use this algorithm on a motivating task in biology---inferring latent ancestry from human genomes---and show that it outperforms strong baselines on the 1000 Genomes dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=BkvdAYhyqm": {
    "title": "Explaining black box text modules in natural language with language models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A text module is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. Black box indicates that we only have access to the module's inputs/outputs. We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the model's internals. Finally, we show that SASC can generate explanations for the response of individual fMRI voxels to language stimuli, with potential applications to fine-grained brain mapping",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=fw1oizreEF": {
    "title": "Convexifying Transformers: Improving optimization and understanding of transformer networks",
    "volume": "review",
    "abstract": "Understanding the fundamental mechanism behind the success of transformer networks is still an open problem in the deep learning literature. Although their remarkable performance has been mostly attributed to the self-attention mechanism, the literature still lacks a solid analysis of these networks and interpretation of the functions learned by them. To this end, we study the training problem of attention/transformer networks and introduce a novel convex analytic approach to improve the understanding and optimization of these networks. Particularly, we first introduce a convex alternative to the self-attention mechanism and reformulate the regularized training problem of transformer networks with our alternative convex attention. Then, we cast the reformulation as a convex optimization problem that is interpretable and easier to optimize. Moreover, as a byproduct of our convex analysis, we reveal an implicit regularization mechanism, which promotes sparsity across tokens. Therefore, we not only improve the optimization of attention/transformer networks but also provide a solid theoretical understanding of the functions learned by them. We also demonstrate the effectiveness of our theory through several numerical experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=DyclWshWvf": {
    "title": "Causal-based Analysis on Credibility of Feedforward Neural Network",
    "volume": "review",
    "abstract": "Feedforward neural network (FNN) has been widely used in various fields. However, its credibility in some risk-sensitive fields remains questionable. The complex causal relations between neurons in the input layer is hard to be observed. These causal relations affect the credibility of FNN directly or indirectly. We transform FNN into a causal structure with different causal relations in the input layer. In order to analyze the causal structure accurately, we categorize it into three causal sub-structures based on different causal relations between input and output neurons. With different levels of intervention, we analyze the causal effect by calculating average treatment effect for each neuron in the input layer. We conduct experiments in the field of pediatric ophthalmology. The results demonstrate the validity of our causal-based analysis method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BlkxbI6vzl": {
    "title": "A Fast and Provable Algorithm for Sparse Phase Retrieval",
    "volume": "review",
    "abstract": "We study the sparse phase retrieval problem, which aims to recover a sparse signal from a limited number of phaseless measurements. Existing algorithms for sparse phase retrieval primarily rely on first-order methods with linear convergence rate. In this paper, we propose an efficient second-order algorithm based on Newton projection, which maintains the same per-iteration computational complexity as popular first-order methods. The proposed algorithm is theoretically guaranteed to converge to the ground truth (up to a global sign) at a quadratic convergence rate after at most $\\mathcal{O}\\big(\\log (\\Vert\\boldsymbol{x}^{\\natural} \\Vert /x_{\\min}^{\\natural})\\big)$ iterations, provided a sample complexity of $\\mathcal{O}(s^2\\log n)$, where $\\boldsymbol{x}^{\\natural} \\in \\mathbb{R}^n$ represents an $s$-sparse ground truth signal. Numerical experiments demonstrate that our algorithm outperforms state-of-the-art methods in terms of achieving a significantly faster convergence rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNZJyEDxy4": {
    "title": "MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data",
    "volume": "review",
    "abstract": "This paper addresses the problem of anomaly detection in tabular data, which is usually implemented in an one-class classification setting where the training set only contains normal samples. Inspired by the success of masked image/language modeling in vision and natural language domains, we extend masked modeling methods to address this problem by capturing intrinsic correlations between features in training set. Thus, a sample deviate from such correlations are related to a high possibility of anomaly. To obtain multiple and diverse correlations, we propose a novel masking strategy which generates multiple masks by learning, and design a diversity loss to reduce the similarity of different masks. Extensive experiments show our method achieves state-of-the-art performance. We also discuss the interpretability from the perspective of each individual feature and correlations between features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=MNFKm4AStf": {
    "title": "Representation Disentanglement via Regularization by Causal Identification",
    "volume": "review",
    "abstract": "In this work, we argue modern deep representation learning models for disentanglement are ill-posed with collider bias behavior; a source of bias producing dependencies between the underlying generating variables. Under the rubric of causal inference, we show this issue can be explained and reconciled under the condition of causal identification; attainable from a combination of a causal graphical model encoding the data generation process assumptions and data. For this, we propose regularization by identification (ReI), a modular regularization engine designed to align the behavior of large scale models with the disentanglement constraints imposed by causal identification. Empirical evidence on standard disentanglement benchmarks demonstrates the superiority of ReI in removing the effects of collider-bias. In a real-world dataset we show that enforcing ReI in a variational framework results in interpretable representations robust to out-of-distribution examples and that align with the true expected effect from domain knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=KNvubydSB5": {
    "title": "HiGen: Hierarchical Graph Generative Networks",
    "volume": "review",
    "abstract": "Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using separate neural networks. This modular approach enables scalable graph generation for large and complex graphs. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution. This enables us to generate community graphs with integer-valued edge weights in an autoregressive manner. Empirical studies demonstrate the effectiveness and scalability of our proposed generative model, achieving state-of-the-art performance in terms of graph quality across various benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=iP8ig954Uz": {
    "title": "HART: Efficient Adaptation via Regularized Autoregressive Parameter Generation",
    "volume": "review",
    "abstract": "Fine-tuning is an effective approach for adapting a pre-trained language model to downstream tasks, but it incurs a high computational cost. To achieve an extremely efficient task adaptation, \\citet{phang2022hypertuning} have proposed to use an auxiliary hypernetwork to generate task-specific weights without any backpropagation. A hypernetwork can generate weights for parameter-efficient fine-tuning (PEFT) modules, such as prefixes \\citep{li2021prefix} and LoRAs \\citep{hu2021lora}, for any unseen task based on a few task-specific demonstration examples, at the cost of a single forward pass. However, hypernetwork training is challenging. Firstly, it is sample inefficient due to the under-exploitation of the dependencies between PEFT weights across layers. Secondly, it exhibits training instability due to the high diversity of few-shot demonstration inputs. To address these limitations, we propose a novel hypernetwork training approach, named HART. It exploits layerwise dependencies by autoregressively generating weights for individual layers, and stabilizes the training by regularizing the consistency between weights generated based on different demonstrations. We train the hypernetwork on a diverse collection of tasks \\citep{wang2022super,sanh2021multitask} and evaluate its performance on unseen tasks. HART notably outperforms \\citet{phang2022hypertuning} on both T5-Large and T5-XL models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=8BAkNCqpGW": {
    "title": "A Policy Gradient Method for Confounded POMDPs",
    "volume": "review",
    "abstract": "In this paper, we propose a policy gradient method for confounded partially observable Markov decision processes (POMDPs) with continuous state and observation spaces in the offline setting. We first establish a novel identification result to non-parametrically estimate any history-dependent policy gradient under POMDPs using the offline data. The identification enables us to solve a sequence of conditional moment restrictions and adopt the min-max learning procedure with general function approximation for estimating the policy gradient. We then provide a finite-sample non-asymptotic bound for estimating the gradient uniformly over a pre-specified policy class in terms of the sample size, length of horizon, concentratability coefficient and the measure of ill-posedness in solving the conditional moment restrictions. Lastly, by deploying the proposed gradient estimation in the gradient ascent algorithm, we show the global convergence of the proposed algorithm in finding the history-dependent optimal policy under some technical conditions. To the best of our knowledge, this is the first work studying the policy gradient method for POMDPs under the offline setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xoZ29eXUk7": {
    "title": "A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. ‘Ending the HIV Epidemic' initiative",
    "volume": "review",
    "abstract": "Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 'Ending the HIV Epidemic (EHE)' initiative aims to reduce new infections by 90\\% by 2030, by improving coverage of diagnoses, treat, and prevent interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conducted on jurisdictions within California and Florida, optimal policies from MARL were significantly different than those generated from single-agent RL, highlighting the influence of jurisdictional variations and interactions. By using comprehensive modeling of HIV and formulations of state space, action space, and reward functions, this work helps demonstrate the strengths and applicability of MARL for informing public health policies, and provides a framework for expanding to the national-level to inform the EHE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=yoVq2BGQdP": {
    "title": "Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning",
    "volume": "review",
    "abstract": "Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=tAcEidZ1Y2": {
    "title": "Self-supervision Meets Bootstrap Estimation: New Paradigm for Unsupervised Reconstruction with Uncertainty Quantification",
    "volume": "review",
    "abstract": "Deep learning-based self-supervised reconstruction (SSR) plays a vital role in diverse domains, including unsupervisedly reconstructing magnetic resonance imaging (MRI). Current powerful methodologies for self-supervised MRI reconstruction usually rely on capturing the relationships between different views or transformations of the same data such as serving as inputs and labels respectively, which show notable influence from analogous approaches in computer vision. Although yielding somewhat promising results, their designs are often heuristic without deep insights into reconstructed object characteristics, and the analytical and mathematical principles of such methods are not expressive. This paper addresses these issues by a novel SSR paradigm, BootRec, that not only provides a theoretical foundation for self-supervised reconstruction but also facilitates the development of downstream algorithms. Self-supervised MRI reconstruction is modeled as error-oriented parameter estimation - Bootstrap estimation for SSR (BootRec). In BootRec, we demonstrate the mathematical equivalence between bootstrapping in a sample set and the commonly used re-undersampling operation for SSR. This insight is further incorporated into designing models to estimate the variances and errors of MRI SSR results without accessing labeled data. The error estimation serves as the loss function for unsupervisedly training the models. Empirical experiments show that our new paradigm BootRec enables effective uncertainty quantification and advanced MRI reconstruction performance against other zero-shot methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=M2oUA4XBq4": {
    "title": "Learning to ignore: Single Source Domain Generalization via Oracle Regularization",
    "volume": "review",
    "abstract": "Machine learning frequently suffers from the discrepancy in data distribution, commonly known as domain shift. Single-source Domain Generalization (sDG) is a task designed to simulate domain shift artificially, in order to train a model that can generalize well to multiple unseen target domains from a single source domain. A popular approach is to learn robustness via the alignment of augmented samples. However, prior works frequently overlooked what is learned from such alignment. In this paper, we study the effectiveness of augmentation-based sDG methods by analyzing the data generating process. We highlight issues in using augmentation for OOD generalization, namely, the distinction between domain invariance and augmentation invariance. To alleviate these issues, we introduce a novel regularization method that leverages pretrained models to guide the learning process via a feature-level regularization of mutual information, which we name PROF (Progressive mutual information Regularization for Online distillation of Frozen oracles). PROF can be applied to conventional augmentation-based methods to moderate the stochasticity of models repeatedly trained on augmented data. We show that PROF stabilizes the learning process for sDG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=mHv6wcBb0z": {
    "title": "Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization",
    "volume": "review",
    "abstract": "Multi-View Representation Learning (MVRL) aims to learn a unified representation of an object from multi-view data. Deep Canonical Correlation Analysis (DCCA) and its variants share simple formulations and demonstrate state-of-the-art performance. However, with extensive experiments, we observe the issue of model collapse, i.e., the performance of DCCA-based methods will drop drastically when training proceeds. The model collapse issue could significantly hinder the wide adoption of DCCA-based methods because it is challenging to decide when to early stop. To this end, we develop NR-DCCA, which is equipped with a novel noise regularization approach to prevent model collapse. Theoretical analysis shows that the full-rank property is the key to preventing model collapse, and our noise regularization constrains the neural network to be \"full-rank\". A framework to construct synthetic data with different common and complementary information is also developed to compare MVRL methods comprehensively. The developed NR-DCCA outperforms baselines stably and consistently in both synthetic and real-world datasets, and the proposed noise regularization approach can also be generalized to other DCCA-based methods such as DGCCA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ndR8Ytrzhh": {
    "title": "Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning",
    "volume": "review",
    "abstract": "Self-consistency (SC) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple and scalable sampling process, Early-Stopping Self-Consistency (ESC), to greatly reduce the cost of SC without sacrificing performance. On this basis, one control scheme for ESC is further derivated to dynamically choose the performance-cost balance for different tasks and models. To demonstrate ESC's effectiveness, we conducted extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning over language models with varying scales. The empirical results show that ESC reduces the average number of sampling of chain-of-thought reasoning by a significant margin on six benchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%), CommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while attaining comparable performances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=VJEcAnFPqC": {
    "title": "Toward a Mechanistic Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
    "volume": "review",
    "abstract": "Taking correct steps through elementary logical operations is the essence of log- ical reasoning, culminating in precise planning outcomes. While such step- wise inference approaches have demonstrated benefits in Large Language Mod- els (LLMs), conducting an accurate quantitative evaluation is challenging, given their extensive scale, complexity, and lack of accessibility. Here, we introduce and explore a paradigm casting stepwise inference as a graph navigation problem. We introduce a minimal synthetic setup, where an autoregressive language model solves a navigation task on directed acyclic graphs (DAGs), taking inspiration from computational graphs and execution traces. Despite its apparent simplicity, we demonstrate that our synthetic model effectively recapitulates phenomena ob- served in LLMs. By implementing training with sample paths from start to goal node in a 'step-by-step' manner, we perform systematic experiments and develop novel analyses illustrating that stepwise navigation proves advantageous when the underlying graph is hierarchical and generalization necessitates the stitching of subpaths observed during pretraining. Further, we observe a diversity-precision tradeoff while varying sampling temperature and a bias towards generating shorter paths. We next elucidate how in-context chain-of-thought exemplars can steer the model's navigation. Importantly, these exemplars can guide the model to follow a path of reasoning we provide, instead of relying on its potentially biased pri- ors. Together, this work showcases the utility and adaptability of this paradigm in exploring the complexities of logical reasoning and planning in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=MdHDUsP2lt": {
    "title": "Information-Theoretic World Model learning for Denoised Predictions",
    "volume": "review",
    "abstract": "Humans excel at isolating relevant information from noisy data to predict the behavior of dynamic systems, effectively disregarding non-informative, temporally-correlated noise. In contrast, existing reinforcement learning algorithms face challenges in generating noise-free predictions within high-dimensional, noise-saturated environments, especially when trained on world models featuring realistic background noise extracted from natural video streams. We propose a novel information-theoretic approach that learns world models based on minimising the past information and retaining maximal information about the future, aiming at simultaneously learning control policies and at producing denoised predictions. Utilizing Soft Actor-Critic agents augmented with an information-theoretic auxiliary loss, we validate our method's effectiveness on complex variants of the standard DeepMind Control Suite tasks, where natural videos filled with intricate and task-irrelevant information serve as a background. Experimental results indicate that our model surpasses four state-of-the-art approaches across six distinct scenarios where natural videos serve as dynamic background noise, all while maintaining competitive performance in traditional settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xLRAQiqd9I": {
    "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
    "volume": "review",
    "abstract": "Molecular modeling, a central topic in quantum mechanics, aims to accurately calculate the properties and simulate the behaviors of molecular systems. The molecular model is governed by physical laws, which impose geometric constraints such as invariance and equivariance to coordinate rotation and translation. While numerous deep learning approaches have been developed to learn molecular representations under these constraints, most of them are built upon heuristic and costly modules. We argue that there is a strong need for a general and flexible framework for learning both invariant and equivariant features. In this work, we introduce a novel Transformer-based molecular model called GeoMFormer to achieve this goal. Using the standard Transformer modules, two separate streams are developed to maintain and learn invariant and equivariant representations. Carefully designed cross-attention modules bridge the two streams, allowing information fusion and enhancing geometric modeling in each stream. As a general and flexible architecture, we show that many previous architectures can be viewed as special instantiations of GeoMFormer. Extensive experiments are conducted to demonstrate the power of GeoMFormer. All empirical results show that GeoMFormer achieves strong performance on both invariant and equivariant tasks of different types and scales. Code and models will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=3zQo5oUvia": {
    "title": "Retrieval-Based Reconstruction For Time-series Contrastive Learning",
    "volume": "review",
    "abstract": "The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that REBAR error is a predictor for mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method is able to learn an embedding that achieves state-of-the-art performance on downstream tasks across diverse modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1IbewSnqq": {
    "title": "FedQV: Leveraging Quadratic Voting in Federated Learning",
    "volume": "review",
    "abstract": "Federated Learning (FL) permits different parties to collaboratively train a global model without disclosing their respective local labels. A crucial step of FL, that of aggregating local models to produce the global one, shares many similarities with public decision-making, and elections in particular. In that context, a major weakness of FL, namely its vulnerability to poisoning attacks, can be interpreted as a consequence of the one person one vote (henceforth 1p1v) principle underpinning most contemporary aggregation rules. In this paper, we propose FedQV, a novel aggregation algorithm built upon the quadratic voting scheme, recently proposed as a better alternative to 1p1v-based elections. Our theoretical analysis establishes that FedQV is a truthful mechanism in which bidding according to one's true valuation is a dominant strategy that achieves a convergence rate that matches those of state-of-the-art methods. Furthermore, our empirical analysis using multiple real-world datasets validates the superior performance of FedQV against poisoning attacks. It also shows that combining FedQV with unequal voting \"budgets\" according to a reputation score increases its performance benefits even further. Finally, we show that FedQV can be easily combined with Byzantine-robust privacy-preserving mechanisms to enhance its robustness against both poisoning and privacy attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Itc7v0pnA": {
    "title": "Quantile-Free Regression: A Flexible Alternative to Quantile Regression",
    "volume": "review",
    "abstract": "Constructing valid prediction intervals rather than point estimates is a well-established method for uncertainty quantification in the regression setting. Models equipped with this capacity output an interval of values in which the ground truth target will fall with some prespecified probability. This is an essential requirement in many real-world applications in which simple point predictions' inability to convey the magnitude and frequency of errors renders them insufficient for high-stakes decisions. Quantile regression is well-established as a leading approach for obtaining such intervals via the empirical estimation of quantiles in the (non-parametric) distribution of outputs. This method is simple, computationally inexpensive, interpretable, assumption-free, and highly effective. However, it does require that the quantiles being learned are chosen a priori. This results in either (a) intervals that are arbitrarily symmetric around the median which is sub-optimal for real-world skewed distributions or (b) learning an excessive number of intervals. In this work, we propose Quantile-Free Regression (QFR), a direct replacement for quantile regression which liberates it from this limitation whilst maintaining its strengths. We demonstrate that this added flexibility results in intervals with an improvement in desirable qualities (e.g. sharpness) whilst maintaining the essential coverage guarantees of quantile regression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=DbRfXmzwjc": {
    "title": "MAGNet: Motif-Agnostic Generation of Molecules from Shapes",
    "volume": "review",
    "abstract": "Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from *in silico* predictions. Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods struggle to represent substructures beyond their known motif set. To alleviate this issue and increase flexibility across datasets, we propose MAGNet, a graph-based model that generates abstract shapes before allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that accounts for the molecules' global context and facilitates learning adequate assignments of atoms and bonds onto shapes. Despite the added complexity of shape abstractions, MAGNet outperforms most other graph-based approaches on standard benchmarks. Importantly, we demonstrate that MAGNet's improved expressivity leads to molecules with more topologically distinct structures and, at the same time, diverse atom and bond assignments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=HqQctXKI7W": {
    "title": "Casting Light on Large Generative Networks: Taming Epistemic Uncertainty in Diffusion Models",
    "volume": "review",
    "abstract": "Epistemic uncertainty plays a pivotal role in contemporary machine learning, serving as a fundamental element that underlies decision-making processes, risk evaluations, and the overall generalizability of models. In this work, we introduce an innovative framework, diffusion ensembles for capturing uncertainty (DECU), designed for estimating epistemic uncertainty within the realm of large high-performing generative diffusion models. These models typically encompass over 100 million parameters and generate outputs within a high-dimensional image space. Consequently, applying conventional methods for estimating epistemic uncertainty is unrealistic without vast computing resources. To address this gap, this paper first presents a novel method for training ensembles of conditional diffusion models in a computationally efficient manner. This is achieved by fitting an ensemble within the conditional networks while using a static set of pre-trained parameters for the remainder of the model. As a result, we significantly reduce the computational load, enabling us to train only a fraction (one thousandth) of the entire network. Furthermore, this substantial reduction in the number of parameters to be trained leads to a marked decrease (87%) in the required training steps compared to a full model on the same dataset. Second, we employ Pairwise-Distance Estimators (PaiDEs) to accurately capture epistemic uncertainty with these ensembles. PaiDEs efficiently gauge the mutual information between model outputs and weights in high-dimensional output space. To validate the effectiveness of our framework, we conducted experiments on the Imagenet dataset. The results demonstrate our ability to capture epistemic uncertainty, particularly for under-sampled image classes. This study represents a significant advancement in detecting epistemic uncertainty for conditional diffusion models, thereby casting new light on the $\\textit{black box}$ of these models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=7oYpj8BOLW": {
    "title": "BackBench: Are Vision Language Models Resilient to Object-to-Background Context?",
    "volume": "review",
    "abstract": "In this paper, we evaluate the resilience of modern vision and multimodal foundational models against object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Our approach, on the other hand, can change the background of real images using text prompts thus allowing diverse changes to the background. We achieve this while preserving the original appearance and semantics of the object of interest. This allows us to quantify the role of background context in understanding the robustness and generalization of deep neural networks. To achieve this goal, we harness the generative capabilities of text-to-image, image-to-text, and image-to-segment models to automatically generate a broad spectrum of object-to-background changes. By using textual guidance for control, we produce various versions of standard vision datasets (ImageNet, COCO), incorporating either diverse and realistic backgrounds into the images or introducing variations in the color and texture of the background. Additionally, we craft adversarial backgrounds by optimizing the latent variables and text embeddings within text-to-image models. We conduct thorough experimentation and provide an in-depth analysis of the robustness of vision and language models against object-to-background context variations across different tasks. Our code and evaluation benchmark along with the datasets will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oi6BhzIu7R": {
    "title": "REAL: Rectified Adversarial Sample via Max-Min Entropy for Test-Time Defense",
    "volume": "review",
    "abstract": "Adversarial attacks expose the vulnerability of neural networks. But it is difficult for existing defense methods to defend against all attacks, which leads to the lack of generalization in adversarial robustness. Inspired by test-time adaptation which leverages model's prediction entropy to generalize naturally distributed samples during testing, we try to rationally utilize adversarial samples' entropy for sample rectification, and then achieve test-time defense. In this article, we investigate the entropy properties of adversarial samples and obtain two observations: 1) adversarial samples are often confidently misclassified despite having low prediction entropy and 2) samples with higher attack strength typically show lower prediction entropy. Therefore, we believe directly minimizing the entropy of adversarial samples is not reasonable and propose a two-stage self-adversarial rectification approach: \\underline{Re}ctified \\underline{A}dversaria\\underline{l} Sample via Max-Min Entropy for Test-Time Defense (REAL), consisting of a max-min entropy optimization scheme and an attack-aware weighting mechanism, which can be embedded in the existing models as a plugged-played block. Experiments on several datasets show that REAL can greatly improve the performance of existing sample rectification model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=JdWpIe70FL": {
    "title": "Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators",
    "volume": "review",
    "abstract": "In machine learning, the ability to assess uncertainty in model predictions is crucial for decision-making, safety-critical applications, and model generalizability. This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy, which are then used as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100 times faster, over a larger input space (up to 100 times) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, $\\textit{Pendulum-v0}$, $\\textit{Hopper-v2}$, $\\textit{Ant-v2}$ and $\\textit{Humanoid-v2}$. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=fGAIgO75dG": {
    "title": "CoLiDE: Concomitant Linear DAG Estimation",
    "volume": "review",
    "abstract": "We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the \\emph{unknown} SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\\textbf{Co}$ncomitant $\\textbf{Li}$near $\\textbf{D}$AG $\\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of exogenous noise levels in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=WDQ9ZzsgDL": {
    "title": "PromptNER : Prompting For FewShot Named Entity Recognition",
    "volume": "review",
    "abstract": "In a surprising turn, Large Language Models (LLMs), together with a growing arsenal of prompt-based heuristics, provide powerful few-shot solutions to myriad classic NLP problems. However, despite promising early results, current LLM based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, an algorithm for few-shot and cross-domain NER. To adapt to a new NER task, PromptNER requires a set of entity definitions, and a set of few-shot examples, along with explanatory text justifying the applicability of each entity tag. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. PromptNER achieves state-of-the-art performance on few-shot NER, achieving improvements in F1 score (absolute) of 4% on the ConLL dataset, 9% on GENIA, 4% on FewNERD, 5% on FaBNER and 24% on TweetNER. PromptNER also achieves state-of-the-art performance on Cross Domain NER beating even methods not restricted to the few-shot setting on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CgpiO0DRrk": {
    "title": "Video Caching at Data-drifting Network Edge: A KD-based Cross-domain Collaborative Solution",
    "volume": "review",
    "abstract": "The explosive growth of video content streaming has led to network congestion and quality decline, making efficient content delivery a significant challenge. To address this, edge caching has emerged as a solution, utilizing mobile edge caching servers like edge base stations (EBS) as a cost-effective approach. Collaborative edge caching has been proposed to address the space limitation of edge servers by enabling cooperation and content sharing among multiple servers, thereby improving caching hit rates (CHR). However, little attention has been paid to the impact of request characteristics on different servers. To tackle this issue, we conducted a study using data collected from Kuaishou company over a period of four weeks, comprising 350 million video requests. Our research findings indicate that request-sparse EBSs significantly impede the overall CHR improvement in the edge caching problem. Knowledge distillation (KD), a technique that transfers knowledge from strong models to weak models, is expected to solve this bottleneck problem. However, traditional KD methods often rely on the assumption of independent and identically distributed data, which may not hold true in real-world scenarios where data drift occurs. We identify two major types of data drift in caching data: temporal drift and spatial drift. To overcome these challenges, we propose an adaptive KD-based cross-domain collaborative edge caching framework, called KDCdCEC, which incorporates three specifically tailored components: i) a slot-wise reinforcement learning agent capable of adapting to EBSs with varying storage sizes, ii) a deep deterministic policy gradient-based algorithm that adaptively configures the reference weights of servers on their partners, and iii) a content-aware request routing mechanism that enhances the decision-making of edge servers. Experimental results show that KDCdCEC outperforms state-of-the-art approaches in terms of average CHR, average latency, and traffic cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LzPWWPAdY4": {
    "title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models",
    "volume": "review",
    "abstract": "Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrep- ancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural lan- guage understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and out- performs existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=ikmuHqugN7": {
    "title": "Scaling Convex Neural Networks with Burer-Monteiro Factorization",
    "volume": "review",
    "abstract": "It has been demonstrated that the training problem for a variety of (non) linear two-layer neural networks (such as two-layer perceptrons, convolutional networks, and self-attention) can be posed as equivalent convex optimization problems, with an induced regularizer which encourages low rank. However, this regularizer becomes prohibitively expensive to compute at moderate scales, impeding training convex neural networks. To this end, we propose applying the Burer-Monteiro factorization to convex neural networks, which for the first time enables a Burer-Monteiro perspective on neural networks with non-linearities. This factorization leads to an equivalent yet computationally tractable non-convex alternative with no spurious local minima. We develop a novel relative optimality bound of stationary points of the Burer-Monteiro factorization, providing verifiable conditions under which any stationary point is a global optimum. Further, for the first time, we show that linear self-attention with sufficiently many heads has no spurious local minima. Our experiments validate the novel relative optimality bound and the utility of the Burer-Monteiro factorization for scaling convex neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.6,
    "authors": []
  },
  "https://openreview.net/forum?id=vogtAV1GGL": {
    "title": "Simple mechanisms for representing, indexing and manipulating concepts",
    "volume": "review",
    "abstract": "Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via grading descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. Concepts can be 'intersected' to find a common theme in a number of related concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=r42tSSCHPh": {
    "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
    "volume": "review",
    "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as \"jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the \\emph{generation exploitation} attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from $0\\\\%$ to more than $95\\\\%$ across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Jl0sjmZx9": {
    "title": "Large Multimodal Model for Real-World Radiology Report Generation",
    "volume": "review",
    "abstract": "While automatic report generation has demonstrated promising results using deep learning-based methods, deploying these algorithms in real-world scenarios remains challenging. Compared to conventional report generation, real-world report generation requires model to follow the instruction from the radiologists and consider contextual information. Thus, this paper focuses on developing a practical report generation method that supports real-world clinical practice. To tackle the challenges posed by the limited availability of clinical data, we propose a GPT-based unified data generation pipeline designed to produce high-quality data. Consequently, we present a new benchmark dataset MIMIC-R3G, comprising five representative tasks pertinent to real-world medical report generation. We propose Domain-enhanced Multi-modal Model (DeMMo), where an additional medical domain vision encoder is incorporated into the general domain multimodal LLM to enhance its ability on specific domains. This approach aims to harness the specialized capabilities of the medical domain vision encoder while leveraging the robustness and versatility of the general domain multi-modal LLM. Comprehensive experiments demonstrate that our approach attains competitive performance across all real-world tasks compared to existing interactive report generation frameworks and state-of-the-art encoder-decoder style report generation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=y21ZO6M86t": {
    "title": "PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters",
    "volume": "review",
    "abstract": "Recently, Graph Contrastive Learning (GCL) has achieved significantly superior performance in self-supervised graph representation learning. However, the existing GCL technique has inherent smooth characteristics because of its low-pass GNN encoder and objective based on homophily assumption, which poses a challenge when applying it to heterophilic graphs. In supervised learning tasks, spectral GNNs with polynomial approximation excel in both homophilic and heterophilic settings by adaptively fitting graph filters of arbitrary shapes. Yet, their applications in unsupervised learning are rarely explored. Based on the above analysis, a natural question arises: \\textit{Can we incorporate the excellent properties of spectral polynomial filters into graph contrastive learning?} In this paper, we address the question by studying the necessity of introducing high-pass information for heterophily from a spectral perspective. We propose PolyGCL, a GCL pipeline that utilizes polynomial filters to achieve contrastive learning between the low-pass and high-pass views. Specifically, PolyGCL utilizes polynomials with learnable filter functions to generate different spectral views and an objective that incorporates high-pass information through a linear combination. We theoretically prove that PolyGCL outperforms previous GCL paradigms when applied to graphs with varying levels of homophily. We conduct extensive experiments on both synthetic and real-world datasets, which demonstrate the promising performance of PolyGCL on homophilic and heterophilic graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=I7FPVqlwSe": {
    "title": "Reward Translation via Reward Machine in Semi-Alignable MDPs",
    "volume": "review",
    "abstract": "Deep reinforcement learning often relies heavily on the quality of dense rewards, which can necessitate significant engineering effort. Reusing human-designed rewards across similar tasks in different domains can enhance learning efficiency in reinforcement learning. Current works have delved into an assortment of domains characterized by divergent embodiments, differing viewpoints, and dynamic disparities. However, these studies require either alignment or alignable demonstrations in which states maintain a bijective map, consequently restricting the applicability to more generalized reward reusing across disparate domains. It becomes crucial to identify the latent structural similarities through coarser-grained alignments between distinct domains, as this enables a reinforcement learning agent to harness its capacity for abstract transfer in a manner akin to human navigation based on maps. To address this challenge, semi-alignable Markov Decision Processes (MDPs) is introduced as a fundamental underpinning to delineate the coarse-grained latent structural resemblances amidst varying domains Subsequently, the Neural Reward Translation (NRT) framework is established, which employs reward machines to resolve cross-domain reward transfer problem within semi-alignable MDPs, thus facilitating more versatile reward reusing that supports reinforcement learning across diverse domains. Our methodology is corroborated through several semi-alignable environments, highlighting NRT's efficacy in domain adaptation undertakings involving semi-alignable MDPs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=6LLho5X6xV": {
    "title": "UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model in Data Science",
    "volume": "review",
    "abstract": "Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to facilitating the prediction over tables in data science, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the establishment of a universal pretraining protocol for tables with varied structures, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a straightforward yet effective method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit. This is subsequently followed by a Transformer encoder to refine the representation. Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts. In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform. This research primarily centers on classification and regression tasks involving tabular data, and conducts rigorous experimental testing and analyses to validate the effectiveness of our methodology. The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets. This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride for tabular data analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=qi88abxiE4": {
    "title": "Large-Scale Spectral Graph Neural Networks via Laplacian Sparsification",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) play a pivotal role in graph-based tasks for their proficiency in representation learning. Among the various GNN methods, spectral GNNs employing polynomial filters have shown promising performance on both homophilous and heterophilous graph structures. The scalability of spectral GNNs is limited because forward propagation requires multiple graph propagation executions, corresponding to the degree of the polynomial. On the other hand, scalable spectral GNNs detach the graph propagation and linear layers, allowing the message-passing phase to be pre-computed and ensuring effective scalability on large graphs. However, this pre-computation can disrupt end-to-end training, possibly impacting performance, and becomes impractical when dealing with high-dimensional input features. In response to these challenges, we propose a novel graph spectral sparsification method to approximate the propagation pattern of spectral GNNs. We prove that our proposed methods generate Laplacian sparsifiers for the random-walk matrix polynomial, incorporating both static and learnable polynomial coefficients. By considering multi-hop neighbor interactions into one-hop operations, our approach facilitates the use of scalable techniques. To empirically validate the effectiveness of our methods, we conduct an extensive experimental analysis on datasets spanning various graph scales and properties. The results show that our method yields superior results in comparison with the corresponding approximated base models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Q9vYgjcvrX": {
    "title": "SASS: Self-Alignment with Semi-Supervised Instruction Data Generation",
    "volume": "review",
    "abstract": "Instruction tuning is instrumental in enabling Large Language Models~(LLMs) to follow user instructions to complete various open-domain tasks. The success of instruction tuning depends on the availability of high-quality instruction data. Owing to the exorbitant cost and substandard quality of human annotation, recent works have been deeply engaged in the exploration of the utilization of powerful closed-source models to generate instruction data automatically. However, these methods carry potential risks arising from the usage requirements of powerful closed-source models, which strictly forbid the utilization of their outputs to develop machine learning models. To deal with this problem, in this work, we explore alternative approaches to generate high-quality instruction data that do not rely on closed-source models. Our exploration includes an investigation of various existing instruction generation methods, culminating in the integration of the most efficient variant with two novel strategies to enhance the quality further. Evaluation results from two benchmarks and the GPT-4 model demonstrate the effectiveness of our generated instruction data, which can outperform Alpaca, a method reliant on closed-source models. We hope that more progress can be achieved in generating high-quality instruction data without using closed-source models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=8JCn0kmS8W": {
    "title": "WavJourney: Compositional Audio Creation with Large Language Models",
    "volume": "review",
    "abstract": "Despite breakthroughs in audio generation models, their capabilities are often confined to domain-specific conditions such as speech transcriptions and audio captions. However, real-world audio creation aims to generate harmonious audio containing various elements such as speech, music, and sound effects with controllable conditions, which is challenging to address using existing audio generation systems. We present WavJourney, a novel framework that leverages Large Language Models (LLMs) to connect various audio models for audio creation. WavJourney allows users to create storytelling audio content with diverse audio elements simply from textual descriptions. Specifically, given a text instruction, WavJourney first prompts LLMs to generate an audio script that serves as a structured semantic representation of audio elements. The audio script is then converted into a computer program, where each line of the program calls a task-specific audio generation model or computational operation function. The computer program is then executed to obtain a compositional and interpretable solution for audio creation. Experimental results suggest that WavJourney is capable of synthesizing realistic audio aligned with textually-described semantic, spatial and temporal conditions, achieving state-of-the-art results on text-to-audio generation benchmarks. Additionally, we introduce a new multi-genre story benchmark. Subjective evaluations demonstrate the potential of WavJourney in crafting engaging storytelling audio content from text. We further demonstrate that WavJourney can facilitate human-machine co-creation in multi-round dialogues. To foster future research, the code and synthesized audio are available at: https://anonymous.4open.science/w/WavJourney_Anonymous/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=hCrFG9cyuC": {
    "title": "PolyVoice: Language Models for Speech to Speech Translation",
    "volume": "review",
    "abstract": "With the huge success of GPT models in natural language processing, there is a growing interest in applying language modeling approaches to speech tasks. Currently, the dominant architecture in speech-to-speech translation (S2ST) remains the encoder-decoder paradigm, creating a need to investigate the impact of language modeling approaches in this area. In this study, we introduce PolyVoice, a language model-based framework designed for S2ST systems. Our framework comprises three decoder-only language models: a translation language model, a duration language model, and a speech synthesis language model. These language models employ different types of prompts to extract learned information effectively. By utilizing unsupervised semantic units, our framework can transfer semantic information across these models, making it applicable even to unwritten languages. We evaluate our system on Chinese $\\rightarrow$ English and English $\\rightarrow$ Spanish language pairs. Experimental results demonstrate that PolyVoice outperforms the state-of-the-art encoder-decoder model, producing voice-cloned speech with high translation and audio quality. Speech samples are available at \\url{https://polyvoice.github.io}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=IOEEDkla96": {
    "title": "Adversarial Feature Map Pruning for Backdoor",
    "volume": "review",
    "abstract": "Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground-truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender can not successfully reproduce the trigger. Consequently, the DNN model will not be repaired since the trigger is not effectively removed. In this work, we propose Adversarial Feature Map Pruning for Backdoor~(FMP) to mitigate backdoor from the DNN. Different from existing defense strategies, which focus on reproducing backdoor triggers, FMP tries to prune the backdoor feature maps, which are trained to extract backdoor information from the inputs. After pruning these backdoor feature maps, FMP will fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMP can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers~(e.g., FMP decreases the ASR to 2.86\\% in CIFAR10, 19.2\\%-65.41\\% lower than previous arts). Second, unlike conventional defense methods that tend to exhibit low Robust Accuracy (i.e., the model's accuracy on the poisoned data), FMP achieves higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks~(e.g., FMP obtains 87.40\\% RA in CIFAR10). Third, compared to existing feature map pruning techniques, FMP can cover more backdoor feature maps~(e.g., FMP removes 83.33\\% of backdoor feature maps from the model in the CIFAR10 \\& BadNet scenario)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7HdtLgsvys": {
    "title": "Tube Loss: A Novel Approach for High Quality Prediction Interval Estimation",
    "volume": "review",
    "abstract": "This paper proposes a continuous loss function termed 'tube loss' for Prediction Interval (PI) estimation. The minimizer of the proposed tube loss is a pair of functions $\\mu_1(x)$ and $\\mu_2(x)$ such that the interval $[\\mu_1(x),\\mu_2(x)]$ contains $t$ fraction of $y_i$ values. The tube loss function also facilitates an upward or downward movement of the PI tube so that the estimated PI may cover the densest regions of response values, thus allowing the sharpening of the width of PI, especially when the distribution of the response is skewed. The tube loss function-based machine learning models also have the privilege of trading off the calibration error and the width of PI by solving a single optimization problem. We have illustrated the use of tube loss functions in kernel machines, neural networks, and sequential deep learning models. Our numerical experiments show that the tube loss function is effective in yielding narrow and more accurate PIs compared to the existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=awHTL3Hpto": {
    "title": "Expressivity of ReLU-Networks under Convex Relaxations",
    "volume": "review",
    "abstract": "Convex relaxations are a key component of training and certifying provably safe neural networks. However, despite substantial progress, a wide and poorly understood accuracy gap to standard networks remains, raising the question of whether this is due to fundamental limitations of convex relaxations. Initial work investigating this question focused on the simple and widely used IBP relaxation. It revealed that some univariate, convex, continuous piecewise linear (CPWL) functions cannot be encoded by any ReLU network such that its IBP-analysis is precise. To explore whether this limitation is shared by more advanced convex relaxations, we conduct the first in-depth study on the expressive power of ReLU networks across all commonly used convex relaxations. We show that: (i) more advanced relaxations allow a larger class of univariate functions to be expressed as precisely analyzable ReLU networks, (ii) more precise relaxations can allow exponentially larger solution spaces of ReLU networks encoding the same functions, and (iii) even using the most precise single-neuron relaxations, it is impossible to construct precisely analyzable ReLU networks that express multivariate, convex, monotone CPWL functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=FDfq0RRkuz": {
    "title": "WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data",
    "volume": "review",
    "abstract": "The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies these key properties due to our algorithmic designs. Our WASA framework enables an LLM to learn an accurate mapping from the texts of different data providers to their corresponding unique watermarks, which sets the foundation for effective source attribution (and hence data provenance). Extensive empirical evaluations show that our WASA framework achieves effective source attribution and data provenance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oO6FsMyDBt": {
    "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
    "volume": "review",
    "abstract": "Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=wsjNCPqziJ": {
    "title": "Learning Latent Causal Semantics from Text: An Empirical Study of Next-Token Predictors Trained on Programs",
    "volume": "review",
    "abstract": "We present evidence that language models can learn to represent the semantics latent in text despite being trained only to perform next token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of (textual) input-output examples, and hence the semantics of the programming language enter as a \\emph{latent causal variable} in the data generation process. We then probe the trained model's hidden states as it generates a program given a specification. Despite providing no inductive bias toward learning the semantics of the programming language, we find that a linear probe is able to extract abstractions of the program states from the model states, which suggests the model acquires an emergent ability to \\emph{interpret} programs in the formal sense. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the model's ability to generate a program that correctly implements the specification. To evaluate whether the semantics are represented in the model states rather than learned by the probe, we propose a causal framework for analyzing the effects of probing, and perform interventional experiments that allow us to precisely attribute the accuracy of the probe to the semantics latent in the model's training data (rather than, e.g., the signal used to supervise the probe). In summary, this paper does not propose any new techniques for training language models, but develops an empirical framework for and provides insights into the acquisition and representation of semantics in language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=YqyTXmF8Y2": {
    "title": "Emerging Pixel-level Semantic Knowledge in Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have recently received increasing research attention for their impressive transfer abilities to semantic segmentation tasks. However, previous works rely on additional supervision to produce fine-grained segmentation maps, leaving it unclear how much diffusion models alone understand the semantic relations of their generated images. To help answer this question, we exploit the semantic knowledge extracted from Stable Diffusion (SD) and build an image segmentor that can generate fine-grained segmentation maps without any additional training. The major issue that makes this task challenging for previous works is that semantically meaningful feature maps usually exist only in the spatially lower-dimensional layers, which makes it infeasible to extract pixel-level semantic relations directly from the feature maps. To overcome this challenge, our framework identifies semantic correspondences between image pixels and spatial locations of low-dimensional feature maps by analyzing SD's generation process and utilizes them to construct image-resolution segmentation maps. In extensive experiments, the produced segmentation maps are shown to be well delineated and capture detailed parts of the images, indicating the existence of highly accurate pixel-level semantic knowledge in the diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wXpSidPpc5": {
    "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
    "volume": "review",
    "abstract": "Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4x training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=3husFxdHI1": {
    "title": "Duality of Information Flow: Insights in Graphical Models and Neural Networks",
    "volume": "review",
    "abstract": "This research highlights the convergence of probabilistic graphical models and neural networks, shedding light on their inherent similarities and interactions. By interpreting Bayesian neural networks within the framework of Markov random fields, we uncovered deep connections between message passing and neural network propagation. Our exploration unveiled a striking equivalence between gradients in neural networks and posterior-prior differences in graphical models. Empirical evaluations across diverse scenarios and datasets showcased the efficacy and generalizability of our approach. This work introduces a novel perspective on Bayesian Neural Networks and probabilistic graphical models, offering insights that could pave the way for enhanced models and a deeper understanding of their relationship",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=LixGd92Wri": {
    "title": "GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts",
    "volume": "review",
    "abstract": "Geometric deep learning (GDL) has gained significant attention in various scientific fields, chiefly for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many relevant applications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark designed for evaluating the performance of GDL models in scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics and material science to biochemistry, and encapsulates a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) testing data, including no OOD information, only OOD features without labels, and OOD features with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 GDL backbones and 11 learning algorithms in each setting. A thorough analysis of the evaluation results is provided, poised to illuminate insights for DGL researchers and domain practitioners who are to use DGL in their applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=HBEjrlu7Aa": {
    "title": "Object-level Data Augmentation for Visual 3D Object Detection in Autonomous Driving",
    "volume": "review",
    "abstract": "Data augmentation plays an important role in visual-based 3D object detection. Existing detectors typically employ image/BEV-level data augmentation techniques, failing to utilize flexible object-level augmentations because of 2D-3D inconsistencies. This limitation hinders us from increasing the diversity of training data. To alleviate this issue, we propose an object-level data augmentation approach that incorporates scene reconstruction and neural scene rendering. Specifically, we reconstruct the scene and objects by extracting image features from sequences and aligning them with associated LiDAR point clouds. This approach is intended to conduct the editing process within a 3D space, allowing for flexible object manipulation. Additionally, we introduce a neural scene renderer to project the edited 3D scene onto a specified camera plane and render it onto a 2D image. Combining with the scene reconstruction, it overcomes the challenges stemming from 2D/3D inconsistencies, enabling the generation of object-level augmented images with corresponding labels for model training. To validate the proposed method, we apply our method to two popular multi-camera detectors: PETRv2 and BEVFormer, consistently boosting the performance. Codes will be public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=YEPlTU5mZC": {
    "title": "Implicit Gaussian process representation of vector fields over arbitrary latent manifolds",
    "volume": "review",
    "abstract": "Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds, appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-density neural dynamics derived from low-density EEG recordings in healthy individuals and Alzheimer's patients. We show that vector field singularities are important disease markers and that their reconstruction leads to a comparable classification accuracy of disease states to high-density recordings. Thus, our method overcomes a significant practical limitation in experimental and clinical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=IGzaH538fz": {
    "title": "GraphGuard: Provably Robust Graph Classification against Adversarial Attacks",
    "volume": "review",
    "abstract": "Graph classification, which aims to predict a label for a graph, has many real-world applications such as malware detection, fraud detection, and healthcare. However, many studies show an attacker could carefully perturb the structure and/or node features in a graph such that a graph classifier misclassifies the perturbed graph. Such vulnerability impedes the deployment of graph classification in security/safety-critical applications. Existing empirical defenses lack formal robustness guarantees and could be broken by adaptive or unknown attacks. Existing provable defenses have the following limitations: 1) they achieve sub-optimal robustness guarantees for graph structure perturbation, 2) they cannot provide robustness guarantees for arbitrarily node feature perturbations, 3) their robustness guarantees are probabilistic, meaning they could be incorrect with a non-zero probability, and 4) they incur large computation costs. We aim to address those limitations in this work. We propose GraphGuard, a certified defense against both graph structure and node feature perturbations for graph classification. Our GraphGuard provably predicts the same label for a graph when the number of perturbed edges and the number of nodes with perturbed features are bounded. Our results on 8 benchmark datasets show GraphGuard outperforms three state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfaPgIQTul": {
    "title": "Learning HJB Viscosity Solutions with PINNs for Continuous-Time Reinforcement Learning",
    "volume": "review",
    "abstract": "Despite recent advances in Reinforcement Learning (RL), the Markov Decision Processes are not always the best choice to model complex dynamical systems requiring interactions at high frequency. Being able to work with arbitrary time intervals, Continuous Time Reinforcement Learning (CTRL) is more suitable for those problems. Instead of the Bellman equation operating in discrete time, it is the Hamiltonian Jacobi Bellman (HJB) equation that describes value function evolution in CTRL. Even though the value function is a solution of the HJB equation, it may not be its unique solution. To distinguish the value function from other solutions, it is important to look for the viscosity solutions of the HJB equation. The viscosity solutions constitute a special class of solutions that possess uniqueness and stability properties. This paper proposes a novel approach to approximate the value function by training a Physics Informed Neural Network (PINN) through a speciﬁc $\\epsilon$-scheduling iterative process constraining the PINN to converge towards the viscosity solution and shows experimental results with classical control tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=wMXH8tTQE3": {
    "title": "ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting",
    "volume": "review",
    "abstract": "Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further exploration. Our analyses point to new avenues for research, aiming for more effective time-series forecasting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=cYksYKbf6K": {
    "title": "Imagine Within Practice: Conservative Rollout Length Adaptation for Model-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "Model-based reinforcement learning (MBRL) algorithms achieve high sample efficiency by leveraging imagined rollouts from a world model for policy optimization. A crucial hyperparameter in MBRL is the rollout length, which represents a trade-off between data quality and efficiency by limiting the imaginary horizon. While longer rollout length offers enhanced efficiency, it introduces more unrealistic data due to compounding error, potentially leading to catastrophic performance deterioration. To prevent significant deviations between imagined rollouts and real transitions, most model-based methods manually tune a fixed rollout length for the entire training process. However, the fixed rollout length is not optimal for all rollouts and does not effectively prevent the generation of unrealistic data. To tackle this problem, we propose a novel method called Conservative Rollout Length Adaptation (CRLA), which conservatively restricts the agent from selecting actions that are rarely taken in the current state. CRLA truncates the rollout to preserve safety when there is a high probability of selecting infrequently taken actions. We apply our method to DreamerV3 and evaluate it on the Atari 100k benchmark. The results demonstrate that CRLA can effectively balance data quality and efficiency by adjusting rollout length and achieve significant performance gains in most Atari games compared to DreamerV3 in the default setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=61DYdiyQqk": {
    "title": "Two Heads Are Better Than One: Exploiting Both Sequence and Graph Models in AMR-To-Text Generation",
    "volume": "review",
    "abstract": "Abstract meaning representation (AMR) is a special semantic representation language, which can capture the core meaning of a sentence with a syntax-irrelevant graph. AMR-to-text generation, which aims to generate a sentence according to a given AMR graph, is a well-studied task and has shown its helpfulness in various other NLP tasks. Existing AMR-to-text generation methods can be roughly divided into two categories, while either has its own advantages and disadvantages. The first one adopts a sequence-to-sequence model, especially a pretrained language model (PLM). It has good text generation ability but cannot cope with the structural information of AMR graphs well. The second category of method is based on graph neural networks (GNNs), whose advantages and disadvantages are exactly the opposite. To combine the strengths of the two kinds of models, in this paper, we propose a dual encoder-decoder model named \\modelName, which integrates a specially designed GNN into a pre-trained sequence-to-sequence model. We conduct extensive experiments as well as human evaluation and a case study, finding that it achieves the desired effect and yields state-of-the-art performance in the AMR-to-text generation task. We also demonstrate that it outperforms the most powerful general-purpose PLM GPT-4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xVbke7yC07": {
    "title": "Enhancing Tropical Cyclone Formation Prediction Using Graph Neural Networks",
    "volume": "review",
    "abstract": "Tropical cyclones are among the most powerful and destructive weather events on Earth, and the formation and evolution of these systems is crucial to the resilience and safety of coastal populations. Although physical models have historically been used to research tropical cyclones, these models frequently fail to capture the complex interactions between many atmospheric and oceanic factors that influence cyclonic systems' behavior. In this research, we suggest a unique method of employing graph neural networks (GNNs) to analyze the development and evolution of tropical cyclones. GNNs are an effective machine learning technique that can learn from huge and complex datasets, which makes them well-suited to capture the underlying patterns in the behavior of tropical cyclones. In our method, a GNN is used to estimate cyclone formation, forecast whether it will become stronger or weaker in the following time step, and match the evolution pattern of cyclones in the training set. We tested our method on a substantial dataset of tropical cyclones and showed that it outperformed conventional physical models in predicting the genesis of tropical cyclones. Our research also shown that the intricate connections between atmospheric and oceanic factors that affect tropical cyclones are better captured by the GNN-based method, leading to a better understanding of their behavior. As a result of our research, better early warning systems and disaster response planning will be possible, allowing for more precise forecasts of tropical cyclone development and behavior. Our work also shows how machine learning methods may improve our comprehension of intricate meteorological processes, presenting new avenues for research in atmospheric science",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=gbrHZq07mq": {
    "title": "Logical Languages Accepted by Transformer Encoders with Hard Attention",
    "volume": "review",
    "abstract": "We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\\sf TC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from ${\\sf AC}^0$. We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms. We apply these results to derive new results on the expressive power of UHAT and AHAT up to permutation of letters (a.k.a. Parikh images)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=2Kf1AIdeyt": {
    "title": "Balancing Information Preservation and Computational Efficiency: L2 Normalization and Geodesic Distance in Manifold Learning",
    "volume": "review",
    "abstract": "Distinguishable metric of similarity plays a fundamental role in unsupervised learning, particularly in manifold learning and high-dimensional data visualization tasks, by which differentiate between observations without labels. However, conventional metrics like Euclidean distance after L1-normalization may fail by losing distinguishable information when handling high-dimensional data, where the distance between different observations gradually converges to a shrinking interval. In this article, we discuss the influence of normalization by different p-norms and the defect of Euclidean distance. We discover that observation differences are better preserved when normalizing data by a higher p-norm and using geodesic distance rather than Euclidean distance as the similarity measurement. We further identify that L2-normalization onto the hypersphere is often sufficient in preserving delicate differences even in relatively high dimensional data while maintaining computational efficiency. Subsequently, we present HS-SNE (HyperSphere-SNE), a hypersphere-representation-system-based augmentation to t-SNE, which effectively addresses the intricacy of high-dimensional data visualization and similarity measurement. Our results show that this hypersphere representation system has improved resolution to identify more subtle differences in high-dimensional data, while balancing information preservation and computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=apQukvJHFE": {
    "title": "Lightweight uncertainty modelling using function space particle optimization",
    "volume": "review",
    "abstract": "Deep ensembles have shown remarkable empirical success in quantifying uncertainty, albeit at considerable computational cost and memory footprint. Meanwhile, deterministic single-network uncertainty methods have proven as computationally effective alternatives, providing uncertainty estimates based on distributions of latent representations. While those methods are successful at out-of-domain detection, they exhibit poor calibration under distribution shifts. In this work, we propose a method that provides calibrated uncertainty by utilizing particle-based variational inference in function space. Rather than using full deep ensembles to represent particles in function space, we propose a single multi-headed neural network that is regularized to preserve bi-Lipschitz conditions. Sharing a joint latent representation enables a reduction in computational requirements, while prediction diversity is maintained by the multiple heads. We achieve competitive results in disentangling aleatoric and epistemic uncertainty for active learning, detecting out-of-domain data, and providing calibrated uncertainty estimates under distribution shifts while significantly reducing compute and memory requirements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qNrJJZAKI3": {
    "title": "FairSeg: A Large-scale Medical Image Segmentation Dataset for Fairness Learning with Fair Error-Bound Scaling",
    "volume": "review",
    "abstract": "Fairness in artificial intelligence models has gained significantly more attention in recent years, especially in the area of medicine, as fairness in medical models is critical to people's well-being and lives. High-quality medical fairness datasets are needed to promote fairness learning research. Existing medical fairness datasets are all for classification tasks, and no fairness datasets are available for medical segmentation, while medical segmentation is an equally important clinical task as classifications, which can provide detailed spatial information on organ abnormalities ready to be assessed by clinicians. In this paper, we propose the first fairness dataset for medical segmentation named FairSeg with 10,000 subject samples. In addition, we propose a fair error-bound scaling approach to reweight the loss function with the upper error-bound in each identity group. We anticipate that the segmentation performance equity can be improved by explicitly tackling the hard cases with high training errors in each identity group. To facilitate fair comparisons, we propose new equity-scaled segmentation performance metrics, such as the equity-scaled Dice coefficient, which is calculated as the overall Dice coefficient divided by one plus the standard deviation of group Dice coefficients. Through comprehensive experiments, we demonstrate that our fair error-bound scaling approach either has superior or comparable fairness performance to the state-of-the-art fairness learning models. The dataset and code are publicly accessible via https://github.com/anonymous-for-science/FairSeg",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=G1Hlubz1fR": {
    "title": "Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning",
    "volume": "review",
    "abstract": "Modular skill learning is an emerging direction in the field of Parameter Efficient Fine-Tuning (PEFT), as it enables neural networks to better organize and clarify various aspects of knowledge, leading to improved knowledge transfer for new tasks. In this paper, we introduce a novel approach that categorizes skills into shared domain skills and specialized skills, with the skill parameters being highly parameterized using low-rank or sparse techniques. Each task is associated with an exclusive specialized skill while also benefiting from shared domain skills. Moreover, tasks can selectively utilize specialized skills from other tasks as needed. To facilitate this approach, we propose a skill assignment matrix that can be jointly learned, and the task network is instantiated based on the skill parameters. To evaluate the effectiveness of our approach, we conducted extensive experiments on the Super Natural Instructions and SuperGLUE datasets. Our results demonstrate that compared to fully-shared, task-specific, or skill-indistinguishable baselines. Modular learning with skill-type discrimination significantly enhances the sample efficiency of multi-task learning. Furthermore, the freezing of a substantial number of base model parameters greatly improves parameter efficiency, leading to boosted training efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9gyDdCKTDJ": {
    "title": "Gaitor: Learning a Unified Representation for Continuous Gait Transition and Terrain Traversal for Quadruped Robots",
    "volume": "review",
    "abstract": "The current state-of-the-art in quadruped locomotion is able to produce robust motion for terrain traversal but requires the segmentation of a desired trajectory into a discrete set of skills such as trot, crawl and pace. This misses the opportunity to leverage commonalities between individual gait types for efficient learning and are unable to smoothly transition between them. Here we present Gaitor, which creates a learnt representation capturing correlations across multiple distinct gait types resulting in the discovery of smooth transitions between motions. In particular, this representation is compact meaning that information common to all gait types is shared. The emerging structure is interpretable in that it encodes phase correlations between the different gait types which can be leveraged to produce smooth gait transitions. In addition, foot swing characteristics are disentangled and directly addressable. Together with a rudimentary terrain encoding and a learned planner operating in this structured latent representation, Gaitor is able to take motion commands including gait type and characteristics from a user while reacting to uneven terrain. We evaluate Gaitor in both simulated and real-world settings, such as climbing over raised platforms, on an ANYmal C platform. To the best of our knowledge, this is the first work learning an interpretable unified-latent representation for multiple gaits, resulting in smooth and natural looking gait transitions between trot and crawl on a real quadruped robot",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IiimxXqxNP": {
    "title": "Efficient and scalable reinforcement learning via hypermodel",
    "volume": "review",
    "abstract": "Data-efficient reinforcement learning(RL) requires deep exploration. Thompson sampling is a principled method for deep exploration in reinforcement learning. However, Thompson sampling need to track the degree of uncertainty by maintaining the posterior distribution of models, which is computationally feasible only in simple environments with restrictive assumptions. A key problem in modern RL is how to develop data and computation efficient algorithm that is scalable to large-scale complex environments. We develop a principled framework, called HyperFQI, to tackle both the computation and data efficiency issues. HyperFQI can be regarded as approximate Thompson sampling for reinforcement learning based on hypermodel. Hypermodel in this context serves as the role for uncertainty estimation of action-value function. HyperFQI demonstrates its ability for efficient and scalable deep exploration in DeepSea benchmark with large state space. HyperFQI also achieves super-human performance in Atari benchmark with 2M interactions with low computation costs. We also give a rigorous performance analysis for the proposed method, justifying its computation and data efficiency. To the best of knowledge, this is the first principled RL algorithm that is provably efficient and also practically scalable to complex environments such as Arcade learning environment that requires deep networks for pixel-based control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=anG8cNYQAs": {
    "title": "INCYDE: A large scale cyclone detection and intensity estimation dataset using satellite infrared imagery",
    "volume": "review",
    "abstract": "Tropical cyclones are devastating natural phenomena that cause a significant amount of damage every year. Conventionally, the Dvorak technique is used to detect cyclones and estimate cyclone intensity from satellite infrared imagery by observing cloud patterns. Satellite infrared imagery provides valuable information for detecting cyclonic storms. Recently, deep CNN models have proven to be highly efficient in detecting relevant patterns in the images. In this work, a novel cyclone detection and intensity estimation dataset called INCYDE (INSAT-based Cyclone Detection and Intensity Estimation) dataset is presented. The cyclone images in the dataset are captured from INSAT 3D/3DR satellites over the Indian Ocean. The proposed INCYDE dataset contains over 21k cyclone images taken from cyclones over the Indian Ocean from the year 2013 to 2021. The dataset pertains to two specific tasks: cyclone detection as an object detection task, and intensity estimation as a regression task. In addition to the dataset, this study in troduces baseline models that were trained on the newly presented dataset. The results of this research would help develop innovative cyclone detection and intensity estimation models, which in turn could help save lives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8DLVrWL78S": {
    "title": "Streamlining Generative Models for Structure-Based Drug Design",
    "volume": "review",
    "abstract": "Generative models for structure-based drug design (SBDD) aim to generate novel 3D molecules for specified protein targets $\\textit{in silico}$. The prevailing paradigm focuses on model expressivity - typically with powerful Graph Neural Network (GNN) models - but is agnostic to binding affinity during training, potentially overlooking better molecules. We address this issue with a two-pronged approach: learn an economical surrogate for affinity to infer an unlabeled molecular graph, and optimize for labels conditioned on this graph and desired molecular properties (e.g., QED, SA). The resulting model FastSBDD achieves state-of-the-art results as well as streamlined computation and model size (up to 1000x faster and with 100x fewer trainable parameters compared to existing methods), paving way for improved docking software. We also establish rigorous theoretical results to expose the representation limits of GNNs in SBDD contexts and the generalizability of our affinity scoring model, advocating more emphasis on generalization going forward",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nu9mOSq7eH": {
    "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists",
    "volume": "review",
    "abstract": "Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=pTqmVbBa8R": {
    "title": "Generative Modeling of Individual Behavior at Scale",
    "volume": "review",
    "abstract": "Recent years have seen a growing interest in using AI to model human behavior, particularly in domains where humans learn from or collaborate with this AI. While most existing work attempts to model human behavior at an aggregate level, our goal is to model behavior at the individual level. Recent work in the domain of chess has shown that behavioral stylometry, or the task of identifying a person from their actions alone, can be achieved with high accuracy among a pool of a few thousand players. We provide a new perspective on behavioral stylomery by connecting it to the vast literature of transfer learning in NLP. Specifically, by casting the stylometry problem as a multi-task learning problem---where each task is a distinct person---we show that parameter efficient fine-tuning (PEFT) methods can be adapted to perform stylometry at an unprecedented scale (47,864 players), while enabling few-shot learning for unseen players. Our approach leverages recent modular PEFT methods to learn a set of skill parameters that can be combined in different ways using style vectors. Style vectors enable two important capabilities. First, they make our approach generative, in that we can generate actions in the style of a player by simply indexing into that player's style vector. Second, they induce a latent style space that we can interpreted and manipulated algorithmically. This allows us to compare different player styles, as well as synthesize new (human-like) styles, e.g., merging the styles of two players or interpolating between their styles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=rAX55lDjtt": {
    "title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities",
    "volume": "review",
    "abstract": "The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of vision and language understanding tasks through their massive open-world knowledge and inter-task homogeneity, only a few of them can be generalised to the audio domain without compromising their domain-specific capacity. Meanwhile, a majority of existing multimodal foundation models (e.g., VLMs) structure their input sequences as [Multimedia, Question, Answer], constraining their applicability to more comprehensive tasks, such as natural language visual reasoning. In this work, we introduce Acoustic Prompt Turning (APT), an acoustic adapter leveraging a multi-task learning framework to extend LLMs and VLMs to the audio domain. APT uses an instruction-aware aligner to acquire a fixed number of acoustic embeddings by cross-attending audio feature maps generated from an audio encoder. Diverse audio-related tasks are formulated in a sequence-to-sequence manner without imposing any constraints on input sequences, and therefore, allowing APT to be seamlessly trained by combining the present multi-task learning with in-context learning. Experiments show that LLMs coupled with APT (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the targeted datasets) across various tasks. Additionally, we evaluate APT-LLMs to a novel audio reasoning task involving comparative analysis and summarisation of two audio clips. We also demonstrate the ability of APT to extend frozen VLMs to the audio domain, yielding promising results in the audio-visual understanding task even without finetuned on any audio-visual datasets. Our code and model weights will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=Koh0i2u8qX": {
    "title": "Mitigating Severe Robustness Degradation on Graphs",
    "volume": "review",
    "abstract": "Although graph neural networks have exhibited remarkable performance in various graph tasks, a significant concern is their vulnerability to adversarial attacks. Consequently, many defense methods have been proposed to alleviate the deleterious effects of adversarial attacks and learn robust graph representations. However, most of them are difficult to *simultaneously* avoid two major limitations: (i) an emergent and severe degradation in robustness when exposed to very intense attacks, and (ii) heavy computation complexity hinders them from scaling to large graphs. In response to these challenges, we introduce an innovative graph defense method for unpredictable real-world scenarios by *designing a graph robust learning framework that is resistant to robustness degradation* and *refraining from unscalable designs with heavy computation*: specifically, our method employs a denoising module, which eliminates edges that are associated with attacked nodes to reconstruct a cleaner graph; Then, it applies Mixture-of-Experts to select differentially private noises with varying magnitudes to counteract the hidden features attacked at different intensities toward robust predictions; Moreover, our overall design avoids the reliance on heavy adjacency matrix computations, such as SVD, thus facilitating its applicability even on large graphs. Comprehensive experiments have been conducted to demonstrate the anti-degraded robustness and scalability of our method, as compared to popular graph adversarial learning methods, under diverse attack intensities and various datasets of different sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=p4S5Z6Sah4": {
    "title": "Traveling Waves Encode The Recent Past and Enhance Sequence Learning",
    "volume": "review",
    "abstract": "Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically grounded hypothesis suggests that the cortical sheet may act like a wave-field capable of invertibly storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface, and indeed many experimental results from neuroscience correlate wave activity with memory tasks. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and reach significantly lower error than wave-free counterparts. We further explore the implications of this memory storage system on more complex sequence modeling tasks such as sequential image classification and find that wave-based models not only again outperform comparable wave-free RNNs while using significantly fewer parameters, but additionally perform comparably to more complex gated architectures such as LSTMs and GRUs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=I0wEUVzbNY": {
    "title": "A Critical Study of What Pre-trained Code Models (do not) Learn",
    "volume": "review",
    "abstract": "Parallel to the recent success of self-attention-based language models across a range of coding assistance tasks, several studies have underscored that pre-trained code models (PCMs) utilize self-attention and hidden representations to encode relations among input tokens. Our research extends upon these insights by understanding the properties of code that PCMs may not fully encode and by broadening the scope to encompass data flow relations. Our study reveals that while PCMs do encode syntactic and data flow relations in self-attention, they only encode relations within specific subsets of input tokens. Specifically, by categorizing input tokens into syntactic tokens and identifiers, we find that models encode relations among syntactic tokens and among identifiers but fail to encode relations between syntactic tokens and identifiers. We show that this limitation results in hidden representations not encoding enough information about input tokens to discriminate between different identifier types and syntax structures. Importantly, we observe that this learning gap persists across different model architectures, datasets, and pre-training objectives. Our findings shed light on why PCMs fail to generalize beyond dataset they are trained on and in real world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=E64ZqVCr72": {
    "title": "Active Domain Adaptation Of Medical Images Using Feature Disentanglement",
    "volume": "review",
    "abstract": "State-of-the-art deep learning models often fail to generalize in the presence of distribution shifts between training (source) data and test (target) data. Domain adaptation techniques have been developed to address this challenge, leveraging either labeled data (supervised domain adaptation) or unlabeled data (unsupervised domain adaptation). The careful selection of target domain samples can significantly enhance model performance and robustness, while also reducing the overall data requirements. Active learning, a strategy for intelligently choosing informative samples with minimal annotation effort, offers a means to maximize performance. In this paper, we introduce an innovative method for active learning in the presence of domain shifts. We propose a novel feature disentanglement approach to decompose image features into domain-specific and task-specific components. Thereafter we define multiple novel cost functions that identify informative samples under domain shift. We test our proposed method for medical image classification using one histopathology dataset and two chest x-ray datasets. Experiments show our proposed approach achieves state-of-the-art performance when compared to both domain adaptation methods and other active domain adaptation techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=YkEW5TabYN": {
    "title": "Perturbed examples reveal invariances shared by language models",
    "volume": "review",
    "abstract": "An explosion of work in language is leading to ever-increasing numbers of available natural language processing models, with little understanding of how new models compare to better-understood models. One major reason for this difficulty is saturating benchmark datasets, which may not reflect well differences in model performance in the wild. In this work, we propose a novel framework for comparing two natural language processing models by revealing their shared invariance to interpretable input perturbations that are designed to target a specific linguistic capability (e.g., Synonym-Invariance, Typo-Invariance). Via experiments on models from within the same and across different architecture families, this framework offers a number of insights about how changes in models (e.g. distillation, increase in size, amount of pre-training) affect multiple well-defined linguistic capabilities. Furthermore, we also demonstrate how our framework can enable evaluation of the invariances shared between models that are available as commercial black-box APIs (e.g., InstructGPT family) and models that are relatively better understood (e.g., GPT-2). Across several experiments, we observe that large language models share many of the invariances encoded by models of various sizes, whereas the invariances encoded by large language models are only shared by other large models. Possessing a wide variety of invariances may be a key reason for the recent successes of large language models, and our framework can shed light on the types of invariances that are retained by or emerge in new models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FT4gAPFsQd": {
    "title": "How Sparse Can We Prune A Deep Network: A Geometric Viewpoint",
    "volume": "review",
    "abstract": "Network pruning constitutes an effective measure to alleviate the storage and computational burden of deep neural networks which arises from its overparameterization. A fundamental question is: How sparse can we prune a deep network without sacrifice on the performance? To address this problem, in this work we take a first principles approach, specifically, by directly enforcing the sparsity constraint on the original loss function and exploiting the universal \\textit{concentration} effect in the high-dimensional world, we're able to characterize the sharp phase transition point of pruning ratio, which turns out to equal one minus the normalized squared Gaussian width of a convex set determined by the $l_1$-regularized loss function. Meanwhile, we provide efficient countermeasures to address the challenges in computing the involved Gaussian width, including the spectrum estimation of a large-scale Hessian matrix and dealing with the non-definite positiveness of a Hessian matrix. Moreover, through the lens of the pruning ratio threshold, we're able to identify the key factors that impact the pruning performance, thus providing intuitive explanations on many phenomena of existing pruning algorithms. Extensive experiments are performed which demonstrate that the theoretical pruning ratio threshold coincides very well with the experimental one. All codes are available at: \\url{https://anonymous.4open.science/r/Global-One-shot-Pruning-BC7B/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=6IjN7oxjXt": {
    "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training",
    "volume": "review",
    "abstract": "Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We, therefore, propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating \"robust overfitting\". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=l5ouuojPGe": {
    "title": "Red Pill or Blue Pill? Thresholding Strategies for Neural Network Monitoring",
    "volume": "review",
    "abstract": "With the increasing deployment of neural networks in critical systems, runtime monitoring plays a critical role in rejecting unsafe predictions during inference. Various techniques have emerged to establish rejection scores that aim to maximize the separability between the distributions of safe and unsafe predictions. In most works, the efficacy of these approaches is evaluated using threshold-agnostic metrics, such as the area under the receiver operating characteristic curve. However, in real-world applications, the effectiveness of a monitor also requires identifying a good threshold to transform these scores into meaningful binary decisions. Despite the pivotal importance of threshold optimization in practice, this problem has received little to no attention in the literature. In this work, we address this question by comparing four strategies for constructing threshold optimization datasets, each reflecting a different assumption about the data available for threshold tuning. We present rigorous experiments on various image datasets and conclude that: 1. Knowledge about runtime threats actually impacting the system helps greatly in identifying an optimal threshold. 2. Without this information, relying solely on in-distribution data is advised, as adding unrelated generic threat data produces worse thresholds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=CWoIj2XJuT": {
    "title": "Unbalanced Diffusion Schrödinger Bridge",
    "volume": "review",
    "abstract": "_Schrödinger bridges_ (SBs) provide an elegant framework for modeling the temporal evolution of populations in physical, chemical, or biological systems. Such natural processes are commonly subject to changes in population size over time due to the emergence of new species or birth and death events. However, existing neural parameterizations of SBs such as _diffusion Schrödinger bridges_ ( DSBs) are restricted to settings in which the endpoints of the stochastic process are both _probability measures_ and assume _conservation of mass_ constraints. To address this limitation, we introduce _unbalanced_ DSBs which model the temporal evolution of marginals with arbitrary finite mass. This is achieved by deriving the time reversal of _stochastic differential equations_ (SDEs) with killing and birth terms. We present two novel algorithmic schemes that comprise a scalable objective function for training unbalanced DSBs and provide a theoretical analysis alongside challenging applications on predicting heterogeneous molecular single-cell responses to various cancer drugs and simulating the emergence and spread of new viral variants",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rKPK2Rn6y8": {
    "title": "Autonomous Tree-search Ability of Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have excelled in remarkable reasoning capabilities with advanced prompting techniques (e.g., Chain-of-Thought), but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making. Recent works propose to utilize external programs (e.g., Python codes) to define search logic, such that LLMs can perform passive tree search to solve more challenging reasoning tasks. Though impressive results have been achieved, there are several fundamental limitations of these approaches. First, passive tree searches are not efficient as they usually require multiple rounds of LLM API calls to solve one single problem. Moreover, passive search methods are not flexible since they need task-specific program designs. Then a natural question arises: can we maintain the tree-search capability of LLMs without the aid of external programs, and can still generate responses that clearly demonstrate the process of a tree-structure search? To this end, we propose a new concept called autonomous tree-search ability of LLM, which can automatically generate a response containing search trajectories for the correct answer. Concretely, we first perform both BFS and DFS style search trajectories using more capable LLM API (e.g. GPT-4 and GPT-3.5) via a fixed system prompt, allowing them to perform autonomous tree-search (ATS) right out of the box. Experiments on 4 challenge puzzle games demonstrate our method can achieve huge improvements. The ATS-BFS method outperforms the Chain of Thought approach by achieving an average accuracy improvement of 33\\%. Compared to Tree of Thoughts, it requires 65.6\\% or 47.7\\% less GPT-api cost to attain a comparable level of accuracy. Moreover, we have collected a dataset using the ATS prompt method and fine-tuned LLaMA with this dataset. This approach has shown to yield a greater improvement compared to the ones fine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an average of 40.6\\% and 38.5\\% for LLaMA2-7B and LLaMA2-13B, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mHYkcQzdae": {
    "title": "A Novel Approach for Micro-Expression Recognition Incorporating Vertical Attention and Position Localization",
    "volume": "review",
    "abstract": "Micro-expression (ME) is a kind of facial expression that is short-lived and difficult for ordinary people to detect. Micro-expression can reflect the real emotion that people try to hide. It is difficult to identify micro-expression due to the fact that the duration is short and it only involves partial muscle motions, which brings great challenges to the accurate identification of micro-expression. To address these issues, we propose a novel neural network for micro-expression recognition (MER), focusing on subtle changes in facial movements using a CVA (Continuously Vertical Attention) block, which models the local muscle changes with minimal identity information. Additionally, we propose a facial position localization module called FPF (Facial Position Focalizer) based on Swin Transformer, which incorporates spatial information into the facial muscle movement pattern features used for MER. We also proved that including AU (Action Units) can further enhance accuracy, and therefore we have incorporated AU information to assist in micro-expression recognition. The experimental results indicate that the model achieved an average recognition accuracy of 94.35\\% and 86.76\\% on the popular CASME II and SAMM micro-expression datasets, improved by 6\\% and 1.98\\% compared to state-of-the-art models, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kaAtQwhnM2": {
    "title": "Perturb and Learn: Energy-Based Modelling in Discrete Spaces without MCMC",
    "volume": "review",
    "abstract": "Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on discrete data poses significant challenges, primarily due to the intricacies of sampling in such spaces. In this work, we propose to train discrete EBMs with Energy Discrepancy which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for demanding sampling techniques like Markov chain Monte Carlo. Energy discrepancy offers theoretical guarantees applicable to a broad class of perturbation processes, of which we investigate three types: perturbations based on Bernoulli noise, deterministic transforms, and neighbourhood structures. We estimate the energy discrepancy loss effectively using importance sampling with two types of proposal distributions: uninformed and gradient-informed. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including Ising models training, discrete density estimation, graph generation, and discrete image modelling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=hB2hXtxIPH": {
    "title": "Greedy Sequential Execution: Solving Homogeneous and Heterogeneous Cooperative Tasks with a Unified Framework",
    "volume": "review",
    "abstract": "Effectively handling both homogeneous and heterogeneous tasks is crucial for the practical application of cooperative agents. However, existing solutions have not been successful in addressing both types of tasks simultaneously. On one hand, value-decomposition-based approaches demonstrate superior performance in homogeneous tasks. Nevertheless, they tend to produce agents with similar policies, which is unsuitable for heterogeneous tasks. On the other hand, solutions based on personalized observation or assigned roles are well-suited for heterogeneous tasks. However, they often lead to a trade-off situation where the agent's performance in homogeneous scenarios is negatively affected due to the aggregation of distinct policies. An alternative approach is to adopt sequential execution policies, which offer a flexible form for learning both types of tasks. However, learning sequential execution policies poses challenges in terms of credit assignment, and the lack of sufficient information about subsequently executed agents can lead to sub-optimal solutions. To tackle these issues, this paper proposes Greedy Sequential Execution (GSE) as a solution to learn the optimal policy that covers both scenarios. In the proposed GSE framework, we introduce an individual utility function into the framework of value decomposition to consider the complex interactions between agents. This function is capable of representing both the homogeneous and heterogeneous optimal policies. Furthermore, we utilize a greedy marginal contribution calculated by the utility function as the credit value of the sequential execution policy to address the credit assignment problem. We evaluated GSE in both homogeneous and heterogeneous scenarios. The results demonstrate that GSE achieves significant improvement in performance across multiple domains, especially in scenarios involving both homogeneous and heterogeneous tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=U2ZIgcrg7Z": {
    "title": "ZOOPFL: EXPLORING BLACK-BOX FOUNDATION MODELS FOR PERSONALIZED FEDERATED LEARNING",
    "volume": "review",
    "abstract": "When personalized federated learning (FL) meets large foundation models, new challenges arise from various limitations in resources. In addition to typical limitations such as data, computation, and communication costs, access to the models is also often limited. This paper endeavors to solve both the challenges of limited resources and personalization. i.e., distribution shifts between clients. To do so, we propose a method named ZOOPFL that uses Zeroth-Order Optimization for Personalized Federated Learning. ZOOPFL avoids direct interference with the foundation models and instead learns to adapt its inputs through zeroth-order optimization. In addition, we employ simple yet effective linear projections to remap its predictions for personalization. To reduce the computation costs and enhance personalization, we propose input surgery to incorporate an auto-encoder with low-dimensional and client-specific embeddings. We provide theoretical support for ZOOPFL to analyze its convergence. Extensive empirical experiments on computer vision and natural language processing tasks using popular foundation models demonstrate its effectiveness for FL on black-box foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=NLevOah0CJ": {
    "title": "Hindsight PRIORs for Reward Learning from Human Preferences",
    "volume": "review",
    "abstract": "Preference based Reinforcement Learning (PbRL) removes the need to hand specify a reward function by learning one from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most contributed to a preference resulting in data intensive approaches and subpar reward models. We address such limitations by introducing a credit assignment strategy (PRIOR) that uses a forward dynamics world model to approximate state importance within a trajectory and then guides rewards to be proportional to state importance through an auxiliary predicted return redistribution objective. Incorporating state importance into reward learning improves the speed of policy learning, overall policy performance, and reward recovery on both locomotion and manipulation tasks. For example, PRIOR achieves 80% success rate with half the amount of data compared to baselines. The performance gains and our ablations demonstrate the benefits even a simple credit assignment strategy can have on reward learning and that state importance in forward dynamics prediction is a strong proxy for a state's contribution to a preference decision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=ONnZVUrFBT": {
    "title": "Communication-Efficient Algorithm for Asynchronous Multi-Agent Bandits",
    "volume": "review",
    "abstract": "We study the cooperative asynchronous multi-agent multi-armed bandits problem, where the active (arm pulling) decision rounds of each agent are asynchronous. In each round, only a subset of agents is active to pull arms, and this subset is unknown and time-varying. We propose a fully distributed algorithm that relies on novel asynchronous communication protocols. This algorithm attains near-optimal regret with constant (time-independent) communications for adversarial asynchronicity among agents. Furthermore, to protect the privacy of the learning process, we extend our algorithms to achieve local differential privacy with rigorous guarantees. Lastly, we report numerical simulations of our new asynchronous algorithms with other known baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=mbPvdO2dxb": {
    "title": "Meta-Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems",
    "volume": "review",
    "abstract": "In the realm of medical imaging, inverse problems aim to infer high-quality images from incomplete, noisy measurements, with the objective of minimizing expenses and risks to patients in clinical settings. The Diffusion Models have recently emerged as a promising approach to such practical challenges, proving particularly useful for the zero-shot inference of images from partially acquired measurements in Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). A central challenge in this approach, however, is how to guide an unconditional prediction to conform to the measurement information. In this paper, we propose a Meta-Guided Diffusion Model (MGDM) that tackles this challenge through a \\emph{bi-level} guidance strategy, where the \\emph{outer level} solves a proximal optimization problem to impose measurement consistency and the \\emph{inner level} approximates the measurement-conditioned posterior mean as the initial prediction. Furthermore, we introduce a refinement phase, termed the \"discrepancy gradient'', designed to reduce the distance between the outputs of the aforementioned levels, thereby acting as an effective regularizer to further enhance data consistency in the recovered samples. Empirical results on publicly available medical datasets in MRI and CT highlight the superior performance of our proposed algorithm, faithfully reproducing high-fidelity medical images consistent with measurements, and notably mitigating the generation of hallucinatory images observed in state-of-the-art methods under similar conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=nN1bEm8cna": {
    "title": "Are Spiking Neural Networks more expressive than Artificial Neural Networks?",
    "volume": "review",
    "abstract": "This article studies the expressive power of spiking neural networks with firing-time-based information encoding, highlighting their potential for future energy-efficient AI applications when deployed on neuromorphic hardware. The computational power of a network of spiking neurons has already been studied via their capability of approximating any continuous function. By using the Spike Response Model as a mathematical model of a spiking neuron and assuming a linear response function, we delve deeper into this analysis and prove that spiking neural networks generate continuous piecewise linear mappings. We also show that they can emulate any multi-layer (ReLU) neural network with similar complexity. Furthermore, we show that the maximum number of linear regions generated by a spiking neuron scales exponentially with respect to the input dimension, a characteristic that distinguishes it significantly from an artificial (ReLU) neuron. Our results further extend the understanding of the approximation properties of spiking neural networks and open up new avenues where spiking neural networks can be deployed instead of artificial neural networks without any performance loss",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=kzGuiRXZrQ": {
    "title": "Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation",
    "volume": "review",
    "abstract": "Deep generative diffusion models are a promising avenue for 3D $\\textit{de novo}$ molecular design in material science and drug discovery. However, their utility is still constrained by suboptimal performance with large molecular structures and limited training data. Addressing this gap, we explore the design space of E(3) equivariant diffusion models, focusing on previously blank spots. Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. Out of this investigation, we introduce the EQGAT-diff model, which consistently surpasses the performance of established models on the QM9 and GEOM-Drugs datasets by a large margin. Distinctively, EQGAT-diff takes continuous atomic positions while chemical elements and bond types are categorical and employ a time-dependent loss weighting that significantly increases training convergence and the quality of generated samples. To further strengthen the applicability of diffusion models to limited training data, we examine the transferability of EQGAT-diff trained on the large PubChem3D dataset with implicit hydrogens to target distributions with explicit hydrogens. Fine-tuning EQGAT-diff for a couple of iterations further pushes state-of-the-art performance across datasets. We envision that our findings will find applications in structure-based drug design, where the accuracy of generative models for small datasets of complex molecules is critical",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=zNzVhX00h4": {
    "title": "Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape",
    "volume": "review",
    "abstract": "We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parametrization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=DUkYDXqxKp": {
    "title": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model",
    "volume": "review",
    "abstract": "In the past decade, autonomous driving has experienced rapid development in both academia and industry. However, its limited interpretability remains a significant unsolved problem, severely hindering autonomous vehicle commercialization and further development. Previous approaches utilizing small language models have failed to address this issue due to their lack of flexibility, generalization ability, and robustness. Recently, multimodal large language models (LLMs) have gained considerable attention from the research community for their capability to process and reason non-text data (e.g., images and videos) by text. In this paper, we present DriveGPT4, an interpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is capable of interpreting vehicle actions and providing corresponding reasoning, as well as answering diverse questions posed by human users for enhanced interaction. Additionally, DriveGPT4 predicts vehicle low-level control signals in an end-to-end fashion. These capabilities stem from a customized visual instruction tuning dataset specifically designed for autonomous driving. To the best of our knowledge, DriveGPT4 is the first work focusing on interpretable end-to-end autonomous driving. When evaluated on multiple tasks alongside conventional methods and video understanding LLMs, DriveGPT4 demonstrates superior qualitative and quantitative performance. Additionally, DriveGPT4 can be generalized in a zero-shot fashion to accommodate more unseen scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mRw9BuNO9i": {
    "title": "Effortless Cross-Platform Video Codec: A Codebook-Based Method",
    "volume": "review",
    "abstract": "Under certain circumstances, advanced neural video codecs can surpass the most complex traditional codecs in their rate-distortion (RD) performance. One of the main reasons for the high performance of existing neural video codecs is the use of the entropy model, which can provide more accurate probability distribution estimations for compressing the latents. This also implies the rigorous requirement that entropy models running on different platforms should use consistent distribution estimations. However, in cross-platform scenarios, entropy models running on different platforms usually yield inconsistent probability distribution estimations due to floating point computation errors that are platform-dependent, which can cause the decoding side to fail in correctly decoding the compressed bitstream sent by the encoding side. In this paper, we propose a cross-platform video compression framework based on codebooks, which avoids autoregressive entropy modeling and achieves video compression by transmitting the index sequence of the codebooks. Moreover, instead of using optical flow for context alignment, we propose to use the conditional cross-attention module to obtain the context between frames. Due to the absence of autoregressive modeling and optical flow alignment, we can design an extremely minimalist framework that can greatly benefit computational efficiency. Importantly, our framework no longer contains any distribution estimation modules for entropy modeling, and thus computations across platforms are not necessarily consistent. Experimental results show that our method can outperform the traditional H.265 (medium) even without any entropy constraints, while achieving the cross-platform property intrinsically",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RzOm9oOSzm": {
    "title": "Unveiling Linear Mode Connectivity of Re-basin from Neuron Distribution Perspective",
    "volume": "review",
    "abstract": "In deep learning, stochastic gradient descent (SGD) finds many minima that are functionally similar but divergent in parameter space, and connecting the two SGD solutions will depict a loss landscape called linear mode connectivity (LMC), where barriers usually exist. Improving LMC plays an important role in model ensemble, model fusion, and federated learning. Previous works of re-basin map different solutions into the same basin to reduce the barriers in LMC, using permutation symmetry. It is found that the re-basin methods work poorly in early training and emerge to improve LMC after several epochs. Also, the performances of re-basins are usually suboptimal that they can find permutations to reduce the barrier but cannot eliminate it (or the reduction is marginal). However, there is no unified theory on when and why re-basins will improve LMC above chance, and unveiling the behind mechanism is fundamental to improving re-basin approaches and further understanding the loss landscape and training dynamics of deep learning. Therefore, in this paper, we propose a theory from the neuron distribution perspective to demystify the mechanism behind the LMC of re-basin. In our theory, we use Shannon entropy to depict the uniformity of neuron distributions and derive that non-uniformity (entropy decrease) will result in better LMC after re-basin. In accordance with our theory, we present the following observations, all of which can be aptly explained by our theory. i) The LMC of re-basin changes in various non-uniform initializations. ii) The re-basin's LMC improvement emerges after training due to the neuron distribution change. iii) The LMC of re-basin changes when pruning with different pruning ratios. Building upon these findings, we further showcase how to apply our theory to refine the performances of other neuron alignment methods beyond re-basin, e.g., OTFusion and FedMA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Xkf2EBj4w3": {
    "title": "Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data",
    "volume": "review",
    "abstract": "Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV) and natural language processing (NLP), our work builds on prior work showing that the reinforcement learning (RL) itself can be cast as a self-supervised problem: learning to reach any goal without human-specified rewards or labels. Despite the seeming appeal, little (if any) prior work has demonstrated how self-supervised RL methods can be practically deployed on robotic systems. By first studying a challenging simulated version of this task, we discover design decisions about architectures and hyperparameters that increase the success rate by $2 \\times$. These findings lay the groundwork for our main result: we demonstrate that a self-supervised RL algorithm based on contrastive learning can solve real-world, image-based robotic manipulation tasks, with tasks being specified by a single goal image provided after training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=BqHaLnans2": {
    "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
    "volume": "review",
    "abstract": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our LLM-CXR trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=fszrlQ2DuP": {
    "title": "Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?",
    "volume": "review",
    "abstract": "Unsupervised domain adaptation (UDA) involves adapting a model trained on a label-rich source domain to an unlabeled target domain. However, in real-world scenarios, the absence of target-domain labels makes it challenging to evaluate the performance of UDA models. Furthermore, prevailing UDA methods relying on adversarial training and self-training could lead to model degeneration and negative transfer, further exacerbating the evaluation problem. In this paper, we propose a novel metric called the Transfer Score to address these issues. The proposed metric enables the unsupervised evaluation of UDA models by assessing the spatial uniformity of the classifier via model parameters, as well as the transferability and discriminability of deep representations. Based on the metric, we achieve three novel objectives without target-domain labels: (1) selecting the best UDA method from a range of available options, (2) optimizing hyperparameters of UDA models to prevent model degeneration, and (3) identifying which checkpoint of UDA model performs optimally. Our work bridges the gap between data-level UDA research and practical UDA scenarios, enabling a realistic assessment of UDA model performance. We validate the effectiveness of our metric through extensive empirical studies on UDA datasets of different scales and imbalanced distributions. The results demonstrate that our metric robustly achieves the aforementioned goals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=AMCaG2TAeg": {
    "title": "Causal Influence-Aware Counterfactual Data Augmentation",
    "volume": "review",
    "abstract": "Pre-recorded data and human-collected demonstrations are both valuable and practical resources for teaching robots complex behaviors. Ideally, learning agents should not be constrained by the scarcity of available demonstrations, but rather generalize to as many new situations as possible. However, the combinatorial nature of real-world scenarios typically requires a huge amount of data to prevent neural network policies from picking up on spurious and non-causal factors. We propose CAIAC, a data augmentation method that can create feasible synthetic samples from a fixed dataset without the need to perform new environment interactions. Motivated by the fact that an agent may only modify the environment through its actions, we swap causally $\\textit{action}$-unaffected parts of the state-space from different observed trajectories in the dataset. In high-dimensional benchmark environments, we observe an increase in generalization capabilities and sample efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=XT2yAa6Bbp": {
    "title": "Sinkhorn Output Perturbations: Structured Pseudo-Label Noise in Semi-Supervised Segmentation",
    "volume": "review",
    "abstract": "In semi-supervised segmentation, the strong-weak augmentation scheme has gained significant traction. Typically, a teacher model predicts a pseudo-label or consistency target from a weakly augmented image, while the student is tasked with matching the prediction when given a strong augmentation. However, this approach, popularized in self-supervised learning, is constrained by the model's current state. Even though the approach has led to state-of-the-art improvements as part of various algorithms, the inherent limitation, being confined to what the teacher model can predict, remains. In Sinkhorn Output Perturbations, we introduce an algorithm that adds structured pseudo-label noise to the training, extending the strong-weak scheme to perturbations of the output beyond just input and feature perturbations. Our strategy softens the inherent limitations of the student-teacher methodologies by constructing noisy yet plausible pseudo-labels. Sinkhorn Output Perturbations impose no specific architectural requirements and can be integrated into any segmentation model and combined with other semi-supervised strategies. Our method achieves state-of-the-art results on Cityscapes and presents competitive performance on Pascal VOC 2012, further improved upon combining our with another recent algorithm. The experiments also show the efficacy of the reallocation algorithm and provide further empirical insights into pseudo-label noise in semi-supervised segmentation. Code is available at:",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bcHty5VvkQ": {
    "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference",
    "volume": "review",
    "abstract": "Autoregressive large language models (LLMs) have made remarkable progress in various natural language generation tasks. However, the high computing cost and latency resulting from token-by-token generation impede their widespread adoption. To address this issue, several approaches have been proposed that reduce computational cost using early-exit strategies. These strategies enable faster text generation using reduced computation without applying the full computation graph to each token. While existing token-level early exit methods show promising results for online inference – they cannot be readily applied for batch inferencing and Key-Value caching. This is because they have to wait until the last token in a batch exits before they can stop computing. This severely limits the practical application of such techniques. In this paper, we propose a simple and effective token-level early exit method, SkipDecode, designed to work seamlessly with batch inferencing and KV caching. It overcomes prior constraints by setting up a singular exit point for every token in a batch at a each sequence position. It also guarantees a monotonic decrease in exit points, thereby eliminating the need to recompute KV Caches for preceding tokens. Rather than terminating computation prematurely as in prior works, our approach bypasses lower to middle layers, devoting most of the computational resources to upper layers, allowing later tokens to benefit from the compute expenditure by earlier tokens. Our experimental results show that SkipDecode can obtain 2x to 5x inference speedups with negligible regression across a variety of tasks. This is achieved using OPT models of 1.3 billion and 6.7 billion parameters, all the while being directly compatible with batching and KV caching optimization techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=49ZYkhEGmv": {
    "title": "Scalabale AI Safety via Doubly-Efficient Debate",
    "volume": "review",
    "abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety, as tasks can become too complicated for humans to judge directly. Irving et al. (2018) proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=fyTPWfXtcc": {
    "title": "Global Optimality for Non-linear Constrained Restoration Problems via Invexity",
    "volume": "review",
    "abstract": "Signal restoration is an important constrained optimization problem with significant applications in various domains. Although non-convex constrained optimization problems have been shown to perform better than convex counterparts in terms of reconstruction quality, convex constrained optimization problems have been preferably for its global optima guarantees. Despite the success of non-convex methods in a large number of applications, it is not an overstatement to say that there is little or no hope for non-convex problems to ensure global optima. In this paper, for the first time, we develop invex constrained optimization theory to mitigate the loss of guarantees for global optima in non-convex constrained inverse problems, where the invex function is a mapping where any critical point is a global minimizer. We also develop relevant theories to extend the global optima guarantee to a set of quasi-invex functions - the largest optimizable mappings. More specifically, we propose a family of invex/quasi-invex of functions for handling constrained inverse problems using the non-convex setting along with guarantees for their global optima. Our experimental evaluation shows that the proposed approach is very promising and can aid in extending existing convex optimization algorithms, such as the alternating direction method of multipliers, and accelerated proximal gradient methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=MiMxv6ijvC": {
    "title": "CARENET : A NOVEL ARCHITECTURE FOR LOW DATA REGIME MIXING CONVOLUTIONS AND ATTENTION",
    "volume": "review",
    "abstract": "In the rapidly evolving landscape of deep learning for computer vision, var- ious architectures have been proposed to achieve state-of-the-art performance in tasks such as object recognition, image segmentation, and classification. While pretrained models on large datasets like ImageNet have been the corner- stone for transfer learning in many applications, this paper introduces CAReNet (Convolutional Attention Residual Network), a novel architecture that was trained from scratch, in the absence of available pretrained weights. CAReNet incorpo- rates a unique blend of convolutional layers, attention mechanisms, and residual connections to offer a holistic approach to feature extraction and representation learning. Notably, CAReNet closely follows the performance of ResNet50 on the same training set while utilizing fewer parameters. Training CAReNet from scratch proved to be necessary, particularly due to architectural differences that render feature representations incompatible with those from pretrained models. Furthermore, we highlight that training new models on large, general-purpose databases to obtain pretrained weights requires time, accurate labels, and pow- erful machines, which causes significant barriers in many domains. Therefore, the absence of pretrained weights for CAReNet is not only a constraint but also an op- portunity for architecture-specific optimization. We also emphasize that in certain domains, such as space and medical fields, the features learned from ImageNet images are vastly different and can introduce bias during training, given the gap that exists between the domains of pretraining and the task of transfer learning. This work focuses on the importance of architecture-specific training strategies for optimizing performance and also demonstrates the efficacy of CAReNet in achieving competitive results with a more compact model architecture. Experi- ments were carried out on several benchmark datasets, including Tiny ImageNet, for image classification tasks. Signifying a groundbreaking stride in efficiency and performance, CAReNet not only outpaces ResNet50 by achieving a lead of 2.61% on Tiny-Imagenet and 1.9% on STL10, but it does so with a model that's nearly half the size of ResNet50. This impressive balance between compactness and elevated accuracy highlights the prowess of CAReNet in the realm of deep learning architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=U0c2IaQhHk": {
    "title": "Exploring the State and Action Space in Reinforcement Learning with Infinite-Dimensional Confidence Balls",
    "volume": "review",
    "abstract": "Reinforcement Learning (RL) is a powerful tool for solving complex decision-making problems. However, existing RL approaches suffer from the curse of dimensionality when dealing with large or continuous state and action spaces. This paper introduces a non-parametric online RL algorithm called RKHS-RL that overcomes these challenges by utilizing reproducing kernels and the RKHS-embedding assumption. The proposed algorithm can handle both finite and infinite state and action spaces, as well as nonlinear relationships in transition probabilities. The RKHS-RL algorithm estimates the transition core using ridge regression and balances exploration and exploitation through infinite-dimensional confidence balls. The paper provides theoretical guarantees, demonstrating that RKHS-RL achieves a sublinear regret bound of $\\tilde{\\mathcal{O}}(H\\sqrt{T})$, where $T$ denotes the time step of the algorithm and $H$ represents the horizon of the Markov Decision Process (MDP), making it an effective approach for RL problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=DHCp41nv1M": {
    "title": "Seeing Video Through Optical Scattering Media using Spatio-Temporal Diffusion Models",
    "volume": "review",
    "abstract": "Optical scattering causes light rays to deviate from their trajectory, posing challenges for imaging through scattering media such as fog and biological tissues. Although diffusion models have been extensively studied for various inverse problems in recent years, its extension to video recovery, especially through highly scattering media, has been an open problem due to the lack of a closed-form forward model and the difficulty of exploiting the spatio-temporal correlation. To address this, here we present a novel inverse scattering solver using a video diffusion model. In particular, by deriving a closed-form forward model from the shower-curtain effect in a dynamic scattering medium, we develop a video diffusion posterior sampling scheme using a diffusion model with temporal attention that maximally exploits the statistical correlation between a series of frames and a series of scattered signals. Unlike previous end-to-end approaches only relied on spatial correlation between a scene and a scattered signal at a specific time point, the adaptability of the proposed method is highly extendable to various types of scenes, various thicknesses of scattering media, and varying distances between a target scene and a medium. In particular, the use of temporal correlation is shown to be critical to faithfully retrieve high-frequency components which are often missed by inverse operations only in spatial domain. Experimental results using the video datasets of moving sperm cells verify the effectiveness of the proposed method. To the best of our knowledge, this is the first video diffusion model to jointly utilize the correlations in both spatial and temporal domains in solving the inverse scattering problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=TeeyHEi25C": {
    "title": "Value function estimation using conditional diffusion models for control",
    "volume": "review",
    "abstract": "A fairly reliable trend in deep reinforcement learning is that the performance scales with the number of parameters, provided a complimentary scaling in amount of training data. As the appetite for large models increases, it is imperative to address, sooner than later, the potential problem of running out of high-quality demonstrations. In this case, instead of collecting only new data via costly human demonstrations or risking a simulation-to-real transfer with uncertain effects, it would be beneficial to leverage vast amounts of readily-available low-quality data. Since classical control algorithms such as behavior cloning or temporal difference learning cannot be used on reward-free or action-free data out-of-the-box, this solution warrants novel training paradigms for continuous control. We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model. This model can be efficiently learned from state sequences (i.e., without access to reward functions nor actions), and subsequently used to estimate the value of each action out-of-the-box. We show how DVF can be used to efficiently capture the state visitation measure for multiple controllers, and show promising qualitative and quantitative results on challenging robotics benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jYHRP6nj9Q": {
    "title": "CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model",
    "volume": "review",
    "abstract": "The social graphs synthesized by the generative models are increasingly in demand due to data scarcity and concerns over user privacy. One of the key performance criteria for generating social networks is the fidelity to specified conditionals, such as users with certain membership and financial status. While recent diffusion models have shown remarkable performance in generating images, their effectiveness in synthesizing graphs has not yet been explored in the context of conditional social graphs. In this paper, we propose the first kind of conditional diffusion model for social networks, CDGraph, which trains and synthesizes graphs based on two specified conditions. We propose the co-evolution dependency in the denoising process of CDGraph to capture the mutual dependencies between the dual conditions and further incorporate social homophily and social contagion to preserve the connectivity between nodes while satisfying the specified conditions. Moreover, we introduce a novel classifier loss, which guides the training of the diffusion process through the mutual dependency of dual conditions. We evaluate CDGraph against four existing graph generative methods, i.e., SPECTRE, GSM, EDGE, and DiGress, on four datasets. Our results show that the generated graphs from CDGraph achieve much higher dual-conditional validity and lower discrepancy in various social network metrics than the baselines, thus demonstrating its proficiency in generating dual-conditional social graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OGtnhKQJms": {
    "title": "Multi-View Causal Representation Learning with Partial Observability",
    "volume": "review",
    "abstract": "We present a unified framework for studying the identifiability of representations learned from simultaneously observed views, such as different data modalities. We allow a partially observed setting in which each view constitutes a nonlinear mixture of a subset of underlying latent variables, which can be causally related. We prove that the information shared across all subsets of any number of views can be learned up to a smooth bijection using contrastive learning and a single encoder per view. We also provide graphical criteria indicating which latent variables can be identified through a simple set of rules, which we refer to as identifiability algebra. Our general framework and theoretical results unify and extend several previous work on multi-view nonlinear ICA, disentanglement, and causal representation learning. We experimentally validate our claims on numerical, image, and multi-modal data sets. Further, we demonstrate that the performance of prior methods is recovered in different special cases of our setup. Overall, we find that access to multiple partial views offers unique opportunities for identifiable representation learning, enabling the discovery of latent structures from purely observational data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqovbCMrOp": {
    "title": "On the Recoverability of Causal Relations from Temporally Aggregated I.I.D Data",
    "volume": "review",
    "abstract": "Real-world data in fields such as economics, finance and neuroscience often exhibit a lower resolution compared to the underlying causal process, with temporally aggregated data being a common example. While the impact of temporally aggregated time series on temporal causal discovery has received attention, the effects of highly aggregated data, which yield independent and identically distributed (i.i.d.) observations, on instantaneous (non-temporal) causal discovery have been largely overlooked by the research community. There is substantial evidence suggesting that temporally aggregated i.i.d. data are prevalent in reality. This prevalence arises because the time required for causal interactions is often considerably shorter than the observational interval, leading to a large aggregation factor and subsequently rendering the temporally aggregated data i.i.d. The critical question arises: are causal discovery results obtained from such data consistent with the true causal process? In this paper, we provide theoretical conditions necessary to ensure the consistency of causal discovery results when analyzing temporally aggregated i.i.d. data. Through a combination of theoretical analysis and experimental validation, we demonstrate that conducting causal discovery on such data often leads to erroneous results. Our primary objective is to bring attention to the risks associated with performing causal discovery on highly aggregated i.i.d. data and advocate for a cautious and meticulous approach when interpreting causal discovery outcomes derived from such data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=anek0q7QPL": {
    "title": "Exploring the Combined Power of Covariance and Hessian Matrices Eigenanalysis for Binary Classification",
    "volume": "review",
    "abstract": "Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our method outperforms traditional methods. Our method stands out by addressing both LDA criteria, unlike PCA and the Hessian method, which predominantly emphasize one criterion each. This comprehensive approach captures intricate patterns and relationships, enhancing classification performance. Furthermore, through the utilization of both LDA criteria, our method outperforms LDA itself by leveraging higher-dimensional feature spaces, in accordance with Cover's theorem, which favors linear separability in higher dimensions. Our approach sheds light on complex DNN decision-making, rendering them comprehensible within a 2D space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=DkYQHewNcp": {
    "title": "Unsupervised Detection of Recurrent Patterns in Neural Recordings with Constrained Filters",
    "volume": "review",
    "abstract": "Structured spontaneous neural activity, characterized by the expression of repetitive patterns, is crucial for memory, learning and spatial navigation. However, investigating the functional role of these patterns has been challenging due to a lack of scalable methods for detecting them in large-scale recordings. To address this challenge, we propose an unsupervised approach that utilizes backpropagation to optimize the parameters of a predefined number of spatiotemporal filters, which serve as pattern detectors. We demonstrate the scalability and efficiency of our approach for detecting place cell sequences in biologically plausible synthetic and real datasets obtained from the mouse hippocampus. Our speed benchmarks demonstrate that our method significantly outperforms prior art, enabling the study of spontaneous activity in larger recordings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=iriEqxFB4y": {
    "title": "DOS: Diverse Outlier Sampling for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "Modern neural networks are known to give overconfident predictions for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing the sampling strategy for outlier datasets. However, the OOD samples selected solely based on predictive uncertainty can be biased towards certain types, which may fail to capture the full outlier distribution. In this work, we empirically show that diversity is critical in sampling outliers for OOD detection performance. Motivated by the observation, we propose a straightforward and novel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse and informative outliers. Specifically, we cluster the normalized features at each iteration, and the most informative outlier from each cluster is selected for model training with absent category loss. With DOS, the sampled outliers efficiently shape a globally compact decision boundary between ID and OOD data. Extensive experiments demonstrate the superiority of DOS, reducing the average FPR95 by up to 25.79% on CIFAR-100 with TI-300K",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7n360rsYAq": {
    "title": "Towards Dynamic Trend Filtering through Trend Points Detection with Reinforcement Learning",
    "volume": "review",
    "abstract": "Trend filtering simplifies complex time series data by prioritizing proximity to the original data while applying smoothness to filter out noise. However, the inherent smoothness of trend filtering filters out the tail distribution of time series data, characterized as extreme values, thereby failing to reflect abrupt changes in the trend. In this paper, we introduce Trend Point Detection, a novel approach to trend filtering that directly identifies essential points that should be reflected in the trend including abrupt changes. We refer to these essential points as Dynamic Trend Points (DTPs) and extract trends from connecting these points. To identify DTPs, we formalize the Trend Point Detection problem as a Markov Decision Process (MDP). We solve the Trend Point Detection problem using Reinforcement Learning (RL) algorithms operating within a discrete action space, referred to as the Dynamic Trend Filtering network (DTF-net). DTF-net incorporates flexible noise filtering, preserving important original sub-sequences while removing noise as needed for other sub-sequences. We demonstrate that DTF-net excels at capturing abrupt changes compared to other trend filtering algorithms, using synthetic data and the Nasdaq intraday dataset. Furthermore, when we utilize DTF-net's trend as an additional feature for Time Series Forecasting (TSF) in non-stationary data, we demonstrate performance improvements, as abrupt changes are captured rather than smoothed out",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ws0F5NTzGw": {
    "title": "AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler",
    "volume": "review",
    "abstract": "In real-world applications, tabular data often suffer from distribution shifts due to their widespread and abundant nature, leading to a significant impact on the performance of machine learning models during testing. However, addressing these shifts in the tabular domain has been relatively underexplored due to unique challenges such as varying attributes and dataset sizes, as well as limitations representation learning capabilities of deep learning models for tabular data. Particularly, with the recent promising paradigm of test-time adaptation (TTA), where we adapt the off-the-shelf model to the unlabeled target domain during the inference phase without accessing the source domain, we observe that directly adopting commonly used TTA methods from other domains often leads to model collapse. We systematically explore challenges in tabular data test-time adaptation, including skewed entropy, complex latent space decision boundaries, confidence calibration issues with both overconfident and under-confident, and model bias towards source label distributions along with class imbalances. Based on these insights, we introduce AdapTable, a novel tabular test-time adaptation method that directly modifies output probabilities by estimating target label distributions and adjusting initial probabilities based on calibrated uncertainty. Extensive experiments on both real-world distribution shifts and synthetic corruptions demonstrate the adaptation efficacy of the proposed method using unlabeled test data alone",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=cTOL99p5HL": {
    "title": "Ghost in the Minecraft: Hierarchical Agents for Minecraft via Large Language Models with Text-based Knowledge and Memory",
    "volume": "review",
    "abstract": "As modern computer games continue to evolve, there is a growing need for adaptive agents that can effectively navigate, make decisions, and interact within vast, ever-changing worlds. While recently developed agents based on Large Language Models (LLMs) show promise in adaptability for controlled text environments, expansive and dynamic open worlds like Minecraft still pose challenges for their performance. To address this, we introduce Ghost in the Minecraft (GITM), a novel hierarchical agent that integrates LLMs with text-based knowledge and memory. Structured actions are constructed to enable LLMs to interact in Minecraft using textual descriptions, bridging the gap between desired agent behaviors and LLM limitations. The hierarchical agent then decomposes goals into sub-goals, actions, and operations by leveraging text knowledge and memory. A text-based in-context learning method is also designed to enhance future planning. GITM demonstrates the potential of LLMs in Minecraft's evolving open world. Notable milestones are collecting 99.2\\% of items and a 55\\% success rate on the popular ``ObtainDiamond'' task. GITM also shows impressive learning efficiency, requiring minimal computational resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nddtu94uX": {
    "title": "PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator",
    "volume": "review",
    "abstract": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, due to challenges in gathering conversations involving human participation, current endeavors like Baize and UltraChat aim to automatically generate conversational data. They primarily rely on ChatGPT conducting roleplay to simulate human behaviors based on instructions rather than genuine learning from humans, resulting in limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator called Socratic to produce a high-quality human-centric synthetic conversation dataset. Subsequently, this dataset was used to train our assistant model, named PlatoLM. PlatoLM achieves the SOTA performance among 7B models (including LLaMA-2-7B-chat and Vicuna-7B) in both Vicuna-Bench and pairwise comparison in MT-Bench; the effectiveness of PlatoLM is also evidenced by manual evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=r6NMqADLGQ": {
    "title": "How To Train Your Covariance",
    "volume": "review",
    "abstract": "We study the problem of _unsupervised heteroscedastic covariance estimation_, where the goal is to learn the multivariate target distribution $\\mathcal{N}(y, \\Sigma_y | x )$ given an observation $x$. This problem is particularly challenging as $\\Sigma_{y}$ varies for different samples (heteroscedastic) and no annotation for the covariance is available (unsupervised). Typically, state-of-the-art methods predict the mean $f(x ; \\theta)$ and covariance $Cov(f(x); \\Theta)$ of the target distribution through two neural networks trained using the negative log-likelihood. This raises two questions: (1) Does the predicted covariance truly capture the randomness of the predicted mean? (2) In the absence of ground-truth annotation, how can we quantify the performance of covariance estimation? We address (1) by developing the __Spatial Variance__, a formulation of $Cov(f(x); \\Theta)$ that captures the randomness in $ f(x ; \\theta)$ by incorporating its curvature around $x$. Furthermore, we tackle (2) by introducing the _Conditional Mean Absolute Error (C-MAE)_, a metric which leverages well-known properties of the normal distribution. We verify the effectiveness of our approach through multiple experiments spanning synthetic (univariate, multivariate) and real-world datasets (UCI Regression, LSP, and MPII Human Pose Estimation). Our experiments provide evidence that our approach outperforms the state of the art across these datasets and multiple network architectures, and accurately learns the relation underlying the target random variables",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=MY0qlcFcUg": {
    "title": "Denoising Task Routing for Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timesteps. (2) Task Weights: During the early stages (higher timesteps) of the denoising process, DTR assigns a greater number of task-specific channels, leveraging the insight that diffusion models prioritize reconstructing global structure and perceptually rich contents in earlier stages, and focus on simple noise removal in later stages. Our experiments demonstrate that DTR consistently enhances the performance of diffusion models across various evaluation protocols, all without introducing additional parameters. Furthermore, DTR contributes to accelerating convergence during training. Finally, we show the complementarity between our architectural approach and existing MTL optimization techniques, providing a more complete view of MTL within the context of diffusion training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=48CXLrx7K3": {
    "title": "Revealing Unintentional Information Leakage in Low-Dimensional Facial Portrait Representations",
    "volume": "review",
    "abstract": "We evaluate the information that can unintentionally leak into the low dimensional output of a neural network, by reconstructing an input image from a 40- or 32-element feature vector that intends to only describe abstract attributes of a facial portrait. The reconstruction uses blackbox-access to the image encoder which generates the feature vector. Other than previous work, we leverage recent knowledge about image generation and facial similarity, implementing a method that outperforms the current state-of-the-art. Our strategy uses a pretrained StyleGAN and a new loss function that compares the perceptual similarity of portraits by mapping them into the latent space of a FaceNet embedding. Additionally, we present a new technique that fuses the output of an ensemble, to deliberately generate specific aspects of the recreated image",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=SQrHpTllXa": {
    "title": "CABINET: Content Relevance-based Noise Reduction for Table Question Answering",
    "volume": "review",
    "abstract": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) – a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJDlRzQh7x": {
    "title": "Neural Networks and Solomonoff Induction",
    "volume": "review",
    "abstract": "Solomonoff Induction (SI) is the most powerful universal predictor given unlimited computational resources. Naive SI approximations are challenging and require running vast amount of programs for extremely long. Here we explore an alternative path to SI consisting in meta-training neural networks on universal data sources. We generate the training data by feeding random programs to Universal Turing Machines (UTMs) and guarantee convergence in the limit to various SI variants (under simplifying assumptions). We provide novel results on how a non-uniform distribution over programs still maintain the universality property. Experimentally, we investigate the effect neural network architectures (i.e. LSTMs, Transformers, etc.) and sizes on their performance on algorithmic data, crucial for SI. First, we consider variable-order Markov sources where the Bayes-optimal predictor is the well-known Context Tree Weighting (CTW) algorithm. Second, we evaluate on challenging algorithmic tasks on Chomsky hierarchy that require different memory structures. Finally, we test on the UTM domain following our theoretical results. We show that scaling network size always improves performance on all tasks, Transformers outperforming all others, even achieving optimality on par with CTW. Promisingly, large Transformers and LSTMs trained on UTM data exhibit transfer to the other domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=dcjtMYkpXx": {
    "title": "Reward Model Ensembles Help Mitigate Overoptimization",
    "volume": "review",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the \"true\" reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger \"gold\" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TyFrPOKYXw": {
    "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
    "volume": "review",
    "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowd workers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations. Warning: This paper contains example data that may be offensive or harmful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=S4zpk61r6G": {
    "title": "DiffMaSIF: Score-Based Diffusion Models for Protein Surfaces",
    "volume": "review",
    "abstract": "Predicting protein-protein complexes is one of the central challenges of computational structural biology. Inspired by recent generative machine learning (ML) techniques which have shown promise in the realms of protein docking, we introduce DiffMaSIF, a novel score-based diffusion model for rigid protein-protein docking. While existing methods rely on co-evolution learned on a residue level, this information is not sufficient for transient, weakly evolved, or newly designed interfaces. DiffMaSIF's efficacy hinges on its surface-based molecular representation which can capture the complementarity inherent in the physical surfaces of interacting protein interfaces. We follow an end-to-end two-tier prediction schema: initially identifying contact sites on the protein surface and constraining each molecular graph to these sites, followed by an equivariant network to position the two proteins. This data reduction step enables the use of more sophisticated networks and more training steps. In addition to developing this model, we introduce new dataset splits accounting for structural leakage at the interface and thus tailored for benchmarking protein-protein interface prediction performance. Our results demonstrate that DiffMaSIF not only outperforms contemporary ML methods in rigid protein docking, but also matches traditional docking tools at considerably fewer numbers of generated decoys. Through DiffMaSIF, we pave the way for surface-centric interface prediction methods, thus advancing accurate prediction of protein interactions across a wide spectrum of difficult and novelty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=KTL534o7Ot": {
    "title": "Programmable Synthetic Data Generation",
    "volume": "review",
    "abstract": "Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While generating synthetic data resembling the original distribution addresses some of these issues, most applications would benefit from additional customization on the generated data. However, existing synthetic data generation approaches are limited to particular constraints, e.g., differential privacy (DP) or fairness. In this work, we introduce ProgSyn, the first programmable and flexible synthetic tabular data generation framework. Customization is achieved via programmatically declared statistical and logical expressions, supporting a wide range of requirements (e.g., DP or fairness, among others). To ensure high synthetic data quality in the presence of custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications using novel relaxations. We conduct an extensive experimental evaluation of ProgSyn over four datasets and on numerous custom specifications, where we outperform state-of-the-art specialized approaches on several tasks, while being more general. For instance, at the same fairness level we achieve 2.3% higher downstream accuracy than the state-of-the-art in fair synthetic data generation on the Adult dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=HKGQDDTuvZ": {
    "title": "Frequency-Aware Transformer for Learned Image Compression",
    "volume": "review",
    "abstract": "Learned image compression (LIC) has gained traction as an effective solution for image storage and transmission in recent years. However, existing LIC methods are redundant in latent representation due to limitations in capturing anisotropic frequency components and preserving directional details. To overcome these challenges, we propose a novel frequency-aware transformer (FAT) block that for the first time achieves multiscale directional ananlysis for LIC. The FAT block comprises frequency-decomposition window attention (FDWA) modules to capture multiscale and directional frequency components of natural images. Additionally, we introduce frequency-modulation feed-forward network (FMFFN) to adaptively modulate different frequency components, improving rate-distortion performance. Furthermore, we present a transformer-based channel-wise autoregressive (T-CA) model that effectively exploits channel dependencies. Experiments show that our method achieves state-of-the-art rate-distortion performance compared to existing LIC methods, and evidently outperforms latest standardized codec VTM-12.1 by 14.5\\%, 15.1\\%, 13.0\\% in BD-rate on the Kodak, Tecnick, and CLIC datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMFSUjxMIl": {
    "title": "CircuitNet 2.0: An Advanced Dataset for Promoting Machine Learning Innovations in Realistic Chip Design Environment",
    "volume": "review",
    "abstract": "Integrated circuits or chips are key to enable computing in modern industry. Designing a chip relies on human experts to produce chip data through professional electronic design automation (EDA) software and complicated procedures. Nowadays, prompted by the wide variety of machine learning (ML) datasets, we have witnessed great advancement of ML algorithms in computer vision, natural language processing, and other fields. However, in chip design, high human workload and data sensitivity cause the lack of public datasets, which hinders the progress of ML development for EDA. To this end, we introduce an advanced large-scale dataset, CircuitNet 2.0, which targets promoting ML innovations in a realistic chip design environment. In order to approach the realistic chip design space, we collect more than 10,000 samples with a variety of chip designs (e.g., CPU, GPU, and AI Chip). All the designs are conducted through complete commercial design flows in a widely-used technology node, 14nm FinFET. We collect comprehensive data, including routability, timing, and power, from the design flow to support versatile ML tasks in EDA. Besides, we also introduce some realistic ML tasks with CircuitNet 2.0 to verify the potential for boosting innovations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=gEdg9JvO8X": {
    "title": "BDQL: Offline RL via Behavior Diffusion Q-learning without Policy Constraint",
    "volume": "review",
    "abstract": "Offline reinforcement learning (RL) algorithms often constrain the policy or regularize the value function within an off-policy actor-critic framework to overcome the overestimation on out-of-distribution (OOD) actions. And the on-policy style offline algorithms also cannot escape from these constraints (or regularization). In this paper, we propose an on-policy style algorithm, Behavior Diffusion Q-Learning (BDQL), which has the potential to solve offline RL without introducing any potential constraints. BDQL first recovers the behavior policy through the diffusion model and then updates this diffusion-based behavior policy using the behavior Q-function learned by SARSA. The update of BDQL exhibits a special two-stage pattern. At the beginning of the training, thanks to the precise modeling of the diffusion model, the on-policy guidance of the behavior Q-function over the behavior policy is effective enough to solve the offline RL. As training processes, BDQL suffers from the OOD issue, causing the training fluctuation or even collapse. Consequently, OOD issue arises after BDQL solves the offline problem which means the policy constraint is not necessary for solving offline RL in BDQL. Although the policy constraint can overcome the OOD issue and then completely address the training fluctuation, it also has a negative impact on solving the offline problem in the first stage. Therefore, we introduce the stochastic weight averaging (SWA) to mitigate the training fluctuation without affecting the offline solution. Experiments on D4RL demonstrate the special two-stage training phenomenon, where the first stage does have the capability to solve offline RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=OHll7EfuSi": {
    "title": "Weight-Based Performance Estimation for Diverse Domains",
    "volume": "review",
    "abstract": "One of the limitations of applying machine learning methods in real-world scenarios is the existence of a domain shift between the source (i.e., training) and target (i.e., test) datasets, which typically entails a significant performance drop. This is further complicated by the lack of annotated data in the target domain, making it impossible to quantitatively assess the model performance. As such, there is a pressing need for methods able to estimate a model's performance on unlabeled target data. Most of the existing approaches addressing this train a linear performance predictor, taking as input either an activation-based or a performance-based metric. As we will show, however, the accuracy of such predictors strongly depends on the domain shift. By contrast, we propose to use a weight-based metric as input to the linear predictor. Specifically, we measure the difference between the model's weights before and after fine-tuning it on a self-supervised loss, which we take to be the entropy of the network's predictions. This builds on the intuition that target data close to the source domain will produce more confident predictions, thus leading to small weight changes during fine-tuning. Our extensive experiments on standard object recognition benchmarks, using diverse network architectures, demonstrate the benefits of our method, outperforming both activation-based and performance-based baselines by a large margin. Our code is available in an anonymous repository: https://anonymous.4open.science/r/79E9/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=PdTe8S0Mkl": {
    "title": "Humans vs ChatGPT: Uncovering the Non-trivial Distinctions by Evaluating Parallel Responses",
    "volume": "review",
    "abstract": "The advent of ChatGPT and similar Large Language Models has set the world in an uproar as it is able to generate human-like natural language. Due to the high similarity between the human text and ChatGPT text, it begs the question if the two are truly indistinguishable. In this study, the human-generated content is compared to ChatGPT-3.5, ChatGPT-4, and Davinci-3 using the same technical questions as found on StackOverflow and general questions found on Yahoo Answers. We leveraged Roget's thesaurus to uncover thematic similarities and differences between the human corpora and GPT corpora. We performed a chi-square test on Roget's 1034 categories and found a significant difference in the appearance of words for 365 of them. To uncover the differences in the neighborhoods of the word embedding we utilized the MIT Embedding Comparator to distinguish GloVe base vectors with respect to its trained version on human and ChatGPT corpora. Pre-trained BERT and Sentence-BERT were used to measure the semantic similarity in the answers (on the same questions) given by humans and ChatGPT, which came out highly similar. While that might indicate difficulty in distinguishing ChatGPT and human text, the significant differences in the appearance of words suggested a move towards classification using machine learning models. We observed that various machine learning models performed very well. In summary, we discern disparities and parallels that can be attributed to conceptual, contextual, or lexicographic factors. We endeavor to establish connections between each methodology and these respective categories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyQO9RPhwN": {
    "title": "Geometry-Guided Conditional Adaption for Surrogate Models of Large-Scale 3D PDEs on Arbitrary Geometries",
    "volume": "review",
    "abstract": "Deep learning surrogate models aim to accelerate the solving of partial differential equations (PDEs) and have achieved certain promising results. Although several main-stream models through neural operator learning have been applied to delve into PDEs on varying geometries, they were designed to map the complex geometry to a latent uniform grid, which is still challenging to learn by the networks with general architectures. In this work, we rethink the critical factors of PDE solutions and propose a novel model-agnostic framework, called 3D Geometry-Guided Conditional Adaption (3D-GeoCA), for solving PDEs on arbitrary 3D geometries. Starting with a 3D point cloud geometry encoder, 3D-GeoCA can extract the essential and robust representations of any kind of geometric shapes, which is regarded as a conditioning key to guiding the adaption of hidden features in the surrogate model. We conduct experiments on the public Shape-Net Car computational fluid dynamics dataset using several surrogate models as the backbones with various point cloud geometry encoders to simulate corresponding large-scale Reynolds Average Navier-Stokes equations. Equipped with 3D-GeoCA, these backbone models can reduce their L-2 errors by a large margin. Moreover, this 3D-GeoCA is model-agnostic so that it can be applied to any surrogate model. Our experimental results further show that its overall performance is positively correlated to the power of the applied backbone model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=wprSv7ichW": {
    "title": "Benchmarking Algorithms for Federated Domain Generalization",
    "volume": "review",
    "abstract": "While prior federated learning (FL) methods mainly consider client heterogeneity, we focus on the *Federated Domain Generalization (DG)* task, which introduces train-test heterogeneity in the FL context. Existing evaluations in this field are limited in terms of the scale of the clients and dataset diversity. Thus, we propose a Federated DG benchmark that aim to test the limits of current methods with high client heterogeneity, large numbers of clients, and diverse datasets. Towards this objective, we introduce a novel data partitioning method that allows us to distribute any domain dataset among few or many clients while controlling client heterogeneity. We then introduce and apply our methodology to evaluate $13$ Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG on $7$ datasets. Our results suggest that, despite some progress, significant performance gaps remain in Federated DG, especially when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets. Furthermore, our extendable benchmark code will be publicly released to aid in benchmarking future Federated DG approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=T23HYw6lta": {
    "title": "Forget-Me-Not: Making Backdoor Hard to be Forgotten in Fine-tuning",
    "volume": "review",
    "abstract": "Backdoor attacks are training time attacks that fool deep neural networks (DNNs) into misclassifying inputs containing a specific trigger, thus representing serious security risks. However, due to catastrophic forgetting, the backdoor inside the poisoned models can be gradually removed under advanced finetuning methods. It reduces the practicality of backdoor attacks since the pretrained models often undergo extra finetuning instead of being used as is, and the attacks gradually lose their robustness given various finetuning-based backdoor defenses. Particularly, recent work reveals that finetuning with a cyclical learning rate scheme can effectively mitigate almost all backdoor attacks. In this paper, we propose a new mechanism for developing backdoor models that significantly strengthens the durability of the generated backdoor. The key idea in this design is to coach the backdoor to become more robust by exposing it to a wider range of learning rates and clean-data-only training epochs. The backdoor models developed with our mechanism can bypass finetuning-based defenses and maintain the backdoor effect even under long and sophisticated finetuning processes. In addition, the backdoor in our backdoored models can persist even if the whole model is finetuned end-to-end with another task, causing a notable accuracy drop when the trigger is present. We demonstrate the effectiveness of our technique through empirical evaluation with various backdoor triggers on three popular benchmarks, including CIFAR-10, CelebA, and ImageNet-10",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=q9jQPA6zPK": {
    "title": "Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design",
    "volume": "review",
    "abstract": "Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design space, resulting in robots with complicated morphologies that are hard to control. In response, this paper presents a novel coarse-to-fine method for designing multi-cellular robots. Initially, this strategy seeks optimal coarse-grained robots and progressively refines them. To mitigate the challenge of determining the precise refinement juncture during the coarse-to-fine transition, we introduce the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies robots of various granularity within a shared hyperbolic space and leverages a refined Cross-Entropy Method for optimization. This framework enables our method to autonomously identify areas of exploration in hyperbolic space and concentrate on regions demonstrating promise. Finally, the extensive empirical studies on various challenging tasks sourced from EvoGym show our approach's superior efficiency and generalization capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IzrLkbq1dc": {
    "title": "Analyzing Local Representations of Self-supervised Vision Transformers",
    "volume": "review",
    "abstract": "In this paper, we present a comparative analysis of various self-supervised Vision Transformers (ViTs), focusing on their local representative power. Inspired by large language models, we examine the abilities of ViTs to perform various computer vision tasks with little to no fine-tuning. We design evaluation framework to analyze the quality of local, i.e. patch-level, representations in the context of few-shot semantic segmentation, instance identification, object retrieval and tracking. We discover that contrastive learning based methods like DINO produce more universal patch representations that can be immediately applied for downstream tasks with no parameter tuning, compared to masked image modeling. The embeddings learned using the latter approach, e.g. in masked autoencoders, have high variance features that harm distance-based algorithms, such as k-NN, and do not contain useful information for most downstream tasks. Finally, we find an object instance retrieval setting where DINOv2, a model pretrained on two orders of magnitude more data, performs worse than its less compute intensive counterpart DINO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=veIzQxZUhF": {
    "title": "Deep concept removal",
    "volume": "review",
    "abstract": "We address the problem of concept removal in deep neural networks, aiming to learn representations that do not encode certain specified concepts (e.g., gender etc.) We propose a novel method based on adversarial linear classifiers trained on a concept dataset, which helps to remove the targeted attribute while maintaining model performance. Our approach Deep Concept Removal incorporates adversarial probing classifiers at various layers of the network, effectively addressing concept entanglement and improving out-of-distribution generalization. We also introduce an implicit gradient-based technique to tackle the challenges associated with adversarial training using linear classifiers. We evaluate the ability to remove a concept on a set of popular distributionally robust optimization (DRO) benchmarks with spurious correlations, as well as out-of-distribution (OOD) generalization tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HAMBmtKLc8": {
    "title": "Graph Neural Networks on Symmetric Positive Definite Manifold",
    "volume": "review",
    "abstract": "Geometric deep learning equips graph neural networks (GNNs) with some symmetry aesthetics from its underlying principles, which draw the structural properties of graphs. However, modeling in Euclidean or hyperbolic geometry, or even their combinations, usually hypothesizes that the graph nodes satisfy the preferred geometric properties, which ignores the actual graph structures. This prompted us to consider a more solid expression to relieve the above significant hypothesis for the geometric graph embeddings. In this study, we generalize the fundamental components of GNNs on the Symmetric Positive Definite (SPD) manifold, which could be approximately observed by the integration of Euclidean and non-Euclidean geometric structures. This motivates us to reconstruct the GNNs with manifold-preserving linear transformation, neighborhood aggregation, non-linear activation, and multinomial logistic regression, in which the Log-Cholesky metric derives the closed-form Fréchet mean representation for neighborhood aggregation and computational tractability for learning geometric embeddings. Experiments demonstrate that the SPDGNN can learn superior representations for grid and hierarchical node structures, leading to significant performance improvements in subsequent classifications compared to the Euclidean and Hyperbolic analogs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Dk10QugVHb": {
    "title": "Causal analysis of social bias in CLIP",
    "volume": "review",
    "abstract": "We propose the first experimental study to causally measure bias in social perception in the latent space of multi-modal models. Previous studies compute correlations between a model's social judgments and protected attributes, such as race, age, and gender, using observational wild-collected human-annotated datasets, such as FairFace. In order to establish causal links between protected attributes and algorithmic bias, we use a synthetic dataset of face images instead, CausalFace, where both legally protected attributes and potential confound attributes, such as facial expression, lighting, and pose, are controlled independently and systematically, and thus allow an experimental exploration, which lets us reach causal conclusions. Our analysis is based on measuring cosine similarities between images and word prompts, including valence words drawn from the two leading social psychology theories elucidating human stereotypes: The ABC Model and the Stereotype Content Model. We find that non-protected attributes are powerful confounds and profoundly influence social perception, injecting variability in measurements whose size is comparable to that induced by legally protected attributes. Clear intersecting biases of race, gender, and age only emerge when these unprotected attributes are controlled for, which is only possible using CausalFace. FairFace does not permit a similar level of insight due to spurious correlations introduced by uncontrolled attributes and a lack of specific annotations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=epFk8e470p": {
    "title": "Deep Models modelled after human brain boost performance in action classification",
    "volume": "review",
    "abstract": "Recognizing actions from visual input is a fundamental cognitive ability. Perceiving what others are doing is a gateway to inferring their goals, emotions, beliefs and traits. Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks make use of information about the body and information about the background remains unclear. In particular, since these two sources of information may be correlated within a training dataset, deep networks might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike deep networks, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that deep networks trained using the Human Atomic Actions 500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel deep network architecture patterned after domain specificity in the brain, that utilizes separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.7,
    "authors": []
  },
  "https://openreview.net/forum?id=3mDe5o24BM": {
    "title": "HFDream: Improving 3D Generation via Human-Assisted Multi-view Text-to-Image Models",
    "volume": "review",
    "abstract": "Large-scale text-to-image models have demonstrated the potential for performing text-to-3D synthesis. However, existing approaches, e.g., DreamFusion, suffer from unstable 3D optimization due to the limitations of current text-to-image models that they struggle to synthesize images from certain viewpoints even when specified in the text prompt. Obtaining a view-aligned image-text pair dataset is challenging due to the limited availability of such data, and the inherent subjectivity and ambiguity of view-alignment. In this paper, we propose to enhance text-to- 3D generation by learning from human feedback for generating desired views. We generate multi-view images with the text-to-image model and engage human labelers to select a valid viewpoint. Using the human-labeled dataset, we train a reward model designed to verify whether the generated image aligns with the viewpoint specified in the text prompt. Finally, we fine-tune the text-to-image model to maximize the reward score. We find that our text-to-image diffusion models fine-tuned with human feedback, coined HFDream, consistently generate diverse viewpoints without the need for multi-view datasets created from 3D assets. This leads to high-quality text-to-3D generations with consistent geometry, when combined with view-dependent prompting in DreamFusion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xvhjRjoFCN": {
    "title": "BiXT: Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers",
    "volume": "review",
    "abstract": "We present a novel bi-directional Transformer architecture (BiXT) for which computational cost and memory consumption scale linearly with input size, but without suffering the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (‘what') and location (‘where') to develop alongside each other over multiple layers – allowing its direct application to dense and instance-based tasks alike. By combiningefficiency with the generality and performance of a full Transformer architecture, BiXT can processes longer sequences like point clouds or images at higher feature resolutions. Our model achieves accuracies up to 82.0% for classification on ImageNet1K with tiny models and no modality-specific internal components, and performs competitively on semantic image segmentation (ADE20K) and point cloud part segmentation (ShapeNetPart) even against modality-specific methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=kwn9ySjbc1": {
    "title": "Variable resolution: improving scene visual question answering with a limited pixel budget",
    "volume": "review",
    "abstract": "Artificial intelligence (AI) scene understanding systems can benefit from utilizing a large visual field of view (FOV). Some existing systems already employ multiple cameras to extend their FOV, however, increasing image size and quality presents an overwhelming challenge to the acquisition and computing resources for such systems. An effective solution is to sub-sample the FOV, without impairing the model's performance on complex visual tasks. In this paper, we show that a variable sampling scheme, inspired by human vision, remarkably outperforms a uniform sampling scheme by 2% accuracy (65% vs. 63%) in the challenging task of scene visual question answering (VQA), under a limited samples budget (3% of the full resolution baseline). The improvement is achieved without any image scanning, and the variable resolution peaks at an arbitrarily chosen fixed image location. Our study also compared basic visual sub-tasks, in particular image classification and object detection. Comparing the variable and uniform models revealed differences in the representations learned by the different models which yield a consistently improved performance of the variable resolution models. We show that the variable sampling scheme allows the models to benefit in low resolution areas, by propagating information from the finer resolution areas, and at the same time higher resolution areas benefit from contextual information at lower resolution in the periphery. The results show the potential of the biologically-inspired image representation to improve the design of visual acquisition and processing models in future AI-based systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3wpuxd7u9": {
    "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results. Code, data and models will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=PczQtTsTIX": {
    "title": "Cross$Q$: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity",
    "volume": "review",
    "abstract": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ's contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LY1eOfqU16": {
    "title": "A Soft Labeling Approach for Fairness-aware Learning Under Partially Annotated Sensitive Attributes",
    "volume": "review",
    "abstract": "In light of AI's growing ubiquity, concerns about its societal impact have prompted extensive efforts to mitigate different types of bias, often relying on the assumption of complete information regarding individuals' sensitive attributes. In this work, we tackle the problem of algorithmic fairness under partially annotated sensitive attributes. Previous approaches often rely on an attribute classifier as a proxy model to infer \"hard\" pseudo labels, which are then used to optimize the final model using fairness-aware regularization techniques. In contrast, we propose a novel regularization approach, that leverages the output probability of the attribute classifier as \"soft\" pseudo labels, derived from the definition of the fairness criteria. Additionally, we study the effect of the uncertainty on the attribute classifier parameters that naturally arise in the case of limited available sensitive attribute annotations. We adopt the Bayesian viewpoint and we propose to optimize our model with respect to the marginal model of the attribute classifier, while our second approach optimizes the fairness objective with respect to each model of the decision maker's belief. To validate our approach, we conduct extensive experiments on Adult and CelebA datasets with tabular and image modalities, respectively. The results of our study highlight the effectiveness of our method as well as the significance of incorporating uncertainty, in improving both utility and fairness compared to a variety of different baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=scFfMOOGD8": {
    "title": "Learnable Invisible Backdoor for Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have shown tremendous potential for high-quality image generation in recent years. Accordingly, there has been a rising focus on security threats associated with diffusion models, primarily because of their potential for malicious utilization. Recent studies have shown diffusion models are vulnerable to backdoor attack, which can make diffusion models generate designated target images given corresponding triggers. However, current backdoor attacks depend on manually designed trigger generation functions, which are usually visible patterns added to input noise, making them easily detected by human inspection. In this paper, we propose a novel and general optimization framework to learn invisible trigger, making the inserted backdoor more stealthy and robust. Our proposed framework can be applied to both unconditional and conditional diffusion models. In addition, for conditional diffusion models, we are the first to show how to backdoor diffusion models in text-guided image editing/inpainting pipeline. Extensive experiments on various commonly used samplers and datasets verify the effectiveness and stealthiness of the proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=yhvtZdqBNm": {
    "title": "Pruning Attention Heads with Almost-sure Sparsity Targets",
    "volume": "review",
    "abstract": "Transformer-based architectures have been widely used to obtain high accuracy values in multiple fields including natural language processing (NLP), computer vision, and more. Multi-head attention is the key factor in the success of Transformer-based architectures that has been found to be computationally expensive. Significant research effort has been devoted to improve attention compute efficiency by reducing the self-attention complexity or pruning redundant attention heads. Previous pruning work either presents training-testing inconsistency or enforces hard structural constraints which limit model performance. We propose the notion of almost-sure sparsity to overcome these limitations and develop a generic framework for Pruning with Almost-Sure Sparsity (PASS) targets over attention heads. To further boost efficiency, we design a novel technique, concentrator, based on which we develop PASSCONC (PASS with CONCentrator). We investigate PASS and PASSCONC on two widely studied architectures: encoder-decoder (ED) Transformer and BERT. Experiments on IWSLT14 German-to-English translation and GLUE benchmark tasks demonstrate that our approaches outperform the SOTA by up to 1.33 higher BLEU scores, 1.44% higher accuracy, and 60% higher attention layer speedups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=8WH6ZlDad6": {
    "title": "EWoK: Tackling Robust Markov Decision Processes via Estimating Worst Kernel",
    "volume": "review",
    "abstract": "Robust Markov Decision Processes (RMDPs) provide a framework for sequential decision-making that is robust to perturbations on the transition kernel. However, current RMDP methods are often limited to small-scale problems, hindering their use in realistic high-dimensional domains. To bridge this gap, we present **EWoK**, a novel approach for the online RMDP setting that **E**stimates the **Wo**rst transition **K**ernel to learn robust policies. Unlike previous works that regularize the policy or value updates, EWoK achieves robustness by simulating the worst scenarios for the agent while retaining complete flexibility in the learning process. Notably, EWoK can be applied on top of any off-the-shelf *non-robust* RL algorithm, enabling easy scaling to high-dimensional domains. Our experiments, spanning from simple Cartpole to high-dimensional MinAtar and DeepMind Control Suite environments, demonstrate the effectiveness and applicability of the EWoK paradigm as a practical method for learning robust policies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=aup1BV78Gq": {
    "title": "A New Type of Associative Memory Network with Exponential Storage Capacity",
    "volume": "review",
    "abstract": "Recent developments have sought to overcome the inherent limitations of traditional associative memory models, like Hopfield networks, where storage capacity scales linearly with input dimension. In this paper, we present a new extension of Hopfield networks that grants precise control over inter-neuron interactions while allowing control of the level of connectivity within the network. This versatile framework encompasses a variety of designs, including classical Hopfield networks, models with polynomial activation functions, and simplicial Hopfield networks as particular cases. Remarkably, a specific instance of our construction, resulting in a new self-attention mechanism, is characterized by quasi-exponential storage capacity and a sparse network structure, aligning with biological plausibility. To our knowledge, our proposed construction introduces the first biologically-plausible associative memory model with exponential storage capacity. Furthermore, the resulting model admits a very efficient implementation via vectorization; therefore, it can fully exploit modern numerical computation hardware like GPUs. This work not only advances the theoretical foundations of associative memory but also provides insights into the development of neurobiologically inspired associative memory systems with unprecedented capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LjivA1SLZ6": {
    "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZ6r9GMT1n": {
    "title": "Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks",
    "volume": "review",
    "abstract": "Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model's output, an attacker can employ black-box attacks to generate such adversarial examples. In this work, we propose a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Our theoretical analysis confirms that this method effectively enhances the model's resilience against both score-based and decision-based black-box attacks. Importantly, our defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function, which can be varied during the attack. We demonstrate the robustness of our defense against multiple black-box attacks through extensive empirical experiments involving diverse models with various architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=02f3mUtqnM": {
    "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
    "volume": "review",
    "abstract": "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3cuJwmPxXj": {
    "title": "Identifying Representations for Intervention Extrapolation",
    "volume": "review",
    "abstract": "The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome variable $Y$, observed features $X$, which are generated as a non-linear transformation of latent features $Z$, and exogenous action variables $A$, which influence $Z$. The objective of intervention extrapolation is then to predict how interventions on $A$ that lie outside the training support of $A$ affect $Y$. Here, extrapolation becomes possible if the effect of $A$ on $Z$ is linear and the residual when regressing Z on A has full support. As $Z$ is latent, we combine the task of intervention extrapolation with identifiable representation learning, which we call $\\texttt{Rep4Ex}$: we aim to map the observed features $X$ into a subspace that allows for non-linear extrapolation in $A$. We show using Wiener's Tauberian theorem that the hidden representation is identifiable up to an affine transformation in $Z$-space, which, we prove, is sufficient for intervention extrapolation. The identifiability is characterized by a novel constraint describing the linearity assumption of $A$ on $Z$. Based on this insight, we propose a flexible method that enforces the linear invariance constraint and can be combined with any type of autoencoder. We validate our theoretical findings through a series of synthetic experiments and show that our approach can indeed succeed in predicting the effects of unseen interventions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmq67R2PIu": {
    "title": "DockGame: Cooperative Games for Multimeric Rigid Protein Docking",
    "volume": "review",
    "abstract": "Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sampling from the Gibbs distribution of the true potential by learning a diffusion generative model over the action spaces (rotations and translations) of all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset, DockGame has much faster runtimes than traditional docking methods, can generate multiple plausible assembly structures, and achieves comparable performance to existing binary docking baselines, despite solving the harder task of coordinating multiple protein chains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=yZBpnKpBCw": {
    "title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities",
    "volume": "review",
    "abstract": "We propose FALCUN, a novel deep batch active learning method that is label- and time-efficient. Our proposed acquisition uses a natural, self-adjusting balance of uncertainty and diversity: It slowly transitions from emphasizing uncertain instances at the decision boundary to emphasizing batch diversity. In contrast, established deep active learning methods often have a fixed weighting of uncertainty and diversity. Moreover, most methods demand intensive search through a deep neural network's high-dimensional latent embedding space. This leads to high acquisition times during which experts are idle as they wait for the next batch to label. We overcome this structural problem by exclusively operating on the low-dimensional probability space, yielding much faster acquisition times. In extensive experiments, we show FALCUNs suitability for diverse use cases, including image and tabular data. Compared to state-of-the-art methods like BADGE, CLUE, and AlfaMix, FALCUN consistently excels in quality and speed: while FALCUN is among the fastest methods, it has the highest average label efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ck4SG9lnrQ": {
    "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
    "volume": "review",
    "abstract": "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming simultaneously more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of 60% even, which is the pass mark for Chinese exams. This highlights that there is significant room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models in the Chinese context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=j5JvZCaDM0": {
    "title": "Feasibility-Guided Safe Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "Safe offline reinforcement learning is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning, which can be effectively extracted with a guided diffusion model thanks to its expressiveness. We compare FISOR against baselines on DSRL benchmark for safe offline RL. Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=tKu7NNu0Yq": {
    "title": "DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance",
    "volume": "review",
    "abstract": "The Earth Mover's Distance (EMD) is the measure of choice between point clouds. However the computational cost to compute it makes it prohibitive as a training loss, and the standard approach is to use a surrogate such as the Chamfer distance. We propose an attention-based model to compute an accurate approximation of the EMD that can be used as a training loss for generative models. To get the necessary accurate estimation of the gradients we train our model to explicitly compute the matching between point clouds instead of EMD itself. We cast this new objective as the estimation of an attention matrix that approximates the ground truth matching matrix. Experiments show that this model provides an accurate estimate of the EMD and its gradient with a wall clock speed-up of more than two orders of magnitude with respect to the exact Hungarian matching algorithm and one order of magnitude with respect to the standard approximate Sinkhorn algorithm, allowing in particular to train a point cloud VAE with the EMD itself. Extensive evaluation show the remarkable behaviour of this model when operating out-of-distribution, a key requirement for a distance surrogate. Finally, the model generalizes very well to point clouds during inference several times larger than during training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=EraNITdn34": {
    "title": "Unlocking the Transferability of Tokens in Deep Models for Tabular Data",
    "volume": "review",
    "abstract": "Fine-tuning a pre-trained deep neural network has become a successful paradigm in various machine learning tasks. However, such a paradigm becomes particularly challenging with tabular data when there are discrepancies between the feature sets of pre-trained models and the target tasks. In this paper, we propose TabToken, a method aims at enhancing the quality of feature tokens (\\ie, embeddings of tabular features). TabToken allows for the utilization of pre-trained models when the upstream and downstream tasks share overlapping features, facilitating model fine-tuning even with limited training examples. Specifically, we introduce a contrastive objective that regularizes the tokens, capturing the semantics within and across features. During the pre-training stage, the tokens are learned jointly with top-layer deep models such as transformer. In the downstream task, tokens of the shared features are kept fixed while TabToken efficiently fine-tunes the remaining parts of the model. TabToken not only enables knowledge transfer from a pre-trained model to tasks with heterogeneous features, but also enhances the discriminative ability of deep tabular models in standard classification and regression tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=S5EqslEHnz": {
    "title": "Do Generated Data Always Help Contrastive Learning?",
    "volume": "review",
    "abstract": "Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under data inflation. Drawing from these insights, we propose **Adaptive Inflation (AdaInf)**, a purely data-centric strategy without introducing any extra computation cost. On benchmark datasets, AdaInf can bring significant improvements for various contrastive learning methods. Notably, without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=1P92J25hdf": {
    "title": "Going Deeper with General and Specific Inductive Bias for Real-Time Stereo Matching",
    "volume": "review",
    "abstract": "Inductive Bias (IB) has sparked a revolutionary transformation by incorporating the advantages of CNNs and Transformers, including scale invariance and integration of locality and long-range dependencies, which is called general IB for its wide applicability. However, its efficacy is currently not enjoyed by stereo matching, one of the geometric vision tasks, because of the ignorance of volume-level scale invariance and the limitation of high real-time requirement. In contrast, a specific IB is adopted by constructing volume structure in stereo matching task, which helps to finally generate a confidence volume to predict disparity map (output), but fewer studies go into the specific volume structure. Based on the above issues, this paper develops a novel model named UStereo to introduce the general IB to stereo matching. Technically, we adopt inter-layer fusion to break down volume-level scale invariance to a recurrence strategy in initialization for information at low resolution and refinement process for the high, which further extends to capture long-range dependencies after shallow stacks of convolutions and normalization without time-consuming Transformers. Additionally, to reveal the role that the volume structure constructed by specific IB plays during inference, we propose the first-time in-depth study of volume at low resolution through varying degrees of restraint as well as 3 original statistic indicators to reflect the characteristics of representation within volumes. Experiments demonstrate UStereo has competitive performance with both fast speed and robust generalization, and ablation studies show the effectiveness of introducing general IB. Moreover, our analysis of the volumes at low resolution suggests they can be viewed as confidence volumes and a concentrated distribution of the disparity within volumes leads to enhanced performance, which could extend the role of the specific IB",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.6,
    "authors": []
  },
  "https://openreview.net/forum?id=7JRbs3i9Ei": {
    "title": "Machine Learning for PROTAC Engineering",
    "volume": "review",
    "abstract": "PROTACs are a promising therapeutic technology that harnesses the cell's built-in degradation processes to degrade specific proteins. Despite their potential, developing new PROTAC molecules is challenging and requires significant expertise, time, and cost. Meanwhile, machine learning has transformed various scientific fields, including drug development. In this work, we present a strategy for curating open-source PROTAC data and propose an open-source toolkit for predicting the degradation effectiveness, i.e., activity, of novel PROTAC molecules. We organized the curated data into 16 different datasets ready to be processed by machine learning models. The datasets incorporate important features such as $pDC_{50}$, $D_{max}$, E3 ligase type, POI amino acid sequence, and experimental cell type. Our toolkit includes a configurable PyTorch dataset class tailored to process PROTAC features, a customizable machine learning model for processing various PROTAC features, and a hyperparameter optimization mechanism powered by Optuna. To evaluate the system, three surrogate models were developed utilizing different PROTAC representations. Using our automatically-curated public datasets, the best models achieved a 71.4% validation accuracy and a 0.73 ROC-AUC validation score. This is not only comparable to state-of-the-art models for protein degradation prediction, but also open-source, easily-reproducible, and less computationally complex than existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=2LhCPowI6i": {
    "title": "Self-Supervised Pseudodata Filtering for Improved Replay with Sub-Optimal Generators",
    "volume": "review",
    "abstract": "Continual learning on a sequence of tasks without forgetting previously acquired knowledge is one of the main challenges faced by modern deep neural networks. In the class-incremental scenario, one of the most difficult continual learning problems, new classes are presented to a classifier over time. The model needs to be able to learn and recognize these new classes while also retaining its knowledge of previously witnessed ones. To achieve this, the model has to revisit previous classes in some form, either by analysing stored exemplars or by using artificially generated samples. The latter approach, Generative Replay, usually relies on a separate generator trained alongside the main classifier. Since the generator also needs to learn continually, it is retrained on every task, using its own generated samples as training data representing older classes. This can lead to error propagation and accumulating features unimportant or confusing for the classifier, reducing the overall performance for larger numbers of tasks. We propose a simple filtering mechanism for mitigating this issue – whenever pseudodata is generated for a new task, the classifier can reject samples it is not able to classify with sufficient confidence, thus preventing itself from retraining on poor-quality data. We tested this mechanism using combinations of Bayesian neural classifiers and two different generators: a Variational Autoencoder and Real-value Non-Volume Preserving Normalizing Flow. We show that the improvement in the classification accuracy grows with the number of tasks, suggesting this approach is particularly useful for the most challenging continual learning scenarios, where very many tasks are learned in a sequence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=vst5P4Pve2": {
    "title": "Towards Global Interaction Efficiency of Graph Networks",
    "volume": "review",
    "abstract": "A graph inherently embodies comprehensive interactions among all its nodes when viewed globally. Hence, going beyond existing studies in long-range interactions, which focus on interactions between individual node pairs, we study the interactions in a graph through a global perspective. Traditional GNNs acquire such interactions by leveraging local connectivities through aggregations. While this approach has been prevalent, it has shown limitations, such as under-reaching, and over-squashing. In response, we introduce a global interaction perspective and propose interaction efficiency as a metric for assessing GNN performance. This metric provides a unified insight for understanding several key aspects of GNNs, including positional encodings in Graph Transformers, spectral graph filter expressiveness, over-squashing, and the role of nonlinearity in GNNs. Inspired by the global interaction perspective, we present Universal Interaction Graph Convolution, which exhibits superior interaction efficiency. This new architecture achieves highly competitive performance on a variety of graph-level learning tasks. Code is available at https://github.com/iclrsubmission-towards/UIGC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=kMp8zCsXNb": {
    "title": "ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference",
    "volume": "review",
    "abstract": "Coordinate network or implicit neural representation (INR) is a fast-emerging method for encoding natural signals (such as images and videos) with the benefits of a compact neural representation. While numerous methods have been proposed to increase the encoding capabilities of an INR, an often overlooked aspect is the inference efficiency, usually measured in multiply-accumulate (MAC) count. This is particularly critical in use cases where inference bandwidth is greatly limited by hardware constraints. To this end, we propose the Activation-Sharing Multi-Resolution (ASMR) coordinate network that combines multi-resolution coordinate decomposition with hierarchical modulations. Specifically, an ASMR model enables the sharing of activations across grids of the data. This largely decouples its inference cost from its depth which is directly correlated to its reconstruction capability, and renders a near $O(1)$ inference complexity irrespective of the number of layers. Experiments show that ASMR can reduce the MAC of a vanilla SIREN model by up to 350$\\times$ while achieving an even higher reconstruction quality than its SIREN baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=eJ0dzPJq1F": {
    "title": "Blending Imitation and Reinforcement Learning for Robust Policy Improvement",
    "volume": "review",
    "abstract": "While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. To address the demand for robust policy improvement in real-world scenarios, we introduce a novel algorithm, Robust Policy Improvement (RPI), which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration—an aspect that is notably challenging in sparse-reward RL—particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the oracles or learn from its own value function when the learner's performance surpasses that of the oracles in a specific state. Empirical evaluations and theoretical analysis validate that RPI excels in comparison to existing state-of-the-art methodologies, demonstrating superior performance across various benchmark domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=L9kwewFGQZ": {
    "title": "Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning",
    "volume": "review",
    "abstract": "Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=0uUASYeXav": {
    "title": "Graphical Object-Centric Actor-Critic",
    "volume": "review",
    "abstract": "There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-the-art model-free actor-critic algorithm built upon transformer architecture and the state-of-the-art monolithic model-based algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=JsnR0YO4Fq": {
    "title": "Exploring Weight Balancing on Long-Tailed Recognition Problem",
    "volume": "review",
    "abstract": "Recognition problems in long-tailed data, in which the sample size per class is heavily skewed, have gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various methods have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance compared with existing methods devised in various ways. However, there is a lack of understanding as to why this method is effective for long-tailed data. In this study, we analyze weight balancing by focusing on neural collapse and the cone effect at each training stage and found that it can be decomposed into an increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-balanced loss. Our analysis enables the training method to be further simplified by reducing the number of training stages to one while increasing accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=tuzTN0eIO5": {
    "title": "Zero Bubble Pipeline Parallelism",
    "volume": "review",
    "abstract": "Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 15\\% in throughput under a similar memory limit. This number can be further pushed to 30\\% when the memory constraint is relaxed. We believe our results mark a major step forward in harnessing the true potential of pipeline parallelism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ynguffsGfa": {
    "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes",
    "volume": "review",
    "abstract": "Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. This challenge is pronounced in low-to-middle income countries where access to large datasets is often limited or even absent. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this technical challenge, we introduce $\\texttt{CLLM}$, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. While diverse, not all the data generated by LLMs will help increase utility for a downstream task, as for any generative model. Consequently, we introduce a principled curation process, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of LLMs in the low-data regime compared to conventional generators. We further show our curation mechanism improves the downstream performance for all generators, including LLMs. Additionally, we provide insights and understanding into the LLM generation and curation mechanism, shedding light on the features that enable them to output high-quality augmented datasets. $\\texttt{CLLM}$ paves the way for wider usage of ML in data scarce domains and regions, by allying the strengths of LLMs with a robust data-centric approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Zh047FhXqI": {
    "title": "Effective Offline Environment Reconstruction when the Dataset is Collected from Diversified Behavior Policies",
    "volume": "review",
    "abstract": "In reinforcement learning, it is crucial to have an accurate environment dynamics model to evaluate different policies' value in tasks like offline policy optimization and policy evaluation. However, the learned model is known to have large value gaps when evaluating target policies different from data-collection policies. This issue has hindered the wide adoption of models as various policies are needed for evaluation in these downstream tasks. In this paper, we focus on one of the typical offline environment model learning scenarios where the offline dataset is collected from diversified policies. We utilize an implicit multi-source nature in this scenario and propose an easy-to-implement yet effective algorithm, policy-conditioned model (PCM) learning, for accurate model learning. PCM is a meta-dynamics model that is trained to be aware of the evaluation policies and on-the-fly adjust the model to match the evaluation policies' state-action distribution to improve the prediction accuracy. We give a theoretical analysis and experimental evidence to demonstrate the feasibility of reducing value gaps by adapting the dynamics model under different policies. Experiment results show that PCM outperforms the existing SOTA off-policy evaluation methods in the DOPE benchmark with \\textit{a large margin}, and derives significantly better policies in offline policy selection and model predictive control compared with the standard model learning method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=gqtbL7j2JW": {
    "title": "You Only Submit One Image to Find the Most Suitable Generative Model",
    "volume": "review",
    "abstract": "Deep generative models have achieved promising results in image generation, and various generative model hubs, e.g., Hugging Face and Civitai, have been developed that enable model developers to upload models and users to download models. However, these model hubs lack advanced model management and identification mechanisms, resulting in users only searching for models through text matching, download sorting, etc., making it difficult to efficiently find the model that best meets user requirements. In this paper, we propose a novel setting called *Generative Model Identification* (GMI), which aims to enable the user to identify the most appropriate generative model(s) for the user's requirements from a large number of candidate models efficiently. To our best knowledge, it has not been studied yet. In this paper, we introduce a comprehensive solution consisting of three pivotal modules: a weighted Reduced Kernel Mean Embedding (RKME) framework for capturing the generated image distribution and the relationship between images and prompts, a pre-trained vision-language model aimed at addressing dimensionality challenges, and an image interrogator designed to tackle cross-modality issues. Extensive empirical results demonstrate the proposal is both efficient and effective. For example, users only need to submit a single example image to describe their requirements, and the model platform can achieve an average top-4 identification accuracy of more than 80%. The code and benchmark are all released to promote the research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSxE03S4fp": {
    "title": "Learn to Achieve Out-of-the-Box Imitation Ability from Only One Demonstration",
    "volume": "review",
    "abstract": "Imitation learning (IL) enables agents to mimic expert behaviors. Most previous IL techniques focus on precisely imitating one policy through mass demonstrations. However, in many applications, what humans require is the ability to perform various tasks directly through a few demonstrations of corresponding tasks, where \\textit{the agent would meet many unexpected changes when deployed}. In this scenario, the agent is expected to not only imitate the demonstration but also adapt to unforeseen environmental changes. This motivates us to propose a new topic called imitator learning (ItorL), which aims to derive an imitator module that can \\textit{on-the-fly} reconstruct the imitation policies based on very \\textit{limited} expert demonstrations for different unseen tasks, without any extra adjustment. In this work, we focus on imitator learning based on only one expert demonstration. To solve ItorL, we propose Demo-Attention Actor-Critic (DAAC), which integrates IL into a reinforcement-learning paradigm that can regularize policies' behaviors in unexpected situations. Besides, for autonomous imitation policy building, we design a demonstration-based attention architecture for imitator policy that can effectively output imitated actions by adaptively tracing the suitable states in demonstrations. We develop a new navigation benchmark and a robot environment for \\topic and show that DAAC outperforms previous imitation methods \\textit{with large margins} both on seen and unseen tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mWf3RGc6HG": {
    "title": "Revisiting Ternary Neural Networks towards Asymmetric Thresholds and Uniform Distribution",
    "volume": "review",
    "abstract": "Recently, researchers have made significant progress in ternary logic circuits, which has spurred the utilization of Ternary Neural Network (TNN) due to its compatibility with ternary coding instead of the 2-bit coding used in binary system. However, TNN exhibits significant accuracy degradation compared to its full-precision counterpart. Therefore, we are motivated to revisit ternary neural networks and enhance their performance. To fully leverage the limited representation space, we apply a uniform distribution to three quantized values {-1,0,+1} to maximize the information entropy. To balance the representation ability of TNN while considering convenient hardware implementation, we adopt the asymmetric thresholds and symmetric scaling factors quantization scheme and introduce the bi-STE optimization method. Moreover, a two-stage knowledge distillation scheme is employed to further enhance the performance. Experimental results demonstrate the effectiveness of the proposed method for TNNs, achieving a top-1 accuracy of 74.5% for ResNet-50 on ImageNet. This outperforms previous ternary quantization methods by a large margin and even surpasses representative 2-bit quantization methods such as LSQ (73.7%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3NBqtrQat": {
    "title": "Learning Object-Centric Representation via Reverse Hierarchy Guidance",
    "volume": "review",
    "abstract": "Object-Centric Learning (OCL) seeks to enable Neural Networks to identify individual objects in a visual scene in an unsupervised manner, which is a meaningful task because the ability to recognize objects and understand their relationships is the foundation of interpretable visual comprehension and reasoning. Due to humans' strong ability to split visual scenes into object sets, incorporating the mechanism of human visual perception into model architecture is a potential way to enhance object representation. According to Reverse Hierarchy Theory (RHT), the human visual system comprises two reverse processes: a bottom-up process rapidly extracting the gist of scenes and a top-down process integrating detailed information into consciousness. Inspired by RHT, We propose Reverse Hierarchy Guided Network (RHGNet) that enhances the models' object-centric representations through an extra top-down pathway as described in RHT. This pathway allows for more decisive semantic information to be included in extracted low-level features, as well as helps search for optimal solutions to distinguish objects from low-level features. We demonstrate with experiments that the model benefits from our method and achieves a stronger ability to differentiate objects, especially the easily ignored small and occluded ones, than current models following a pure bottom-up fashion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TCJbcjS0c2": {
    "title": "LASER: Linear Compression in Wireless Distributed Optimization",
    "volume": "review",
    "abstract": "Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce ${\\bf LASER}$: ${\\bf L}$ine${\\bf A}$r Compre${\\bf S}$sion in Wir${\\bf E}$less Dist${\\bf R}$ibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to that of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64$ % improvement in perplexity over our baselines for noisy channels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=G3OCarOfxx": {
    "title": "Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training",
    "volume": "review",
    "abstract": "Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low robust training error, there still exists a significant $\\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this puzzling phenomenon (CGRO) through $\\textit{feature learning theory}$. Specifically, we prove that, under our theoretical framework (patch-structured dataset and one-hidden-layer CNN model) , a $\\textit{three-stage phase transition}$ happens from adversarial training dynamics, and the network learner provably partially learns the true feature but exactly memorizes the spurious features from training-adversarial examples, which thereby results in CGRO phenomenon. Besides, for more general data assumption, we then show the efficiency of CGRO classifier from the perspective of $\\textit{representation complexity}$. On the empirical side, we also verify our theoretical analysis about learning process in real-world vision dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2bF381xEke": {
    "title": "MapSelect: Sparse & Interpretable Graph Attention Networks",
    "volume": "review",
    "abstract": "Graph Attention Networks (GATs) have shown remarkable performance in capturing complex graph structures by assigning dense attention weights over all neighbours of a node. Attention weights can act as an inherent explanation for the model output, by highlighting the most important neighbours for a given input graph. However, the dense nature of the attention layer causes a lack of focus as all edges receive some probability mass. To overcome this, we introduce MapSelect, a new method providing a fully differentiable sparse attention mechanism. Through user-defined constraints, MapSelect enables precise control over the attention density, acting as a continuous relaxation of the popular top-k operator. We propose two distinct variants of MapSelect: a local approach maintaining a fixed degree per node, and a global approach preserving a percentage of the full graph. Upon conducting a comprehensive evaluation of five sparse GATs in terms of sparsity, performance, and interpretability, we provide insights on the sparsity-accuracy and sparsity-interpretability trade-offs. Our results show that MapSelect outperforms robust baselines in terms of interpretability, especially in the local context, while also leading to competitive task performance on real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMjflI1aL0": {
    "title": "Imbalanced data robust online continual learning based on evolving class aware memory selection and built-in contrastive representation learning",
    "volume": "review",
    "abstract": "Continual Learning (CL) aims to learn and adapt continuously to new information while retaining previously acquired knowledge. Most state of the art CL methods currently emphasize class incremental learning. In this approach, class data is introduced and processed only once within a defined task boundary. However, these methods often struggle in dynamic environments, especially when dealing with imbalanced data, shifting classes, and evolving domains. Such challenges arise from changes in correlations and diversities, necessitating ongoing adjustments to previously established class and data representations. In this paper, we introduce a novel online CL algorithm, dubbed as Memory Selection with Contrastive Learning (MSCL), based on evolving intra-class diversity and inter-class boundary aware memory selection and contrastive data representation learning. Specifically, we propose a memory selection method called Feature-Distance Based Sample Selection (FDBS), which evaluates the distance between new data and the memory set to assess the representability of new data to keep the memory aware of evolving inter-class similarities and intra-class diversity of the previously seen data. Moreover, as the data stream unfolds with new class and/or domain data and requires data representation adaptation, we introduce a novel built-in contrastive learning loss (IWL) that seamlessly leverages the importance weights computed during the memory selection process, and encourages instances of the same class to be brought closer together while pushing instances of different classes apart. We tested our method on various datasets such as MNIST, Cifar-100, PACS, DomainNet, and mini-ImageNet using different architectures. In balanced data scenarios, our approach either matches or outperforms leading memory-based CL techniques. However, it significantly excels in challenging settings like imbalanced class, domain, or class-domain CL. Additionally, our experiments demonstrate that integrating our proposed FDBS and IWL techniques enhances the performance of existing rehearsal-based CL methods with significant margins both in balanced and imbalanced scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=qW9GVa3Caa": {
    "title": "Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability",
    "volume": "review",
    "abstract": "We introduce Prototype Generation, a stricter and more robust form of feature visualisation for model-agnostic, data-independent interpretability of image classification models. We demonstrate its ability to generate inputs that result in natural activation paths, countering previous claims that feature visualisation algorithms are untrustworthy due to the unnatural internal activations. We substantiate these claims by quantitatively measuring similarity between the internal activations of our generated prototypes and natural images. We also demonstrate how the interpretation of generated prototypes yields important insights, highlighting spurious correlations and biases learned by models which quantitative methods over test-sets cannot identify",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=vtyasLn4RM": {
    "title": "CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs",
    "volume": "review",
    "abstract": "Graph Visualization, also known as Graph Drawing, aims to find geometric embeddings of graphs that optimize certain criteria. Stress is a widely used metric; stress is minimized when every pair of nodes is positioned at their shortest path distance. However, stress optimization presents computational challenges due to its inherent complexity and is usually solved using heuristics in practice. We introduce a scalable Graph Neural Network (GNN) based Graph Drawing framework with sub-quadratic runtime that can learn to optimize stress. Inspired by classical stress optimization techniques and force-directed layout algorithms, we create a coarsening hierarchy for the input graph. Beginning at the coarsest level, we iteratively refine and un-coarsen the layout, until we generate an embedding for the original graph. To enhance information propagation within the network, we propose a novel positional rewiring technique based on intermediate node positions. Our empirical evaluation demonstrates that the framework achieves state-of-the-art performance while remaining scalable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=eKGEsFdpin": {
    "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
    "volume": "review",
    "abstract": "Potential harms of Large Language Models such as mass misinformation and plagiarism can be partially mitigated if there exists a reliable way to detect machine generated text. In this paper, we propose a new watermarking method to detect machine-generated texts. Our method embeds a unique pattern within the generated text, ensuring that while the content remains coherent and natural to human readers, it carries distinct markers that can be identified algorithmically. Specifically, we intervene with the token sampling process in a way which enables us to trace back our token choices during the detection phase. We show how watermarking affects textual quality and compare our proposed method with a state-of-the-art watermarking method in terms of robustness and detectability. Through extensive experiments, we demonstrate the effectiveness of our watermarking scheme in distinguishing between watermarked and non-watermarked text, achieving high detection rates while maintaining textual quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=LYG6tBlEX0": {
    "title": "H-GAP: Humanoid Control with a Generalist Planner",
    "volume": "review",
    "abstract": "Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations. The daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability introduced by the bipedal morphology of humanoids. However, the extensive collection of human motion-captured data and the derived datasets of humanoid trajectories, such as MoCapAct, paves the way to tackle these challenges. In this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a state-action trajectory generative model trained on humanoid trajectories derived from human motion-captured data, capable of adeptly handling downstream control tasks with Model Predictive Control (MPC). For 56 degrees of freedom humanoid, we empirically demonstrate that H-GAP learns to represent and generate a wide range of motor behaviors. Further, without any learning from online interactions, it can also flexibly transfer these behaviours to solve novel downstream control tasks via planning. Notably, H-GAP excels established MPC baselines with access to the ground truth model, and is superior or comparable to offline RL methods trained for individual tasks. Finally, we do a series of empirical studies on the scaling properties of H-GAP, showing the potential for performance gains via additional data but not computing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=HRkyLbBRHI": {
    "title": "Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "Offline Reinforcement learning (RL) is a compelling framework for learning optimal policies without additional environmental interaction. Nevertheless, offline RL inevitably faces the problem of distributional shifts, where the states and actions encountered during policy execution are not in the training dataset. A common solution involves incorporating conservatism into either the policy or value function, which serves as a safeguard against uncertainties and unknowns. In this paper, we also focus on achieving the same objectives of conservatism but from a different perspective. We propose COmpositional COnservatism with Anchor-seeking ($\\text{\\textit{COCOA}}$) for offline RL, an approach that pursues conservatism in a compositional manner on top of the transductive reparameterization (Netanyahu et al., 2023). In this reparameterization, the input variable (the state in our case) is viewed as the combination of an anchor and its difference from the original input. Independently of and agnostically to the prevalent $\\text{\\textit{behavioral}}$ conservatism in offline RL, COCOA learns to seek both in-distribution anchors and differences with the learned dynamics model, encouraging conservatism in the $\\text{\\textit{compositional input space}}$ for the function approximators of the Q-function and policy. Our experimental results show that our method generally improves the performance of four state-of-the-art offline RL algorithms on the D4RL benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=B5CgCJY2po": {
    "title": "Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing",
    "volume": "review",
    "abstract": "Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more efficient in terms of message complexity. We study the proposed model and provide both empirical evidence and theoretical insights in terms of its expressiveness, efficiency, information exchange and ability to extrapolate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=VeFmnRmoaW": {
    "title": "MetroGNN: Metro Network Expansion with Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "Selecting urban regions for metro network expansion that serve maximal transportation demands is critical to urban development, while computationally challenging to solve. First, metro network expansion is dependent on multiple complicated features, such as urban demographics, origin-destination (OD) flow, and relationships with existing metro lines, requiring a unified model to incorporate these correlated features for region selection. Second, it is a complex decision-making task with an enormous solution space and various constraints, due to the large number of candidate regions and restrictions on urban geography. In this paper, we present a reinforcement learning framework to solve a Markov decision process on an urban heterogeneous multi-graph, achieving metro network expansion by intelligently selecting a set of nodes on the graph. A novel graph neural network is proposed, which unifies the complicated features and learns effective representations for urban regions. In addition, we design an attentive reinforcement learning agent with action masks to efficiently search the large solution space and avoid infeasible solutions indicated by the various constraints. Experiments on real-world urban data of Beijing and Changsha show that our proposed approach can improve the satisfied transportation demands substantially by over 30\\% compared with state-of-the-art reinforcement learning methods. Further in-depth analysis demonstrates that MetroGNN can provide explainable results in scenarios with much more complicated initial conditions and expansion requirements, indicating its applicability in real-world metro network design tasks. Codes are released at https://anonymous.4open.science/r/MetroGNN-31DD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfHISoWo2m": {
    "title": "Meta-Learning Nonlinear Dynamical Systems with Deep Kernels",
    "volume": "review",
    "abstract": "Scientific processes are often modelled by sets of differential equations. As datasets grow, individually fitting these models and quantifying their uncertainties becomes a computationally challenging task. In this paper, we focus on improving the scalability of a particular class of stochastic dynamical model, called latent force models. These offer a balance between data-driven and mechanistic inference in dynamical systems, achieved by deriving a kernel function over a low-dimensional latent force. However, exact computation of posterior kernel terms is rarely tractable, requiring approximations for complex scenarios such as nonlinear dynamics. We overcome this issue by posing the problem as meta-learning the class of latent force models corresponding to a set of differential equations. By employing a deep kernel along with a sensible function embedding, we demonstrate the ability to extrapolate from simulations to real experimental datasets. Finally, we show how our model scales compared with other approximations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwtMhMSybu": {
    "title": "Unlocking the Power of Representations in Long-term Novelty-based Exploration",
    "volume": "review",
    "abstract": "We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes. We further propose a novel generalization of the inverse dynamics loss, which leverages masked transformer architectures for multi-step prediction; which in conjunction with \\DETOCS achieves a new state-of-the-art in a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new state-of-the-art in hard exploration Atari games, and is the first agent to reach the end screen in \"Pitfall!",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=FNCFiXKYoq": {
    "title": "MAAD Private: Multi-Attribute Adversarial Debiasing with Differential Privacy",
    "volume": "review",
    "abstract": "Balancing the trade-offs between algorithmic fairness, individual privacy, and model utility, is pivotal for the advancement of ethical artificial intelligence. In this work, we explore fair classification through the lens of differential privacy. We present an enhancement to the adversarial debiasing approach, enabling it to account for multiple sensitive attributes while upholding a privacy-conscious learning paradigm. Empirical results from two tabular datasets and a natural language dataset demonstrate our model's ability to concurrently debias up to four sensitive attributes and meet various fairness criteria, within the constraints of differential privacy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oUF3uGIVo": {
    "title": "Exploring High-Order Message-Passing in Graph Transformers",
    "volume": "review",
    "abstract": "The Transformer architecture has demonstrated promising performance on graph learning tasks. However, the existing attention mechanism used in Graph Transformers (GT) cannot capture high-order correlations that exist in complex graphs, thereby limiting their expressiveness. In this paper, we present a High-Order message-passing strategy within the Transformer architecture (HOtrans) to learn long-range, high-order relationships for graph representation. Recognizing that some nodes share similar properties, we extract communities from the entire graph and introduce a virtual node to connect all nodes in the community. Operating on the community, we adopt a three-step message-passing approach: capture the high-order information of the community into a virtual node; propagate long-range dependent information between communities; aggregate community-level representations back to graph nodes. This facilitates effective global information passing. Virtual nodes capture the high-order community information and support the long-range information passing as the bridge. We demonstrate that many existing GTs can be regarded as special cases of this framework. Our experimental results illustrate that our proposed HOtrans consistently achieves highly competitive results across several node classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcZ9VadFd5": {
    "title": "Emergence of Equivariance in Deep Ensembles",
    "volume": "review",
    "abstract": "We demonstrate that a generic deep ensemble is emergently equivariant under data augmentation in the large width limit. Specifically, the ensemble is equivariant at any training step for any choice of architecture, provided that data augmentation is used. This equivariance also holds off-manifold and is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. As such, the deep ensemble is indistinguishable from a manifestly equivariant predictor. We prove this theoretically using neural tangent kernel theory and verify our theoretical insights using detailed numerical experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8JizpeY4y": {
    "title": "Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations",
    "volume": "review",
    "abstract": "We introduce a novel modeling approach for time series imputation and forecasting, tailored to address the challenges often encountered in real-world data, such as irregular samples, missing data, or unaligned measurements from multiple sensors. Our method relies on a continuous-time-dependent model of the series' evolution dynamics. It leverages adaptations of conditional, implicit neural representations for sequential data. A modulation mechanism, driven by a meta-learning algorithm, allows adaptation to unseen samples and extrapolation beyond observed time-windows for long-term predictions. The model provides a highly flexible and unified framework for imputation and forecasting tasks across a wide range of challenging scenarios. It achieves state-of-the-art performance on classical benchmarks and outperforms alternative time-continuous models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBqowcUwFP": {
    "title": "L(M)V-IQL: Multiple Intention Inverse Reinforcement Learning for Animal Behavior Characterization",
    "volume": "review",
    "abstract": "In the pursuit of comprehending decision-making, behavioral neuroscience has made significant progress, aided by mathematical models in recent years. Among various approaches, Inverse Reinforcement Learning (IRL) stands out as a promising technique, distinguishing itself from other paradigms through its ability to circumvent the necessity for a reward function in characterizing observed behavior. Nevertheless, the widespread adoption of IRL within the field of neuroscience remains limited. This constraint may be attributed, in part, to the prevailing assumption in many existing IRL frameworks that animals exhibit a singular intention throughout a given task, wherein their behavior is optimized based on a single static reward function. In an effort to overcome this limitation, we propose the class of Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL) algorithms, a novel IRL framework designed to accommodate multiple discrete intrinsic rewards. We formulate an Expectation-Maximization approach to cluster observed trajectories into multiple intentions, and subsequently solve the IRL problem independently for each intention. We illustrate the application of L(M)V-IQL through simulated experiments, followed by its utilization on a dataset of mice engaged in a two-armed bandit task. Our methods exhibit exceptional proficiency in discerning animal intentions and yield interpretable reward functions corresponding to each identified intention. We anticipate that this progress will open up new possibilities in neuroscience and psychology, serving as an important advancement in elucidating the intricacies of animal decision-making and uncovering underlying brain mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=lQgm3UvGNY": {
    "title": "Synergistic Information Retrieval: Interplay between Search and Large Language Models",
    "volume": "review",
    "abstract": "Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern retrieval models (RMs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural languages. In this paper, we explore the advantages and disadvantages of LLMs and RMs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose $\\textbf{InteR}$, a novel framework that facilitates information refinement through synergy between RMs and LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using retrieved documents. This iterative refinement process augments the inputs of RMs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=T1Y2KmVtUn": {
    "title": "Differentiable Sensor Layouts for End-to-End Learning of Task-Specific Camera Parameters",
    "volume": "review",
    "abstract": "Computational imaging concepts based on integrated edge AI and and neural sensor concepts solve vision problems in an end-to-end, task-specific manner, by jointly optimizing the algorithmic and hardware parameters to sense data with high information value. They yield energy, data, and privacy efficient solutions, but rely on novel hardware concepts, yet to be scaled up. In this work, we present the first truly end-to-end trained imaging pipeline that optimizes imaging sensor parameters, available in standard CMOS design methods, jointly with the parameters of a given neural network on a specific task. Specifically, we derive an analytic, differentiable approach for the sensor layout parameterization that allows for task-specific, local varying pixel resolutions. We present two pixel layout parameterization functions: rectangular and curvilinear grid shapes that retain a regular topology. We provide a drop-in module that approximates sensor simulation given existing high-resolution images to directly connect our method with existing deep learning models. We show that network predictions benefit from learnable pixel layouts for two different downstream tasks, classification and semantic segmentation. Moreover, we give a fully featured design for the hardware implementation of the learned chip layout for a semantic segmentation task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=EG68RSznLT": {
    "title": "Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation",
    "volume": "review",
    "abstract": "Offline preference-based reinforcement learning (PbRL) offers an effective solution to overcome the challenges associated with designing rewards and the high costs of online interactions. In offline PbRL, agents are provided with a fixed dataset containing human preferences between pairs of trajectories. Previous studies mainly focus on recovering the rewards from the preferences, followed by policy optimization with an off-the-shelf offline RL algorithm. However, given that preference label in PbRL is inherently trajectory-based, accurately learning transition-wise rewards from such label can be challenging, potentially leading to misguidance during subsequent offline RL training. To address this issue, we introduce our method named $\\textit{Flow-to-Better (FTB)}$, which leverages the pairwise preference relationship to guide a generative model in producing preferred trajectories, avoiding Temporal Difference (TD) learning with inaccurate rewards. Conditioning on a low-preference trajectory, $\\textit{FTB}$ uses a diffusion model to generate a better one with a higher preference, achieving high-fidelity full-horizon trajectory improvement. During diffusion training, we propose a technique called $\\textit{Preference Augmentation}$ to alleviate the problem of insufficient preference data. As a result, we surprisingly find that the model-generated trajectories not only exhibit increased preference and consistency with the real transition but also introduce elements of $\\textit{novelty}$ and $\\textit{diversity}$, from which we can derive a desirable policy through imitation learning. Experimental results on D4RL benchmarks demonstrate that FTB achieves a remarkable improvement compared to state-of-the-art offline PbRL methods. Furthermore, we show that FTB can also serve as an effective data augmentation method for offline RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ro4CgvfUKy": {
    "title": "Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping",
    "volume": "review",
    "abstract": "Deep Neural Networks (DNNs) that achieve human-level performance in general tasks like object segmentation typically require supervised labels. In contrast, humans are able to perform these tasks effortlessly without supervision. To accomplish this, the human visual system makes use of perceptual grouping: for example, the black and white stripes of a zebra are perceptually grouped together despite their vastly different colors. Understanding how perceptual grouping arises in an unsupervised manner is critical for improving both models of the visual system, and computer vision models. In this work, we propose a counterintuitive approach to unsupervised perceptual grouping and segmentation: that they arise because of neural noise, rather than in spite of it. We (1) mathematically demonstrate that under realistic assumptions, neural noise can be used to separate objects from each other, and (2) show that adding noise in a DNN enables the network to segment images even though it was never trained on any segmentation labels. Interestingly, we find that (3) segmenting objects using noise results in segmentation performance that aligns with the perceptual grouping phenomena observed in humans. We introduce the Good Gestalt (GG) datasets --- six datasets designed to specifically test perceptual grouping, and show that our DNN models reproduce many important phenomena in human perception, such as illusory contours, closure, continuity, proximity, and occlusion. Finally, we (4) demonstrate the ecological plausibility of the method by analyzing the sensitivity of the DNN to different magnitudes of noise. We find that some model variants consistently succeed with remarkably low levels of neural noise ($\\sigma<0.001$), and surprisingly, that segmenting this way requires as few as a handful of samples. Together, our results suggest a novel unsupervised segmentation method requiring few assumptions, a new explanation for the formation of perceptual grouping, and a potential benefit of neural noise in the visual system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.6,
    "authors": []
  },
  "https://openreview.net/forum?id=UpgRVWexaD": {
    "title": "Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling",
    "volume": "review",
    "abstract": "Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely **S**orting **K**rylov **R**ecycling (**SKR**), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov subspace recycling, a powerful technique for solving a series of interrelated systems by leveraging their inherent similarities. Specifically, SKR employs a sorting algorithm to arrange these systems in a sequence, where adjacent systems exhibit high similarities. Then it equips a solver with Krylov subspace recycling to solve the systems sequentially instead of independently, thus effectively enhancing the solving efficiency. Both theoretical analysis and extensive experiments demonstrate that SKR can significantly accelerate neural operator data generation, achieving a remarkable speedup of up to 13.9 times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mkdwvl3Y8L": {
    "title": "Discovering Knowledge-Critical Subnetworks in Neural Language Models",
    "volume": "review",
    "abstract": "Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various *knowledge-critical* subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98\\%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized relational knowledge) but struggles to express the removed knowledge, and suffers performance drops on examples needing this removed knowledge on downstream tasks after finetuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTFdNLHE7k": {
    "title": "Kernelised Normalising Flows",
    "volume": "review",
    "abstract": "Normalising Flows are non-parametric statistical models known for their dual capabilities of density estimation and generation. They are distinguished by their inherently invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=HdAoLSBYXj": {
    "title": "Topic modeling as multi-objective optimization with Setwise Contrastive Learning",
    "volume": "review",
    "abstract": "Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive leraning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that balances the trade-off between the ELBO and the contrastive objective. Extensive experiments demonstrate that our framework consistently produces higher-performing neural topic models in terms of topic coherence, topic diversity, and downstream performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9DvDRTTdlu": {
    "title": "ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF",
    "volume": "review",
    "abstract": "Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain. This novel loss function surpasses the well-known score distillation sampling (SDS) loss in terms of suitability for editing purposes. Our experimental results demonstrate that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LxCPyLREX5": {
    "title": "Federated Learning under Label Shifts with Guarantees",
    "volume": "review",
    "abstract": "We consider the problem of training a global model in a distributed setting and develop an unbiased estimate of the overall *true risk* minimizer of multiple clients under challenging inter-client and intra-client *label shifts* as a stepping stone to provably address distribution shifts in real world. We generalize the family of Maximum Likelihood Label Shift (MLLS) density estimation methods inspired by a board family of Integral Probability Metrics and introduce the Variational Regularized Label Shift (VRLS) family of density ratio estimation methods and show all MLLS methods are special cases of VRLS under specific latent spaces. Our theory shows high-probability estimation error bounds achieved through a versatile regularization term in VRLS. Our extensive numerical experiments demonstrate that VRLS establishes *a new SotA in density ratio estimation* surpassing all baselines in MNIST, Fashion MNIST, CIFAR-10 datasets and *relaxed label shifts* as a proxy of real-world settings. In distributed settings, our importance-weighted empirical risk minimization with VRLS outperforms federated averaging and other baselines in imbalanced settings under drastic and challenging label shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=8FHWkY0SwF": {
    "title": "Learning Personalized Causally Invariant Representations for Heterogeneous Federated Clients",
    "volume": "review",
    "abstract": "Personalized federated learning (PFL) has gained great success in tackling the scenarios where target datasets are heterogeneous across the local clients. However, the application of the existing PFL methods to real-world setting is hindered by the common assumption that the test data on each client is in-distribution (IND) with respect to its training data. Due to the bias of training dataset, the modern machine learning model prefers to rely on shortcut which can perform well on the training data but fail to generalize to the unseen test data that is out-of-distribution (OOD). This pervasive phenomenon is called shortcut learning and has attracted plentiful efforts in centralized situations. In PFL, the limited data diversity on federated clients makes mitigating shortcut and meanwhile preserving personalization knowledge rather difficult. In this paper, we analyse this challenging problem by formulating the structural causal models (SCMs) for heterogeneous federated clients. From the proposed SCMs, we derive two significant causal signatures which inspire a provable shortcut discovery and removal method under federated learning, namely FedSDR. Specifically, FedSDR is divided into two steps: 1) utilizing the available training data distributed among local clients to discover all the shortcut features in a collaborative manner. 2) developing the optimal personalized causally invariant predictor for each client by eliminating the discovered shortcut features. We provide theoretical analysis to prove that our method can draw complete shortcut features and produce the optimal personalized invariant predictor that can generalize to unseen OOD data on each client. The experimental results on diverse datasets validate the superiority of FedSDR over the state-of-the-art PFL methods on OOD generalization performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=dn87xnULwF": {
    "title": "Maximally Expressive GNNs for Outerplanar Graphs",
    "volume": "review",
    "abstract": "We propose a _linear time_ graph transformation that enables the Weisfeiler-Leman (WL) test and message passing graph neural networks (MPNNs) to be maximally expressive on _outerplanar_ graphs. Our approach is motivated by the fact that most pharmaceutical molecules correspond to outerplanar graphs. Existing research predominantly enhances the expressivity of graph neural networks without specific graph families in mind. This often leads to methods that are impractical due to their computational complexity. In contrast, the restriction to outerplanar graphs enables us to encode the Hamiltonian cycle of each biconnected component in linear time. As the main contribution of the paper we prove that our method achieves maximum expressivity on outerplanar graphs. Experiments confirm that our graph transformation improves the predictive performance of MPNNs on molecular benchmark datasets at negligible computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=lWXedJyLuL": {
    "title": "A Unified Causal View of Instruction Tuning",
    "volume": "review",
    "abstract": "Instruction tuning on a mixture of tasks has improved zero-shot capabilities in natural language processing (NLP). Nevertheless, existing methods often learn features that exhibit correlations between instruction-formatted samples and target labels, rather than causal relationships. Termed as \"spurious correlation'' in statistics, such a correlation may change drastically in a new task, making the effect from the learned features to be misleading. To this end, we develop a meta Structural Causal Model (meta-SCM) to integrate different NLP tasks under a single causal structure of the data. Specifically, the meta-SCM introduces multiple latent factors that represent properties of source context language, only some of which causally influence the target labels for a specific task. The key idea is to learn task-required causal factors and only use those to make predictions for a given task. Theoretically, we prove the causal factor can be identified without mixing information from others. Guided by the identifiability, we propose a Structural Instruction Tuning (SIT) method to learn the task-required causal representations that can mimic the causal factors for each task. The utility of our approach is verified by improvements of zero-shot ability on a range of unseen datasets and tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=cJs4oE4m9Q": {
    "title": "Deep Orthogonal Hypersphere Compression for Anomaly Detection",
    "volume": "review",
    "abstract": "A common assumption of many anomaly detection methods is that a reasonable decision boundary has a hypersphere shape, which is difficult to obtain in practice and is not sufficiently compact, especially when the data are in high-dimensional spaces. In this paper, we first propose a novel deep anomaly detection model that improves the original hypersphere learning through an orthogonal projection layer, which ensures that the training data distribution is consistent with the hypersphere hypothesis, thereby increasing the true positive rate and decreasing the false negative rate. Moreover, we propose a bi-hypersphere compression method to obtain a hyperspherical shell that yields a more compact decision region than a hyperball, which is demonstrated theoretically and numerically. Note that the proposed methods are not confined to common datasets, such as image and tabular data, but are also extended to a more challenging but promising scenario, graph-level anomaly detection, which learns graph representation with maximum mutual information between the substructure and global structure features while exploring orthogonal single- or bi-hypersphere anomaly decision boundaries. The numerical and visualization results on benchmark datasets demonstrate the effectiveness and superiority of our methods in comparison with many baselines and the state-of-the-arts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhNXGWVH1N": {
    "title": "LeanFlex-GKP: Advancing Hassle-Free Structured Pruning with Simple Flexible Group Count",
    "volume": "review",
    "abstract": "Densely structured pruning methods — which generate pruned models in a fully dense format, allowing immediate compression benefits without additional demands — are evolving owing to their practical significance. Traditional techniques in this domain mainly revolve around coarser granularities, such as filter pruning, thereby limiting their performance due to restricted pruning freedom. Recent advancements in *Grouped Kernel Pruning (GKP)* have enabled the utilization of finer granularity while maintaining the densely structured format. We observed that existing GKP methods often introduce dynamic operations to different aspects of their procedures, where many were done so at the cost of adding complications and/or imposing limitations — e.g., requiring an expensive mixture of clustering schemes; or having dynamic pruning rates and sizes among groups, which lead to reliance on custom architecture support for its pruned models. In this work, we argue the best practice to introduce such dynamic operation to GKP is to make `Conv2d(groups)` (a.k.a. group count) flexible under an integral optimization, leveraging its ideal alignment with the infrastructure support of *Grouped Convolution*. Pursuing such direction, we present a one-shot, post-train, data-agnostic GKP method that is more performant, adaptive, and efficient than its predecessors; while simultaneously being a lot more user-friendly with little-to-no hyper-parameter tuning or handcrafted criteria required",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Op1XmdxFk8": {
    "title": "ProtoReg: Prioritizing Discriminative Information for Fine-grained Transfer Learning",
    "volume": "review",
    "abstract": "Transfer learning leverages a pre-trained model with rich features to fine-tune it for downstream tasks, thereby improving generalization performance. However, we point out the \"granularity gap\" in fine-grained transfer learning, a mismatch between the level of information learned by a pre-trained model and the semantic details required for a fine-grained downstream task. Under these circumstances, excessive non-discriminative information can hinder the sufficient learning of discriminative semantic details. In this study, we address this issue by establishing class-discriminative prototypes and refining the prototypes to gradually encapsulate more fine-grained semantic details, while explicitly aggregating each feature with the corresponding prototype. This approach allows the model to prioritize fine-grained discriminative information, even when the pre-trained model contains excessive non-discriminative information due to the granularity gap. Our proposed simple yet effective method, ProtoReg, significantly outperforms other transfer learning methods in fine-grained classification benchmarks with an average performance improvement of 6.4\\% compared to standard fine-tuning. Particularly in limited data scenarios using only 15\\% of the training data, ProtoReg achieves an even more substantial average improvement of 13.4\\%. Furthermore, ProtoReg demonstrates robustness to shortcut learning when evaluated on out-of-distribution data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=7v3tkQmtpE": {
    "title": "Rethinking Decision Transformer via Hierarchical Reinforcement Learning",
    "volume": "review",
    "abstract": "Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the Transformer architecture in sequential decision making. However, a notable limitation of DT is its reliance on {recalling} trajectories from datasets, without the capability to seamlessly stitch them together. In this work, we introduce a general sequence modeling framework for studying sequential decision making through the lens of \\emph{Hierarchical Reinforcement Learning}. At the time of making decisions, a \\emph{high-level} policy first proposes an ideal \\emph{prompt} for the current state, a \\emph{low-level} policy subsequently generates an action conditioned on the given prompt. We show how DT emerges as a special case with specific choices of high-level and low-level policies and discuss why these choices might fail in practice. Inspired by these observations, we investigate how to jointly optimize the high-level and low-level policies to enable the stitching capability. This further leads to the development of new algorithms for offline reinforcement learning. Finally, our empirical studies clearly demonstrate the proposed algorithms significantly surpass DT on several control and navigation benchmarks. We hope that our contributions can inspire the integration of Transformer architectures within the field of RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=R7rZUSGOPD": {
    "title": "PAE: Reinforcement Learning from External Knowledge for Efficient Exploration",
    "volume": "review",
    "abstract": "Human intelligence is adept at absorbing valuable insights from external knowledge. This capability is equally crucial for artificial intelligence. In contrast, classical reinforcement learning agents lack such capabilities and often resort to extensive trial and error to explore the environment. This paper introduces $\\textbf{PAE}$: $\\textbf{P}$lanner-$\\textbf{A}$ctor-$\\textbf{E}$valuator, a novel framework for teaching agents to $\\textit{learn to absorb external knowledge}$. PAE integrates the Planner's knowledge-state alignment mechanism, the Actor's mutual information skill control, and the Evaluator's adaptive intrinsic exploration reward to achieve 1) effective cross-modal information fusion, 2) enhanced linkage between knowledge and state, and 3) hierarchical mastery of complex tasks. Comprehensive experiments in six challenging sparse reward environments demonstrate PAE's superior exploration efficiency with good interpretability compared to existing methods. We provide the source code in the supplementary for further study and application",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=2h3m61LFWL": {
    "title": "Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs",
    "volume": "review",
    "abstract": "We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\\widetilde{O}(d\\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computationally more efficient as it only requires solving one optimization problem in each time step. In our regret analysis, we offer a generic convergence result of MLE in linear MDPs through a novel supermartingale construct and uncover an interesting connection between linear MDPs and online learning, which could be of independent interest. Finally, the simulation results show that VBMLE significantly outperforms the benchmark method in terms of both empirical regret and computation time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=EOTgj37XNM": {
    "title": "Classifiers are Forgetful! Balancing the Mutual Causal Effects in Class-Incremental Learning",
    "volume": "review",
    "abstract": "Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence. Pre-Trained Models (PTMs) have recently led to breakthroughs in both visual and natural language processing (NLP) tasks. Despite recent studies showing PTMs' potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we reveal that the problem lies in the imbalance effect between new and old data, which leads to the forgetting of classifiers. To alleviate this problem, we propose BaCE, a method retrieving the causal effects from new data to the adaptation of old classes and from old data to the adaptation of new classes. By balancing the causal effect, BaCE enables the causal effects from new and old data mutually help the adaptation to each class. We conduct extensive experiments on three different tasks (Image Classification, Text Classification, and Named Entity Recognition) with various backbones (ResNet-18, ViT, BERT) in the CIL setting. Empirical results show the proposed method outperforms a series of CIL methods on different tasks and settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=nrDRBhNHiB": {
    "title": "A multiobjective continuation method to compute the regularization path of deep neural networks",
    "volume": "review",
    "abstract": "Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=lgmCGI2IpI": {
    "title": "An Efficient Query Strategy for Active Learning via Optimal Transport",
    "volume": "review",
    "abstract": "Active Learning (AL) aims to reduce labeling costs by iteratively querying instances. Existing AL methods typically query instances based on either informativeness or representativeness. Only considering informativeness leads to sample bias. Only considering representativeness leads to query amount of instances before the optimal decision boundary is found. It is essential to consider both when querying instances. However, current hybrid methods are also time-consuming. To query instance efficiently while considering both informativeness and representativeness, we propose an efficient active query strategy based on optimal transport called Active Query by Optimal Transport (AQOT). Optimal Transport (OT) enables us to measure the difference between two distributions efficiently, allowing us considering the distribution of instances easily. Via entropy regularization, we can solve OT efficiently. Specifically, we make use of the sparseness of the solution of OT to querying the most informative instance while considering representativeness. Additionally, we introduce a dynamic adjustment to AQOT. By concatenating AQOT to multiple classification models, we show AQOT is a broad-spectrum active query strategy. Experimental results demonstrate that our method surpasses state-of-the-art active learning methods and shows high efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Gny0PVtKz2": {
    "title": "ConvFormer: Revisiting Token-mixers for Sequential User Modeling",
    "volume": "review",
    "abstract": "Sequential user modeling is essential for building recommender systems, aiming to predict users' subsequent preferences based on their historical behavior. Despite the widespread success of the Transformer architecture in various domains, we observe that its self-attentive token mixer is outperformed by simpler strategies in the realm of sequential user modeling. This observation motivates our study, which aims to revisit and optimize the design of token mixers for this specific application. We start by examining the core building blocks of the self-attentive token mixer, identifying three empirically-validated criteria essential for designing effective token mixers in sequential user models. To validate the utility of these criteria, we develop ConvFormer, a streamlined modification to the Transformer architecture that satisfies the proposed criteria simultaneously. We also present an acceleration technique to handle the computational cost of processing long sequences. Experimental results on four public datasets reveal that even a simple model, when designed in accordance with the proposed criteria, can surpass various complex and delicate solutions, validating the efficacy of the proposed criteria",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=7iCUSBlOgh": {
    "title": "Toward Generalizability of Graph-based Imputation on Bio-Medical Missing Data",
    "volume": "review",
    "abstract": "Recent work on graph-based imputation methods for missing features has garnered significant attention, largely due to the effectiveness of their ability to aggregate and propagate information through graph structures. However, these methods generally assume that the graph structure is readily available and manually mask the original features to simulate the scenario of missing features. This set of assumptions narrows the applicability of such techniques to real-world tabular data, where graph structure is not readily available and missing data is a prevalent issue, such as in cases involving confidential patient information. In light of this situation, and with the aim of enhancing generalizability, we propose GRASS that bridges the gap between recent graph-based imputation methods and real-world scenarios involving missing data in their initial states. Specifically, our approach begins with tabular data and employs a simple Multi-Layer Perceptron (MLP) layer to extract feature gradient, which serves as an additional resource for generating graph structures. Leveraging these gradients, we construct a graph from a feature (i.e., column) perspective and carry out column-wise feature propagation to impute missing values based on their similarity to other features. Once the feature matrix is imputed, we generate a second graph, but this time from a sample-oriented (i.e., row) perspective, which serves as the input for existing graph-based imputation models. We evaluate GRASS using real-world medical and bio-domain datasets, demonstrating their effectiveness and generalizability in handling versatile missing scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=rHzapPnCgT": {
    "title": "Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models",
    "volume": "review",
    "abstract": "Recent work has showcased the significant potential of diffusion models in pose-guided person image synthesis. However, owing to the inconsistency in pose between the source and target images, synthesizing an image with a distinct pose, relying exclusively on the source image and target pose information, remains a formidable challenge. This paper presents Progressive Conditional Diffusion Models (PCDMs) that incrementally bridge the gap between person images under the target and source poses through three stages. Specifically, in the first stage, we design a simple prior conditional diffusion model that predicts the global features of the target image by mining the global alignment relationship between pose coordinates and image appearance. Then, the second stage establishes a dense correspondence between the source and target images using the global features from the previous stage, and an inpainting conditional diffusion model is proposed to further align and enhance the contextual features, generating a coarse-grained person image. In the third stage, we propose a refining conditional diffusion model to utilize the coarsely generated image from the previous stage as a condition, achieving texture restoration and enhancing fine-detail consistency. The three-stage PCDMs work progressively to generate the final high-quality and high-fidelity synthesized image. Both qualitative and quantitative results demonstrate the consistency and photorealism of our proposed PCDMs under challenging scenarios. Our code and models will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=d2TOOGbrtP": {
    "title": "Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions",
    "volume": "review",
    "abstract": "Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distributions, including the invariant posterior and the posteriors on training domains. Furthermore, we develop a lite version of PTG for widespread applications. PTG shows competitive performance on various domain generalization benchmarks on DomainBed. Additionally, PTG can use any existing domain generalization methods as its prior, and combined with previous state-of-the-art method the performance can be further improved. Code will be made public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=XaqaitclOA": {
    "title": "Investigating the Ability of PINNs To Solve Burgers' PDE Near Finite-Time BlowUp",
    "volume": "review",
    "abstract": "Physics Informed Neural Networks (PINNs) have been achieving ever newer feats of solving complicated PDEs numerically while offering an attractive trade-off between accuracy and speed of inference. A particularly challenging aspect of PDEs is that there exist simple PDEs which can evolve into singular solutions in finite time starting from smooth initial conditions. In recent times some striking experiments have suggested that PINNs might be good at even detecting such finite-time blow-ups. In this work, we embark on a program to investigate this stability of PINNs from a rigorous theoretical viewpoint. Firstly, we derive generalization bounds for PINNs for Burgers' PDE, in arbitrary dimensions, under conditions that allow for a finite-time blow-up. Then we demonstrate via experiments that our bounds are significantly correlated to the $\\ell_2$-distance of the neurally found surrogate from the true blow-up solution, when computed on sequences of PDEs that are getting increasingly close to a blow-up",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ZWxBU9sYG": {
    "title": "How to Craft Backdoors with Unlabeled Data Alone?",
    "volume": "review",
    "abstract": "Relying only on unlabeled data, Self-supervised learning (SSL) can learn rich features in an economical and scalable way. As the drive-horse for building foundation models, SSL has received a lot of attention recently with wide applications, which also raises security concerns where backdoor attack is a major type of threat: if the released dataset is maliciously poisoned, backdoored SSL models can behave badly when triggers are injected to test samples. The goal of this work is to investigate this potential risk. We notice that existing backdoors all require a considerable amount of *labeled* data that may not be available for SSL. To circumvent this limitation, we explore a more restrictive setting called no-label backdoors, where we only have access to the unlabeled data alone, where the key challenge is how to select the proper poison set without using label information. We propose two strategies for poison selection: clustering-based selection using pseudolabels, and contrastive selection derived from the mutual information principle. Experiments on CIFAR-10 and ImageNet-100 show that both no-label backdoors are effective on many SSL methods and outperform random poisoning by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Pa6SiS66p0": {
    "title": "Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning",
    "volume": "review",
    "abstract": "While humans excel at continual learning (CL), deep neural networks (DNNs) exhibit catastrophic forgetting. A salient feature of the brain that allows effective CL is that it utilizes multiple modalities for learning and inference, which is underexplored in DNNs. Therefore, we study the role and interactions of multiple modalities in mitigating forgetting and introduce a benchmark for multi-modal continual learning. Our findings demonstrate that leveraging multiple views and complementary information from multiple modalities enables the model to learn more accurate and robust representations of the objects that are less vulnerable to modality-specific regularities and considerably mitigates forgetting. Furthermore, we observe that individual modalities exhibit varying degrees of robustness to distribution shift. Finally, we propose a method for integrating and aligning the information from different modalities by utilizing the relational structural similarities between the data points in each modality. Our method sets a strong baseline that enables both single- and multimodal inference. Our study provides a promising case for further exploring the role of multiple modalities in enabling CL and provides a standard benchmark for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Xd46Q82QEO": {
    "title": "Exploring Pointwise Similarity of Representations",
    "volume": "review",
    "abstract": "Representation similarity measures have emerged as a popular tool for examining learned representations. Many existing studies have focused on analyzing aggregate estimates of similarity at a global level, i.e. over a set of representations for N input examples. In this work, we shed light on the importance of investigating similarity of representations at a local level, i.e. representations of a single input example. We show that peering through the lens of similarity of individual data points can reveal previously overlooked phenomena in deep learning. Specifically, we investigate the similarity in learned representations of inputs by architecturally identical models that only differ in random initialization. We find that while standard models represent (most) inputs similarly only when they are drawn from training data distribution, adversarially trained models represent a wide variety of out-of-distribution inputs similarly, thus indicating that these models learn more \"stable\" representations. We design an instantiation of such a pointwise measure, named Pointwise Normalized Kernel Alignment (PNKA), that provides a way to quantify the similarity of an individual point across distinct representation spaces. Using PNKA, we additionally show how we can further understand the effects of data (e.g. corruptions) and model (e.g. fairness constraints) interventions on the model's representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tB7p0SM5TH": {
    "title": "GraSP: Simple yet Effective Graph Similarity Predictions",
    "volume": "review",
    "abstract": "Graph similarity computation (GSC) is considered one of the essential operations because of its wide range of applications in various fields. Graph Edit Distance (GED) and Maximum Common Subgraph (MCS) are the most popular graph similarity metrics. However, calculating exact GED and MCS is a complex task that falls under the category of NP-hard problems. Consequently, state-of-the-art methodologies learn data-driven models leveraging graph neural networks (GNNs) for estimating GED and MCS values. A perceived limitation of these approaches includes reliance on computationally expensive cross-graph node-level interaction components but to little avail. Instead of building up complicated components, we aim to make the complicated simple and present GraSP, a simple yet highly effective approach for GSC. In particular, to achieve higher expressiveness, we design techniques to enhance node features via positional encoding, employ a graph neural network backbone with a gating mechanism and residual connections, and develop a multi-scale pooling technique to generate meaningful representations. We theoretically prove that our method is more expressive and passes 1-WL test performance capabilities. Notably, GraSP is versatile in accurately predicting GED and MCS metrics. In extensive experiments against numerous competitors on real-world datasets, we demonstrate the superiority of GraSP over prior arts regarding effectiveness and efficiency. The source code is available at https://anonymous.4open.science/r/GraSP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=JGP1GlTnLF": {
    "title": "Learning from Distinction: Mitigating backdoors using a low-capacity model",
    "volume": "review",
    "abstract": "Deep neural networks (DNNs) are susceptible to backdoor attacks due to their black-box nature and lack of interpretability. Backdoor attacks intend to manipulate the model's prediction when hidden backdoors are activated by predefined triggers. Although considerable progress has been made in backdoor detection and removal at the model deployment stage, an effective defense against backdoor attacks during the training time is still under-explored. In this paper, we propose a novel training-time backdoor defense method called Learning from Distinction (LfD), allowing training a backdoor-free model on the backdoor-poisoned data. LfD uses a low-capacity model as a teacher to guide the learning of a backdoor-free student model via a dynamic weighting strategy. Extensive experiments on CIFAR-10, GTSRB and ImageNet-subset datasets show that LfD significantly reduces attack success rates by $0.52\\%$, $11.31\\%$ and $1.42\\%$, respectively, with minimal impact on clean accuracy (less than $1$%, $3$% and $1$%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=1dY11GyZdp": {
    "title": "Signed-Binarization: Unlocking Efficiency Through Repetition-Sparsity Trade-Off",
    "volume": "review",
    "abstract": "Efficient inference of Deep Neural Networks (DNNs) on resource-constrained edge devices is essential. Quantization and sparsity are key algorithmic techniques that translate to repetition and sparsity within tensors at the hardware-software interface. This paper introduces the concept of repetition-sparsity trade-off that helps explain computational efficiency during inference. We propose Signed Binarization, a unified co-design framework that synergistically integrates hardware-software systems, quantization functions, and representation learning techniques to address this trade-off. Our results demonstrate that Signed Binarization is more accurate than binary models with the same number of non-zero weights. Detailed analysis indicates that signed binarization generates a smaller distribution of effectual (non-zero) parameters nested within a larger distribution of total parameters, both of the same type, for a DNN block. Finally, our approach achieves a 26\\% speedup on real hardware, doubles energy efficiency, and reduces density by 2.8x compared to binary methods for ResNet 18, presenting an alternative solution for deploying efficient models in resource-limited environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=dxI1HLatWw": {
    "title": "Generalized Temporal Difference Learning Models for Supervised Learning",
    "volume": "review",
    "abstract": "In conventional statistical learning settings, data points are typically assumed to be independently and identically distributed (i.i.d.) according to some unknown probability distribution. Various supervised learning algorithms, such as generalized linear models, are derived by making different assumptions about the conditional distribution of the response variable given the independent variables. In this paper, we propose an alternative formulation in which data points in a typical supervised learning dataset are treated as interconnected, and we model the data sampling process by a Markov reward process. Accordingly, we view the original supervised learning problem as a classic on-policy policy evaluation problem in reinforcement learning, and introduce a generalized temporal difference (TD) learning algorithm to address it. Theoretically, we establish the convergence of our generalized TD algorithms under linear function approximation. We then explore the relationship between TD's solution and the original linear regression solution. This connection suggests that the probability transition matrix does not significantly impact optimal solutions in practice and hence can be easy to design. In our empirical evaluations, we examine critical designs of our generalized TD algorithm, and demonstrate the competitive generalization performance across a variety of benchmark datasets, including regression, binary classification, and image classification within a deep learning context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=N134PpnlKs": {
    "title": "Twinned Interventional Flows",
    "volume": "review",
    "abstract": "Real-world problems in continuously evolving settings, such as predicting the efficacy of medical treatment, often require estimating the causal effects of interventions. Issues such as irregularly-sampled and missing data, unobserved factors, and ethical concerns make such settings especially challenging. The existing methodology relies on low-dimensional embeddings, potentially incurring information loss. We circumvent this limitation with a novel approach ``twinning\" that augments the partial observations with additional latent variables and appeals to conditional continuous normalizing flows to model the system dynamics, obtaining accurate density estimates. We also introduce a new approach to overcome a key technical challenge, namely, mitigating stiffness of the underlying neural ODE. The model provably benefits from auxiliary non-interventional data during training. We showcase the flexibility of the proposed method with tasks like anomaly detection and counterfactual prediction, and benchmark on standard reinforcement learning (Half-Cheetah) and treatment effect prediction (tumor growth) contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1yll8U12GT": {
    "title": "Enhancing Decision Tree Learning with Deep Networks",
    "volume": "review",
    "abstract": "Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric. We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=CJnyR3M6Oh": {
    "title": "Sparse hyperbolic representation learning",
    "volume": "review",
    "abstract": "Minimizing the space complexity of entity representations without the loss of information makes data science procedures computationally efficient and effective. For the entities with the tree structure, hyperbolic-space-based representation learning (HSBRL) has successfully reduced the space complexity of representations by using low-dimensional space. Nevertheless, it has not minimized the space complexity of each representation since it has used the same dimension for all representations and has not selected the best dimension for each representation. This paper, for the first time, constructs a sparse learning scheme to minimize the dimension for each representation in HSBRL. The most significant difficulty is that we cannot construct a well-defined sparse learning scheme for HSBRL based on a coordinate system since there is no canonical coordinate system that reflects geometric structure perfectly, unlike in linear space. Forcibly applying a linear sparse learning method on a coordinate system of hyperbolic space causes a non-uniform sparsity. Another difficulty is that existing Riemannian gradient descent cannot reach a sparse solution since the algorithm oscillates on a non-smooth function, which is essential in sparse learning. To overcome the above issue, for the first time, we geometrically define the sparseness and sparse regularization in hyperbolic space, to achieve geometrically uniform sparsity. Also, we propose the first optimization algorithm that can avoid the oscillation problem and obtain sparse representations in hyperbolic space by the geometric shrinkage-thresholding idea",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=i43XCU54Br": {
    "title": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization",
    "volume": "review",
    "abstract": "Large language model (LLM) agents have been shown effective on a wide range of tasks, and by ensembling multiple LLM agents, their performances could be further improved. Existing approaches employ a fixed set of agents to interact with each other in a static architecture, which limits their generalizability to various tasks and requires strong human prior in designing these agents. In this work, we propose to construct a strategic team of agents communicating in a dynamic interaction architecture based on the task query. Specifically, we build a framework named Dynamic LLM-Agent Network (**DyLAN**) for LLM-agent collaboration on complicated tasks like reasoning and code generation. DyLAN enables agents to interact for multiple rounds in a dynamic architecture with inference-time agent selection and an early-stopping mechanism to improve performance and efficiency. We further design an automatic agent team optimization algorithm based on an unsupervised metric termed *Agent Importance Score*, enabling the selection of best agents based on the contribution each agent makes. Empirically, we demonstrate that DyLAN performs well in both reasoning and code generation tasks with reasonable computational cost. DyLAN achieves 13.0\\% and 13.3\\% improvement on MATH and HumanEval, respectively, compared to a single execution on GPT-35-turbo. On specific subjects of MMLU, agent team optimization in DyLAN increases accuracy by up to 25.0\\%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=x6gnuUXpxM": {
    "title": "Constructing Sparse Neural Architecture with Deterministic Ramanujan Graphs",
    "volume": "review",
    "abstract": "We present a sparsely connected neural network architecture constructed using the theory of Ramanujan graphs which provide comparable performance to a dense network. The method can be considered as a before-training, deterministic, weight free, pruning at initialization (PaI) technique. The deterministic Ramanujan graphs occur either as Cayley graphs of certain algebraic groups or as Ramanujan $r$-coverings of the full $(k,l)$ bi-regular bipartite graph on $k + l$ vertices. Sparse networks are constructed for bipartite graphs representing both the convolution and the fully connected layers. We experimentally show that the proposed sparse architecture provides comparable accuracy with a lower sparsity ratio than those achieved by previous approaches based on non-deterministic methods for benchmark datasets. In addition, they retain other desirable properties such as path connectivity and symmetricity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFQe52N40m": {
    "title": "Online Feature Updates Improve Online (Generalized) Label Shift Adaptation",
    "volume": "review",
    "abstract": "This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we delve into the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel Online Label Shift adaptation with Online Feature Updates (OLS-OFU) method harnesses self-supervised learning to refine the feature extraction process, thus improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical tests on CIFAR-10 and CIFAR-10C datasets, under both online label shift and generalized label shift conditions, underscore OLS-OFU's effectiveness and robustness, especially in cases of domain shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=8p3fu56lKc": {
    "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention",
    "volume": "review",
    "abstract": "Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity (Akyurek et al., 2023), while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective (von Oswald et al., 2022). However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian distribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss now implements a single step of $\\textit{pre-conditioned}$ GD. However, if only the distribution of the responses is changed, then this does not have a large effect on the learned algorithm: even when the response comes from a more general family of $\\textit{nonlinear}$ functions, the global minimizer of the pre-training loss still implements a single step of GD on a least-squares linear regression objective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yYylDyLnzt": {
    "title": "Dantzig-Wolfe Decomposition and Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "The 3D bin packing problem is an NP-hard optimisation problem. RL solutions found in the literature tackle simplified versions of the full problem due to its large action space and long episode lengths. This work uses a Danzig-Wolfe formulation to decompose the full problem into a set partition and 3D knapsack problem. The RL agent is used to solve the 3D knapsack problem and CPLEX (a mixed integer linear programming solver) is used to solve the set partition problem. This removes the bin selection action from the action space of the agent and reduces the episode length to be only the number of items required to fill 1 bin rather than all items in the inference. We thereby simplify the learning problem compared to the full 3D bin-packing case. The trained agent is used at inference time to iteratively generate columns of the Danzig-Wolfe formulation using the column generation procedure. This algorithm provided improved solutions on up to 28/47 instances compared to those obtained by successively applying the RL agent to optimize volume occupation in a bin with the remaining items. RL solutions alone cannot provide valid lower bounds for solutions. This work also uses the Danzig-Wolfe formulation and column generation to improve on existing SOTA lower bounds by replacing the RL agent with an integer linear program for the 3D knapsack problem. An improved lower bound compared to SOTA was found on 17/47 instances by using CPLEX to solve both master and sub-problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KIq6p9iv2q": {
    "title": "Towards Perpetually Trainable Neural Networks",
    "volume": "review",
    "abstract": "Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly inoccuous assumption: that the networkis trained on a stationary data distribution. In settings where this assumption is violated, e.g. deep reinforcement learning, learning algorithms become unstableand brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. In this paper, we conduct a thorough analysis of the mehcnaisms of plasticity loss in neural networks trained on nonstationary learning problems, identify solutions to each of these pathologies, and integrate these solutions into a straightforward training protocol designed to maintain plasticity. We validate this approach in a variety of synthetic continual learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=KskgLM728l": {
    "title": "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints",
    "volume": "review",
    "abstract": "The ever-growing biomedical publications magnify the challenge of extracting structured data from unstructured texts. This task involves two components: biomedical entity identification (Named Entity Recognition) and their interrelation determination (Relation Extraction). However, pre-existing methods often neglect unique features of the biomedical literature, such as ambiguous entities, nested proper nouns, and overlapping relation triplets, and underutilize prior knowledge, leading to an intolerable performance decline in the biomedical domain, especially with limited annotated training data. In this paper, we propose the **Bio**medical **R**elation-**F**irst E**X**traction (Bio-RFX) model by leveraging sentence-level relation classification before entity extraction to tackle entity ambiguity. Moreover, we exploit structural constraints between entities and relations to guide the model's hypothesis space, enhancing extraction performance across different training scenarios. Comprehensive experiments on multiple biomedical datasets show that Bio-RFX achieves significant improvements on both named entity recognition and relation extraction tasks, especially under low-resource training scenarios, achieving a remarkable **5.13%** absolute improvement on average in NER, and **7.20%** absolute improvement on average in RE compared to baselines. The source code and pertinent documentation are readily accessible on established open-source repositories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ZS6lgCLr2B": {
    "title": "Tackling Byzantine Clients in Federated Learning",
    "volume": "review",
    "abstract": "The possibility of adversarial (a.k.a., {\\em Byzantine}) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\\mathsf{FedAvg}$ algorithm by a \\emph{robust averaging rule}. While a significant amount of work has been devoted to studying the convergence of federated {\\em robust averaging} (which we denote by $\\mathsf{FedRo}$), prior work has largely ignored the impact of {\\em client subsampling} and {\\em local steps}, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis of $\\mathsf{FedRo}$ with two-sided step-sizes, tightly analyzing the impact of client subsampling and local steps. Specifically, we present a sufficient condition on client subsampling for nearly-optimal convergence of $\\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show that the rate of improvement in learning accuracy {\\em diminishes} with respect to the number of clients subsampled, as soon as the sample size exceeds a threshold value. Interestingly, we also observe that under a careful choice of step-sizes, the learning error due to Byzantine clients decreases with the number of local steps. We validate our theory by experiments on the FEMNIST image classification task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=dVq2StlcnY": {
    "title": "Interpretable and Generalizable Graph Neural Networks via Subgraph Multilinear Extension",
    "volume": "review",
    "abstract": "Interpretable graph neural networks (XGNNs) are widely adopted in scientific applications involving graph-structured data. Previous approaches predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, which we term as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability measures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=om5z1n0mXA": {
    "title": "Rethinking the Effectiveness of Graph Classification Datasets in Benchmarks for Assessing GNNs",
    "volume": "review",
    "abstract": "Graph classification benchmarks, vital for assessing and developing graph neural network (GNN) models, have recently been scrutinized, as simple methods like MLPs have demonstrated comparable performance on certain datasets. This leads to an important question: Do these benchmarks effectively distinguish the advancements of GNNs over other methodologies? If so, how do we quantitatively measure this effectiveness? In response, we propose an empirical protocol based on a fair benchmarking framework to investigate the performance discrepancy between simple methods and GNNs. We further propose a novel metric to quantify the effectiveness of a dataset by utilizing the performance gaps and considering dataset complexity. Through extensive testing across 16 real-world datasets, we found our metric to align with existing studies and intuitive assumptions. Finally, to explore the causes behind the low effectiveness, we investigated the relationship between intrinsic graph properties and task labels and developed a novel technique for generating more synthetic datasets that can precisely control these correlations. Our findings shed light on the current understanding of benchmark datasets, and our new platform backed by an effectiveness validation protocol could fuel the future evolution of graph classification benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=MBIGXMT0qC": {
    "title": "Multi-Scale Protein Language Model for Unified Molecular Modeling",
    "volume": "review",
    "abstract": "Protein language models have shown great potential in protein engineering. However, the current protein language models mainly work in the residue scale, which cannot offer information in the atom scale. The strong power of protein language models could not be fully exploited to benefit the applications that cross protein and small molecules. In this paper, we propose msESM(multi-scale ESM) to realize the multi-scale unified molecular modeling by pre-training on multi-scale code-switch protein sequence and describing relationships among residues and atoms with a multi-scale position encoding. Experimental results show that msESM outperforms previous methods in protein-molecule tasks and is on par with the state-of-the-art in protein-only and molecule-only tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJ2PQ9QaDF": {
    "title": "Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data",
    "volume": "review",
    "abstract": "Modern deep learning models are usually highly over-parameterized so that they can overfit the training data. Surprisingly, such overfitting neural networks can usually still achieve high prediction accuracy. To study this ``benign overfitting'' phenomenon, a line of recent works has theoretically studied the learning of linear models and two-layer neural networks. However, most of these analyses are still limited to the very simple learning problems where the Bayes-optimal classifier is linear. In this work, we investigate a class of XOR-type classification tasks with label-flipping noises. We show that, under a certain condition on the sample complexity and signal-to-noise ratio, an over-parameterized ReLU CNN trained by gradient descent can achieve near Bayes-optimal accuracy. Moreover, we also establish a matching lower bound result showing that when the previous condition is not satisfied, the prediction accuracy of the obtained CNN is an absolute constant away from the Bayes-optimal rate. Our result demonstrates that CNNs have a remarkable capacity to efficiently learn XOR problems, even in the presence of highly correlated features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=8euJaTveKw": {
    "title": "Prometheus: Inducing Evaluation Capability in Language Models",
    "volume": "review",
    "abstract": "Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset's versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=fMzO6vcmhy": {
    "title": "QORA: Zero-Shot Transfer via Interpretable Object-Relational Model Learning",
    "volume": "review",
    "abstract": "Although neural networks have demonstrated significant success in various reinforcement-learning tasks, even the highest-performing deep models often fail to generalize. As an alternative, object-oriented approaches offer a promising path towards better efficiency and generalization; however, they typically address narrow problem classes and require extensive domain knowledge. To overcome these limitations, we introduce *QORA*, an algorithm that constructs models expressive enough to solve a variety of domains, including those with stochastic transition functions, directly from a domain-agnostic object-based state representation. We also provide a novel benchmark suite to evaluate learners' generalization capabilities. In our test domains, QORA achieves 100% predictive accuracy using almost four orders of magnitude fewer observations than a neural-network baseline, demonstrates zero-shot transfer to modified environments, and adapts rapidly when applied to tasks involving previously unseen object interactions. Finally, we give examples of QORA's learned rules, showing them to be easily interpretable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Lxc4nBkJuq": {
    "title": "Dissecting Gradient Masking and Denoising in Diffusion Models for Adversarial Purification",
    "volume": "review",
    "abstract": "Diffusion models exhibit remarkable empirical robustness in adversarial purification. The mechanisms underlying such improvements remain unclear. It is possible that diffusion models effectively purify the adversarial examples via the learned stimuli prior. Alternatively, the substantial randomness added in the diffusion models may cause gradient masking that contaminates the empirical estimate of adversarial robustness. Here, we seek to dissect the contribution of these two potential factors. Theoretically, we illustrate how a purification system with randomness can cause gradient masking, which can not be addressed by the standard expectation-over-time (EOT) method. Inspired by this, we propose and justify that a simple procedure, randomness replay, can provide a better robustness estimate when randomness is involved. Experimentally, we verify that gradient masking indeed happens under previous evaluations of diffusion models. After properly controlling the effect of randomness, the reverse-only diffusion model (RevPure) provides a better robustness improvement than the previous DiffPure framework, suggesting that the robustness improvement is solely attributed to the reverse process. Furthermore, our analyses reveal that robustness improvement is caused by a sequential denoising mechanism that transforms the stimulus to a direction orthogonal to the original adversarial perturbation, rather than reducing the $\\ell_2$ distance between the transformed and clean stimuli. Our results shed new light on the mechanisms underlying the empirical robustness from diffusion models, and shall inform future development of more efficient adversarial purification systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xuY33XhEGR": {
    "title": "ClimODE: Climate Forecasting With Physics-informed Neural ODEs",
    "volume": "review",
    "abstract": "Climate prediction traditionally relies on complex numerical simulations of atmospheric physics. Deep learning approaches, such as transformers, have recently challenged the simulation paradigm with complex network forecasts. However, they often act as data-driven black-box models that neglect the underlying physics and lack uncertainty quantification. We address these limitations with ClimODE, a spatiotemporal continuous-time process that implements a key principle of advection from statistical mechanics, namely, weather changes due to a spatial movement of quantities over time. ClimODE models precise weather evolution with value-conserving dynamics, learning global weather transport as a neural flow, which also enables estimating the uncertainty in predictions. Our approach outperforms existing data-driven methods in global and regional forecasting with an order of magnitude smaller parameterization, establishing a new state of the art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=THUBTfSAS2": {
    "title": "Querying Easily Flip-flopped Samples for Deep Active Learning",
    "volume": "review",
    "abstract": "Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the *least disagree metric* (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Experimental results show that our LDM-based active learning algorithm obtains state-of-the-art *overall* performance on all considered datasets and deep architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=DKfcxPxunu": {
    "title": "Multi-Task Learning for Routing Problem with Zero-Shot Generalization",
    "volume": "review",
    "abstract": "Vehicle routing problems (VRPs) are widely studied due to their significant practical importance. In the last decade, leveraging neural networks to solve VRPs in an end-to-end manner has gained substantial research attention. However, current works require building separate neural models for each routing problem, which hinders its practicality in solving diverse problems. In this study, we treat the VRPs as different combinations of a set of shared underlying attributes and propose to solve them simultaneously as multi-task learning. By training a unified model on multiple VRPs with varying attributes, we can effectively solve unseen problems in a zero-shot manner. Our experimental results on eleven VRPs show that our unified model performs comparably to single-task models trained specifically for each problem. More importantly, our model exhibits promising zero-shot generalization to new VRPs, reducing the average gap to 4.6\\% and 7.0\\% for sizes 50 and 100, respectively, compared to over 20\\% in the single-task approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FDb2JQZsFH": {
    "title": "Attention-based Iterative Decomposition for Tensor Product Representation",
    "volume": "review",
    "abstract": "In recent research, Tensor Product Representation (TPR) is applied for the systematic generalization task of deep neural networks by learning the compositional structure of data. However, such prior works show limited performance in discovering and representing the symbolic structure from unseen test data because of the incomplete bindings to the structural representations. In this work, we propose an Attention-based Iterative Decomposition (AID) module that can effectively improve the binding for the structured representations encoded from the sequential input features with TPR. Our AID can be easily adapted to any TPR-based model and provides enhanced systematic decomposition through a competitive attention mechanism between input features and structured representations. In our experiments, AID shows effectiveness by significantly improving the performance of TPR-based prior works on the series of systematic generalization tasks. Moreover, in the quantitative and qualitative evaluations, AID produces more compositional and well-bound structural representations than other works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=er7VhmqZEA": {
    "title": "NOISY MULTI-VIEW CONTRASTIVE LEARNING FRAMEWORK FOR ENHANCING TOP-K RECOMMENDATION",
    "volume": "review",
    "abstract": "Recommender systems have become an essential component of various online plat- forms, providing personalized recommendations to users. Collaborative filtering- based methods, such as matrix factorization, have been widely used to capture latent user-item preferences. Recently, graph-based methods have shown promising results by modeling the interactions between users and items as a graph and lever- aging knowledge graphs (KG) to learn the user and item embeddings. Motivated by the recent success of contrastive learning in mining supervised signals from data itself, in this paper, we focus on establishing a noisy contrastive learning framework in Knowledge-aware recommendation systems and propose a self-supervised novel noisy multi-view contrastive learning framework for improving top-K recommen- dation. In this paper, we propose a novel recommendation system architecture that generates three different views of user-item interactions for improved recommenda- tion along with a noise addition module. The global-level structural view leverages attention-based aggregation network Wang et al. (2019d) to capture collaborative information in the entity-item-user graph. In the item-item semantic view, we use a K-nearest Neighbour item-item semantic module to incorporate semantic relations among items. In the local view, we apply LightGCN He et al. (2020) with noisy perturbations to generate robust user-item representations. We then use two more signals such as representation loss and uniformity loss in positive pairs to improve the quality of the representations and ensure uniform representations in the representational space. Experimental results on two benchmark datasets demonstrate that our proposed method achieves superior performance compared to state-of-the-art methods. Additionally, we conducted extensive experiments on CTR task-based datasets to demonstrate the robustness of our framework's generalization in learning better user-item representations which can be seen in the supplementary material. All the codes to generate reproducible results are available in this anonymous repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzAuFCKiov": {
    "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behaviors without aligning with human values. The dominant approach for steering LLMs towards beneficial behaviors involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexities in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pairwise Proximal Policy Optimization (P3O) that operates directly on comparative rewards. We theoretically show that P3O is invariant to equivalent rewards and avoids the complexities of PPO. Empirical evaluations demonstrate that P3O outperforms PPO in the KL-Reward trade-off and can align with human preferences as well as or better than prior methods. In summary, this work introduces a simpler yet effective approach for aligning LLMs to human preferences through relative feedback",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SKfBx2rv2c": {
    "title": "Feasible Algorithmic Recourse Without Explicit Structure Prior",
    "volume": "review",
    "abstract": "To ensure that vulnerable end-users have a clear understanding of decisions made by black-box models, algorithmic recourse has made significant progress by identifying small changes in input features that can alter predictions. However, the recoursed examples in real-world scenarios are only feasible and actionable for end-users if they preserve the realistic constraints among input features. Previous works have highlighted the importance of incorporating causality into algorithmic recourse to capture these constraints as causal relationships. Existing methods often rely on inaccessible prior Structural Causal Models (SCMs) or complete causal graphs. To maintain the causal relationships without such prior knowledge, we propose a novel approach that focuses on identifying and constraining the variation of exogenous noise by leveraging recent advancements in non-linear Independent Component Analysis (ICA). Based on this idea, we introduce two methods: Algorithmic Recourse with L2 norm (AR-L2) and Algorithmic Recourse with Nuclear norm (AR-Nuc). Experimental results on synthetic, semi-synthetic, and real-world data demonstrate the effectiveness of our proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RR70yWYenC": {
    "title": "Efficient Instance-Optimal Finite-Sum Minimization",
    "volume": "review",
    "abstract": "Given a sequence of functions $f_1,\\ldots,f_n$ with $f_i:\\mathcal{D}\\mapsto \\mathbb{R}$, finite-sum minimization seeks a point ${x}^\\star \\in \\mathcal{D}$ minimizing $\\sum_{j=1}^nf_j(x)/n$. In this work, we propose a key twist into the finite-sum minimization, dubbed as *instance-optimal finite-sum minimization*, that asks for a sequence of points $x_1^\\star, \\ldots, x_n^\\star \\in D$ such that each ${x}^\\star_i \\in D$ minimizes the prefix-sum $\\sum_{j=1}^if_j(x)/i$. Assuming that each prefix-sum is strongly convex, we develop a first-order stochastic instance optimal gradient method $\\mathrm{SIOPT}-\\mathrm{Grad}$ producing an $\\epsilon$-optimal sequence with $\\tilde{\\mathcal{O}}(n/\\epsilon^{1/3} + 1/\\sqrt{\\epsilon})$ overall *first-order oracles* (FO). An FO corresponds to the computation of a single gradient $\\nabla f_j(x)$ at a given $x \\in \\mathcal{D}$ for some $j \\in [n]$. Our approach significantly improves upon the $\\mathcal{O}(n/\\epsilon)$ FOs that $\\mathrm{StochasticGradientDescent}$ requires and the $\\mathcal{O}(n^2 \\log (1/\\epsilon))$ FOs that state-of-the-art variance reduction methods such as $\\mathrm{Katyusha}$ require. We also prove that there is no natural first-order method with $\\mathcal{O}\\left(n/\\epsilon^\\alpha\\right)$ gradient complexity for $\\alpha < 1/4$, establishing that the first-order complexity of our method is nearly tight",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2iGiSHmeAN": {
    "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics",
    "volume": "review",
    "abstract": "Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=BIglOUjfXX": {
    "title": "Forked Diffusion for Conditional Graph Generation",
    "volume": "review",
    "abstract": "We introduce a novel score-based diffusion framework that incorporates forking for conditional generation. In this framework, a single parent diffusion process is associated with a primary variable (e.g., structure), while multiple child diffusion processes are employed, each dedicated to a dependent variable (e.g., property). The parent process guides the co-evolution of its child processes towards segregated representation spaces. This approach allows our models to manage conditional information flow effectively, uncover intricate interactions and dependencies, and ultimately unlock new generative capabilities. Our experimental results demonstrate the significant superiority of our method over contemporary baselines in the context of conditional graph generation, highlighting the potential of forking diffusion for enhancing conditional generation tasks and inverse molecular design tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6WtaLj8O1": {
    "title": "Fully Hyperbolic Representation Learning on Knowledge Hypergraph",
    "volume": "review",
    "abstract": "Knowledge hypergraphs generalize knowledge graphs in terms of utilizing hyperedges to connect multiple entities and represent complicated relations within them. Existing methods either transform hyperedges into an easier to handle set of binary relations or view hyperedges as isolated and ignore their adjacencies. Both approaches have information loss and may lead to sub-optimal models. To fix these issues, we propose the Hyperbolic Hypergraph GNN (H2GNN), whose essential part is the hyper-star message passing, a novel scheme motivated by a lossless expansion of hyperedges into hierarchies, and implement a direct embedding which explicitly takes adjacent hyperedges and entity positions into account. As the name suggests, H2GNN works in the fully hyperbolic space, which can further reduce distortion and boost efficiency. We compare H2GNN with 15 baselines on both homogeneous and heterogeneous knowledge hypergraphs, and it outperforms state-of-the-art approaches in both node classification and link prediction tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=95joD3Yc5t": {
    "title": "Generative Semantic Communication: Diffusion Models Beyond Bit Recovery",
    "volume": "review",
    "abstract": "Semantic communication is expected to be one of the cores of next-generation AI-based communications. One of the possibilities offered by semantic communication is the capability to regenerate, at the destination side, images or videos semantically equivalent to the transmitted ones, without necessarily recovering the transmitted sequence of bits. The current solutions still lack the ability to build complex scenes from the received partial information. Clearly, there is an unmet need to balance the effectiveness of generation methods and the complexity of the transmitted information, possibly taking into account the goal of communication. In this paper, we aim to bridge this gap by proposing a novel generative diffusion-guided framework for semantic communication that leverages the strong abilities of diffusion models in synthesizing multimedia content while preserving semantic features. Concurrently, we propose a novel strategy to make diffusion models resilient to corrupted conditioning data, avoiding that heavily noise-affected conditioning may mislead the generation process. We reduce bandwidth usage by sending highly-compressed semantic information only. Then, the diffusion model learns to synthesize semantic-consistent scenes from such semantic information. We prove, through an in-depth assessment of multiple scenarios, that our method outperforms existing solutions in generating high-quality images with preserved semantic information even in cases where the received conditioning content is significantly degraded. More specifically, our results show that objects, locations, and depths are still recognizable even in the presence of extremely noisy conditions of the communication channel",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FFvCjbhpDq": {
    "title": "The Role of Forgetting in Fine-Tuning Reinforcement Learning Models",
    "volume": "review",
    "abstract": "Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning pre-trained reinforcement learning (RL) agents remains a challenge. This work conceptualizes one specific cause of poor transfers in the RL setting: *forgetting of pre-trained capabilities*. Namely, due to the distribution shift between the pre-training and fine-tuning data, the pre-trained model can significantly deteriorate before the agent reaches parts of the state space known by the pre-trained policy. In many cases, re-learning the lost capabilities takes as much time as learning them from scratch. We identify conditions when this problem occurs, perform a thorough analysis, and identify potential solutions. Namely, we propose to counteract deterioration by applying techniques that mitigate forgetting. We experimentally confirm this to be an efficient solution; for example, it allows us to significantly improve the fine-tuning process on Montezuma's Revenge as well as on the challenging NetHack domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ki39vo5x1T": {
    "title": "Federated Offline Policy Learning with Heterogeneous Observational Data",
    "volume": "review",
    "abstract": "We consider the problem of learning personalized decision policies from observational bandit feedback data across multiple heterogeneous data sources. Moreover, we examine the practical considerations of this problem in the federated setting where a central server aims to train a policy on data distributed across the heterogeneous sources, or clients, without collecting any of their raw data. We present a policy learning algorithm amenable to federation based on the aggregation of local policies trained with doubly robust offline policy evaluation and learning strategies. We provide a novel regret analysis for our approach that establishes a finite-sample upper bound on a notion of global regret against a mixture distribution of clients. In addition, for any individual client, we establish a corresponding local regret upper bound characterized by measures of relative distribution shift to all other clients. Our analysis and supporting experimental results provide insights into tradeoffs in the participation of heterogeneous data sources in policy learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=5jWsW08zUh": {
    "title": "Some Intriguing Aspects about Lipschitz Continuity of Neural Networks",
    "volume": "review",
    "abstract": "Lipschitz continuity is a crucial functional property of any predictive model, that naturally governs its robustness, generalisation, as well as adversarial vulnerability. Contrary to other works that focus on obtaining tighter bounds and developing different practical strategies to enforce certain Lipschitz properties, we aim to thoroughly examine and characterise the Lipschitz behaviour of Neural Networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, datasets, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. As a highlight of this investigation, we showcase a remarkable fidelity of the lower Lipschitz bound, identify a striking Double Descent trend in both upper and lower bounds to the Lipschitz and explain the intriguing effects of label noise on function smoothness and generalisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=OVPoEhbsDm": {
    "title": "Thermodynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling",
    "volume": "review",
    "abstract": "Modeling protein-protein interactions (PPI) represents a central challenge within the field of biology, and accurately predicting the consequences of mutations in this context is crucial for various applications, such as drug design and protein engineering. Recent advances in deep learning (DL) have shown promise in forecasting the effects of such mutations. However, the effectiveness of these models is hindered by two primary constraints. First and foremost, obtaining the structures of mutant proteins is a persistent challenge, as they are often elusive to acquire. Secondly, interactions take place dynamically, but thermodynamics is rarely integrated into the DL architecture design. To address these obstacles, we present a novel framework known as Refine-PPI, which incorporates two key enhancements. On the one hand, we introduce a structure refinement module that is trained by a mask mutation modeling (MMM) task on available wide-type structures and then is transferred to hallucinate the inaccessible mutant protein structures. Additionally, we employ a new kind of geometric networks to capture the dynamic 3D variations and encode the uncertainty associated with PPI. Through comprehensive experiments conducted on the established benchmark dataset SKEMPI, our results substantiate the superiority of the Refine-PPI framework. These findings underscore the effectiveness of our hallucination strategy to address the absence of mutant protein structure and hope to shed light on the prediction of the free energy change",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=pNfniUgXJt": {
    "title": "WASSERSTEIN-GUIDED SYMBOLIC REGRESSION: MODEL DISCOVERY OF NETWORK DYNAMICS",
    "volume": "review",
    "abstract": "Real-world complex systems often miss high-fidelity physical descriptions and are typically subject to partial observability. Learning dynamics of such systems is a challenging and ubiquitous problem, encountered in diverse critical applications which require interpretability and qualitative guarantees. Our paper addresses this problem in the case of probability distribution flows governed by ODEs. Specifically, we devise a ${\\it white}$ ${\\it box}$ approach -dubbed Symbolic Distribution Flow Learner ($\\texttt{SDFL}$)- combining symbolic search with a Wasserstein-based loss function, resulting in robust model recovery scheme which naturally lends itself to cope with partial observability. Additionally, we furnish the proposed framework with theoretical guarantees on the number of required ${\\it snapshots}$ to achieve a certain level of fidelity in the model-discovery. We illustrate the performance of the proposed scheme on the prototypical problem of Kuramoto networks and a standard benchmark of single-cell population trajectory data. The numerical experiments demonstrate the computational advantage of $\\texttt{SDFL}$ in comparison to the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZqKxMHcAg": {
    "title": "Evaluating Language Models Through Negotiations",
    "volume": "review",
    "abstract": "Commercial interests are racing to exploit language models' remarkable capability to display agent-like behavior. Indeed, a future where personal LM-based agents are widely adopted to perform complicated tasks involving planning and negotiating appears increasingly plausible. Current, predominantly static evaluation methods are ill-suited to evaluate such dynamic, multi-step applications. In this work, we therefore propose jointly evaluating LM performance and alignment through the lens of negotiation games. We argue that this common real-world task provides scalable, difficult-to-hack performance metrics while offering non-trivial insights into model decision-making. Crucially, negotiation games allow us to study both competitive and cooperative performance, modulate complexity, and side-step accidental evaluation data leakage. Using our evaluation setup, we report results for publicly accessible LMs from all major providers on a variety of negotiation games. Noteworthy takeaways include: (i) open-source models are currently unable to complete this task, (ii) cooperative bargaining games prove challenging, and (iii) the most powerful models do not always 'win'. Evaluation through negotiations complements existing evaluation efforts by providing a novel evaluation paradigm to study evolving language model agency. We release an open-source library to accelerate research in this critical direction and lower the technical boundaries for researchers outside of the machine learning field to contribute",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=JKpk2p4O99": {
    "title": "Towards robust unlearnable examples via deep hiding",
    "volume": "review",
    "abstract": "Ensuring data privacy and protection has become paramount in the era of deep learning. Unlearnable examples are proposed to mislead the deep learning models and prevent data from unauthorized exploration by adding small perturbations to data. However, such perturbations (e.g., noise, texture, color change) predominantly impact low-level features, making them vulnerable to countermeasures like adversarial training, data augmentations, and preprocessing. In contrast, semantic images with intricate shapes have a wealth of high-level features, making them more resilient to countermeasures and potential for producing robust unlearnable examples. In this paper, we propose a Deep Hiding (DH) scheme that adaptively hides semantic images enriched with high-level features. We employ an Invertible Neural Network (INN) to invisibly integrate predefined images, inherently hiding them with deceptive perturbations. To enhance data unlearnability, we introduce a Latent Feature Concentration module, designed to work with the INN, regularizing the intra-class variance of these perturbations. To further boost the robustness of unlearnable examples, we design a Semantic Images Generation module that produces hidden semantic images. By utilizing similar semantic information, this module generates similar semantic images for samples within the same classes, thereby enlarging the inter-class distance and narrowing the intra-class distance. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-subset, against 12 countermeasures, reveal that our proposed method exhibits state-of-the-art ro- bustness for unlearnable examples, demonstrating its efficacy in data protection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=JshLcbPI9J": {
    "title": "Deep Backtracking Counterfactuals for Causally Compliant Explanations",
    "volume": "review",
    "abstract": "Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. We demonstrate these properties experimentally on a modified version of MNIST and CelebA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JSS9rKHySk": {
    "title": "On the Role of General Function Approximation in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "We study offline reinforcement learning (RL) with general function approximation. General function approximation is a powerful tool for algorithm design and analysis, but its adaptation to offline RL encounters several challenges due to varying approximation targets and assumptions that blur the real meanings of function assumptions. In this paper, we try to formulate and clarify the treatment of general function approximation in offline RL in two aspects: (1) analyzing different types of assumptions and their practical usage, and (2) understanding its role as a restriction on underlying MDPs from information-theoretic perspectives. Additionally, we introduce a new insight for lower bound establishing: one can exploit model-realizability to establish general-purposed lower bounds that can be generalized into other functions. Building upon this insight, we propose two generic lower bounds that contribute to a better understanding of offline RL with general function approximation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ZS4m74kZpH": {
    "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
    "volume": "review",
    "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oEzY6fRUMH": {
    "title": "State Chrono Representation for Enhancing Generalization in Reinforcement Learning",
    "volume": "review",
    "abstract": "Developing a robust and generalizable state representation is essential for overcoming the challenges posed by reinforcement learning tasks that rely on images as input. Recent developments in metric learning, including techniques like deep bisimulation metric approaches, have facilitated the transformation of states into structured representation spaces, allowing the measurement of distances based on task-relevant features. However, these approaches face challenges in handling demanding generalization tasks and scenarios characterized by sparse rewards. Their limited one-step update strategy often falls short of capturing adequate long-term behaviors within their representations. To address these challenges, we present the State Chrono Representation (SCR) approach, which enhances state representations by integrating long-term information alongside the bisimulation metric. SCR learns state distances and measurements within a temporal framework, considering future dynamics and accumulated rewards across current and long-term future states. The resulting representation space not only captures sequential behavioral information but also integrates distances and measurements from the present to the future. This temporal-aware learning strategy does not introduce a significant number of additional parameters for modeling dynamics, ensuring the efficiency of the entire learning process. Comprehensive experiments conducted within DeepMind Control environments reveal that SCR achieves state-of-the-art performance in demanding generalization tasks and scenarios characterized by sparse rewards",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=EwAGztBkJ6": {
    "title": "On the Generalization of Gradient-based Neural Network Interpretations",
    "volume": "review",
    "abstract": "Feature saliency maps are commonly used for interpreting neural network predictions. This approach to interpretability is often studied as a post-processing problem independent of training setups, where the gradients of trained models are used to explain their output predictions. However, in this work, we observe that gradient-based interpretation methods are highly sensitive to the training set: models trained on disjoint datasets without regularization produce inconsistent interpretations across test data. Our numerical observations pose the question of how many training samples are required for accurate gradient-based interpretations. To address this question, we study the generalization aspect of gradient-based explanation schemes and show that the proper generalization of interpretations from training samples to test data requires more training data than standard deep supervised learning problems. We prove generalization error bounds for widely-used gradient-based interpretations, suggesting that the sample complexity of interpretable deep learning is greater than that of standard deep learning. Our bounds also indicate that Gaussian smoothing in the widely-used SmoothGrad method plays the role of a regularization mechanism for reducing the generalization gap. We evaluate our findings on various neural net architectures and datasets, to shed light on how training data affect the generalization of interpretation methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KqTzfiNjWU": {
    "title": "Restorer Guided Diffusion Models for Variational Inverse Problems",
    "volume": "review",
    "abstract": "Diffusion models have made remarkable progress in solving various inverse problems, attributing to the generative modeling capability of the data manifold. Posterior sampling from the conditional score function enable the precious data consistency powered by the measurement-based likelihood term. However, most prevailing approaches confined to the insufficient expressive ability of the measurement model with merely digitized measuring deterioration, regardless of complicated unpredictable disturbance in real-world sceneries. To address this, we show that the measurement-based likelihood can be renewed with restoration-based likelihood, licencing the patronage of various off-the-shelf restoration models for powerful diffusion solvers, in what we call restorer guidance. Particularly, assembled with versatile restorer guidance optionally, we can resolve inverse problems with bunch of choices for assorted sample quality and realize the proficient deterioration control with assured realistic. We show that our work can be analogous to the transition from the classifier guidance to classifier-free guidance in the field of inverse problem solver. Experiments on various complicated inverse problems illustrate the effectiveness of our method, including image dehazing, rain streak removal, and motion deblurring. Code will be available soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8T7m27VC3S": {
    "title": "3D Dense Captioning beyond Nouns: A Middleware for Autonomous Driving",
    "volume": "review",
    "abstract": "Recently, language foundation models have revolutionized many fields and how they could enable smarter and safer autonomous vehicles remains elusive. We believe one major obstacle is the lack of a comprehensive and standard middleware representation that links perception and planning. We rethink the limitations of existing middleware (e.g., 3D boxes or occupancy) and propose 3\\textbf{D} d\\textbf{e}n\\textbf{s}e capt\\textbf{i}onin\\textbf{g} beyond \\textbf{n}ouns (or abbreviated as DESIGN). For each input scenario, DESIGN refers to a set of 3D bounding boxes with a language description for each. Notably, the \\textbf{comprehensive} description involves not only what the box is (noun) but also its attribute (adjective), location (preposition) and moving status (adverb). We design a scalable rule-based auto-labelling methodology to generate DESIGN ground truth, guaranteeing that the middleware is \\textbf{standard}. Using this methodology, we construct a large-scale dataset nuDesign based upon nuScenes, which consists of an unprecedented number of 2300k sentences. We also present an extensive benchmarking on nuDesign, featuring a model named DESIGN-former that takes multi-modal inputs and predicts reliable DESIGN outputs. Through qualitative visualizations, we demonstrate that DEISGN, as a novel 3D scene understanding middleware, has good interpretability. We release our code, data and models, hoping this middleware could trigger better autonomous driving algorithms and systems that benefit from the power of language foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYWsyTuiRp": {
    "title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map",
    "volume": "review",
    "abstract": "Given that Transformers are ubiquitous in wide tasks, interpreting their internals is a pivotal issue. Still, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts. We analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme. Our experiments with both masked- and causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. In addition, FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9sVJ17zvB": {
    "title": "VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation",
    "volume": "review",
    "abstract": "Creating stable, controllable videos is a complex task due to the need for significant variation in temporal dynamics and cross-frame temporal consistency. To address this, we enhance the spatial-temporal capability and introduce a versatile video generation model, VersVideo, which leverages textual, visual, and stylistic conditions. Current video diffusion models typically extend image diffusion architectures by supplementing 2D operations (such as convolutions and attentions) with temporal operations. While this approach is efficient, it often restricts spatial-temporal performance due to the oversimplification of standard 3D operations. To counter this, we incorporate two key elements: (1) multi-excitation paths for spatial-temporal convolutions with dimension pooling across different axes, and (2) multi-expert spatial-temporal attention blocks. These enhancements boost the model's spatial-temporal performance without significantly escalating training and inference costs. We also tackle the issue of information loss that arises when a variational autoencoder is used to transform pixel space into latent features and then back into pixel frames. To mitigate this, we incorporate temporal modules into the decoder to maintain inter-frame consistency. Lastly, by utilizing the innovative denoising UNet and decoder, we develop a unified ControlNet model suitable for various conditions, including image, Canny, HED, depth, and style. Examples of the videos generated by our model can be found at https://anonymous-pages.github.io/video_demos/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=SIZWiya7FE": {
    "title": "Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models",
    "volume": "review",
    "abstract": "Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. This challenge prompts us to propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. Specifically, we introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further address the issue of lacking supervision information, which hinders alignment with ground truth, we introduce a contrastive loss to facilitate the matching of representations between the remaining data and those of the original model, thus preserving predictive performance. Experimental results across various unlearning tasks demonstrate the effectiveness of our proposed method, Label-Agnostic Forgetting (LAF) without using any labels, which achieves comparable performance to state-of-the-art methods that rely on full supervision information. Furthermore, our approach excels in semi-supervised scenarios, leveraging limited supervision information to outperform fully supervised baselines. This work not only showcases the viability of supervision-free unlearning in deep models but also opens up a new possibility for future research in unlearning at the representation level",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3vnnLeajU": {
    "title": "Controlling Vision-Language Models for Universal Image Restoration",
    "volume": "review",
    "abstract": "Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a universal framework for image restoration. More specifically, DA-CLIP trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings. By integrating the embedding into an image restoration network via cross-attention, we are able to pilot the model to learn a high-fidelity image reconstruction. The controller itself will also output a degradation feature that matches the real corruptions of the input, yielding a natural classifier for different degradation types. In addition, we construct a mixed degradation dataset with synthetic captions for DA-CLIP training. Our approach advances state-of-the-art performance on both degradation-specific and unified image restoration tasks, showing a promising direction of prompting image restoration with large-scale pretrained vision-language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=6W35Wcs077": {
    "title": "Decomposition Ascribed Synergistic Learning for Unified Image Restoration",
    "volume": "review",
    "abstract": "Learning to restore multiple image degradations within a single model is quite beneficial for real-world applications. Nevertheless, existing works typically concentrate on regarding each degradation independently, while their relationship has been less exploited to ensure the synergistic learning. To this end, we revisit the diverse degradations through the lens of singular value decomposition, with the observation that the decomposed singular vectors and singular values naturally undertake the different types of degradation information, dividing various restoration tasks into two groups, \\ie, singular vector dominated and singular value dominated. The above analysis renders a more unified perspective to ascribe the diverse degradations, compared to previous task-level independent learning. The dedicated optimization of degraded singular vectors and singular values inherently utilizes the potential relationship among diverse restoration tasks, attributing to the Decomposition Ascribed Synergistic Learning (DASL). Specifically, DASL comprises two effective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue Operator (SVAO), to favor the decomposed optimization, which can be lightly integrated into existing convolutional image restoration backbone. Moreover, the congruous decomposition loss has been devised for auxiliary. Extensive experiments on blended five image restoration tasks demonstrate the effectiveness of our method, including image deraining, image dehazing, image denoising, image deblurring, and low-light image enhancement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ox2ATRM90I": {
    "title": "Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML",
    "volume": "review",
    "abstract": "Medical applications of machine learning (ML) have experienced a surge in popularity in recent years. Given the abundance of available data from electronic health records, the intensive care unit (ICU) is a natural habitat for ML. Models have been proposed to address numerous ICU prediction tasks like the early detection of complications. While authors frequently report state-of-the-art performance, it is challenging to verify claims of superiority. Datasets and code are not always published, and cohort definitions, preprocessing pipelines, and training setups are difficult to reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular framework that allows researchers to define reproducible and comparable clinical ML experiments; we offer an end-to-end solution from cohort definition to model evaluation. The framework natively supports most open-access ICU datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future ICU datasets. Combined with a transparent preprocessing pipeline and extensible training code for multiple ML and deep learning models, YAIB enables unified model development, transfer, and evaluation. Our benchmark comes with five predefined established prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and length of stay) developed in collaboration with clinicians. Adding further tasks is straightforward by design. Using YAIB, we demonstrate that the choice of dataset, cohort definition, and preprocessing have a major impact on the prediction performance — often more so than model class — indicating an urgent need for YAIB as a holistic benchmarking tool. We provide our work to the clinical ML community to accelerate method development and enable real-world clinical implementations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=NvJxTjTQtq": {
    "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations",
    "volume": "review",
    "abstract": "Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs' inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=U9NHClvopO": {
    "title": "SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition of Multi Token Embeddings",
    "volume": "review",
    "abstract": "Soft prompt tuning techniques have recently gained traction as an effective strategy for the parameter-efficient tuning of pretrained language models, particularly minimizing the required adjustment of model parameters. Despite their growing use, achieving optimal tuning with soft prompts, especially with smaller datasets, remains a substantial challenge. This study makes two contributions in this domain: (i) we introduce SuperPos-Prompt, a new reparameterization technique employing the superposition of multiple pretrained vocabulary embeddings to improve the learning of soft prompts. Our experiments across several GLUE and SuperGLUE benchmarks consistently highlight SuperPos-Prompt's superiority over \\textit{Residual Prompt} tuning, exhibiting an average score increase of +4.7 in T5-Small and $+3.9$ in T5-Base along with a faster convergence. Remarkably, SuperPos-Prompt occasionally outperforms even full fine-tuning methods. (ii) Additionally, we demonstrate enhanced performance and rapid convergence by omitting dropout from the frozen network, yielding consistent improvements across various scenarios and tuning methods. Unlike many existing strategies, our approach does not rely on the availability of a proficient pretrained source prompt for initialization, thereby ensuring notable flexibility and more effective combination of related prompt candidates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=i9Vs5NGDpk": {
    "title": "Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning",
    "volume": "review",
    "abstract": "We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. Our results hold for a broad class of asymptotically free sketches under very mild data assumptions. For squared prediction risk, we provide a decomposition into an unsketched equivalent implicit ridge bias and a sketching-based variance, and prove that the risk can be globally optimized by only tuning sketch size in infinite ensembles. For general subquadratic prediction risk functionals, we extend GCV to construct consistent risk estimators, and thereby obtain distributional convergence of the GCV-corrected predictions in Wasserstein-2 metric. This in particular allows construction of prediction intervals with asymptotically correct coverage conditional on the training data. We also propose an \"ensemble trick\" whereby the risk for unsketched ridge regression can be efficiently estimated via GCV using small sketched ridge ensembles. We empirically validate our theoretical results using both synthetic and real large-scale datasets with practical sketches including CountSketch and subsampled randomized discrete cosine transforms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=70A6oo3Il2": {
    "title": "AdaFlood: Adaptive Flood Regularization",
    "volume": "review",
    "abstract": "Although neural networks are conventionally optimized towards zero training loss, it has been recently learned that targeting a non-zero training loss threshold, referred to as a flood level, often enables better test time generalization. Current approaches, however, apply the same constant flood level to all training samples, which inherently assumes all the samples have the same difficulty. We present AdaFlood, a novel flood regularization method that adapts the flood level of each training sample according to the difficulty of the sample. Intuitively, since training samples are not equal in difficulty, the target training loss should be conditioned on the instance. Experiments on datasets covering four diverse input modalities &mdash; text, images, asynchronous event sequences, and tabular &mdash; demonstrate the versatility of AdaFlood across data domains and noise levels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=zUDbPgskDS": {
    "title": "Crystals with Transformers on Graphs, for predictions of crystal material properties",
    "volume": "review",
    "abstract": "Graph neural networks (GNN) has found extensive applications across diverse domains, notably in the modeling molecules. Crystals differ from molecules by the ionic bonding across the lattice and the highly ordered microscopic structure, which provides crystals unique symmetry and determines the macroscopic properties. Therefore, long-range orders are essential in predicting the physical and chemical properties of crystals. GNNs successfully model the local environment of atoms in crystals, however, they struggle to capture long-range interactions due to a limitation of depth. In this paper, we propose CrysToGraph ($\\textbf{Crys}$tals with $\\textbf{T}$ransformers $\\textbf{o}$n $\\textbf{Graph}$s), a novel transformer-based geometric graph network designed specifically for crystalline systems. CrysToGraph effectively captures short-range dependencies with transformer-based graph convolution blocks and long-range dependencies with graph-wise transformer blocks. Our model outperforms most existing methods by achieving new state-of-the-art results on the MatBench benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=iPWxqnt2ke": {
    "title": "Identifying Policy Gradient Subspaces",
    "volume": "review",
    "abstract": "Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we demonstrate the existence of such gradient subspaces for policy gradient algorithms despite the continuously changing data distribution inherent to reinforcement learning. Our findings reveal promising directions for more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=sDmjlpphdB": {
    "title": "Mixture-of-Experts in Prompt Optimization",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) exhibit strong generalization power in adapting to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design process. While these methods demonstrated promising results, they also restricted the output space of the search problem to a demo-free instruction. Such simplification significantly limits their performance, as a single demo-free instruction might not be able to cover the entire problem space of the targeted task due to its complexity. To alleviate this issue, we adopt the Mixture-of-Expert paradigm to divide the problem space into homogeneous regions, each governed by a specialized expert. To further improve the coverage of each expert, we expand their prompts to contain both an instruction and several demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into clusters based on their semantic similarity and assign a cluster to each expert; (2) instruction assignment: A region-based joint search is applied to optimize an instruction complementary to the demo cluster for each expert, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), outperforms prior art by up to 43% on benchmark NLP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=uvZDQvjULn": {
    "title": "A bi-objective perspective on controllable language models: reward dropout improves off-policy control performance",
    "volume": "review",
    "abstract": "We study the theoretical aspects of CLMs (Controllable Language Models) from a bi-objective optimization perspective. Specifically, we consider the CLMs as an off-policy RL problem that requires simultaneously maximizing the reward and likelihood objectives. Our main contribution consists of three parts. First, we establish the theoretical foundations of CLM by presenting reward upper bound and Pareto improvement/optimality conditions. Second, we analyze conditions that improve and violate Pareto optimality itself, respectively. Finally, we propose Reward Dropout, a simple yet powerful method to guarantee policy improvement based on a Pareto improvement condition. Our theoretical outcomes are supported by not only deductive proofs but also empirical results. The performance of Reward Dropout was evaluated on five CLM benchmark datasets, and it turns out that the Reward Dropout significantly improves the performance of CLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2D8aW4son": {
    "title": "Capturing The Channel Dependency Completely Via Knowledge-Episodic Memory For Time Series Forecasting",
    "volume": "review",
    "abstract": "The forecasting of Multivariate Time Series (MTS) has long been an important but challenging task, and recent advancements in MTS forecasting methods try to discover both temporal and channel-wise dependencies. However, we explore the nature of MTS and observe two kinds of existed channel dependencies that current methods have difficulty to capture completely. One is the evident channel dependency, which can be captured by mixing the channel information directly, and another is the latent channel dependency, which should be captured by finding the intrinsic variable that caused the same changes within MTS. To address this issue, we introduce the knowledge and episodic memory modules, which gain the specific knowledge and hard pattern memories with a well-designed recall method, to capture the latent and evident channel dependency respectively. Further, based on the proposed memory modules, we develop a pattern memory network, which recalls both memories for capturing different channel dependencies completely, for MTS forecasting. Extensive experiments on eight datasets all verify the effectiveness of the proposed memory-based forecasting method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lv9KZ5qCSG": {
    "title": "Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling",
    "volume": "review",
    "abstract": "Fairness or equity in machine learning is profoundly important for societal well-being, but limited public datasets hinder its progress, especially in the area of medicine. It is undeniable that fairness in medicine is one of the most important areas for fairness learning's applications. Currently, no large-scale public medical datasets with 3D imaging data for fairness learning are available, while 3D imaging data in modern clinics are standard tests for disease diagnosis. In addition, existing medical fairness datasets are actually repurposed datasets, and therefore they typically have limited demographic identity attributes with at most three identity attributes of age, gender and race for fairness modeling. To address this gap, we introduce our Eye Fairness dataset with 30,000 subjects (EyeFairness-30k) covering three major eye diseases including age-related macular degeneration, diabetic retinopathy and glaucoma affecting 380 million patients globally. Our EyeFairness dataset include both 2D fundus photos and 3D optical coherence tomography scans with six demographic identity attributes including age, gender, race, ethnicity, preferred language, and marital status. We also propose a fair identity scaling (FIS) approach combining group and individual scaling together to improve model fairness. Our FIS approach is compared with various the-state-of-the-art fairness learning methods with superior performance in the racial, gender, and ethnicity fairness tasks with 2D and 3D imaging data, which demonstrate the utilities of our EyeFairness dataset for fairness learning. To facilitate fairness comparisons between different models, we propose performance-scaled disparity measures, which can be to compare model fairness account for overall performance levels. The dataset and code are publicly accessible via https://github.com/anonymous4science/EyeFairness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=13D1zn0mpd": {
    "title": "Effective and Parameter-Efficient Reusing Fine-Tuned Models",
    "volume": "review",
    "abstract": "Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific fine-tuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for merging multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose parameter-efficient methods for Reusing fine-tuned models. For reusing fully fine-tuned models, we inject sparse task vectors to a merged model by magnitude pruning. For reusing LoRA fine-tuned models, we use a lower-rank matrix to approximate the LoRA matrix by singular value decomposition. Extensive experiments conducted on computer vision and natural language process tasks demonstrate the effectiveness and parameter-efficiency of the proposed methods. The proposed methods outperform existing merging models method by a large margin and achieve comparable performance to using a fine-tuned model per task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=zmJDzPh1Dm": {
    "title": "Nemesis: Normalizing the soft-prompt vectors of vision-language models",
    "volume": "review",
    "abstract": "With the prevalence of large-scale pretrained vision-language models (VLMs), such as CLIP, soft-prompt tuning has become a popular method for adapting these models to various downstream tasks. However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the impact of their norms to the performance of VLMs. This motivates us to pose an unexplored research question: ``Do we need to normalize the soft prompts in VLMs?'' To fill this research gap, we first uncover a phenomenon, called the $\\textbf{Low-Norm Effect}$ by performing extensive corruption experiments, suggesting that reducing the norms of certain learned prompts occasionally enhances the performance of VLMs, while increasing them often degrades it. To utilize this effect, we propose a novel method named $\\textbf{N}$ormalizing th$\\textbf{e}$ soft-pro$\\textbf{m}$pt v$\\textbf{e}$ctors of vi$\\textbf{si}$on-language model$\\textbf{s}$ ($\\textbf{Nemesis}$) to normalize soft-prompt vectors in VLMs. To the best of our knowledge, our work is the first to systematically investigate the role of norms of soft-prompt vector in VLMs, offering valuable insights for future research in soft-prompt tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=HXu7oYPOhg": {
    "title": "Memory-efficient particle filter recurrent neural network for object localization",
    "volume": "review",
    "abstract": "This study proposes a novel memory-efficient recurrent neural network (RNN) architecture specified to solve the object localization problem. This problem is to recover the object states along with its movement in a noisy environment. We take the idea of the classical particle filter and combine it with GRU RNN architecture. The key feature of the resulting memory-efficient particle filter RNN model (mePFRNN) is that it requires the same number of parameters to process environments of different sizes. Thus, the proposed mePFRNN architecture consumes less memory to store parameters compared to the previously proposed PFRNN model. To demonstrate the performance of our model, we test it on symmetric and noisy environments that are incredibly challenging for filtering algorithms. In our experiments, the mePFRNN model provides more precise localization than the considered competitors and requires fewer trained parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=j4VMrwgn1M": {
    "title": "Training Graph Transformers via Curriculum-Enhanced Attention Distillation",
    "volume": "review",
    "abstract": "Recent studies have shown that Graph Transformers (GTs) can be effective for specific graph-level tasks. However, when it comes to node classification, training GTs remains challenging, especially in semi-supervised settings with a severe scarcity of labeled data. Our paper aims to address this research gap by focusing on semi-supervised node classification. To accomplish this, we develop a curriculum-enhanced attention distillation method that involves utilizing a Local GT teacher and a Global GT student. Additionally, we introduce the concepts of in-class and out-of-class and then propose two improvements, out-of-class entropy and top-k pruning, to facilitate the student's out-of-class exploration under the teacher's in-class guidance. Taking inspiration from human learning, our method involves a curriculum mechanism for distillation that initially provides strict guidance to the student and gradually allows for more out-of-class exploration by a dynamic balance. Extensive experiments show that our method outperforms many state-of-the-art approaches on seven public graph benchmarks, proving its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=bm1JVsVZVu": {
    "title": "Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning",
    "volume": "review",
    "abstract": "Multi-objective optimization (MOO) has become an influential framework for various machine learning problems, including reinforcement learning and multi-task learning. In this paper, we study the black-box multi-objective optimization problem, where we aim to optimize multiple potentially conflicting objectives with function queries only. To address this challenging problem and find a Pareto optimal solution or the Pareto stationary solution, we propose a novel adaptive stochastic gradient algorithm for black-box MOO, called ASMG. Specifically, we use the stochastic gradient approximation method to obtain the gradient for the distribution parameters of the Gaussian smoothed MOO with function queries only. Subsequently, an adaptive weight is employed to aggregate all stochastic gradients to optimize all objective functions effectively. Theoretically, we explicitly provide the connection between the original MOO problem and the corresponding Gaussian smoothed MOO problem and prove the convergence rate for the proposed ASMG algorithm in both convex and non-convex scenarios. Empirically, the proposed ASMG method achieves competitive performance on multiple numerical benchmark problems. Additionally, the state-of-the-art performance on the black-box multi-task learning problem demonstrates the effectiveness of the proposed ASMG method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=a24gfxA7jD": {
    "title": "Physics Informed Distillation for Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have recently emerged as a potent tool in generative modeling, although their inherent iterative nature often results in sluggish image generation due to the requirement for multiple model evaluations. Recent progress has unveiled the intrinsic link between diffusion models and Probability Flow Ordinary Differential Equations (ODEs), thus enabling us to conceptualize diffusion models as ODE systems. Simultaneously, Physics Informed Neural Networks (PINNs) have substantiated their effectiveness in solving intricate differential equations through implicit modeling of their solutions. Building upon these foundational insights, we introduce Physics Informed Distillation (PID), a novel approach that employs a student model to represent the solution of the ODE system corresponding to the teacher diffusion model, akin to the principles employed in PINNs. Our approach demonstrates remarkable results, such as achieving an FID score of 3.92 on CIFAR-10 for single-step image generation. Additionally, we establish the stability of our method under conditions involving a sufficiently high discretization number, paralleling observations found in the PINN literature, thus highlighting its potential as a streamlined single-step distillation approach without the need for additional methodology-specific hyperparameters. The code will be made available upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=VDkye4EKVe": {
    "title": "Discovering Minimal Reinforcement Learning Environments",
    "volume": "review",
    "abstract": "Human agents often acquire skills under conditions that are significantly different from the context in which the skill is needed. For example, students prepare for an exam not by taking it, but by studying books or supplementary material. Can artificial agents benefit from training outside of their evaluation environment as well? In this project, we develop a novel meta-optimization framework to discover neural network-based synthetic environments. We find that training contextual bandits suffices to train Reinforcement Learning agents that generalize well to their evaluation environment, eliminating the need to meta-learn a transition function. We show that the synthetic contextual bandits train Reinforcement Learning agents in a fraction of time steps and wall clock time, and generalize across hyperparameter settings and algorithms. Using our method in combination with a curriculum on the performance evaluation horizon, we are able to achieve competitive results on a number of challenging continuous control problems. Our approach opens a multitude of new research directions: Contextual bandits are easy to interpret, yielding insights into the tasks that are encoded by the evaluation environment. Additionally, we demonstrate that synthetic environments can be used in downstream meta-learning setups, derive a new policy from the differentiable reward function, and show that the synthetic environments generalize to entirely different optimization settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=We6kIyBOMp": {
    "title": "Delayed Spiking Neural Network and Exponential Time Dependent Plasticity Algorithm",
    "volume": "review",
    "abstract": "Spiking Neural Networks (SNNs) become more similar to artificial neural networks (ANNs) to solve complex machine learning tasks. However, such similarity does not bring superior performances but loses biological plausibility. Moreover, most learning methods of SNNs follow the pattern of gradient descent used in ANNs, which also suffer from low bio-plausibility. To address these issues, a realistic delayed spiking neural network (DSNN) is introduced in this study, which only considers the dendrite and axon delays as the learnable parameters. And a more biologically plausible exponential time-dependent plasticity (ETDP) algorithm is proposed to train the DSNN. The ETDP adjusts the delays according to the global and local time differences between presynaptic and postsynaptic spikes, and the forward and backward propagation time of signals. These biological indicators can surrogate the time-consuming computation of descents precisely. Experimental results demonstrate that the DSNN trained by ETDP achieves very competitive results on various benchmark datasets, compared with other SNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=mIQ2puu82H": {
    "title": "DIFFNAT: IMPROVING DIFFUSION IMAGE QUALITY USING NATURAL IMAGE STATISTICS",
    "volume": "review",
    "abstract": "Diffusion models have advanced generative AI significantly in terms of editing and creating naturalistic images. However, while editing images using text-prompt or image guidance, some unnatural artefacts or effects can be generated by the diffusion model. This problem is more prominent in the context of few-shot personalization of text-to-image diffusion model, where the large diffusion model has to be finetuned from few examples of certain subject identity to produce edited images conditioned on text prompts. In this context, we propose a generic \"naturalness\" preserving loss function, viz., kurtosis concentration (KC) loss, which can be readily applied to any standard diffusion model pipeline to elevate the image quality. Our motivation stems from the projected kurtosis concentration property of natural images, which states that natural images have nearly constant kurtosis values across different band-pass versions of the image. In order to retain the \"naturalness\" of the generated images, we enforce reducing the gap between the highest and lowest kurtosis values across the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note that our approach does not require any additional guidance like classifer or classifer-free guidance in order to improve the image quality. We validate the proposed approach for three diverse tasks, viz., (1) personalized few-shot finetuning using text guidance, (2) unconditional image generation, and (3) image super-resolution. Integrating the proposed KC loss have improved the perceptual quality across all these tasks in terms of both FID, MUSIQ score and user evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9Kgnvknvwd": {
    "title": "A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level Optimization",
    "volume": "review",
    "abstract": "In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO) problem, where the upper-level subproblem is a multi-objective optimization problem and the lower-level subproblem is for scalar optimization. Existing gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the computational inefficient problem. To address this, we propose an efficient first-order multi-gradient method for MOBLO, called FORUM. Specifically, we reformulate MOBLO problems as a constrained multi-objective optimization (MOO) problem via the value-function approach. Then we propose a novel multi-gradient aggregation method to solve the challenging constrained MOO problem. Theoretically, we provide the complexity analysis to show the efficiency of the proposed method and a non-asymptotic convergence result. Empirically, extensive experiments demonstrate the effectiveness and efficiency of the proposed FORUM method in different learning problems. In particular, it achieves state-of-the-art performance on three multi-task learning benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WpMHBIYsUf": {
    "title": "Homeomorphic Model Transformation for Boosting Performance and Efficiency in Object Detection Networks",
    "volume": "review",
    "abstract": "The field of computer vision has witnessed significant advancements in recent years with the development of deep learning networks. However, the fixed architectures of these networks limit their capabilities. For object detection task, existing methods typically rely on fixed architecture. While achieving promising performance, there is potential for further improving network performance with minimal modifications. In this study, we investigate that existing networks with minimal modifications can further boost performance. However, modifying some layers results in pre-trained weight mismatch, the fine-tune process is time-consuming and resource-inefficient. To address this issue, we propose a novel technique called Homeomorphic Model Transformation (HMT), which enables the adaptation of initial weights based on pretrained weights. This approach ensures the preservation of the original model's performance when modifying layers. Additionally, HMT significantly reduces the total training time required to achieve optimal results while further enhancing network performance. Extensive experiments across various object detection tasks validate the effectiveness and efficiency of our proposed HMT solution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=jzdQPKgIWA": {
    "title": "Learning to Explore with In-Context Policy for Fast Peer Adaptation",
    "volume": "review",
    "abstract": "Adapting to different peers in multi-agent settings requires agents to quickly learn about the peer's policy from a few interactions and act accordingly. In this paper, we present a novel end-to-end method that learns an in-context policy that actively explores the peer's policy, recognizes its pattern, and adapts to it. The agent is trained on a diverse set of peer policies to learn how to balance exploration and exploitation based on the observed context, which is the history of interactions with the peer. The agent proposes exploratory actions when the context is uncertain, which can elicit informative feedback from the peer and help infer its preferences. To encourage such exploration behavior, we introduce an intrinsic reward based on the accuracy of the peer identification. The agent exploits the context when it is confident, which can optimize its performance with the peer. We evaluate our method on two tasks that involve competitive (Kuhn Poker) or cooperative (Overcooked) interactions with peer agents. We demonstrate that our method induces active exploration behavior, achieving faster adaptation and better outcomes than existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=zAdUB0aCTQ": {
    "title": "AgentBench: Evaluating LLMs as Agents",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=FDve8qGH3M": {
    "title": "Simple CNN for Vision",
    "volume": "review",
    "abstract": "Traditional Convolutional Neural Networks (CNNs) tend to use 3$\\times$3 small kernels, but can only capture neighboring spatial information in one block. Inspired by the success of Vision Transformers (ViTs) in capturing long-range visual dependencies, recent CNNs have reached a consensus on utilizing large kernel convolutions (e.g., 31$\\times$31 and, astonishingly, 51$\\times$51 kernels). Nevertheless, these approaches necessitate adopting specialized techniques such as re-parameterization or sparsity, which require extra post-processing. And too large kernels are unfriendly to hardware. This paper introduces a Simple Convolutional Neural Network (SCNN) that employs a sequence of stacked 3$\\times$3 convolutions but surpasses state-of-the-art CNNs utilizing larger kernels. Notably, we propose simple yet highly effective designs that enable 3$\\times$3 convolutions to progressively capture visual cues of various sizes, thereby overcoming the limitations of smaller kernels. First, we build a thin and deep model, which encourages more convolutions to capture more spatial information under the same computing complexity instead of opting for a heavier, shallower architecture. Furthermore, we introduce an innovative block comprising two 3$\\times$3 depthwise convolutions to enlarge the receptive field. Finally, we replace the input of the popular Sigmoid Linear Unit (SiLU) activation function with global average pooled features to capture all spatial information. Our SCNN performs superior to state-of-the-art CNNs and ViTs across various tasks, including ImageNet-1K image classification, COCO instance segmentation, and ADE20K semantic segmentation. Remarkably, SCNN outperforms the small version of Swin Transformer, a well-known ViTs, while requiring only 50\\% computation, which further proves that large kernel convolution is not the only choice for high-performance CNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ai4L058yoO": {
    "title": "Is Feature Extraction the most informative dimensionality reduction technique? Revisiting Unsupervised Feature Selection from a Dynamic Approach",
    "volume": "review",
    "abstract": "This paper compares unsupervised feature extraction and unsupervised feature selection techniques in the context of dimensionality reduction without using labeled data. Unsupervised feature extraction transforms the input space into a lower-dimensional representation by creating informative features that capture underlying patterns, leading to improved model performance. On the other hand, unsupervised feature selection chooses a subset of features based on predefined criteria, potentially overlooking important relationships and reducing the model's discriminative power. State-of-the-art researches suggest that feature extraction outperforms feature selection in terms of model accuracy and robustness. Leveraging the intrinsic structure of the data, unsupervised feature extraction provides richer representations, enhancing the model's ability to discern complex patterns. These paper proposes to revisit feature selection algorithms from a dynamic perspective, where the features are selected depending on the specific sample input. Through empirical evaluations, it will be demonstrated that unsupervised feature selection outperforms feature extraction, both in accuracy and data compression. These findings highlight the potential of unsupervised feature selection as a powerful approach for dimensionality reduction and improved model performance, particularly when labeled data is scarce or unavailable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ym0ubZrsmm": {
    "title": "Image Background Serves as Good Proxy for Out-of-distribution Data",
    "volume": "review",
    "abstract": "Out-of-distribution (OOD) detection empowers the model trained on the closed image set to identify unknown data in the open world. Though many prior techniques have yielded considerable improvements in this research direction, two crucial obstacles still remain. Firstly, a unified perspective has yet to be presented to view the developed arts with individual designs, which is vital for providing insights into future work. Secondly, we expect sufficient natural OOD supervision to promote the generation of compact boundaries between the in-distribution (ID) and OOD data without collecting explicit OOD samples. To tackle these issues, we propose a general probabilistic framework to interpret many existing methods and an OOD-data-free model, namely $\\textbf{S}$elf-supervised $\\textbf{S}$ampling for $\\textbf{O}$OD $\\textbf{D}$etection (SSOD). SSOD efficiently exploits natural OOD signals from the ID data based on the local property of convolution. With these supervisions, it jointly optimizes the OOD detection and conventional ID classification in an end-to-end manner. Extensive experiments reveal that SSOD establishes competitive state-of-the-art performance on many large-scale benchmarks, outperforming the best previous method by a large margin, e.g., reporting $\\textbf{-6.28}$% FPR95 and $\\textbf{+0.77}$% AUROC on ImageNet, $\\textbf{-19.01}$% FPR95 and $\\textbf{+3.04}$% AUROC on CIFAR-10, and top-ranked performance on hard OOD datasets, i.e., ImageNet-O and OpenImage-O",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=lvjz7Bm3Ea": {
    "title": "ChronoGAM: An End-to-End One-Class Time Series Gaussian Mixture Model",
    "volume": "review",
    "abstract": "Recently, several algorithms have been proposed for One Class Learning (OCL) with time series. However, several problems can be found in these methods, problems involving the collapse of hyperspheres, manual thresholds, numerical instabilities and even the use of unlabeled instances during training, which directly violates the concept of OCL. To avoid these problems and solve cases like the numerical instability of some methods this paper proposes an end-to-end method for time series one-class learning based on a Gaussian Mixture Model (GMM). The proposed method combines the unsupervised learning technique of an autoencoder adapted to extract temporal and structural features of a time series, combined with distribution learning, to provide better performance than other state-of-the-art methods for the classification of time series data. ChronoGAM is a novel method that is capable of improving the temporal importance of the representations learned by the autoencoding system. We propose a new objective function with modifications to penalize the small values on the covariance matrix without resulting in exploding gradient propagation, causing numerical instabilities, and adapting the energy calculus to avoid the use of exponential functions. The method is tested on over $85$ benchmark datasets, generating $652$ datasets. We gain in $369$ datasets, with an average ranking of $2.68$, being the top-ranked method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=QhoehDVFeJ": {
    "title": "Efficient Meshy Neural Fields for Animatable Human Avatars",
    "volume": "review",
    "abstract": "Efficiently digitizing high-fidelity animatable human avatars from videos is a challenging and active research topic. Recent volume rendering-based neural representations open a new way for human digitization with their friendly usability and photo-realistic reconstruction quality. However, they are inefficient for long optimization times and slow inference speed; their implicit nature results in entangled geometry, materials, and dynamics of humans, which are hard to edit afterward. Such drawbacks prevent their direct applicability to downstream applications, especially the prominent rasterization-based graphic ones. We present EMA, a method that Efficiently learns Meshy neural fields to reconstruct animatable human Avatars. It jointly optimizes explicit triangular canonical mesh, spatial-varying material, and motion dynamics, via inverse rendering in an end-to-end fashion. Each above component is derived from separate neural fields, relaxing the requirement of a template, or rigging. The mesh representation is highly compatible with the efficient rasterization-based renderer, thus our method only takes about an hour of training and can render in real-time. Moreover, only minutes of optimization is enough for plausible reconstruction results. The disentanglement of meshes enables direct downstream applications. Extensive experiments illustrate the very competitive performance and significant speed boost against previous methods. We also showcase applications including novel pose synthesis, material editing, and relighting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=YEhQs8POIo": {
    "title": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images",
    "volume": "review",
    "abstract": "Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference APIs. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider. In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its initial promise on synthetic images. Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods without any model training. For example, on CIFAR10 (with ImageNet as the public data), we achieve FID≤7.9 with privacy cost ε = 0.67, significantly improving the previous SOTA from ε = 32. We further demonstrate the promise of applying PE on large foundation models such as Stable Diffusion to tackle challenging private datasets with a small number of high-resolution images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=XCVuT5Stl5": {
    "title": "SENSITIVITY-INFORMED REGULARIZATION FOR OFFLINE BLACK-BOX OPTIMIZATION",
    "volume": "review",
    "abstract": "Offline optimization is an important task in numerous material engineering domains where online experimentation to collect data is too expensive and needs to be replaced by an in silico maximization of a surrogate of the black-box function. Although such a surrogate can be learned from offline data, its prediction might not be reliable outside the offline data regime, which happens when the surrogate has narrow prediction margin and is (therefore) sensitive to small perturbations of its parameterization. This raises the following questions: (1) how to regulate the sensitivity of a surrogate model; and (2) whether conditioning an offline optimizer with such less sensitive surrogate will lead to better optimization performance. To address these questions, we develop an optimizable sensitivity measurement for the surrogate model, which then inspires a sensitivity-informed regularizer that is applicable to a wide range of offline optimizers. This development is both orthogonal and synergistic to prior research on offline optimization, which is demonstrated in our extensive experiment benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=5COCYDObes": {
    "title": "Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilising extensive human labor, resulting in CoT policies that frequently fail to generalise. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical findings, leading the CoT to consider the anticipated goals. A prompt-generator policy has its own aim in our system, allowing it to adapt to the action policy and automatically root the CoT process towards outputs that lead to decisive, high-performing actions. Meanwhile, the action policy is learning how to use the CoT outputs to take specific actions. Our empirical data reveal that our system outperforms leading methods in agent learning benchmarks such as Overcooked and FourRoom",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhYNXVcZYz": {
    "title": "SketchEdit: Editing Freehand Sketches At The Stroke-Level",
    "volume": "review",
    "abstract": "Freehand sketching is a representation of human cognition of the real world. Recent sketch synthesis methods have demonstrated the capability of generating lifelike outcomes. However, these methods directly encode the whole sketch instances and makes it challenging to decouple the strokes from the sketches and have difficulty in controlling local sketch synthesis, e.g., stroke editing. Besides, the sketch editing task encounters the issue of accurately positioning the edited strokes, because users may not be able to draw on the exact position and the same stroke may appear on various locations in different sketches. We propose SketchEdit to realize flexible editing of sketches at the stroke-level for the first time. To tackle the challenge of decoupling strokes, our SketchEdit divides a drawing sequence of a sketch into a series of strokes based on the pen state, align the stroke segments to have the same starting position, and learns the embeddings of every stroke by a proposed stroke encoder. This design allows users to conveniently select the strokes for editing at any locations. Moreover, we overcome the problem of stroke placement via a diffusion process, which progressively generate the locations for the strokes to be synthesized, using the stroke features as the guiding condition. Both the stroke embeddings and the generated locations are fed into a sequence decoder to synthesize the manipulated sketch. The stroke encoder and the sequence decoder are jointly pre-trained under the autoencoder paradigm, with an extra image decoder to learn the local structure of sketches. Experiments demonstrate that the SketchEdit is effective for stroke-level sketch editing and outperforms state-of-the-art methods in the sketch reconstruction task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=pdJXYfJjz9": {
    "title": "EXPLORING RAIN-/DETAIL-AWARE REPRESENTATION FOR INSTANCE-SPECIFIC IMAGE DE-RAINING",
    "volume": "review",
    "abstract": "Recent advances in image de-raining have focused on training powerful models on mixed multiple datasets comprising diverse rain types and backgrounds. However, this approach tends to overlook the inherent differences between datasets, resulting in suboptimal optimization and poor generalization. To address this limitation, we propose an approach to learn instance-specific de-raining models by exploring meaningful representations that characterize both the rain and background components in rainy images. Leveraging these representations as instructive guidance, we put forth a Context-based Instance-specific Modulation (CoI-M) mechanism which can modulate CNN- or Transformer-based models. Furthermore, we develop a rain-/detail-aware contrastive learning strategy to help extract joint rain-/detail-aware instance-specific representations. By integrating CoI-M with the rain-/detail-aware Contrastive learning, we develop CoIC, an innovative and effective algorithm for training models on mixed datasets. Moreover, CoIC offers insight into modeling relationships of datasets, quantitatively assessing the impact of rain and details on restoration, and revealing different behaviors of models given diverse inputs. Extensive experiments validate the effectiveness of CoIC in boosting the de-raining ability of CNN- and Transformer-based models, as well as significantly improving their generalization ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=4kJfWZChJI": {
    "title": "Generalization or Specificity? Spectral Meta Estimation and Ensemble (SMEE) with Domain-specific Experts",
    "volume": "review",
    "abstract": "Existing domain generalization (DG) methodologies strive to construct a unified model trained on diverse source domains, with the goal of achieving robust performance on any unseen test domain. However, in practice, not all source domains contribute equally to effective knowledge transfer for a specific test domain. Consequently, the reliability of single-model generalization often falls short of classic empirical risk minimization (ERM). This paper departs from the conventional approaches and advocates for a paradigm that prioritizes specificity over broad generalization. We propose the Spectral Meta Estimation and Ensemble (SMEE) approach, which capitalizes on domain-specific expert models and leverages unsupervised ensemble learning to construct a weighted ensemble for test samples. Our comprehensive investigation reveals three key insights: (1) The proposed meta performance estimation strategy for model selection within the sources plays a pivotal role in accommodating stochasticity; (2) The proposed spectral unsupervised ensemble method for transferability estimation excels in constructing robust learners for multi-class classification tasks, while being entirely hyperparameter-free; and (3) Multi-expert test-time transferability estimation and ensemble proves to be a promising alternative to the prevailing single-model DG paradigm. Experiments conducted on the DomainBed benchmark substantiate the superiority of our approach, consistently surpassing state-of-the-art DG techniques. Importantly, our approach offers a noteworthy performance enhancement while maintaining remarkable computational efficiency, executing in mere milliseconds per test sample during inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ecbRyZZmKG": {
    "title": "Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning",
    "volume": "review",
    "abstract": "To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named \"Double-I watermark\". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed \"Double-I watermark\" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Go33RnNiVH": {
    "title": "$\\beta$-DQN: Diverse Exploration via Learning a Behavior Function",
    "volume": "review",
    "abstract": "Efficient exploration remains a pivotal challenge in reinforcement learning (RL). While numerous methods have been proposed, their lack of simplicity, generality and computational efficiency often lead researchers to choose simple techniques such as $\\epsilon$-greedy. Motivated by these considerations, we propose $\\beta$-DQN. This method improves exploration by constructing a set of diverse polices through a behavior function $\\beta$ learned from the replay memory. First, $\\beta$ differentiates actions based on their frequency at each state, which can be used to design strategies for better state coverage. Second, we constrain temporal difference (TD) learning to in-sample data and derive two functions $Q$ and $Q_{\\textit{mask}}$. Function $Q$ may overestimate unseen actions, providing a foundation for bias correction exploration. $Q_{\\textit{mask}}$ reduces the values of unseen actions in $Q$ using $\\beta$ as an action mask, thus yields a greedy policy that purely exploit in-sample data. We combine $\\beta, Q, Q_{\\textit{mask}}$ to construct a set of policies ranging from exploration to exploitation. Then an adaptive meta-controller selects an effective policy for each episode. $\\beta$-DQN is straightforward to implement, imposes minimal hyper-parameter tuning demands, and adds a modest computational overhead to DQN. Our experiments, conducted on simple and challenging exploration domains, demonstrate $\\beta$-DQN significantly enhances performance and exhibits broad applicability across a wide range of tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wDd4Zcnc08": {
    "title": "HP$^3$-NS: Hybrid Perovskite Property Prediction Using Nested Subgraph",
    "volume": "review",
    "abstract": "Many machine learning techniques have demonstrated superiority in large-scale material screening, enabling rapid and accurate estimation of material properties. However, data representation on hybrid organic-inorganic (HOI) crystalline materials poses a distinct challenge due to their intricate nature. Current graph-based representations often struggle to effectively capture the nuanced interactions between organic and inorganic components. Furthermore, these methods typically rely on detailed structural information that hinders the applications of the methods for novel material discovery. To address these, we propose a nested graph representation HP$^3$-NS (Hybrid Perovskite Property Prediction Using Nested Subgraph) that hierarchically encodes the distinct interactions within hybrid crystals. Our encoding scheme incorporates both intra- and inter-molecular interactions and distinguishes between the organic and inorganic components. This hierarchical representation also removes the dependence on detailed structural data, enabling the model application to newly designed materials. We demonstrate the effectiveness and significance of the method on hybrid perovskite datasets, wherein the proposed HP$^3$-NS achieves significant accuracy improvement compared to current state-of-the-art techniques for hybrid material property prediction tasks. Our method shows promising potential to accelerate hybrid perovskite development by enabling effective computational screening and analysis of HOI crystals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Wi0Ys33Nm": {
    "title": "Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes",
    "volume": "review",
    "abstract": "The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that enables a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al., 2018 to a larger class of initial weight distributions (which we call pseudo-iid), including the established cases of iid and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialised with pseudo-iid distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge of Chaos for a broader class of neural networks and tune them at criticality in order to enhance their training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xU0XRbn3b5": {
    "title": "Privacy at Interpolation: Precise Analysis for Random and NTK Features",
    "volume": "review",
    "abstract": "Deep learning models often memorize the training set. This makes them vulnerable to recovery attacks, raising privacy concerns to users, and many widespread algorithms such as empirical risk minimization (ERM) do not directly enforce safety guarantees. In this paper, we study the safety of ERM models when the training samples are interpolated (i.e., *at interpolation*) against a family of powerful black-box information retrieval attacks. Our analysis quantifies this safety via two separate terms: *(i)* the model *stability* with respect to individual training samples, and *(ii)* the *feature alignment* between attacker query and original data. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result characterizes precisely the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. This proves that privacy strengthens with an increase in generalization capability, unveiling the role of the model and of its activation function. Numerical experiments show an agreement with our theory not only for RF/NTK models, but also for deep neural networks trained on standard datasets (MNIST, CIFAR-10)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=i6JcQpiFdR": {
    "title": "Guaranteed Trust Region Optimization via Two-Phase KL Penalization",
    "volume": "review",
    "abstract": "On-policy reinforcement learning (RL) has become a popular framework for solving sequential decision problems due to its computational efficiency and theoretical simplicity. Some on-policy methods guarantee every policy update is constrained to a trust region relative to the prior policy to ensure training stability. These methods often require computationally intensive non-linear optimization or require a particular form of action distribution. In this work, we show that applying KL penalization alone is nearly sufficient to enforce such trust regions. Then, we show that introducing a \"fixup\" phase is sufficient to guarantee a trust region is enforced on every policy update while adding fewer than 5\\% additional gradient steps in practice. The resulting algorithm, which we call FixPO, is able to train a variety of policy architectures and action spaces, is easy to implement, and produces results competitive with other trust region methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4aJg9e4nvF": {
    "title": "What do vision transformers learn? A visual exploration",
    "volume": "review",
    "abstract": "Vision transformers (ViTs) are quickly becoming the de-facto architecture for computer vision, yet we understand very little about why they work and what they learn. While existing studies visually analyze the mechanisms of convolutional neural networks, an analogous exploration of ViTs remains challenging. In this paper, we first address the obstacles to performing visualizations on ViTs. Assisted by these solutions, we observe that neurons in ViTs trained with language model supervision (e.g., CLIP) are activated by semantic concepts rather than visual features. We also explore the underlying differences between ViTs and CNNs, and we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information. On the other hand, both architecture types behave similarly in the way features progress from abstract patterns in early layers to concrete objects in late layers. In addition, we show that ViTs maintain spatial information in all layers except the final layer. In contrast to previous works, we show that the last layer most likely discards the spatial information and behaves as a learned global pooling operation. Finally, we conduct large-scale visualizations on a wide range of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to validate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=A0DI5v6m8O": {
    "title": "Black-Box Gradient Matching for Reliable Offline Black-Box Optimization",
    "volume": "review",
    "abstract": "Offline design optimization problem arises in numerous science and engineering applications including materials engineering, where expensive online experimentation necessitates the use of in silico surrogate functions to predict and maximize the target objective over candidate designs. Although these surrogates can be learned from offline data, their predictions can be potentially inaccurate outside the offline data regime. This challenge raises a fundamental question about the impact of imperfect surrogate model on the performance gap between its optima and the true oracle optima, and to what extent the performance loss can be mitigated. Although prior work developed methods to improve the robustness of surrogate models and their associated optimization processes, a provably quantifiable relationship between an imperfect surrogate and the corresponding performance gap, and whether prior methods directly address it, remain elusive. To shed more light on this important question, we present a novel theoretical formulation to understand offline black-box optimization, by explicitly bounding the optimization quality based on how well the surrogate matches the latent gradient field that underlines the offline data. Inspired by our theoretical analysis, we propose a principled black-box gradient matching algorithm to create effective surrogate models for offline optimization. Experiments on diverse real-world benchmarks demonstrate improved optimization quality using our approach to create surrogates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=HM2E7fnw2U": {
    "title": "Mitigating Mode Collapse in Sequential Disentanglement via an Architecture Bias",
    "volume": "review",
    "abstract": "One of the fundamental representation learning tasks is unsupervised sequential disentanglement, where latent representations of the inputs are decomposed to a single static factor and a sequence of dynamic factors. To extract this latent information, existing variational methods condition the static and dynamic codes on the entire input sequence. Unfortunately, these models often suffer from mode collapse, i.e., the dynamic vectors encode static and dynamic information, leading to a non-meaningful static component. Attempts to alleviate this problem via reducing the dynamic dimension and mutual information loss terms gain only partial success. Often, promoting a certain functionality of the model is better achieved via specific architectural biases instead of incorporating additional loss terms. For instance, convolutional nets gain translation-invariance with shared kernels and attention models realize the underlying correspondence between source and target sentences. Inspired by these successes, we propose in this work a novel model that mitigates mode collapse by conditioning the static component on a single sample from the sequence, and subtracting the resulting code from the dynamic factors. Remarkably, our variational model has less hyper-parameters in comparison to existing work, and it facilitates the analysis and visualization of disentangled latent data. We evaluate our work on multiple data-modality benchmarks including general time series, video, and audio, and we show beyond state-of-the-art results on generation and prediction tasks in comparison to several strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rIx1YXVWZb": {
    "title": "Understanding Addition in Transformers",
    "volume": "review",
    "abstract": "Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=36L7W3ri4U": {
    "title": "Beating Price of Anarchy and Gradient Descent without Regret in Potential Games",
    "volume": "review",
    "abstract": "Arguably one of the thorniest problems in game theory is that of equilibrium selection. Specifically, in the presence of multiple equilibria do self-interested learning dynamics typically select the socially optimal ones? We study a rich class of continuous-time no-regret dynamics in potential games (PGs). Our class of dynamics, *Q-Replicator Dynamics* (QRD), include gradient descent (GD), log-barrier and replicator dynamics (RD) as special cases. We start by establishing *pointwise convergence* of all QRD to Nash equilibria in almost all PGs. In the case of GD, we show a tight average case performance within a factor of two of optimal, for a class of symmetric $2\\times2$ potential games with unbounded Price of Anarchy (PoA). Despite this positive result, we show that GD is not always the optimal choice even in this restricted setting. Specifically, GD outperforms RD, if and only if *risk-* and *payoff-dominance* equilibria coincide. Finally, we experimentally show how these insights extend to all QRD dynamics and that unbounded gaps between average case performance and PoA analysis are common even in larger settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9OevMUdods": {
    "title": "Do Large Language Models Know about Facts?",
    "volume": "review",
    "abstract": "Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to comprehensively evaluate the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs are able to compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=bA5o5eZplk": {
    "title": "New recipes for graph anomaly detection: Forward diffusion dynamics and graph generation",
    "volume": "review",
    "abstract": "Distinguishing atypical nodes in a graph, which is known as graph anomaly detection, is more crucial than the generic node classification in real applications, such as fraud and spam detection. However, the lack of prior knowledge about anomalies and the extremely class-imbalanced data pose formidable challenges in learning the distributions of normal nodes and anomalies, which serves as the foundation of the state of the arts. We introduce a novel paradigm (first recipe) for detecting graph anomalies, stemming from our empirical and rigorous analysis of the significantly distinct evolving patterns between anomalies and normal nodes when scheduled noise is injected into the node attributes, referred to as the forward diffusion process. Rather than modeling the data distribution, we present three non-GNN methods to capture the evolving patterns and achieve promising results on six widely-used datasets, while mitigating the oversmoothing limitation and shallow architecture of GNN methods. We further investigate the generative power of denoising diffusion models to synthesize training samples that align with the original graph semantics (second recipe). In particular, we derive two principles for designing the denoising neural network and generating graphs. With our proposed graph generation method, we attain record-breaking performance while our generated graphs are also capable of enhancing the results of existing methods. All the code and data are available at \\url{https://github.com/DiffAD/DiffAD}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=A6juYCULJO": {
    "title": "Abstractive Summarization through the PRISM of Decoding Strategies",
    "volume": "review",
    "abstract": "In the realm of natural language generation, abstractive summarization (AS) is at the center of an unparalleled evolution driven by transformer-based language models (LMs). However, the significance of decoding strategies is often neglected despite their influence on the generated summaries. Given the abundance of token selection heuristics and their accompanying hyperparameters, the community needs directions to steer well-founded decisions based on the task and the target metrics at hand. To fill this gap, we comparatively assess the effectiveness and efficiency of decoding-time techniques for short, long, and multi-document AS. We explore more than 2500 combinations of 3 widely used million-scale autoregressive encoder-decoder models, 6 datasets, and 9 decoding settings. Our findings shed light on the field, demonstrating that optimized decoding choices can yield substantial performance enhancements. In addition to human evaluation, we quantitatively measure effects using 10 automatic metrics, including dimensions such as semantic similarity, factuality, compression, redundancy, and carbon footprint. We introduce PRISM, a first-of-its-kind dataset that pairs AS gold input-output examples with LM predictions under a wide array of decoding options",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypAT2ixD4X": {
    "title": "In defense of parameter sharing for model-compression",
    "volume": "review",
    "abstract": "When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent capacity advantage that RPS enjoys over sparse models. Theoretically, we establish RPS as a superior technique in terms of memory-efficient representation when compared to pruning for linear models. This paper argues in favor of paradigm shift towards RPS based models. During our rigorous evaluation of RPS, we identified issues in the state- of-the-art RPS technique ROAST, specifically regarding stability (ROAST's sensitivity to initialization hyperparameters, often leading to divergence) and Pareto-continuity (ROAST's inability to recover the accuracy of the original model at zero compression). We provably address both of these issues. We refer to the modified RPS, which incorporates our improvements, as STABLE-RPS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RzY9qQHUXy": {
    "title": "Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning",
    "volume": "review",
    "abstract": "Real-world tasks are universally associated with training samples that exhibit a long-tailed class distribution, and traditional deep learning models are not suitable for fitting this distribution, thus resulting in a biased trained model. To surmount this dilemma, massive deep long-tailed learning studies have been proposed to achieve inter-class fairness models by designing sophisticated sampling strategies or improving existing model structures and loss functions. Habitually, these studies tend to apply data augmentation strategies to improve the generalization performance of their models. However, this augmentation strategy applied to balanced distributions may not be the best option for long-tailed distributions. For a profound understanding of data augmentation, we first theoretically analyze the gains of traditional augmentation strategies in long-tailed learning, and observe that augmentation methods cause the long-tailed distribution to be imbalanced again, resulting in an intertwined imbalance: inherent data-wise imbalance and extrinsic augmentation-wise imbalance, i.e., two 'birds' co-exist in long-tailed learning. Motivated by this observation, we propose an adaptive Dynamic Optional Data Augmentation (DODA) to address this intertwined imbalance, i.e., one 'stone' simultaneously 'kills' two 'birds', which allows each class to choose appropriate augmentation methods by maintaining a corresponding augmentation probability distribution for each class during training. Extensive experiments across mainstream long-tailed recognition benchmarks (e.g., CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018) prove the effectiveness and flexibility of the DODA in overcoming the intertwined imbalance. The code is available in https://anonymous.4open.science/r/Code-for-DODA-FE12",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Wgb8tuu5BI": {
    "title": "Decoupling Intrinsic and Measurement Trends: A Crucial Consideration in Time Series Causal Discovery",
    "volume": "review",
    "abstract": "In the realm of time series data, it is common to encounter time trends, which manifest as a function concerning time within a given data span. Time trends can be classified into intrinsic (real) and measurement (false) trends. Intrinsic trends are inherent to the underlying mechanisms of the variables, while measurement trends are essentially measurement errors unique to the observed values (e.g., an increase in diagnosed thyroid nodule patients due to enhanced medical techniques, despite a stable incidence rate over time). Measurement trends can critically influence the results of a variety of causal discovery methods and hence, necessitate elimination prior to causal analytic procedures. In this study, we introduce a novel framework capable of detecting all trend-influenced variables and distinguishing between intrinsic and measurement trends, called Trend Differentiator (TrendDiff). This approach consists of two primary steps: trend variable identification and trend type differentiation. The first step leverages Constraint-based Causal Discovery from heterogeneous/Nonstationary Data (CD-NOD) to identify variables with trends. Following this, we utilize the structure characteristics to differentiate between intrinsic and measurement trends. Experimental results on various synthetic scenarios and real-world data sets are employed to demonstrate the efficacy of our methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=wCUw8t63vH": {
    "title": "Spectral learning of shared dynamics between generalized-linear processes",
    "volume": "review",
    "abstract": "Across various science and engineering applications, there often arises a need to predict the dynamics of one data stream from another. Further, these data streams may have different statistical properties. Studying the dynamical relationship between such processes, especially for the purpose of predicting one from the other, requires accounting for their distinct statistics while also dissociating their shared dynamical subspace. Existing analytical modeling approaches, however, do not address both of these needs. Here we propose a path forward by deriving a novel analytical multi-step subspace identification algorithm that can learn a model for a primary generalized-linear process (called ``predictor\"), while also dissociating the dynamics shared with a secondary process. We demonstrate a specific application of our approach for modeling discrete Poisson point-processes activity, while finding the dynamics shared with continuous Gaussian processes. In simulations, we show that our algorithm accurately prioritizes identification of shared dynamics. Further, we also demonstrate that the method can additionally model the disjoint dynamics that exist only in the predictor Poisson data stream, if desired. Similarly, we apply our algorithm on a biological dataset to learn models of dynamics in Poisson neural population spiking streams that predict dynamics in movement streams. Compared with existing Poisson subspace identification methods, models learned with our method decoded movements better and with lower-dimensional latent states. Lastly, we discuss regimes in which our assumptions might not be met and provide recommendations and possible future directions of investigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MCNqgUFTHI": {
    "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents",
    "volume": "review",
    "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=9bmTbVaA2A": {
    "title": "Bootstrapping Variational Information Pursuit with Foundation Models for Interpretable Image Classification",
    "volume": "review",
    "abstract": "Variational Information Pursuit (V-IP) is an interpretable-by-design framework that makes predictions by sequentially selecting a short chain of task-relevant, user-defined interpretable queries about the data that are most informative for the task. The selected query-answer chain serves as an explanation for the prediction. Applying the framework to any task requires (i) specification of a query set, and (ii) densely annotated data with query answers to train classifiers to answer queries at test time. This limits V-IP's application to small-scale tasks where manual data annotation is feasible. In this work, we focus on image classification tasks and propose to relieve this bottleneck by leveraging Foundation Models. Specifically, following recent work, we propose to use GPT, a Large Language Model, to propose semantic concepts as queries for a given classification task. To answer these queries, we propose a Concept Question-Answering network (Concept-QA) which learns to answer binary queries about semantic concepts in images. We design pseudo-labels to train our Concept-QA model using GPT and CLIP (a Vision-Language Model). Empirically, we find our Concept-QA model to be competitive with state-of-the-art VQA models in terms of answering accuracy but with an order of magnitude fewer parameters. This allows for seamless integration of Concept-QA into the V-IP framework as a fast-answering mechanism. We name this method Concept-QA+V-IP. Finally, we show on several datasets that Concept-QA+V-IP produces shorter query chains which are more interpretable and accurate than V-IP trained with a baseline CLIP-based answering mechanism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=yqAToOgxgf": {
    "title": "An old dog can learn (some) new tricks: A tale of a three-decade old architecture",
    "volume": "review",
    "abstract": "Designing novel architectures often involves combining or extending familiar components such as convolutions and attention modules. However, this approach can obscure the fundamental design principles as the focus is usually on the entire architecture. Instead, this paper takes an unconventional approach, attempting to rejuvenate an old architecture with modern tools and techniques. Our primary objective is to explore whether a 30-year-old architecture can compete with contemporary models, when equipped with modern tools. Through experiments spanning image recognition datasets, we aim to understand what aspects of the architecture contribute to its performance. We find that while an ensemble of ingredients bears significance in achieving commendable performance, only a few pivotal components have a large impact. We contend that our discoveries offer valuable insights for creating cutting-edge architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJNCnkDRkY": {
    "title": "Generative Pre-Trained Speech Language Model with Efficient Hierarchical Transformer",
    "volume": "review",
    "abstract": "While recent advancements in speech language modeling have achieved significant progress, they face remarkable challenges in modelling the long acoustic sequence of neural audio codecs. Previous speech language models are compelled to learn acoustic tokens through a multi-stage generation process, which hinders their performance due to error propagation and information loss. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-Trained \\textbf{S}peech Language Model (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of raw audio waveforms in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identity unconditionally. When provided a brief 3-second prompt, GPST is able to produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality and speaker similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=92yrETgM6G": {
    "title": "Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration",
    "volume": "review",
    "abstract": "We introduce a new framework of adversarial attacks, named calibration attacks, in which the attacks are generated and organized to trap victim models to be miscalibrated without altering their original accuracy, hence seriously endangering the trustworthiness of the models and any decision-making based on their confidence scores. Specifically, we identify four novel forms of calibration attacks: underconfidence attacks, overconfidence attacks, maximum miscalibration attacks, and random confidence attacks, in both the black-box and white-box setups. We then test these new attacks on typical victim models with comprehensive datasets, demonstrating that even with a relatively low number of queries, the attacks can create significant calibration mistakes. We further provide detailed analyses to understand different aspects of calibration attacks. Building on that, we investigate the effectiveness of widely used adversarial defences and calibration methods against these types of attacks, which then inspires us to devise two novel defences against such calibration attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=lWe3GBRem8": {
    "title": "Offline RL for Online RL: Decoupled Policy Learning for Mitigating Exploration Bias",
    "volume": "review",
    "abstract": "It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when any prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that na\\\"ive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose the Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL), where an optimistic (_exploration_) policy is used to interact with the environment, and a _separate_ pessimistic (_exploitation_) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14\\% to 26\\% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and also improves online RL performance by 165\\% on two OpenAI gym environments. Further, OOO RL can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=eVpjeCNsR6": {
    "title": "EraseDiff: Erasing Data Influence in Diffusion Models",
    "volume": "review",
    "abstract": "In response to data protection regulations and the ``right to be forgotten'', in this work, we introduce an unlearning algorithm for diffusion models. Our algorithm equips a diffusion model with a mechanism to mitigate the concerns related to data memorization. To achieve this, we formulate the unlearning problem as a bi-level optimization problem, wherein the outer objective is to preserve the utility of the diffusion model on the remaining data. The inner objective aims to scrub the information associated with forgetting data by deviating the learnable generative process from the ground-truth denoising procedure. To solve the resulting bi-level problem, we adopt a first-order method, having superior practical performance while being vigilant about the diffusion process and solving a bi-level problem therein. Empirically, we demonstrate that our algorithm can preserve the model utility, effectiveness, and efficiency while removing across two widely-used diffusion models and in both conditional and unconditional image generation scenarios. In our experiments, we demonstrate the unlearning of classes, attributes, and even a race from face and object datasets such as UTKFace, CelebA, CelebA-HQ, and CIFAR10. The source code of our algorithm is available at https://github.com/AnonymousUser-hello/DiffusionUnlearning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=5ep85sakT3": {
    "title": "Contextual Bandits with Online Neural Regression",
    "volume": "review",
    "abstract": "Recent works have shown a reduction from contextual bandits to online regression under a realizability assumption \\citep{foster2020beyond,foster2021efficient}. In this work, we investigate the use of neural networks for such online regression and associated Neural Contextual Bandits (NeuCBs). Using existing results for wide networks, one can readily show a ${\\mathcal{O}}(\\sqrt{T})$ regret for online regression with square loss, which via the reduction implies a ${\\mathcal{O}}(\\sqrt{K} T^{3/4})$ regret for NeuCBs. Departing from this standard approach, we first show a $\\mathcal{O}(\\log T)$ regret for online regression with almost convex losses that satisfy QG (Quadratic Growth) condition, a generalization of the PL (Polyak-\\L ojasiewicz) condition, and that have a unique minima. Although not directly applicable to wide networks since they do not have unique minima, we show that adding a suitable small random perturbation to the network predictions surprisingly makes the loss satisfy QG with unique minima. Based on such a perturbed prediction, we show a ${\\mathcal{O}}(\\log T)$ regret for online regression with both squared loss and KL loss, and subsequently convert these respectively to $\\tilde{\\mathcal{O}}(\\sqrt{KT})$ and $\\tilde{\\mathcal{O}}(\\sqrt{KL^*} + K)$ regret for NeuCB, where $L^*$ is the loss of the best policy. Separately, we also show that existing regret bounds for NeuCBs are $\\Omega(T)$ or assume i.i.d. contexts, unlike this work. Finally, our experimental results on various datasets demonstrate that our algorithms, especially the one based on KL loss, persistently outperform existing algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tr0KidwPLc": {
    "title": "Evaluating Large Language Models at Evaluating Instruction Following",
    "volume": "review",
    "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever-increasing list of models. This paper investigates the efficacy of these \"LLM evaluators\", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the instructions. We introduce a challenging meta-evaluation benchmark, LLMBAR, designed to test the ability of an LLM evaluator to discern instruction-following outputs. The authors curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that could mislead an LLM evaluator. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBAR and even the highest-scoring LLM evaluators have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBAR, we hope to offer more insight into the behavior of LLM evaluators and foster research in developing better instruction-following models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=A81iom2Y41": {
    "title": "Be Your Own Neighborhood: Detecting Adversarial Example by the Neighborhood Relations Built on Self-Supervised Learning",
    "volume": "review",
    "abstract": "Deep Neural Networks (DNNs) have achieved excellent performance in various fields. However, DNNs' vulnerability to Adversarial Examples (AE) hinders their deployments to safety-critical applications. This paper presents a novel AE detection framework, named BEYOND, for trustworthy predictions. BEYOND performs the detection by distinguishing the AE's abnormal relation with its augmented versions, i.e. neighbors, from two prospects: representation similarity and label consistency. An off-the-shelf Self-Supervised Learning (SSL) model is used to extract the representation and predict the label for its highly informative representation capacity compared to supervised learning models. For clean samples, their representations and predictions are closely consistent with their neighbors, whereas those of AEs differ greatly. Furthermore, we explain this observation and show that by leveraging this discrepancy BEYOND can effectively detect AEs. We develop a rigorous justification for the effectiveness of BEYOND. Furthermore, as a plug-and-play model, BEYOND can easily cooperate with the Adversarial Trained Classifier (ATC), achieving the state-of-the-art (SOTA) robustness accuracy. Experimental results show that BEYOND outperforms baselines by a large margin, especially under adaptive attacks. Empowered by the robust relation net built on SSL, we found that BEYOND outperforms baselines in terms of both detection ability and speed. Our code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pJBSzGmb9a": {
    "title": "On the Global Convergence of Natural Actor-Critic with Neural Network Parametrization",
    "volume": "review",
    "abstract": "Despite the empirical effectiveness of natural actor-critic (NAC) algorithms, their theoretical underpinnings remain relatively unexplored, especially with neural network parameterizations. In the existing literature, the non-asymptotic sample complexity bounds for NAC hold only when the critic is either tabular or are represented by a linear function. In this work, we relax such assumptions for NAC and utilize multi-layer neural network parameterization of the critic and an arbitrary smooth function for the actor. We establish the non-asymptotic sample complexity bounds of $\\tilde{\\mathcal{O}}\\left(\\frac{1}{\\epsilon^{4}(1-\\gamma)^{4}}\\right)$ for the global convergence of NAC algorithm. We obtain this result using our unique decomposition of the error incurred at each critic step. The critic error is decomposed into the error incurred in fitting the sampled data, the error incurred due to the lack of knowledge of the transition matrix as well as the error incurred due to the limited approximation power of the class of neural networks. In contrast to the existing works for NAC with neural network parameterization of the critic, our analysis does not require i.i.d sampling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTssMmhC2X": {
    "title": "How to Fine-Tune Vision Models with SGD",
    "volume": "review",
    "abstract": "SGD and AdamW are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we find that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first \"embedding\" layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1% of the parameters) leads to SGD with or without momentum performing slightly better than AdamW while using less memory (e.g., on ViT-L, SGD uses 33% less GPU memory). Our insights result in state-of-the-art accuracies on five popular distribution shift benchmarks: WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=oxjeePpgSP": {
    "title": "Backdoor Contrastive Learning via Bi-level Trigger Optimization",
    "volume": "review",
    "abstract": "Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR). This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g., 99\\% ASR on ImageNet-100) with a very low poisoning rate (1\\%). Besides, our attack can effectively evade existing state-of-the-art defenses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=JpyWPfzu0b": {
    "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
    "volume": "review",
    "abstract": "This paper presents PaLI-3, a smaller, faster and stronger vision language model (VLM) that compares favorably to similar models that are 10x larger. As part of arriving at this strong performance, we compare Vision Transformer (ViT) models pretrained using classification objectives to contrastively pretrained ones (SigLIP). We find that, while slightly underperforming on standard image classification benchmarks, SigLIP-based PaLI shows superior performance across various multimodal benchmarks, especially on localization and text understanding. The SigLIP encoder we use is a scaled-up version using 2 billion parameters, and achieves a new state-of-the-art on multilingual cross-modal retrieval. We consider that PaLI-3, at only 5B parameters, rekindles research on fundamental pieces of complex VLMs, and could fuel a new generation of scaled-up models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=VyWv7GSh5i": {
    "title": "A Novel Variational Lower Bound For Inverse Reinforcement Learning",
    "volume": "review",
    "abstract": "Inverse reinforcement learning (IRL) seeks to learn the reward function from expert trajectories, to understand the task for imitation or collaboration thereby removing the need for manual reward engineering. However, IRL in the context of large, high-dimensional problems with unknown dynamics has been particularly challenging. In this paper, we present a new variational lower bound for IRL, which is derived under the framework of a probabilistic graphical model with an optimality node. Our method simultaneously learns the reward function and policy under the learned reward function by maximizing the lower bound, which is equivalent to minimizing the reverse Kullback–Leibler divergence between an approximated distribution of optimality given the reward function and the true distribution of optimality given trajectories. This leads to a new IRL method that learns a valid reward function such that the policy under the learned reward achieves expert-level performance on several known domains. Importantly, the method outperforms the existing state-of-the-art IRL algorithms on these domains by demonstrating better reward from the learned policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.8,
    "authors": []
  },
  "https://openreview.net/forum?id=hz9TMobz2q": {
    "title": "Push: Concurrent Probabilistic Programming for Bayesian Deep Learning",
    "volume": "review",
    "abstract": "We introduce a library called Push that takes a probabilistic programming approach to Bayesian deep learning (BDL). This library enables concurrent execution of BDL inference algorithms on multi-GPU hardware for neural network (NN) models. To accomplish this, Push introduces an abstraction that represents an input NN as a particle. Push enables easy creation of particles so that an input NN can be replicated and particles can communicate asynchronously so that a variety of parameter updates can be expressed, including common BDL algorithms. Our hope is that Push lowers the barrier to experimenting with BDL by streamlining the scaling of particles across GPUs. We evaluate the scaling behavior of particles on single-node multi-GPU devices on vision and scientific machine learning (SciML) tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=E78OaH2s3f": {
    "title": "CAS: A Probability-Based Approach for Universal Condition Alignment Score",
    "volume": "review",
    "abstract": "Recent conditional diffusion models have shown remarkable advancements and have been widely applied in fascinating real-world applications. However, samples generated by these models often do not strictly comply with user-provided conditions. Due to this, there have been few attempts to evaluate this alignment via pre-trained scoring models to select well-generated samples. Nonetheless, current studies are confined to the text-to-image domain and require large training datasets. This suggests that crafting alignment scores for various conditions will demand considerable resources in the future. In this context, we introduce a universal condition alignment score that leverages the conditional probability measurable through the diffusion process. Our technique operates across all conditions and requires no additional models beyond the diffusion model used for generation, effectively enabling self-rejection. Our experiments validate that our met- ric effectively applies in diverse conditional generations, such as text-to-image, {instruction, image}-to-image, edge-/scribble-to-image, and text-to-audio",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tnAPOvvNzZ": {
    "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",
    "volume": "review",
    "abstract": "Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in various applications, showcasing improved performance, adaptability, robustness, and controllability. By overcoming the limitations of TextTuning, JsonTuning demonstrates significant potential for more effective and reliable LLMs capable of handling diverse scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TDxtP8nxkh": {
    "title": "NAP2: Neural Networks Hyperparameter Optimization Using Weights and Gradients Analysis",
    "volume": "review",
    "abstract": "Recent hyper-parameter tuning methods for deep neural networks (DNNs) generally rely on first using low-fidelity methods to identify promising configurations and then using high-fidelity methods for further evaluation. While effective, existing solutions treat DNNs as `black boxes', which limits their predictive abilities. In this work, we propose Neural Architectures Performance Prediction (NAP2), a `white box' hyperparameter optimization approach. NAP2 models the changes in the weights and gradients of the analyzed networks over time and can predict their final performance with high accuracy, even after a short training period. Our evaluation shows that NAP2 outperforms the current state-of-the-art both in its ability to identify top-performing architectures and in the amount of resources it utilizes. Moreover, we show that our approach is transferable, meaning it is possible to train NAP2 on one dataset and apply it to another",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5GX6s5TpmV": {
    "title": "The Certification Paradox: Certifications Admit Better Evasion Attacks",
    "volume": "review",
    "abstract": "In guaranteeing the absence of adversarial examples in bounded spaces, certification mechanisms play an important role in demonstrating neural network robustness. Within this work we ask if certifications themselves can potentially compromise the very models they help to protect? By demonstrating a new attack surface that exploits certified guarantees to construct norm minimising evasion attacks, we demonstrate the heretofore unexplored risks inherent in releasing certifications. Our new *Certification Aware Attack* produces smaller, more difficult to detect adversarial examples more than $74$% of the time than comparable attacks, while reducing the median perturbation norm by more than $10$%. That this is achievable in significantly less computational time highlights an apparent paradox---that releasing certifications can reduce security",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=IsGsv8qEHp": {
    "title": "Human-oriented Representation Learning for Robotic Manipulation",
    "volume": "review",
    "abstract": "Humans inherently possess generalizable visual representations that empower them to efficiently explore and interact with the environments in manipulation tasks. We advocate that such a representation automatically arises from simultaneously learning about multiple simple perceptual skills that are critical for everyday scenarios (e.g., hand detection, state estimate, etc.) and is better suited for learning robot manipulation policies compared to current state-of-the-art visual representations purely based on self-supervised objectives. We formalize this idea through the lens of human-oriented multi-task fine-tuning on top of pre-trained visual encoders, where each task is a perceptual skill tied to human-environment interactions. We introduce Task Fusion Decoder as a plug-and-play embedding translator that utilizes the underlying relationships among these perceptual skills to guide the representation learning towards encoding meaningful structure for what's important for all perceptual skills, ultimately empowering learning of downstream robotic manipulation tasks. Extensive experiments across a range of robotic tasks and embodiments, in both simulations and real-world environments, show that our Task Fusion Decoder consistently improves the representation of three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for downstream manipulation policy-learning. More demos, datasets, models, and code can be found at https://sites.google.com/view/human-oriented-robot-learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByR3NdDSZB": {
    "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning",
    "volume": "review",
    "abstract": "We present a novel unified bilevel optimization-based framework, \\textsf{PARL}, formulated to address the recently highlighted critical issue of policy alignment in reinforcement learning using utility or preference-based feedback. We identify a major gap within current algorithmic designs for solving policy alignment due to a lack of precise characterization of the dependence of the alignment objective on the data generated by policy trajectories. This shortfall contributes to the sub-optimal performance observed in contemporary algorithms. Our framework addressed these concerns by explicitly parameterizing the distribution of the upper alignment objective (reward design) by the lower optimal variable (optimal policy for the designed reward). Interestingly, from an optimization perspective, our formulation leads to a new class of stochastic bilevel problems where the stochasticity at the upper objective depends upon the lower-level variable. To demonstrate the efficacy of our formulation in resolving alignment issues in RL, we devised an algorithm named \\textsf{A-PARL} to solve PARL problem, establishing sample complexity bounds of order $\\mathcal{O}(1/T)$. Our empirical results substantiate that the proposed \\textsf{PARL} can address the alignment concerns in RL by showing significant improvements (up to 63\\% in terms of required samples) for policy alignment in large-scale environments of the Deepmind control suite and Meta world tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tsE5HLYtYg": {
    "title": "SafeDreamer: Safe Reinforcement Learning with World Models",
    "volume": "review",
    "abstract": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of world models has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. Further details and resources are available on the project website: https://sites.google.com/view/safedreamer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=5BCFlnfE1g": {
    "title": "Demystifying CLIP Data",
    "volume": "review",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its \\textit{data} and \\textit{not} the \\textit{model} architecture or pre-training {objective}. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP's data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8\\% accuracy, surpassing CLIP's 68.3\\% on \\mbox{ViT-B} models. Scaling to 1B data, while maintaining the same training budget, attains \\textbf{72.4\\%}. Our observations hold across various model sizes, exemplified by ViT-H achieving \\textbf{80.5\\%}, without any bells-and-whistles. Curation code and training data distribution over metadata will be made available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=z7usV2BlEE": {
    "title": "Making Large Language Models Better Reasoners with Alignment",
    "volume": "review",
    "abstract": "Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \\textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. In this paper, we introduce an \\textit{Alignment Fine-Tuning (AFT)} paradigm with a novel \\textit{Constrained Alignment Loss} to alleviate the assessment misalignment problem. Specifically, the proposed loss has two objectives: a) Alignment, which guarantees the scores of high-quality COTs surpass that of subpar ones; b) Constraint, which keeps the subpar scores confined to a reasonable range to prevent the model degradation. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT. AFT also performs well in multi-task and out-of-distribution situations. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=DiWRG9JTWZ": {
    "title": "MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation",
    "volume": "review",
    "abstract": "Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC). Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from \\underline{seen} training distributions but recognizes novel classes sampled from unseen testing distributions. In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment. Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks. To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios. Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model. Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning. The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts. We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ufvwhR3XmN": {
    "title": "A Joint Spectro-Temporal Relational Thinking Based Acoustic Modeling Framework",
    "volume": "review",
    "abstract": "Relational thinking refers to the inherent ability of humans to form mental impressions about relations between sensory signals and prior knowledge, and subsequently incorporate them into their model of their world. This ability plays a key role in human understanding of speech, yet it has not been a prominent feature in any artificial speech recognition systems. Recently, there have been some attempts to correct this oversight, but these have been limited to coarse utterance-level models that operate exclusively in the time domain. In an attempt to narrow the gap between artificial systems and human abilities, this paper presents a novel spectro-temporal relational thinking based acoustic modeling framework. Specifically, it first generates numerous probabilistic graphs to model the relations among consecutive speech segments across both time and frequency domains. These graphs are then coupled and transformed into latent representations for downstream tasks, during which meaningful spectro-temporal patterns formed by the co-occurrence of certain node pairs can be uncovered. Models built upon this framework outperform state-of-the-art systems with a 7.82% improvement in phoneme recognition tasks. In-depth analyses further reveal that our proposed relational thinking modeling mainly improves the model's ability to recognize vowel phonemes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=5sixirvG0I": {
    "title": "Whittle Index with Multiple Actions and State Constraint for Inventory Management",
    "volume": "review",
    "abstract": "Whittle index is a heuristic tool that leads to good performance for the restless bandits problem. In this paper, we extend Whittle index to a new multi-agent reinforcement learning (MARL) setting with multiple discrete actions and a possibly changing constraint on the state space, resulting in WIMS (Whittle Index with Multiple actions and State constraint). This setting is common for inventory management where each agent chooses a replenishing quantity level for the corresponding stock-keeping-unit (SKU) such that the total profit is maximized while the total inventory does not exceed a certain limit. Accordingly, we propose a deep MARL algorithm based on WIMS for inventory management. Empirically, our algorithm is evaluated on real large-scale inventory management problems with up to 2307 SKUs and outperforms operation-research-based methods and baseline MARL algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=9vZ8UjP2Mz": {
    "title": "Exploring the Generalization Capabilities of AID-based Bi-level Optimization",
    "volume": "review",
    "abstract": "Bi-level optimization has achieved considerable success in contemporary machine learning applications, especially for given proper hyperparameters. However, due to the two-level optimization structure, commonly, researchers focus on two types of bi-level optimization methods: approximate implicit differentiation (AID)-based and iterative differentiation (ITD)-based approaches. ITD-based methods can be readily transformed into single-level optimization problems, facilitating the study of their generalization capabilities. In contrast, AID-based methods cannot be easily transformed similarly but must stay in the two-level structure, leaving their generalization properties enigmatic. In this paper, although the outer-level function is nonconvex, we ascertain the uniform stability of AID-based methods, which achieves similar results to a single-level nonconvex problem. We conduct a convergence analysis for a carefully chosen step size to maintain stability. Combining the convergence and stability results, we give the generalization ability of AID-based bi-level optimization methods. Furthermore, we carry out an ablation study of the parameters and assess the performance of these methods on real-world tasks. Our experimental results corroborate the theoretical findings, demonstrating the effectiveness and potential applications of these methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=G1DoOVM3xZ": {
    "title": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation",
    "volume": "review",
    "abstract": "The exploration-exploitation dilemma has been a central challenge in reinforcement learning (RL) with complex model classes. In this paper, we propose a new algorithm, Monotonic Q-Learning with Upper Confidence Bound (MQL-UCB) for RL with general function approximation, where the Bellman operator of the underlying Markov decision process (MDP) is assumed to map any value functions into a function class with a bounded eluder dimension. Our key algorithmic design includes: (1) a general deterministic policy-switching strategy that achieves low switching cost, (2) a monotonic value function structure with carefully controlled function class complexity, and (3) a variance weighted regression scheme that exploits historical trajectories with high data efficiency. MQL-UCB achieves minimax optimal regret of $\\tilde{O}(d\\sqrt{HK})$ when $K$ is sufficiently large and near optimal policy switching cost of $\\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$ being the planning horizon, and $K$ being the number of episodes. Our work sheds light on designing provably sample-efficient and deployment-efficient Q-learning with nonlinear function approximation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=HHbRxoDTxE": {
    "title": "Looped Transformers are Better at Learning Learning Algorithms",
    "volume": "review",
    "abstract": "Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. (2022). However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10% of the parameter count",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=mF3cTns4pe": {
    "title": "Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs",
    "volume": "review",
    "abstract": "Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qaKRfobbTg": {
    "title": "Learning Thresholds with Latent Values and Censored Feedback",
    "volume": "review",
    "abstract": "In this paper, we investigate a problem of *actively* learning threshold in latent space, where the *unknown* reward $g(\\gamma, v)$ depends on the proposed threshold $\\gamma$ and latent value $v$ and it can be $only$ achieved if the threshold is lower than or equal to the *unknown* latent value. This problem has broad applications in practical scenarios, e.g., reserve price optimization in online auctions, online task assignments in crowdsourcing, setting recruiting bars in hiring, etc. We first characterize the query complexity of learning a threshold with the expected reward at most $\\epsilon$ smaller than the optimum and prove that the number of queries needed can be infinitely large even when $g(\\gamma, v)$ is monotone with respect to both $\\gamma$ and $v$. On the positive side, we provide a tight query complexity $\\tilde{\\Theta}(1/\\epsilon^3)$ when $g$ is monotone and the CDF of value distribution is Lipschitz. Moreover, we show a tight $\\tilde{\\Theta}(1/\\epsilon^3)$ query complexity can be achieved as long as $g$ satisfies one-sided Lipschitzness, which provides a complete characterization for this problem. Finally, we extend this model to an online learning setting and demonstrate a tight $\\Theta(T^{2/3})$ regret bound using continuous-arm bandit techniques and the aforementioned query complexity results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7uPprVelO": {
    "title": "GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature",
    "volume": "review",
    "abstract": "The generation of diverse but realistic objects that have combinatorial properties has various practical applications across several fields, including computer graphics, animation, industrial design, material science, etc. For instance, we might want to restrict the output of the generator so that it satisfies discrete constraints or encourage certain combinatorial properties as a penalty. However, existing generative models and optimization solvers often struggle to concurrently ensure solution diversity and uphold the underlying combinatorial nature. To address this, we propose $GenCO$, a novel framework that conducts end-to-end training of deep generative models integrated with embedded combinatorial solvers, aiming to uncover high-quality solutions aligned with nonlinear objectives. While structurally akin to conventional generative models, $GenCO$ diverges in its role - it focuses on generating instances of combinatorial optimization problems rather than final objects (e.g., images). This shift allows finer control over the generated outputs, enabling assessments of their feasibility and introducing an additional combinatorial loss component. We demonstrate the effectiveness of our approach on a variety of generative tasks characterized by combinatorial intricacies, including game level generation and map creation for path planning, consistently demonstrating its capability to yield diverse, high-quality solutions that reliably adhere to user-specified combinatorial properties",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=lebNJk3ul9": {
    "title": "A space-continuous implementation of Proper Orthogonal Decomposition by means of Neural Networks",
    "volume": "review",
    "abstract": "In the realm of reduced order modeling, the Proper Orthogonal Decomposition (POD) has established itself as a widely adopted technique for efficiently handling parametric partial differential equations. This approach exploits principles of linear algebra to extract, from a collection of high-fidelity numerical solutions, an optimized reduced space capable of linearly representing the input data. This paper aims to introduce an innovative alternative to replicate the capabilities of POD by harnessing the power of neural networks, thereby overcoming the constraint of exclusively working with solutions confined to the same topological space. Our method centers around the utilization of the DeepONet architecture, which is applied and minimally modified to emulate the POD spatial-temporal (or parametric) decomposition. This novel adaptation enables the creation of a continuous representation of spatial modes. Although the accuracy gap between neural networks and linear algebraic tools is still evident, this architecture exhibits a distinct advantage: it can accept solutions generated through different discretization schemes, contrary to the conventional POD approach. Furthermore, our approach allows various enhancements and variants developed to augment the capabilities of POD. These can be seamlessly integrated into the architecture, offering a versatile and adaptable framework known as PODNet. To validate its effectiveness, we apply it to two distinct test cases: a simple 1D trigonometric problem and a more complex 2-dimensional Graetz problem. In doing so, we conduct a comprehensive comparison between our proposed methodology and established approaches, shedding light on the potential advantages and trade-offs inherent to this innovative fusion of neural networks and traditional reduced order modeling techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WtHKqtHVXo": {
    "title": "Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require accurate movement. % It is an open question how well such approaches can work for high-precision, contact-rich tasks that require controlling contact forces with the environment. % We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks in a zero-shot fashion. % Specifically, we reparameterize the action space to include robot compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. % We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and the IROS 2020 Robotic Grasping and Manipulation Competition, where zero-shot policy generation in this action space improves success rates by greater than 3x and 4x, respectively, over a baseline that uses free space motions. % To further investigate properties that make language models well posed to generate contact-rich tasks, we also analyse language models ability to complete control-relevant arithmetic reasoning tasks over continuous numbers in-context and ablate the importance of different prompt components in generating relevant motion patterns. Project webpage: https://dex-code-gen.github.io/dex-code-gen/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=hac6DzbMa7": {
    "title": "Continual Learning with Orthogonal Weights and Knowledge Transfer",
    "volume": "review",
    "abstract": "Orthogonal projection has been shown highly effective at overcoming *catastrophic forgetting* (CF) in continual learning (CL). Existing orthogonal projection methods are *all* based on *orthogonal gradients* (OG) between tasks. However, this paper shows theoretically that OG cannot guarantee CF elimination, which is a major limitation of the existing OG-based CL methods. Our theory further shows that only the *weight/parameter-level orthogonality* between tasks can guarantee CF elimination as the final classification is computed based on the network weights/parameters only. Existing OG-based methods also have two other *inherent limitations*, i.e., *over-consumption of network capacity* and *limiting knowledge transfer* (KT) across tasks. KT is also a core objective of CL. This paper then proposes a novel *weight-level orthogonal projection* method (called STIL), which ensures that each task occupies a weight subspace that is orthogonal to those of the other tasks. The method also addresses the two other limitations of the OG-based methods. Extensive evaluations show that the proposed STIL not only overcomes CF better than baselines, but also, perhaps more importantly, performs KT much better than them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Qwq4cpLtoX": {
    "title": "Exploring the Relationship Between Model Architecture and In-Context Learning Ability",
    "volume": "review",
    "abstract": "What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and state-space models. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while others demonstrate periods of stagnation followed by abrupt mastery of the task. Finally, and somewhat surprisingly, we find that several state-space model variants are more robust in-context learners than transformers; since state-space models have constant-sized memory footprints at inference time, this result opens the future possibility of scaling up in-context learning to vastly larger numbers of in-context examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TfbpnxTJt3": {
    "title": "Federated Learning with Local Openset Noisy Labels",
    "volume": "review",
    "abstract": "Federated learning is a learning paradigm that allows the central server to learn from different data sources while keeping the data private locally. Without controlling and monitoring the local data collection process, the locally available training labels are likely noisy, $\\textit{i.e.}$, the collected training labels differ from the unobservable ground truth. Additionally, in heterogenous FL, each local client may only have access to a subset of label space (referred to as openset label learning), meanwhile without overlapping with others. In this work, we study the challenge of federated learning with local openset noisy labels. We observe that many existing solutions in the noisy label literature, $\\textit{e.g.}$, loss correction, are ineffective during local training due to overfitting to noisy labels and being not generalizable to openset labels. To address the problems, we design a label communication mechanism that shares randomly selected ``contrastive labels\" among clients. The privacy of the shared contrastive labels is protected by label differential privacy (DP). Both the DP guarantee and the effectiveness of our approach are theoretically guaranteed. Compared with several baseline methods, our solution shows its efficiency in several public benchmarks and real-world datasets under different noise ratios and noise models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=2zoi9YI21Y": {
    "title": "Towards a Self-Made Model: Zero-Shot Self-Supervised Purification for Adversarial Attacks",
    "volume": "review",
    "abstract": "Adversarial purification is an adversarial defense method without robustness training for the classifier and regardless of the form of attacks, aiming to remove the adversarial perturbations on the attacked images. Such methods can defend against various unseen threats without modifying the classifier in contrast to empirical defenses. However, previous purification methods require careful training of a strong generative model or incorporating additional knowledge when training a classifier to be comparable to adversarial training. Retraining promising generative models or classifiers on large-scale datasets (e.g., ImageNet) is extremely challenging and computation-consuming. In this work, following the natural image manifold hypothesis, we propose a zero-shot self-supervised method for adversarial purification named \\textit{ZeroPur}: For an adversarial example that lies beyond the natural image manifold, its corrupted embedding vector is first restored so that it is moved close to the natural image manifold. The embedding is then fine-tuned on finer intermediate-level discrepancies to project it back within the manifold. The whole purification process is done from coarse to fine, which does not rely on any generative model and does not require retraining the classifier to incorporate additional knowledge. Extensive experiments on three datasets including CIFAR-10, CIFAR-100, and ImageNet with various classifier architectures including ResNet and WideResNet, demonstrate that our method achieves state-of-the-art robust performance. Code released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=cLqCZ740vw": {
    "title": "Benchmarking Smoothness and Reducing High-Frequency Oscillations in Continuous Control Policies",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) policies are prone to high frequency oscillations, specially undesirable when deploying to hardware in the real-world. In this paper, we identify, categorize, and compare methods from the literature that aim to mitigate high frequency oscillations in RL. We define two broad classes: loss regularization and architectural methods. At their core, they incentivize learning a smooth mapping, such that nearby states in the input space produce nearby actions in the output space. We present benchmarks in terms of policy performance and smoothness with staple RL environments from Gymnasium, as well as two robotics locomotion tasks that include deployment and evaluations in the real-world. Finally, we also propose hybrid methods that combine elements from both loss regularization and architectural methods, and outperform the existing approaches in the simulation benchmarks as well as in the real-world",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=AZVmYg3LvS": {
    "title": "Improved Function Space Variational Inference with Informative Priors",
    "volume": "review",
    "abstract": "Function space variational inference allows Bayesian neural network (BNN) to introduce the prior distribution on the function space directly. Moreover, Recent linear approximation scheme for KL divergence between two random functions, has presented the tractable training objective and thus facilitates imposing the function space prior on BNNs. On the other hand, despite of its tractability, the existing inference suffers from the interpretability issue because the this function space prior is obtained by mapping the pre-defined weight-space prior to the function output via the complex neural network, and thus seems to be less interpretable. Alternatively, thought the uniform function space prior, that imposes a zero mean prior on the function space to encourage the model to be uncertain for out-of-training set, has been considered, this prior can introduce unnecessary uncertainty into the function outputs of the training datasets. Thus, this can cause the trade-off between the uncertainty estimation performances on the in-training and out-of-training sets. In this work, we aim at refining the function space variational inference to handle the mentioned issue. To this end, we first reconsider the role of the function space prior in view of Bayesian Model prediction, and then build the function space prior to help improve the uncertainty estimation of the BNNs. Additionally, we propose a refined variational distribution on function space to encourage the useful predictive functions in sense of Bayesian model averaging, to be sampled, and thus improving the prediction of the BNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=b8hRudcKQ3": {
    "title": "Performance Adjustment for Federated Learning Marketplace",
    "volume": "review",
    "abstract": "In federated learning, client participation is mainly motivated by performance-gain rewards or monetary rewards. In practice, different clients may have varying preferences over these two types of rewards. However, optimizing the training process to align model performance and monetary rewards with client expectations remains an open challenge. To accommodate diverse reward preferences, we propose Alpha-Tuning, an FL performance adjustment framework guided by dynamic validation loss composition. The core of our framework is a mechanism to decide the weights assigned to clients' local validation loss, each of which is determined by the corresponding client's performance contribution in the given training round and its monetary quotation for biasing this FL course towards its favor. The training hyper-parameters and model aggregation weights are adjusted together with model parameters to minimize the weighted sums of clients' local validation losses in our framework. Paired with a payment rule designed to compensate the clients according to their data contribution, Alpha-Tuning balances the clients' preferences between the performance gain and monetary reward. We demonstrate the effectiveness of our framework by conducting experiments on the federated learning tasks under various client quotation settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=wOMy6J8epf": {
    "title": "A counterfactual-based approach to prevent crowding in intelligent subway systems",
    "volume": "review",
    "abstract": "Today, the cities we live in are far from being truly smart: overcrowding, pollution, and poor transportation management are still in the headlines. With wide-scale deployment of advanced Artificial Intelligence (AI) solutions, however, it is possible to reverse this course and apply appropriate countermeasures to take a step forward on the road to sustainability. In this research, explainable AI techniques are applied to provide public transportation experts with suggestions on how to control crowding on subway platforms by leveraging interpretable, rule-based models enhanced with counterfactual explanations. The experimental scenario relies on agent-based simulations of the De Ferrari Hitachi subway station of Genoa, Italy. Numerical results for both prediction of crowding and counterfactual (i.e., countermeasures) properties are encouraging. Moreover, an assessment of the quality of the proposed explainable methodology was submitted to a team of experts in the field to validate the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=nJnky5K944": {
    "title": "Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?",
    "volume": "review",
    "abstract": "Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that one-layer and single-head Transformers have a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=WSsP7W8tqN": {
    "title": "Grokking Tickets: Lottery Tickets Accelerate Grokking",
    "volume": "review",
    "abstract": "Grokking is one of the most surprising puzzles in neural network generalization: a network first reaches a memorization solution with perfect training accuracy but poor generalization, but with further training, it reaches a perfectly generalizable solution. We aim to analyze the mechanism of grokking from the lottery ticket hypothesis, identifying the process to find the lottery tickets (good sparse subnetworks) as the key to describing the transitional phase between memorization and generalization. Firstly, with the lottery tickets identified via the magnitude pruning after perfect generalization, we show that the lottery tickets drastically accelerate grokking compared to the dense networks on various configurations (MLP and Transformer, and an arithmetic and image classification task). We also show that the speedup is significant even when compared with the dense networks with the same weight norm as the lottery tickets. Besides, the speedup only happens when training ``good'' subnetworks are identified at the generalization solution. Specifically, speedup does not happen when using tickets identified at the memorization solution or transition between memorization and generalization or when pruning networks at the initialization (Random pruning, Grasp, SNIP, and Synflow). The results indicate that the weights norm of network parameters is not enough to explain the process of grokking, but the importance of finding good subnetworks to describe the transition from memorization to generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=p7iVaVidha": {
    "title": "OfflineLight: An Offline Reinforcement Learning Model for Traffic Signal Control",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) is gaining popularity in addressing the traffic signal control (TSC) problems. Yet, the trial and error training with environmental interactions for traditional RL-based methods is costly and time-consuming. Additionally, it is challenging to directly deploy a completely pre-trained RL model for all types of intersections. Inspired by recent advances in decision-making systems from offline RL, we propose a general offline actor-critic framework (Offline-AC) that considers policy and value constraints, and an adaptive decision-making model named OfflineLight based on Offline-AC. Offline-AC is further proved general and suitable for developing new offline RL algorithms. Moreover, we collect, organize and release the first offline interaction dataset for TSC (TSC-OID), which is generated from the state-of-the-art (SOTA) RL models that interact with a traffic simulation environment based on multiple datasets of real-world road intersections and traffic flow. Through numerical experiments on real-world datasets, we demonstrate that: (1) Offline RL can build a high-performance RL model without online interactions with the traffic environment; (2) OfflineLight matches or achieves SOTA among recent RL methods; and (3) OfflineLight shows comprehensive generalization performance after completing training on only 20% of the TSC-OID dataset. The relevant dataset and code are available at anonymous URL:https://anonymous.4open.science/r/OfflineLight-6665/README.md",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=scxDIx6StY": {
    "title": "Adaptive Temperature Enhanced Dual-level Hypergraph Contrastive Learning",
    "volume": "review",
    "abstract": "Hypergraphs, which incorporate hyperedges to link multiple nodes and capture complex high-order relationships, have attracted increasing attention in recent years. Consequently, a bunch of hypergraph neural networks has been proposed to model the high-order relationships between hyperedges and nodes. Inspired by the success of graph contrastive learning, researchers have begun exploring the benefits of contrastive learning over hypergraphs. However, these works still have the following limitations in modeling the high-order relationships over unlabeled data: (i) They primarily focus on maximizing the agreements among individual node embeddings while neglecting the capture of group-wise collective behaviors within hypergraphs; (ii) Most of them disregard the importance of the temperature index in discriminating contrastive pairs during contrast optimization. To address these limitations, we propose a novel \\textbf{Ad}aptive \\textbf{T}emperature enhanced \\textbf{Hy}per\\textbf{G}raph \\textbf{C}ontrastive \\textbf{L}earning framework called \\textbf{AdT-HyGCL} to boost contrastive learning over hypergraphs. Specifically, we first introduce a noise enhancement module to generate relatively challenging augmented hypergraphs for hypergraph contrastive tasks. Unlike most works that merely maximize the agreement of node embeddings in hypergraphs, we then propose a dual-level contrast mechanism that not only captures the individual node behaviors in a local context but also models the group-wise collective behaviors of nodes within hyperedges from a community perspective. Furthermore, we design an adaptive temperature-enhanced contrastive optimization to improve the discrimination ability between positive and negative contrastive pairs, thereby facilitating more effective hypergraph representation learning. Theoretical justifications and empirical experiments conducted on eight benchmark hypergraphs demonstrate that AdT-HyGCL exhibits excellent rationality, generalization, effectiveness, and robustness compared to state-of-the-art baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=Cqrv7Sve7g": {
    "title": "Offline Reward Inference on Graph: A New Thinking",
    "volume": "review",
    "abstract": "In offline reinforcement learning, reward inference is the key to learning effective policies in practical scenarios. Due to the expensive or unethical nature of environmental interactions in domains such as healthcare and robotics, reward functions are rarely accessible, and the task of inferring rewards becomes challenging. To address this issue, our research focuses on developing a reward inference method that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. Initially, we leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks substantiate the efficacy of our approach, wherein the utilization of our inferred rewards yields substantial performance enhancements within the offline reinforcement learning framework, particularly when confronted with limited reward annotations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8F6bws5JBy": {
    "title": "Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication",
    "volume": "review",
    "abstract": "Tabular data in the wild are frequently afflicted with class-imbalance, biasing machine learning models towards major classes. A straightforward, data-centric approach to this problem is oversampling - where synthetic minority samples are generated to balance the classes. Although tabular generative models are capable of generating synthetic samples, their integrity suffers when the number of minority samples is low. To this end, language models primed with rich prior knowledge are a fitting candidate for the task at hand. However, an oversampling strategy utilizing the extensive capabilities of such language models is yet to emerge. In this paper, we propose a novel tabular oversampling framework to channel the power of language interfaces. By leveraging its conditional sampling capabilities, we synthesize minority samples by progressively masking the important features of the majority class samples and imputing them towards the minority distribution. To reduce the inclusion of imperfectly converted samples, we utilize the power of the language model itself to self-authenticate the labels of the samples generated by itself, sifting out ill-converted samples. Extensive experiments on a variety of datasets and imbalance ratios reveal that the proposed method successfully generates reliable minority samples to boost the performance of machine learning classifiers, even under heavy imbalance ratios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5bNYf0CqxY": {
    "title": "Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks",
    "volume": "review",
    "abstract": "The spiking neural networks are inspired by the biological neurons that employ binary spikes to propagate information in the neural network. It has garnered considerable attention as the next-generation neural network, as the spiking activity simplifies the computation burden of the network to a large extent and is known for its low energy deployment enabled by specialized neuromorphic hardware. One popular technique to feed a static image to such a network is rate encoding, where each pixel is encoded into random binary spikes, following a Bernoulli distribution that uses the pixel intensity as bias. By establishing a novel connection between rate-encoding and randomized smoothing, we give the first provable robustness guarantee for spiking neural networks against adversarial perturbation of inputs bounded under $l_1$-norm. We introduce novel adversarial training algorithms for rate-encoded models that significantly improve the state-of-the-art empirical robust accuracy result. Experimental validation of the method is performed across various static image datasets, including CIFAR-10, CIFAR-100 and ImageNet-100",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Gf4KZIqLHD": {
    "title": "A Change of Heart: Backdoor Attacks on Security-Centric Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have been employed as defensive tools to reinforce the security of other models, notably in purifying adversarial examples and certifying adversarial robustness. Meanwhile, the prohibitive training costs often make the use of pre-trained diffusion models an attractive practice. The tension between the intended use of these models and their unvalidated nature raises significant security concerns that remain largely unexplored. To bridge this gap, we present DIFF2, a novel backdoor attack tailored to security-centric diffusion models. Essentially, DIFF2 superimposes a diffusion model with a malicious diffusion-denoising process, guiding inputs embedded with specific triggers toward an adversary-defined distribution, while preserving the normal process for other inputs. Our case studies on adversarial purification and robustness certification show that DIFF2 substantially diminishes both post-purification and certified accuracy across various benchmark datasets and diffusion models, highlighting the potential risks of utilizing pre-trained diffusion models as defensive tools. We further explore possible countermeasures, suggesting promising avenues for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=NY3HzOOL3u": {
    "title": "Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks",
    "volume": "review",
    "abstract": "We study building an agent that solves diverse long-horizon tasks in open-world environments. Without human demonstrations, learning to accomplish tasks in a large open-world environment with reinforcement learning (RL) is extremely inefficient. To tackle this challenge, we convert the multi-task learning problem into learning basic skills and planning over the skills, and propose a Finding-skill to improve the sample efficiency for training all the skills. Using the popular open-world game Minecraft as the testbed, we propose three types of fine-grained basic skills, and use RL with intrinsic rewards to acquire skills with high success rates. For skill planning, we leverage the prior knowledge in Large Language Models to find the relationships between skills and build a skill graph. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 40 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines by a large margin and is the most sample-efficient demonstration-free RL method to solve Minecraft Tech Tree tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=2DJUXmHZ2O": {
    "title": "Generalizing Poincaré Policy Representations in Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "Learning policy representations is essential for comprehending the intricacies of agent interactions and their decision-making processes. Recent studies have found that the evolution of any state under Markov decision processes (MDPs) can be divided into multiple hierarchies based on time sequences. This conceptualization resembles a tree-growing process, where the policy and environment dynamics determine the possible branches. In this paper, the multiple agent's trajectory growing paths can be projected into a Poincaré ball, which requires the tree to grow from the origin to the boundary of the ball, deriving a new geometric idea of learning Poincaré Policy Representations (P2R) for MARL. Specifically, P2R captures the policy representation of the Poincaré ball by a hyperbolic neural network and introduces a contrast objective function that encourages embeddings of the same policy to move closer together while embeddings of different policies to move apart, which enables embed policies with low distortion. Experimental results provide empirical evidence for the effectiveness of the P2R framework in cooperative and competitive games, demonstrating the potential of Poincaré policy representations for optimizing policies in complex multi-agent environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ERTp3iQWPW": {
    "title": "A Framework for PromptOps in GenAI Application Development Lifecycle",
    "volume": "review",
    "abstract": "The use of \"prompts\" in the creation process of Generative Artificial Intelligence (GenAI) systems is receiving increasing interest. The significance of these prompts throughout the development cycle, however, is not properly used by current software development lifecycle approaches. This study proposes a unique methodology for integrating timely engineering and management into the creation of GenAI applications. Organizations may benefit from using \"PromptOps\" to create GenAI applications more quickly, effectively, and securely. It offers a technique to lower the danger of bias, increase the accuracy and dependability of GenAI systems, and decrease the cost of development and implementation.Our platform facilitates the seamless integration of several automated technologies in software development by performing prompt operations (PromptOps). These include Continuous Integration/Continuous Deployment (CI/CD) pipelines, workflows, APIs, and more. Our approach enables developers to easily include automated technologies, leading to a more simplified and efficient process. Furthermore, this study indicates that the framework may enable all stakeholders, including non-engineering units, to convert prompts into services, expanding their use in the building of applications. This study emphasizes the critical significance of prompts in GenAI and shows how their incorporation may improve AI application development, eventually stimulating creativity and driving the adoption of Generative AI technology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=NY3wMJuaLf": {
    "title": "Fake It Till Make It: Federated Learning with Consensus-Oriented Generation",
    "volume": "review",
    "abstract": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods and has the plug-and-play property",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRbLHpLAy4": {
    "title": "RetPur: Diffusion Purification Model for Defending Hash Retrieval Target Attacks",
    "volume": "review",
    "abstract": "Deep Neural Networks (DNNs) have harnessed their formidable representational capabilities to attain remarkable performance in image retrieval models. Nonetheless, in cases where malicious actors introduce adversarial perturbations into the test dataset, the retrieval model may readily yield results that are either irrelevant or intentionally manipulated by the attacker. Specifically, the targeted attack is notable for producing predefined results, thereby inflicting a more adverse impact on retrieval performance. While adversarial purification has demonstrated effectiveness in countering adversarial attacks, its application in retrieval tasks remains unexplored. Addressing these concerns, we introduce a free-trained purification model denoted as RetPur aimed at purifying adversarial test dataset, thereby mitigating the issue of targeted attacks within both uni-modal and cross-modal retrieval systems. RetPur employs a pre-trained diffusion model, offering a plug-and-play convenience, while utilizing adversarial samples as conditioning factors to guide image generation, thereby enhancing task accuracy. In terms of retrieval system architecture, our study pioneers the incorporation of adversarial purification tasks into uni-modal (Image-to-Image) and cross-modal (Image-to-Image, Image-to-Text) hash retrieval systems, specifically tailored to image retrieval scenarios. Furthermore, we explore the application of adversarial purification tasks to a wider array of attacks, including both generative and iterative approaches. Through an extensive series of experiments, it can be concluded that the purified dataset exhibits retrieval performance in the retrieval systems that is closely akin to that of the original dataset, even across different attacks and modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FeqxK6PW79": {
    "title": "Analyzing Deep Transformer Models for Time Series Forecasting via Manifold Learning",
    "volume": "review",
    "abstract": "Deep transformer models consistently achieve groundbreaking results on natural language processing and computer vision problems, among other engineering and scientific domains. However, despite active research that aims to better understand transformer neural networks via e.g., computing saliency scores or analyzing their attention matrix, these models are not well-understood at large. This problem is further exacerbated for deep time series forecasting methods, for which analysis and understanding work is relatively scarce. Indeed, deep time series forecasting methods only recently emerged as state-of-the-art, and moreover, time series data may be less ``natural'' to interpret and analyze, unlike image and text information. Complimentary to existing analysis studies, we employ a manifold learning viewpoint, i.e., we assume that latent representations of time series forecasting models lie next to a low-dimensional manifold. In this work, we study geometric features of latent data manifolds including their intrinsic dimension and principal curvatures. Our results demonstrate that deep transformer models share a similar geometric behavior across layers, and that geometric features are correlated with model performance. Further, untrained models present different structures, which rapidly converge during training. Our geometric analysis and differentiable tools may be used in designing new and improved deep forecasting neural nets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FNq3nIvP4F": {
    "title": "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction",
    "volume": "review",
    "abstract": "Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips (\"shot-level'') depicting a single scene. To deliver a coherent long video (\"story-level''), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, SEINE, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=FiQRgzKl64": {
    "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts",
    "volume": "review",
    "abstract": "Weight-sharing supernet has become a vital component for performance estimation in the state-of-the-art (SOTA) neural architecture search (NAS) frameworks. Although supernet can directly generate different subnetworks without retraining, there is no guarantee for the quality of these subnetworks because of weight sharing. In NLP tasks such as machine translation and pre-trained language modeling, we observe that given the same model architecture, there is a large performance gap between supernet and training from scratch. Hence, supernet cannot be directly used and retraining is necessary after finding the optimal architectures. In this work, we propose mixture-of-supernets, a generalized supernet formulation where mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model, with negligible training overhead. In this way, different subnetworks do not share the model weights directly, but do so indirectly through an architecture-based routing mechanism. As a result, model weights of different subnetworks are customized towards their specific architectures and the weight generation is learned by gradient descent. Compared to existing weight-sharing supernet for NLP, our method can minimize the retraining time, greatly improving training efficiency. In addition, the proposed method achieves the SOTA performance in NAS for building fast machine translation models, yielding better latency-BLEU tradeoff compared to HAT, the state-of-the-art NAS for MT. We also achieve the SOTA performance in NAS for building memory-efficient task-agnostic BERT models, outperforming NAS-BERT and AutoDistil in various model sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=P2Fjm0nIit": {
    "title": "NeRF Compression via Transform Coding",
    "volume": "review",
    "abstract": "Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing detailed 3D scenes through continuous volumetric representations. Recent NeRFs utilize feature grids to improve rendering quality and speed; however, these representations introduce significant storage overhead. This paper presents a novel method for efficiently compressing a grid-based NeRF model. Our approach is based on the non-linear transform coding paradigm, where we compress the model's feature grids using end-to-end optimized neural compression. Since these neural compressors are overfitted to individual scenes, we develop lightweight decoders and encoder-free compression. To exploit the spatial inhomogeneity of the latent feature grids, we introduce an importance-weighted rate-distortion objective and a sparse entropy model using a masking mechanism. Our experimental results validate that our proposed method surpasses existing works in terms of grid-based NeRF compression efficacy and reconstruction quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=vEEWhGjx0M": {
    "title": "Adversarial Attacks on Combinatorial Multi-Armed Bandits",
    "volume": "review",
    "abstract": "We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, which depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDdSRaOiyb": {
    "title": "Explaining Time Series via Contrastive and Locally Sparse Perturbations",
    "volume": "review",
    "abstract": "Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The code is available for review: \\url{https://anonymous.4open.science/r/ContraLSP-1146/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=CThn4xaLDT": {
    "title": "E(3) Equivariant Scalar Interaction Network",
    "volume": "review",
    "abstract": "Equivariant Graph Neural Networks have demonstrated exceptional performance in modeling geometric data frequently observed in natural science research. The fundamental component of such models is the equivariant operation, which involves operations such as tensor product and scalarization. We present a conceptual framework that unifies the equivariant operations via equivariant basis decomposition. Within this framework, we generalize the idea of replacing the equivariant basis with input features to design efficient equivariant operations capable of modeling different type-$l$ features. To implement this, we propose Scalar Interaction and design an equivariant network, Scalar Interaction Network (SINet), with it. SINet's efficacy extends to efficiently mapping high type-$l$ features while maintaining a complexity $O(L^2)$ with the maximum $L$, representing a significant improvement over the $O(L^6)$ of tensor-product methods. Empirical results demonstrate SINet's capability to model complex quantum systems with high precision and computational efficiency. Its performance is competitive with current state-of-the-art methods in the field, showcasing its potential to advance the modeling of geometric data. This work highlights the potential of scalar interaction as an building block for constructing equivariant networks and opens up new avenues for future exploration in these vital fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=nATTIkte9f": {
    "title": "LMO-DP: Accurately Fine-Tuning Language Models with Stronger Differential Privacy",
    "volume": "review",
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) and its variants have been proposed to ensure rigorous privacy for fine-tuning large-scale pre-trained language models. State-of-the-art (SOTA) DP-SGD methods rely heavily on the Gaussian mechanism since its key component – moment accountant (MA) leverages the properties of Gaussian noise to accumulate the overall privacy budget via tight DP composition. However, the privacy constraints imposed in DP-SGD, solely on the Gaussian noise, may still overly perturb the gradients and degrade the fine-tuning accuracy, especially in stronger privacy regimes (e.g., the total privacy budget $\\epsilon < 3$). To address such limitations, we propose a novel Language Model-based Optimal Differential Privacy (LMO-DP) framework, which takes the first step to enable the tight composition of a sub-optimal DP mechanism (non-Gaussian) for accurately fine-tuning language models, even in stronger privacy regimes (e.g., $0.5 \\leq \\epsilon < 3$). Furthermore, LMO-DP efficiently approximates the sub-optimal DP and fast convergence, compared to the SOTA methods. For instance, fine-tuning RoBERTa-large (with 300M parameters) on the SST-2 dataset can achieve the 92.20% accuracy (given the total privacy budgets $\\epsilon = 0.3$ and $\\delta = 0$), compared with the ∼50% accuracy of most SOTA methods. We also draw similar findings on text generation tasks while privately fine-tuning GPT-2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=NX0eNGXezp": {
    "title": "Semi-HyperGraph Benchmark: Enhancing Flexibility of Hypergraph Learning with Datasets and Benchmarks",
    "volume": "review",
    "abstract": "Graphs are widely used to encapsulate a variety of data formats, but real-world networks often involve complex node relations beyond only being pairwise. While hypergraphs have been developed and employed to account for the complex node relations, they reduce the flexibility of machine learning systems by totally disregarding simple edges, which to some extent leads to a drop in performance. Additionally, Graph Neural Networks (GNNs) research are normally separated into simple graphs and hypergraphs, and these two classes of methods tend not to interchange. Therefore, there is a need for a more flexible benchmark that allows GNNs to employ both simple edge and hyperedge information. In this paper, we present the *Semi-HyperGraph Benchmark (SHGB)*, a collection of comprehensive datasets combining hypergraphs and simple edges, with an accessible evaluation framework to fully understand the performance of GNNs on complex graphs. SHGB contains 23 real-world hypergraph datasets with simple edges included, across various domains such as biology, social media, and e-commerce. Furthermore, we provide an extensible evaluation framework and a supporting codebase to facilitate the training and evaluation of GNNs on SHGB. Our empirical study of existing GNNs on SHGB reveals various research opportunities and gaps, including (1) evaluating the actual performance improvement of hypergraph GNNs over simple graph GNNs; (2) comparing the impact of different sampling strategies on hypergraph learning methods; and (3) exploring ways to integrate simple edge and hyperedge information. We make our source code and full datasets publicly available at https://anonymous-url/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2Lnqs4eMJ": {
    "title": "Optimal Neural Network Approximation for High-Dimensional Continuous Functions",
    "volume": "review",
    "abstract": "The original version of the Kolmogorov-Arnold representation theorem states that for any continuous function $f : [0, 1]^d \\rightarrow \\mathbb{R}$, there exist $(d+1)(2d+1)$ univariate continuous functions such that $f$ can be expressed as product and linear combinations of them. So one can use this representation to find the optimum size of a neural network to approximate $f$. Now the important question is to check how does the size of the neural network depends on $d$. It is proved that function space generated by special class of activation function called EUAF (elementary universal activation function), with $\\mathcal{O}(d^2)$ neurons is dense in $C([a, b]^d)$ with 11 hidden layers. In this paper we provide classes of $d$-variate functions for which the optimized neural networks will have $\\mathcal{O}(d)$ number of neurons with elementary superexpressive activation function defined by Yarotsky. We provide a new construction of neural network of $\\mathcal{O}(d)$ neuron size to approximate $d$-variate continuous functions of certain classes. We also prove that the size $\\mathcal{O}(d)$ is optimal in those cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=qhAx0fU9YE": {
    "title": "When Does Bias Transfer in Transfer Learning?",
    "volume": "review",
    "abstract": "Using transfer learning to adapt a pre-trained \"source model\" to a downstream \"target task\" can dramatically increase performance with seemingly no downside. In this work, we demonstrate that there can exist a downside after all: bias transfer, or the tendency for biases of the source model to persist even after adapting the model to the target dataset. Through a combination of synthetic and natural experiments, we show that bias transfer both (a) arises in realistic settings (such as when pre-training on ImageNet or other standard datasets) and (b) can occur even when the target dataset is explicitly de-biased. As transfer-learned models are increasingly deployed in the real world, our work highlights the importance of understanding the limitations of pre-trained source models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=nkUQPOwYy0": {
    "title": "Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences",
    "volume": "review",
    "abstract": "Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n \\log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\\mathcal{O}( \\log n)$ levels of resolution, where groups at greater distance are increasingly larger in size and the weights to compute group quantities are learned. As such, interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\\mathcal{O}(n \\log n)$ or $\\mathcal{O}(n)$, depending on whether the queries are down-sampled or not. This multi-level divide-and-conquer strategy is inspired by fast summation methods from $n$-body physics and the Fast Multipole Method. We perform evaluation on autoregressive and bidirectional language modeling tasks by comparing our Fast Multipole Attention model with other efficient attention variants on medium-size datasets. We find empirically that the Fast Multipole Transformer performs much better than other efficient transformers in terms of memory size and accuracy. The Fast Multipole Attention mechanism has the potential to empower large language models with much greater sequence lengths, taking the full context into account in an efficient, naturally hierarchical manner during training and when generating long sequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wxClzZdjqP": {
    "title": "LLM4GCL: CAN LARGE LANGUAGE MODEL EM-POWER GRAPH CONTRASTIVE LEARNING?",
    "volume": "review",
    "abstract": "Graph contrastive learning (GCL) has made significant strides in pre-training graph neural networks (GNNs) without requiring human annotations. Previous GCL efforts have primarily concentrated on augmenting graphs, assuming the node features are pre-embedded. However, many real-world graphs contain textual node attributes (e.g., citation network), known as text-attributed graphs (TAGs). The existing GCL methods often simply convert the textual attributes into numerical features using shallow or heuristic methods like skip-gram and bag-of-words, which cannot capture the semantic nuances and general knowledge embedded in natural language. Motivated by the exceptional capabilities of large language models (LLMs), like ChatGPT, in comprehending text, in this work, we delve into the realm of GCL on TAGs in the era of LLMs, which we term LLM4GCL. We explore two potential pipelines: \\textit{LLM-as-GraphAugmentor} and \\textit{LLM-as-TextEncoder}. The former aims to directly leverage LLMs to conduct augmentations at the feature and structure levels through prompts. The latter attempts to employ LLMs to encode nodes' textual attributes into embedding vectors. Building on these two pipelines, we conduct comprehensive and systematic studies on six benchmark datasets, exploring various feasible designs. The results show the promise of LLM4GCL in enhancing the performance of state-of-the-art GCL methods. Our code and dataset will be publicly released upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=VJvbOSXRUq": {
    "title": "GnnX-Bench: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
    "volume": "review",
    "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kNGxg8shA1": {
    "title": "Noise Robust Graph Learning under Feature-Dependent Graph-Noise",
    "volume": "review",
    "abstract": "In real-world scenarios, node features frequently exhibit noise due to various factors, making GNNs vulnerable. Various methods enhance robustness, but they make an unrealistic assumption that the noise in node features is independent of the graph structure of node labels, restricting their practicality. To this end, we introduce more realistic noise scenario, called feature-dependent graph-noise (FDGN), where noisy node features may entail both structure and label noise, and propose a generative model to capture these causal relationships. Our proposed method, PRINGLE, outperforms baselines on commonly used benchmark datasets and newly introduced real-world graph datasets that simulate FDGN in e-commerce systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=jrm33chK71": {
    "title": "Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes",
    "volume": "review",
    "abstract": "When training data is scarce, it is common to make use of a feature extractor that has been pre-trained on a large \"base\" dataset, either by fine-tuning its parameters on the \"target\" dataset or by directly adopting its representation as features for a simple classifier. Fine-tuning is ineffective for few-shot learning, since the target dataset contains only a handful of examples. However, directly adopting the features without fine-tuning relies on the distribution of the base dataset being similar enough to that of the target dataset in order to achieve separability and generalization. This paper investigates whether better features for the target dataset can be obtained by training on fewer base classes, in an effort to bring the distribution of the base dataset closer to that of the target dataset. We consider cross-domain few-shot image classification in eight different domains from Meta-Dataset and entertain multiple real-world settings (domain-informed, task-informed and uninformed) where progressively less detail is known about the target task. To our knowledge, this is the first demonstration that fine-tuning on a subset of carefully selected base classes can significantly improve few-shot learning. Our contributions are simple and intuitive methods that can be implemented in any few-shot solution. We also give insights into the conditions in which these solutions are likely to provide a boost in accuracy. We release the code to reproduce all experiments from this paper on GitHub. https://anonymous.4open.science/r/Few-and-Fewer-C978",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=4Ay23yeuz0": {
    "title": "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space",
    "volume": "review",
    "abstract": "Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces TABSYN, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed TabSyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations, (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that TabSyn outperforms existing methods. Specifically, it reduces the error rates by 86% and 67% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines, its superiority in accurately learning the data distributions of tabular data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=o8tjamaJ80": {
    "title": "Adversarial AutoMixup",
    "volume": "review",
    "abstract": "Data mixing augmentation has been widely applied to improve the generalization ability of deep neural networks. Recently, offline data mixing augmentation, e.g. handcrafted and saliency information based mixup, has been gradually replaced by automatic mixing approaches. Through minimizing two sub-tasks, namely, mixed sample generation and mixup classification in an end-to-end way, AutoMix significantly improves accuracy on image classification tasks. However, as the optimization objective is consistent for the two sub-tasks, this approach is prone to generating consistent instead of diverse mixed samples, which results in overfitting for target task training. In this paper, we propose AdAutomixup, an adversarial automatic mixup augmentation approach that generates challenging samples to train a robust vein classifier for palm-vein identification, by alternatively optimizing the classifier and the mixup sample generator. AdAutomixup comprises two modules, a mixed example generator and a target classifier. The mixed sample generator aims to produce hard mixed examples to challenge the target classifier while the target classifier's aim is to learn robust features from hard mixed examples to improve generalization. To prevent the collapse of the inherent meanings of images, we further introduce an exponential moving average (EMA) teacher and a cosine similarity to train AdAutomixup in an end-to-end way. Extensive experiments on five image benchmarks consistently prove that our approach outperforms the state-of-the-art in various classification scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=pBxeZ6pVUD": {
    "title": "Grounded Object-Centric Learning",
    "volume": "review",
    "abstract": "The extraction of object-centric representations for downstream tasks is an emerging area of research. Learning grounded representations of objects that are guaranteed to be stable and invariant promises robust performance across different tasks and environments. Slot Attention (SA) learns object-centric representations by assigning objects to *slots*, but presupposes a *single* distribution from which all slots are randomly initialised. This results in an inability to learn *specialized* slots which bind to specific object types and remain invariant to identity-preserving changes in object appearance. To address this, we present *Conditional Slot Attention* (CoSA) using a novel concept of *Grounded Slot Dictionary* (GSD) inspired by vector quantization. Our proposed GSD comprises (i) canonical object-level property vectors and (ii) parametric Gaussian distributions, which define a prior over the slots. We demonstrate the benefits of our method in multiple downstream tasks such as scene generation, composition, and task adaptation, whilst remaining competitive with SA in object discovery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KEpR8hFzvO": {
    "title": "Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws",
    "volume": "review",
    "abstract": "Neural operators (NOs) have emerged as effective tools for modeling complex physical systems in scientific machine learning. In NOs, a central characteristic is to learn the governing physical laws directly from data. In contrast to other machine learning applications, partial knowledge is often known a priori about the physical system at hand whereby quantities such as mass, energy and momentum are exactly conserved. Currently, NOs have to learn these conservation laws from data and can only approximately satisfy them due to finite training data and random noise. In this work, we introduce conservation law-encoded neural operators (clawNOs), a suite of NOs that endow inference with automatic satisfaction of such conservation laws. ClawNOs are built with a divergence-free prediction of the solution field, with which the continuity equation is automatically guaranteed. As a consequence, clawNOs are compliant with the most fundamental and ubiquitous conservation laws essential for correct physical consistency. As demonstrations, we consider a wide variety of scientific applications ranging from constitutive modeling of material deformation, incompressible fluid dynamics, to atmospheric simulation. ClawNOs significantly outperform the state-of-the-art NOs in both accuracy and generalizability, especially in small-data regimes and long-term prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=htEL8LrrVe": {
    "title": "Communication Bounds for the Distributed Experts Problem",
    "volume": "review",
    "abstract": "In this work, we study the experts problem in the distributed setting where an expert's cost needs to be aggregated across multiple servers. Our study considers various communication models such as the message-passing model and the broadcast model, along with multiple aggregation functions, such as summing and taking the maximum of an expert's cost across servers. We propose the first communication-efficient protocols that achieve near-optimal regret in these settings, even against a strong adversary who can choose the inputs adaptively. Additionally, we give a lower bound showing that the communication of our protocols is nearly optimal. Finally, we implement our protocols and demonstrate empirical savings on real-world benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=c8UABqZfld": {
    "title": "Spatial Matching Loss Function for Mass Segmentation on Whole Mammography Images",
    "volume": "review",
    "abstract": "Breast cancer is one of the cancer types with a high mortality rate among women, and mammography is one of the primary means to improve the identification of breast cancer. Deep-learning-based approaches are among the pioneering methods for mass segmentation in mammography images; in this category of methods, the loss function is one of the core elements. Most of the proposed losses aim to measure pixel-level similarities. While the hard-coded location information is provided in these losses, they mostly neglect to consider higher-level information such as relative distance, sizes, and quantities, which are important for mass segmentation. Motivated by this observation, in this paper we propose a framework for loss calculation in mass segmentation for mammography images that incorporates the higher-level spatial information in the loss by spatial matching between the prediction and the ground truth masks while calculating the loss. The proposed loss calculation framework is termed Spatial Matching (SM) loss. Instead of only calculating the loss over the entire masks that captures the similarity of the segmentation and the ground truth only at the pixel level, SM loss also compares the two in cells in a grid that enables the loss to measure higher-level similarities in the locations, sizes, and quantities. The grid size is selected according to each sample, which enables the method to consider the variation in mass sizes. In this study, Binary Cross Entropy (BCE) and Tversky are used as the core loss in experiments for the SM loss. AU-Net is selected as the baseline approach. We tested our method on the INbreast dataset. The results of our experiments show a significant boost in the performance of the baseline method while outperforming state-of-the-art mass segmentation methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Ts95eXsPBc": {
    "title": "Spatially-Aware Transformers for Embodied Agents",
    "volume": "review",
    "abstract": "Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \\href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3GDKJSQnW2": {
    "title": "Pivotal Prompt Tuning for Video Dynamic Editing",
    "volume": "review",
    "abstract": "Text-conditioned image editing has recently provided high-quality edits on images based on diffusion frameworks. Unfortunately, this success did not carry over to video editing, which continues to be challenging. Video editing is limited to rigid editing such as object overlay and style transfer. This paper proposes pivotal dynamic editing (PDEdit) for performing spatial-temporal non-rigid video editing based only on the target text, which has never been attempted before. PDEdit is capable of changing the motion of an object/person in the video, either at a specific moment or throughout the video, while preserving the temporal consistency of edited motions and a high level of fidelity to the original input video. In contrast to previous works, the proposed method performs editing based only on the input video and target text. It does not require any other auxiliary inputs (e.g., object masks or source video captions). Based on the video diffusion model, PDEdit using the proposed prompt pivoting leverages the target text prompt for editing the input video. The quality and adaptability of the proposed method on numerous input videos from different domains show the proposed to be highly effective. It can produce high-fidelity video edits under a single unified PDEdit framework. The code for this work will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7U5QE9T4hI": {
    "title": "Learning to Extrapolate and Adjust: Two-Stage Meta-Learning for Concept Drift in Online Time Series Forecasting",
    "volume": "review",
    "abstract": "The non-stationary nature of time series data in many real-world applications makes accurate time series forecasting challenging. In this paper, we consider concept drift where the underlying distribution or environment of time series changes. We first classify concepts into two categories, macro-drift corresponding to stable and long-term changes and micro-drift referring to sudden or short-term changes. Next, we propose a unified meta-learning framework called LEAF (Learning to Extrapolate and Adjust for Forecasting). Specifically, an extrapolation module is first meta-learnt to track the dynamics of the prediction model in latent space and extrapolate to the future considering macro-drift. Then an adjustment module incorporates meta-learnable surrogate loss to capture sample-specific micro-drift patterns. Through this two-stage framework, different types of concept drifts can be handled. In particular, LEAF is model-agnostic and can be applied to any deep prediction model. To further advance the research of concept drift on time series, we open source three electric load time series datasets collected from real-world scenarios, which exhibit diverse and typical concept drifts and are ideal benchmark datasets for further research. Extensive experiments on multiple datasets demonstrate the effectiveness of LEAF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xAqcJ9XoTf": {
    "title": "On the Stability of Expressive Positional Encodings for Graph Neural Networks",
    "volume": "review",
    "abstract": "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) *Non-uniqueness*: there are many different eigendecompositions of the same Laplacian, and (2) *Instability*: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be the use of \"hard partition'' of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to ``softly partition'' eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=sawjxRnVpF": {
    "title": "Curvature-Informed SGD via General Purpose Lie-Group Preconditioners",
    "volume": "review",
    "abstract": "We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group's equi-variance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Our proposed approach offers a promising direction for improving the convergence of SGD with low computational overhead. We demonstrate that Preconditioned SGD (PSGD) outperforms SoTA on Vision, NLP, and RL tasks across multiple modern deep-learning architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=dRel8fuUK4": {
    "title": "Low-Cost High-Power Membership Inference by Boosting Relativity",
    "volume": "review",
    "abstract": "We present a membership inference attack game and design a novel attack (RMIA), which effectively leverages both reference models and population data in its likelihood ratio test. Our test amplifies the distinction between members and non-members relative to any target model. Our algorithm exhibits superior test power (true-positive rate) when compared to prior methods, even at extremely low false-positive error rates (as low as 0), and dominates them throughout the TPR-FPR tradeoff curve. It also performs exceptionally well under challenging real-world constraints, where only a limited number of reference models (as few as 1) are available, where the prior attack results approach random guess. Our method lays the groundwork for cost-effective and practical yet powerful privacy risk analysis of machine learning algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5oXp5Kvq5": {
    "title": "A Causal Ordering Prior for Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "Unsupervised representation learning with variational inference relies heavily on independence assumptions over latent variables. Causal representation learning (CRL), however, argues that factors of variation in a dataset are, in fact, causally related. Allowing latent variables to be correlated, as a consequence of causal relationships, is more realistic and generalisable. So far, provably identifiable methods rely on: auxiliary information, weak labels, and interventional or even counterfactual data. Inspired by causal discovery with functional causal models, we propose a fully unsupervised representation learning method that considers a data generation process with a latent additive noise model (ANM). We encourage the latent space to follow a causal ordering via loss function based on the Hessian of the latent distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HiTg16qhxp": {
    "title": "Dynamic Neural Response Tuning",
    "volume": "review",
    "abstract": "Artificial Neural Networks (ANNs) have gained broad applications across various fields due to their brilliant architecture designs. ANNs were initially inspired by the principle of biology. The biological neural system's fundamental response process comprises information acquisition, transmission, and aggregation. The transmission of information in neurons is achieved by triggering action potentials that propagate through axons. ANNs utilize activation functions to simulate such behavior of biological neurons. However, previous studies have only considered static response conditions, while the biological neuron's response conditions are dynamic, depending on neuron properties and the real-time environment. Therefore, the dynamic response conditions of biological neurons could help improve the static ones of the existing activation functions. Additionally, the neuron's aggregated response exhibits high specificity for different categories, allowing the neural system to differentiate and identify distinct objects. Inspired by these biological patterns, we propose a novel Dynamic Neural Response Tuning (DNRT) mechanism, which aligns the response patterns of ANNs with those of biological neurons. DNRT comprises Response-Adaptive Activation (RAA) and Aggregated Response Regularization (ARR), mimicking biological neurons' information transmission and aggregation behaviors. RAA dynamically adjusts the response condition based on the strength and characteristics of the input signal. ARR is devised to enhance the network's ability to learn category specificity. Extensive experiments indicate that the proposed DNRT is highly interpretable, applicable to various mainstream network architectures, and can achieve remarkable performance compared with existing response activation functions in multiple tasks and domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Let8OMe20n": {
    "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models",
    "volume": "review",
    "abstract": "Fine-tuning text-to-image models using reward functions trained on human feedback data has emerged as a powerful approach for aligning model behavior with human intent. However, excessive optimization with such reward models, which are only proxy objectives, can degrade the performance of the fine-tuned models, a phenomenon commonly referred to as reward overoptimization. We introduce the Text-Image Alignment Assessment (TIA2) benchmark, a diverse collection of text prompts, images, and human annotations, for studying the issue in depth. We evaluate several state-of-the-art reward models for text-to-image generation on our benchmark and find that they are often not well-aligned with human assessment. We empirically demonstrate that overoptimization can occur when a poorly aligned reward model is used as a fine-tuning objective. To address this, we introduce a simple method, TextNorm, for inducing confidence calibration in reward models by normalizing the scores across prompts that are semantically different from the original prompt. We demonstrate that using the confidence-calibrated scores in fine-tuning effectively reduces the risk of overoptimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=VzPGV19Bnp": {
    "title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach",
    "volume": "review",
    "abstract": "Recent text-to-image generation models have demonstrated impressive capability of generating text-aligned images with high fidelity. However, generating images of novel concepts specified by a reference image remains a challenging task. To address this problem, researchers have been exploring various methods for customizing pre-trained text-to-image generation models. Currently, most existing methods for customizing pre-trained text-to-image generation models involve the use of regularization techniques to prevent over-fitting. Although regularization will ease the challenge of customization and leads to successful content creation with respect to text guidance, it may restrict the model capability, resulting in the loss of detailed information and inferior performance. In this work, we propose ProFusion, a novel framework for customized text-to-image generation, which can tackle the over-fitting problem without the widely used regularization. Specifically, it consists of an encoder network and a novel sampling method. Given a single user-provided image from an arbitrary domain, the proposed framework can customize a pre-trained text-to-image generation model within half a minute. Empirical results demonstrate that our proposed framework outperforms existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=jvtmdK69KQ": {
    "title": "Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts",
    "volume": "review",
    "abstract": "Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\\ast}$ becomes unknown and the true model is over-specified by a Gaussian mixture of $k$ experts where $k > k_{\\ast}$, our findings suggest that the number of experts selected from the top-K sparse softmax gating function must exceed the total cardinality of a certain number of Voronoi cells associated with the true parameters to guarantee the convergence of the density estimation. Moreover, while the density estimation rate remains parametric under this setting, the parameter estimation rates become substantially slow due to an intrinsic interaction between the softmax gating and expert functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQGUDc9tC8": {
    "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models",
    "volume": "review",
    "abstract": "Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to detect and quantify social biases in PLMs by observing model outputs. As a result, previous debiasing methods mainly finetune or even pre-train PLMs on newly constructed anti-stereotypical datasets, which are high-cost. In this work, we try to unveil the mystery of social bias inside language models by introducing the concept of {\\sc Social Bias Neurons}. Specifically, we propose {\\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e., neurons) in a language model that can be attributed to undesirable behavior, such as social bias. By formalizing undesirable behavior as a distributional property of language, we employ sentiment-bearing prompts to elicit classes of sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus attributes the uneven distribution for different demographics to specific Social Bias Neurons, which track the trail of unwanted behavior inside PLM units to achieve interoperability. Moreover, derived from our interpretable technique, {\\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate social biases. By studying BERT, RoBERTa, and their attributable differences from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified neurons, and further mitigate undesired behaviors. As measured by prior metrics from StereoSet, our model achieves a higher degree of fairness while maintaining language modeling ability with low cost\\footnote{This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xCawdgA8Qr": {
    "title": "Leveraging Behavioral Cloning for Representation Alignment in Cross-Domain Policy Transfer",
    "volume": "review",
    "abstract": "The limited transferability of learned policies is a major challenge that restricts the applicability of learning-based solutions in decision-making tasks. In this paper, we present a simple method for aligning latent state representations across different domains using unaligned trajectories of proxy tasks. Once the alignment process is completed, policies trained on the shared representation can be transferred to another domain without further interaction. Our key finding is that multi-domain behavioral cloning is a powerful means of shaping a shared latent space. We also observe that the commonly used domain discriminative objective for distribution matching can be overly restrictive, potentially disrupting the latent state structure of each domain. As an alternative, we propose to use maximum mean discrepancy for regularization. Since our method focuses on capturing shared structures, it does not require discovering the exact cross-domain correspondence that existing methods aim for. Furthermore, our approach involves training only a single multi-domain policy, making it easy to extend. We evaluate our method across various domain shifts, including cross-robot and cross-viewpoint settings, and demonstrate that our approach outperforms existing methods that employ adversarial domain translation. We also conduct ablation studies to investigate the effectiveness of each loss component for different domain shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=vW1SkPl4kp": {
    "title": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback",
    "volume": "review",
    "abstract": "Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we present a novel risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk (CVaR) objective under both linear and general function approximations, enriched by human feedback. These new formulations provide a principled way to guarantee safety in each decision making step throughout the control process. Moreover, integrating human feedback into risk-sensitive RL framework bridges the gap between algorithmic decision-making and human participation, allowing us to also guarantee safety for human-in-the-loop systems. We propose provably sample-efficient algorithms for this Iterated CVaR RL and provide rigorous theoretical analysis. Furthermore, we establish a matching lower bound to corroborate the optimality of our algorithms in a linear context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=vSwu81S33z": {
    "title": "Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling",
    "volume": "review",
    "abstract": "Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and downstream tasks. Through extensive empirical validations, we demonstrate that our approach surpasses other baselines in BMA performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7mR83Q12cJ": {
    "title": "Counterfactual Data Augmentation with Contrastive Learning",
    "volume": "review",
    "abstract": "Statistical disparity between distinct treatment groups is one of the most significant challenges for estimating Conditional Average Treatment Effects (CATE). To address this, we introduce a model-agnostic data augmentation method that imputes the counterfactual outcomes for a selected subset of individuals. Specifically, we utilize contrastive learning to learn a representation space and a similarity measure such that in the learned representation space \\textit{close} individuals identified by the learned similarity measure have \\textit{similar} potential outcomes. This property ensures reliable imputation of counterfactual outcomes for the individuals with close neighbors from the alternative treatment group. By augmenting the original dataset with these reliable imputations, we can effectively reduce the discrepancy between different treatment groups, while inducing minimal imputation error. The augmented dataset is subsequently employed to train CATE estimation models. Theoretical analysis and experimental studies on synthetic and semi-synthetic benchmarks demonstrate that our method achieves significant improvements in both performance and robustness to overfitting across state-of-the-art models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=B6pQxqUcT8": {
    "title": "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A$^*$ search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution. Extensive experiments on multiple tool-use and reasoning tasks demonstrate that ToolChain* efficiently balances exploration and exploitation within an expansive action space. It outperforms state-of-the-art baselines on planning and reasoning tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RR8y0WKrFv": {
    "title": "Ensemble Distillation for Unsupervised Constituency Parsing",
    "volume": "review",
    "abstract": "We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of ``tree averaging,'' based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.6,
    "authors": []
  },
  "https://openreview.net/forum?id=KOUAayk5Kx": {
    "title": "Defying Multi-model Forgetting: Orthogonal Gradient Learning to One-shot Neural Architecture Search",
    "volume": "review",
    "abstract": "One-shot neural architecture search (NAS) trains an over-parameterized network (termed as supernet) that assembles all the architectures as its subnets by using weight sharing, and thereby reduces much computational budget. However, there is an issue of multi-model forgetting about supernet training in one-shot NAS that some weights of the previously well-trained architecture will be overwritten by that of the newly sampled architecture which has overlapped structures with the old one. To overcome the issue, we propose an orthogonal gradient learning (OGL) guided supernet training paradigm for one-shot NAS, where the novelty lies in the fact that the weights of the overlapped structures of current architecture are updated in the orthogonal direction to the gradient space of these overlapped structures of all previously trained architectures. Moreover, a new approach of calculating the projection is designed to effectively find the base vectors of the gradient space to acquire the orthogonal direction. We have theoretically and experimentally proved the effectiveness of the proposed paradigm in overcoming the multi-model forgetting. Besides, we apply the proposed paradigm to two one-shot NAS baselines, and experimental results have demonstrated that our approach is able to mitigate the multi-model forgetting and enhance the predictive ability of the supernet in one-shot NAS with remarkable efficiency on popular test datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rnxam2SRgB": {
    "title": "Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models",
    "volume": "review",
    "abstract": "In this paper, we propose Describe-and-Dissect, a novel method to describe the roles of hidden neurons in vision networks. Describe-and-Dissect utilizes recent advancements in multimodal deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, Describe-and-Dissect is training-free, meaning we don't train any new models and can easily leverage more capable general purpose models in the future. We show on a large scale user study that our method outperforms the state-of-the-art baseline methods including CLIP-Dissect, MILAN, and Network Dissection. Our method on average provides the highest quality labels and is more than 2$\\times$ as likely to be selected as the best explanation for a neuron than the best baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=SdUUyqakLl": {
    "title": "Exploit Gradient Skew to Circumvent Byzantine Defenses for Federated Learning",
    "volume": "review",
    "abstract": "Federated Learning (FL) is notorious for its vulnerability to Byzantine attacks. Most current Byzantine defenses share a common inductive bias: among all the gradients, the majorities are more likely to be honest. However, such bias is a poison to Byzantine robustness due to a newly discovered phenomenon -- gradient skew. We discover that the majority of honest gradients skew away from the optimal gradient (the average of honest gradients) as a result of heterogeneous data. This gradient skew phenomenon allows Byzantine gradients to hide within the skewed majority of honest gradients and thus be recognized as the majority. As a result, Byzantine defenses are deceived into perceiving Byzantine gradients as honest. Motivated by this observation, we propose a novel skew-aware attack called STRIKE: first, we search for the skewed majority of honest gradients; then, we construct Byzantine gradients within the skewed majority. Experiments on three benchmark datasets validate the effectiveness of our attack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=02Ug9N8DCI": {
    "title": "GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling",
    "volume": "review",
    "abstract": "Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \\log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate-attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on data-controlled cumulative sums for context aggregation, our findings suggest that incorporating data-controlled complex cumulative products may be a crucial step towards more powerful sequence models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=nr0w6CH7v4": {
    "title": "Quality Diversity through Human Feedback",
    "volume": "review",
    "abstract": "Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with human-constructed metrics. Notably, when deployed for a latent space illumination task, QDHF markedly enhances the diversity of images generated by a Diffusion model. The study concludes with an in-depth analysis of QDHF's sample efficiency and the quality of its derived diversity metrics, emphasizing its promise for enhancing exploration and diversity in optimization for complex, open-ended tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=AssIuHnmHX": {
    "title": "Understanding Length Generalization by Thinking Like Transformers",
    "volume": "review",
    "abstract": "Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. In this work, we focus on length generalization, and we propose a unifying framework to understand when and how Transformers can be expected to length generalize on a given task. First, we show that there exist algorithmic tasks for which standard decoder-only Transformers trained from scratch naturally exhibit strong length generalization. For these tasks, we leverage the RASP programming language (Weiss et al., 2021) to show that the correct algorithmic solution which solves the task can be represented by a simple Transformer. We thus propose the RASP-Generalization Conjecture: Transformers tend to learn a length-generalizing solution if there exists a short RASP-L program that works for all input lengths. We present empirical evidence to support the correlation between RASP-simplicity and generalization. We leverage our insights to give new scratchpad formats which yield strong length generalization on traditionally hard tasks (such as parity and addition), and we illustrate how scratchpad can hinder generalization when it increases the complexity of the corresponding RASP-L program. Overall, our work provides a novel perspective on the mechanisms of length generalization and the algorithmic capabilities of Transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zZFGliCl9": {
    "title": "Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders",
    "volume": "review",
    "abstract": "The posterior collapse phenomenon in variational autoencoder (VAE), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAE preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAE performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAE. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via a non-trivial theoretical analysis of linear conditional VAE and hierarchical VAE with two levels of latent, we prove that the cause of posterior collapses in these models includes the correlation between the input and output of the conditional VAE and the effect of learnable encoder variance in the hierarchical VAE. We empirically validate our theoretical findings for linear conditional and hierarchical VAE and demonstrate that these results are also predictive for non-linear cases with extensive experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=X41c4uB4k0": {
    "title": "Training-free Multi-objective Diffusion Model for 3D Molecule Generation",
    "volume": "review",
    "abstract": "Searching for novel and diverse molecular candidates is a critical undertaking in drug and material discovery. Existing approaches have successfully adapted the diffusion model, the most effective generative model in image generation, to create 1D SMILES strings, 2D chemical graphs, or 3D molecular conformers. However, these methods are not efficient and flexible enough to generate 3D molecules with multiple desired properties, as they require additional training for the models for each new property or even a new combination of existing properties. Moreover, some properties may potentially conflict, making it impossible to find a molecule that satisfies all of them simultaneously. To address these challenges, we present a training-free conditional 3D molecular generation algorithm based on off-the-shelf unconditional diffusion models and property prediction models. The key techniques include modeling the loss of property prediction models as energy functions, considering the property relation between multiple conditions as a probabilistic graph, and developing a stable posterior estimation for computing the conditional score function. We conducted experiments on both single-objective and multi-objective 3D molecule generation, focusing on quantum properties, and compared our approach with the trained or fine-tuned diffusion models. Our proposed model achieves superior performance in generating molecules that meet the conditions, without any additional training cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=mvGa1ikBD3": {
    "title": "Graph Neural Networks with Directional Encodings for Anisotropic Elasticity",
    "volume": "review",
    "abstract": "Simulating the behavior of nonlinear and anisotropic materials is a central problem with applications across engineering, computer graphics, robotics, and beyond. While conventional mesh-based simulations provide accurate and reliable predictions, their computational overhead typically prevents their use in interactive applications. Graph neural networks (GNN) have recently emerged as a compelling alternative to conventional simulations for time-critical applications. However, existing GNN-based methods cannot distinguish between deformations in different directions and are thus limited to isotropic materials. To address this limitation, we propose a novel and easy-to-implement GNN architecture based on directional encodings of edge features. By preserving directional information during message passing, our method has access to the full state of deformation and can thus model anisotropic materials. We demonstrate through a set of qualitative and quantitative evaluations that our approach outperforms existing mesh-based GNN approaches for modeling anisotropic materials",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=1WSd408I9M": {
    "title": "Generative AI in healthcare: A trustworthy approach",
    "volume": "review",
    "abstract": "Generative AI in healthcare: A trustworthy approach Abstract: The recent advancements in self-supervised algorithms like Transformer Architecture and Diffusion models have expanded the means of applying AI in healthcare and life sciences. To achieve real world adoption, it is important to measure and audit the trustworthiness of the AI system as per the legal and compliance requirements for privacy, security, fairness, and safety. In this paper, we focus on the method to achieve trustworthiness in an LLM (Large Language Model) based decision support system for physicians. The stakeholders for this decision support system are patients, physicians, regulators, and external auditors. We focus on the limitations of large or foundation models and the method to overcome these limitations, with the aim of accelerating the adoption of this far-reaching technology in the healthcare sector. It also explores possible guardrails for safety and the methods for aligning AI systems to guardrails. Our Solution Approach: We explore an approach to an AI system which can enhance decision capabilities by using the data and EHRs (Electronic Health Record) collected over many years for a vast volume of patients. The longitudinal data consists of biomarkers, disease progression indicators, treatment administered, and patient outcome. The goal of the system is to assist physicians in identifying the best treatment option for a given patient context. The LLM-based system will be able to predict optimal options based on hundreds of similar cases on which it was trained. The paper addresses the transparency, data integrity, model development, and performance validation of the system. In the sections below, we explore the various stages of development and deployment of such a system, the challenges, and the methods to overcome the challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bNt7oajl2a": {
    "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
    "volume": "review",
    "abstract": "The ability to derive the underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through $\\textit{iterative hypothesis refinement}$, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal $\\textit{hypothesis proposers}$ (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling $\\textit{inductive reasoners}$, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through extensive empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CGBfHtFRM": {
    "title": "Mean Field Langevin Actor-Critic: Faster Convergence and Global Optimality beyond Lazy Learning",
    "volume": "review",
    "abstract": "We study how deep reinforcement learning algorithms learn meaningful features when optimized for finding the optimal policy. In particular, we focus on a version of the neural actor-critic algorithm where both the actor and critic are represented by over-parameterized neural networks in the mean-field regime, and are updated via temporal-difference (TD) and policy gradient respectively. Specifically, for the critic neural network to perform policy evaluation, we propose $\\textit{mean-field Langevin TD learning}$ method (MFLTD), an extension of the mean-field Langevin dynamics with proximal TD updates, and compare its effectiveness against existing methods through numerical experiments. In addition, for the actor neural network to perform policy updates, we propose $\\textit{mean-field Langevin policy gradient}$ (MFLPG), which implements policy gradient in the policy space through a version of Wasserstein gradient flow in the space of network parameters. We prove that MFLTD finds the correct value function, and the sequence of actors created by MFLPG created by the algorithm converges linearly to the globally optimal policy of the Kullback Leibler divergence regularized objective. To our best knowledge, we provide the first linear convergence guarantee for neural actor-critic algorithms with $\\textit{global optimality}$ and $\\textit{feature learning}$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Z9AZsU1Tju": {
    "title": "Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning",
    "volume": "review",
    "abstract": "Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Different from most traditional fusion models that incorporate all modalities identically in neural networks, our model designates a prime modality and regards the remaining modalities as detectors in the information pathway, serving to distill the flow of information. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby substantially enhancing the performance of multimodal representation learning. Experimental evaluations on the MUStARD, CMU-MOSI, and CMU-MOSEI datasets demonstrate that our model consistently distills crucial information in multimodal learning scenarios, outperforming state-of-the-art benchmarks. Remarkably, on the CMU-MOSI dataset, ITHP-DeBERTa surpasses human-level performance in the multimodal sentiment binary classification task across all evaluation metrics (i.e., Binary Accuracy, F1 Score, Mean Absolute Error, and Pearson Correlation)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=hF8jnnexSB": {
    "title": "The Power of Minimalism in Long Sequence Time-series Forecasting",
    "volume": "review",
    "abstract": "Recently, transformer-based models have been widely applied to time series forecasting tasks due to their remarkable capability to capture complex interactions within sequential data. However, as the sequence length expands, Transformer-based models suffer from increased memory consumption, overfitting, and performance deterioration in capturing long-range dependencies. Recently, several studies have shown that MLP-based models can outperform advanced Transformer-based models for long-term time series forecasting (LTSF) tasks. Unfortunately, linear mappings often struggle to capture intricate dependencies when handling multivariate time series. Although modeling each channel independently can alleviate this issue, it will significantly increase the computational cost. To this end, we introduce a set of simple yet effective depthwise convolution models named LTSF-Conv to perform LTSF. Specifically, we apply unique filters to each channel to achieve channel independence, which plays a pivotal role in enhancing overall forecasting performance. Experimental results show that LTSF-Conv models outperform the state-of-the-art Transformer-based and MLP-based models across seven real-world LTSF benchmarks. Surprisingly, a two-layer non-stacked network can outperform the state-of-the-art Transformer model in 91\\% of cases with a significant reduction of computing resources. In particular, LTSF-Conv models substantially decrease the average number of trainable parameters (by $\\sim$ 12$\\times$), maximum memory consumption (by $\\sim$ 86$\\times$), running time (by $\\sim$ 18$\\times$), and inference time (by $\\sim$ 2$\\times$) on the Electricity benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=7avlrpzWqo": {
    "title": "Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization",
    "volume": "review",
    "abstract": "Modern ML applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in clusters of nodes, and their updates are aggregated before being applied to the model. However, a distributed setup is prone to Byzantine failures of individual nodes, components, and software. With data augmentation added to these settings, there is a critical need for robust and efficient aggregation systems. We define the quality of workers as reconstruction ratios $\\in (0,1]$, and formulate aggregation as a Maximum Likelihood Estimation procedure using Beta densities. We show that the Regularized form of log-likelihood wrt subspace can be approximately solved using iterative least squares solver, and provide convergence guarantees using recent Convex Optimization landscape results. Our empirical findings demonstrate that our approach significantly enhances the robustness of state-of-the-art Byzantine resilient aggregators. We evaluate our method in a distributed setup with a parameter server, and show simultaneous improvements in communication efficiency and accuracy across various tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=2RGQwJEcAC": {
    "title": "Visual Transformer with Differentiable Channel Selection: An Information Bottleneck Inspired Approach",
    "volume": "review",
    "abstract": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different types of neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Differentiable Channel Selection, or DCS-Transformer. DCS-Transformer features channel selection in the computation of the attention weights and the input/output features of the MLP in the transformer block. Our DCS-Transformer is compatible with many popular and compact transformer networks, such as MobileViT and EfficientViT, and it reduces the FLOPs of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in MobileViT and EfficientViT with DCS-Transformer blocks, leading to DCS-Transformer networks with different backbones. The DCS-Transformer is motivated by reduction of Information Bottleneck, and a novel upper bound for the IB which can be optimized by SGD is derived and incorporated into the training loss of the network with DCS-Transformer. Extensive results on image classification and object detection evidence that DCS-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers. The code of DCS-Transformer is available at \\url{https://anonymous.4open.science/r/IB-DCS-ViT-273C/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=bDooTVT4t2": {
    "title": "Universally Amplifying Randomized Smoothing for Certified Robustness with Anisotropic Noise",
    "volume": "review",
    "abstract": "Randomized smoothing has achieved great success for certified adversarial robustness. However, existing methods (especially the theory for certification guarantee) rely on a fixed i.i.d. noise distribution for all dimensions of the data (e.g., all the pixels in an image), and may result in limited performance of certified robustness. To address this limitation, we propose UCAN: a novel technique that $\\underline{U}$niversally amplifies randomized smoothing for $\\underline{C}$ertified robustness with $\\underline{A}$nisotropic $\\underline{N}$oise. It can theoretically transform any randomized smoothing method with isotropic noise to ensure certified robustness based on different variants of anisotropic noise. The theories universally work for using different noise distributions against different $\\ell_p$ perturbations. Furthermore, we also design a novel framework with three example noise parameter generators (NPGs) for customizing the anisotropic noise. Finally, experimental results demonstrate that UCAN significantly outperforms the state-of-the-art (SOTA) methods, e.g., the certified accuracy can be improved by up to $182.6$\\% at large certified radii on MNIST, CIFAR10, and ImageNet datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ntSP0bzr8Y": {
    "title": "PowerGPT: Foundation Model for Power Systems",
    "volume": "review",
    "abstract": "We propose a foundation model, namely PowerGPT, to model electricity time series (ETS) data, which learns generic representations of load and electricity consumption data by pre-training, providing a large-scale, off-the-shelf model for power systems. PowerGPT is the largest model in the field of power systems and is pre-trained on a large-scale ETS data including load and electricity consumption data. The design of PowerGPT is to capture long-term temporal dependency and hierarchical correlation from massive ETS data, providing information that spans from the fine-grained to coarse-grained scales. As a foundation model, PowerGPT achieves SOTA performance on various downstream tasks in power systems (i.e. forecasting, missing value imputation, and anomaly detection), showing the generalization ability to a wide range of tasks. The low-resource label analysis further illustrates the effectiveness of our pre-training strategy. In addition, we explore the effect of model size to show that a larger-scale model with a higher capacity can lead to performance improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uj2Wjv0pMY": {
    "title": "Put on your detective hat: What's wrong in this video?",
    "volume": "review",
    "abstract": "Following step-by-step procedures is an essential component of various activities carried out by individuals in their everyday lives. These procedures serve as a guiding framework that helps achieve goals efficiently, whether assembling furniture or preparing a recipe. However, the complexity and duration of procedural activities inherently increase the likelihood of making errors. Understanding such procedural activities from a sequence of frames is a challenging task that demands an accurate interpretation of visual information and an ability to reason about the structure of the activity. To this end, we collected a new ego-centric 4D dataset comprising 384 recordings (94.5 hrs) of people performing recipes in kitchen environments. This dataset consists of two distinct activity types: one in which participants adhere to the provided recipe instructions and another where they deviate and induce errors. We provide 5.3K step annotations and 10K fine-grained action annotations for 20% of the collected data and benchmark it on two tasks: error recognition, multi step localization and procedure learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=639DcBewcJ": {
    "title": "Low-Rank Robust Graph Contrastive Learning",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies. In this work, we propose a novel and robust method, Low-Rank Robust Graph Contrastive Learning (LR-RGCL). LR-RGCL performs transductive node classification in two steps. First, a robst GCL encoder named RGCL is trained by prototypical contrastive learning with Bayesian nonparametric Prototype Learning (BPL). Next, using the robust features produced by RGCL, a novel and provable low-rank transductive classification algorithm is used to classify the unlabeled nodes in the graph. Our low-rank transductive classification algorithm is inspired by the low frequency property of the graph data and its labels, and theoretical result on the generalization of our algorithm is provided. To the best of our knowledge, our theoretical result is among the first to demonstrate the advantage of low-rank learning in transductive classification. Extensive experiments on public benchmarks demonstrate the superior performance of LR-RGCL and the robustness of the learned node representations. The code of LR-RGCL is available at \\url{https://anonymous.4open.science/r/LRR-GCL-3B3C/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=PYDOCManeN": {
    "title": "Representation-space diffusion models for generating periodic materials",
    "volume": "review",
    "abstract": "Generative models hold the promise of significantly expediting the materials design process when compared to traditional human-guided or rule-based methodologies. However, effectively generating high-quality periodic structures of materials on limited but diverse datasets remains an ongoing challenge. Here we propose a novel approach for periodic structure generation which fully respect the intrinsic symmetries, periodicity, and invariances of the structure space. Namely, we utilize differentiable, physics-based, structural descriptors which can describe periodic systems and satisfy the necessary invariances, in conjunction with a denoising diffusion model which generates new materials within this descriptor or representation space. Reconstruction is then performed on these representations using gradient-based optimization to recover the corresponding Cartesian positions of the crystal structure. This approach differs significantly from current methods by generating materials in the representation space, rather than in the Cartesian space, which is made possible using an efficient reconstruction algorithm. Consequently, known issues with respecting periodic boundaries and translational and rotational invariances during generation can be avoided, and the model training process can be greatly simplified. We show this approach is able to provide competitive performance on established benchmarks compared to current state of the art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=kaZAKvjLro": {
    "title": "Semi-supervised Long-tailed Recognition using Alternate Sampling",
    "volume": "review",
    "abstract": "Long tailed recognition is confronted by two interven- ing challenges, i.e., the sample scarcity in the tail classes and the imbalanced class distribution. The class geome- try in feature space mainly suffers from the data scarcity, while imbalance distribution biases the decision boundary of classes. Previous work makes assumptions on the under- neath geometric structure of the tail classes to address the data scarcity challenge, and resorts to class balanced sam- pling or reweighting to address the data imbalance chal- lenge. We advocate to leverage the readily available un- labeled data in a semi-supervised setting to approach to long tailed recognition. An alternate sampling strategy is then introduced to overcome the two challenges in a single framework. The feature embedding (geometric structure) and classifier are updated in an iterative fashion. The extra unlabeled data, regularized by a consistency loss, leads to a better geometric structure. The class-balanced sampling is implemented to train the classifier such that it is not af- fected by the imbalance distribution or the quality of pseudo labels. We demonstrate significant accuracy improvements over other competitive methods on two datasets, where we improve on tail classes without much, if at all, degradations on head classes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9m02ib92Wz": {
    "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models",
    "volume": "review",
    "abstract": "Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RfCGvKBmMq": {
    "title": "Representation Matching Information Bottleneck for Text Matching in Asymmetrical Domains",
    "volume": "review",
    "abstract": "Recent studies have shown that the domain matching of text representations will help improve the generalization ability of asymmetrical domains text matching tasks. This requires that the distribution of text representations should be as similar as possible, similar to matching with heterogeneous data domains, in order to make the data after feature extraction indistinguishable. However, how to align the distribution of text representations remains an open question, and the role of text representations distribution alignment is still unclear. In this work, we explicitly narrow the distribution of text representations by aligning them with the same prior distribution. We theoretically prove that narrowing the distribution of text representations in asymmetrical domains text matching is equivalent to optimizing the information bottleneck (IB). Since the interaction between text representations plays an important role in asymmetrical domains text matching, IB does not restrict the interaction between text representations. Therefore, we propose the adequacy of interaction and the incompleteness of a single text representation on the basis of IB and obtain the representation matching information bottleneck (RMIB). We theoretically prove that the constraints on text representations in RMIB is equivalent to maximizing the mutual information between text representations on the premise that the task information is given. On four text matching models and five text matching datasets, we verify that RMIB can improve the performance of asymmetrical domains text matching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=iUD9FklwQf": {
    "title": "G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=I2mIxuXA72": {
    "title": "Understanding Domain Generalization: A Noise Robustness Perspective",
    "volume": "review",
    "abstract": "Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We conjecture that the failure mode of ERM arising from spurious correlations may be less prevalent in practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=lNCnZwcH5Z": {
    "title": "Non-negative Contrastive Learning",
    "volume": "review",
    "abstract": "Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these advantages enable NCL to outperform CL significantly on feature disentanglement, feature selection, as well as downstream classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kT0vIJA8CT": {
    "title": "Can Differentiable Decision Trees Learn Interpretable Reward Functions?",
    "volume": "review",
    "abstract": "There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs). Our experiments across several domains, including Cartpole, Visual Gridworld environments and Atari games, provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which the reward function is aligned with human preferences. We experimentally demonstrate that using reward DDTs results in competitive performance when compared with larger capacity deep neural network reward functions. We also observe that the choice between soft and hard (argmax) output of reward DDT reveals a tension between wanting highly shaped rewards to ensure good RL performance, while also wanting simpler, more interpretable rewards",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2cG3mQqop": {
    "title": "Image Clustering Conditioned on Text Criteria",
    "volume": "review",
    "abstract": "Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified criteria in the form of text by leveraging modern Vision-Language Models and Large Language Models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, significantly outperforming baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNlntv7A9X": {
    "title": "SoftPhy: Soft-Body Physical Concept Learning and Reasoning from Videos",
    "volume": "review",
    "abstract": "We introduce the Soft-Body Physical Dataset (SOPHY), a novel benchmark for evaluating machine models in physical reasoning across diverse scenarios for soft bodies. The SOPHY is specifically designed to be complementary with existing physical reasoning benchmarks by encompassing diverse physical property inferences for soft bodies like physical parameters such as mass and density across dynamic situations and predicting corresponding dynamics. This comprehensive dataset enables the development and assessment of AI models with human-like visual reasoning abilities in understanding both rigid objects and soft objects' visual attributes, physical properties, and dynamics while devising goal-oriented solutions. We evaluated a range of AI models and found that they still struggle to achieve satisfactory performance, which shows that current AI models still lack physical commonsense for soft objects and illustrates the value of the proposed dataset. We hope the SOPHY fosters advancements in AI perception and reasoning in diverse physical environments, bridging the gap between human and machine intelligence in the physical world",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xHmCdSArUC": {
    "title": "Correlated Noise Provably Beats Independent Noise for Differentially Private Learning",
    "volume": "review",
    "abstract": "Differentially private learning algorithms inject noise into the learning process. While the most common private learning algorithm, DP-SGD, adds independent Gaussian noise in each iteration, recent work on matrix factorization mechanisms has shown empirically that introducing correlations in the noise can greatly improve their utility. We characterize the asymptotic learning utility for any choice of the correlation function, giving precise analytical bounds for linear regression and as the solution to a convex program for general convex functions. We show, using these bounds, how correlated noise provably improves upon vanilla DP-SGD as a function of problem parameters such as the effective dimension and condition number. Moreover, our analytical expression for the near-optimal correlation function circumvents the cubic complexity of the semi-definite program used to optimize the noise correlation matrix in previous work. We validate these theoretical results with experiments on private deep learning. Our work matches or outperforms prior work while being efficient both in terms of computation and memory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=6bcAD6g688": {
    "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
    "volume": "review",
    "abstract": "Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of **6.16\\%** label errors in **11** datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mH3yfzIPsL": {
    "title": "XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction",
    "volume": "review",
    "abstract": "Event prediction aims to forecast the time and type of a future event based on a historical event sequence. Despite its significance, several challenges exist, including the irregularity of time intervals between events, cycles, periodicity, and the complex multi-scale nature of event interactions, as well as the potentially high computational costs for long event sequences. However, current neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data. Our model comprises two vital components: a novel Feature-based Cycle-aware Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism. These scales are determined by a bottom-up clustering algorithm. Extensive experiments on several real-world datasets show that our XTSFormer outperforms several baseline methods in prediction performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpyBQn6gJY": {
    "title": "Regularized Optimal Transport for Temporal Trajectory Analysis in Single-Cell Data",
    "volume": "review",
    "abstract": "The temporal relationship between different cellular states and lineages is only partially understood and has major significance for cell differentiation and cancer progression. However, two pain points persist and limit learning-based solutions: ($a$) lack of real datasets and standardized benchmark for early cell developments; ($b$) the complicated transcriptional data fail classic temporal analyses. We integrate $\\texttt{Mouse-RGC}$, a large-scale mouse retinal ganglion cell dataset with annotations for $9$ time stages and $30,000$ gene expressions. Existing approaches show a limited generalization on our datasets. To tackle the modeling bottleneck, we then translate this fundamental biology problem into a machine learning formulation, $\\textit{i.e.}$, temporal trajectory analysis. And an innovative regularized optimal transport algorithm, $\\texttt{TAROT}$, is proposed to fill in the research gap, consisting of ($1$) customized masked autoencoder to extract high-quality cell representations; ($2$) cost function regularization through biology priors for distribution transports; ($3$) continuous temporal trajectory optimization based on discrete matched time stages. Extensive empirical investigations demonstrate that our framework produces superior cell lineages and pesudotime, compared to existing approaches on $\\texttt{Mouse-RGC}$ and another two public benchmarks. Moreover, $\\texttt{TAROT}$ is capable of identifying biologically meaningful gene sets along with the developmental trajectory and its simulated gene knockout results echo the findings in physical wet lab validation. Codes are provided in the supplement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuflmOaxb7": {
    "title": "Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning",
    "volume": "review",
    "abstract": "Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon tabular Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods under softmax parameterization, where gradient tracking is applied to the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, which are nearly independent of the size of the state-action space and illuminate the impacts of network size and connectivity. To the best of our knowledge, this is the first time that global convergence is established for federated multi-task RL using policy optimization. Moreover, the convergence behavior of the proposed algorithms is robust against inexactness of policy evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=43cYe4oogi": {
    "title": "Understanding Expressivity of Neural KG Reasoning from Rule Structure Learning",
    "volume": "review",
    "abstract": "Knowledge graph (KG) reasoning refers to the task of deducing new facts from the existing facts in KG, which has been applied in many fields. Recently, Graph Neural Networks (GNNs) with tail entity scoring achieve the state-of-the-art performance on KG reasoning. However, the theoretical understandings for these GNNs are either lacking or focusing on single-relational graphs, leaving what the kind of rule structures these GNNs can learn an open problem. We propose to fill the above gap in this paper. Specifically, GNNs with tail entity scoring are unified into a common framework. Then, we analyze their expressivity by formally describing the rule structures they can learn and theoretically demonstrating their superiority. These results further inspire us to propose a novel labeling strategy to learn more rule structures in KG reasoning. Experimental results are consistent with our theoretical findings and verify the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=kxgSlyirUZ": {
    "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks",
    "volume": "review",
    "abstract": "Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g. generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g. language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 1,132 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=OhTzuWzO6Q": {
    "title": "A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings",
    "volume": "review",
    "abstract": "In several practical applications of federated learning (FL), the clients are highly heterogeneous in terms of both their data and compute resources, and therefore enforcing the same model architecture for each client is very limiting. The need for uncertainty quantification is also often particularly amplified for clients that have limited local data. This paper presents a unified FL framework based on training customized local Bayesian models that can simultaneously address both these constraints. A Bayesian framework provides a natural way of incorporating supervision in the form of prior distributions. We use priors in the functional (output) space of the networks to facilitate collaboration across heterogeneous clients via an unlabelled auxiliary dataset. We further present a differentially private version of the algorithm along with formal differential privacy guarantees that apply to general settings without any assumptions on the learning algorithm. Experiments on standard FL datasets demonstrate that our approach outperforms strong baselines in both homogeneous and heterogeneous settings and under strict privacy constraints, while also providing characterizations of model uncertainties",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RSincg5RBe": {
    "title": "Hierarchical Graph Latent Diffusion Model for Molecule Generation",
    "volume": "review",
    "abstract": "Recently, generative models based on the diffusion process have emerged as a promising direction for automating the design of molecules. However, directly adding continuous Gaussian noise to discrete graphs leads to the problem of the final noisy data not conforming to the standard Gaussian distribution. Current graph diffusion models either corrupt discrete data through a transition matrix or relax the discrete data to continuous space for the diffusion process. These approaches not only require significant computation resources due to the inclusion of the bond type matrix but also cannot easily perform scalable conditional generation, such as adding cross-attention layers, due to the lack of embedding representations. In this paper, we first introduce the Graph Latent Diffusion Model (GLDM), a novel variant of latent diffusion models that overcomes the mismatch problem of continuous diffusion space and discrete data space. Meanwhile, the latent diffusion framework avoids the issues of computational resource consumption and lack of embeddings for conditional generation faced by current graph diffusion models. However, it only utilizes graph-level embeddings for molecule generation, losing node-level and structural information. Therefore, we further ex- tend the GLDM to the Hierarchical Graph Latent Diffusion Model (HGLDM). By including node embeddings and subgraph embeddings that contain structural in- formation, our model significantly reduces computation time compared to the cur- rent graph diffusion models. We evaluate our model on three benchmarks through unconditional generation and conditional generation tasks, which demonstrate its superior performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=B5kAfAC7hO": {
    "title": "Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning",
    "volume": "review",
    "abstract": "In real-world reinforcement learning problems, the state information is often only partially observable, which breaks the basic assumption in Markov decision processes, and thus, leads to inferior performances. Partially Observable Markov Decision Processes have been introduced to explicitly take the issue into account for learning, exploration, and planning, but presenting significant computational and statistical challenges. To address these difficulties, we exploit the representation view, which leads to a coherent design framework for a practically tractable reinforcement learning algorithm upon partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm. We also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, therefore, pushing reliable reinforcement learning towards more practical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xrFTey4pY6": {
    "title": "Interactive Model Correction with Natural Language",
    "volume": "review",
    "abstract": "In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on spurious correlations that fail to generalize to new data distributions, such as a bird classifier that relies on the background of an image. Preventing models from latching on to spurious correlations necessarily requires additional information beyond labeled data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that far less supervision suffices if we provide targeted feedback about the misconceptions of models trained on a given dataset. We introduce Clarify, a novel natural language interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns, such as ``water background'' for a bird classifier. Then, in an entirely automated way, we use such descriptions to improve the training process by reweighting the training data or gathering additional targeted data. Our empirical results show that non-expert users can successfully describe model misconceptions via Clarify, improving worst-group accuracy by an average of 7.3% in two datasets with spurious correlations. Finally, we use Clarify to find and rectify 31 novel spurious correlations in ImageNet, improving minority-split accuracy from 21.1% to 28.7%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=MNShbDSxKH": {
    "title": "Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules",
    "volume": "review",
    "abstract": "Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate lan- guages into module descriptions, thus achieving strong visual reasoning results while maintaining the model's transparency and efficiency. However, these mod- els usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. On the contrary, human beings grad- ually acquire knowledge that can be reused and grow into more profound skills for fast generalization to new tasks since we are an infant. Inspired by this, we propose generative neuro-symbolic visual reasoning by growing and reusing mod- ules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established mod- ules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module's ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed GNSVR model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring ex- pression comprehension; Second, the visual modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RqUMWdDg52": {
    "title": "FireAct: Toward Language Agent Finetuning",
    "volume": "review",
    "abstract": "Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques, which can result in a lack of robustness in agent performance due to the limited learning support. In this paper, we investigate the less explored direction of fine-tuning LMs to obtain language agents. With a simple, controlled setup that uses a Google search API for question answering (QA), we systematically explore a variety of base LMs, agent methods, fine-tuning data, and QA tasks. Our experiments reveal novel insights around the scaling effects of the base LM and fine-tuning data, combining trajectory data collected from different tasks and different agent methods, as well as robustness to different types of data perturbations. Overall, these findings illustrate overlooked advantages of fine-tuned language agents over existing prompting-based ones, provide empirical guidelines for fine-tuning, and indicate future directions in creating better tasks and methods for language agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=IcVNBR7qZi": {
    "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models",
    "volume": "review",
    "abstract": "Pretrained language models are commonly aligned with human preferences and downstream tasks via finetuning. A prominent finetuning approach is reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights an optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. We then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental in an RFT benchmark for lanaguage models. In particular, we show that in datasets where inputs with small reward standard deviation under the pretrained model are more prevalent, the reward that RFT achieves compared to \\emph{supervised finetuning (SFT)} is worse. Controlled experiments and a theoretical analysis further establish that vanishing gradients in RFT can lead to extremely slow optimization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial SFT phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, our experiments reveal that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2ov9RiAkxE": {
    "title": "Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications",
    "volume": "review",
    "abstract": "Large language models (LLMs) are increasingly deployed as the service backend for LLM-integrated applications such as code completion and AI-powered search. Compared with the traditional usage of LLMs where users directly send queries to an LLM, LLM-integrated applications serve as middleware to refine users' queries with domain-specific knowledge to better inform LLMs and enhance the responses. Despite numerous opportunities and benefits, LLM-integrated applications also introduce new attack surfaces. Understanding, minimizing, and eliminating these emerging attack surfaces is a new area of research. In this work, we consider a setup where the user and LLM interact via an LLM-integrated application in the middle. We focus on the communication rounds that begin with user's queries and end with LLM-integrated application returning responses to the queries, powered by LLMs at the service backend. For this query-response protocol, we identify potential high-risk vulnerabilities that can originate from the malicious application developer or from an outsider threat initiator that is able to control the database access, manipulate and poison data that are high-risk for the user. Successful exploits of the identified vulnerabilities result in the users receiving responses tailored to the intent of a threat initiator (e.g., biased preferences for certain products). We assess such threats against LLM-integrated applications empowered by OpenAI GPT-3.5 and GPT-4. Our empirical results show that the threats can effectively bypass the restrictions and moderation policies of OpenAI, resulting in users receiving responses that contain bias, toxic content, privacy risk, and disinformation. To mitigate those threats, we identify and define four key properties, namely integrity, source identification, attack detectability, and utility preservation, that need to be satisfied by a safe LLM-integrated application. Based on these properties, we develop a lightweight, threat-agnostic defense that mitigates both insider and outsider threats. Our evaluations demonstrate the efficacy of our defense",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RFJGFrMvYj": {
    "title": "TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion",
    "volume": "review",
    "abstract": "In recent years, significant progress has been made in the development of text-to-image generation models. However, these models still face limitations when it comes to achieving full controllability during the generation process. Often, specific training or the use of limited models is required, and even then, they have certain restrictions. To address these challenges, A two-stage method that effectively combines controllability and high quality in the generation of images is proposed. This approach leverages the expertise of pre-trained models to achieve precise control over the generated images, while also harnessing the power of diffusion models to achieve state-of-the-art quality. By separating controllability from high quality, This method achieves outstanding results. It is compatible with both latent and image space diffusion models, ensuring versatility and flexibility. Moreover, This approach consistently produces comparable outcomes to the current state-of-the-art methods in the field. Overall, This proposed method represents a significant advancement in text-to-image generation, enabling improved controllability without compromising on the quality of the generated images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.5,
    "authors": []
  },
  "https://openreview.net/forum?id=yINucFNbcZ": {
    "title": "Improving the efficiency of conformal predictors via test-time augmentation",
    "volume": "review",
    "abstract": "In conformal classification, the goal is to output a _set_ of predicted classes, accompanied by a probabilistic guarantee that the set includes the true class. Conformal approaches have gained widespread traction across domains because they can be composed with existing classifiers to generate predictions with probabilistically valid uncertainty estimates. In practice, however, the utility of conformal prediction is limited by its tendency to yield large prediction sets. We study this phenomenon and provide insights into why large set sizes persist, even for conformal methods designed to produce small sets. Using these insights, we propose a method to reduce prediction set size while maintaining coverage. We use test-time augmentation to replace a classifier's predicted probabilities with probabilites aggregated over a set of augmentations. Our approach is flexible, computationally efficient, and effective. It can be combined with any conformal score, requires no model retraining, and reduces prediction set sizes by up to 30\\%. We conduct an evaluation of the approach spanning three datasets, three models, two established conformal scoring methods, and multiple coverage values to show when and why test-time augmentation is a useful addition to the conformal pipeline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mnHpxTxnYg": {
    "title": "Black-Box Privacy Attacks Against GANs via Detector Networks",
    "volume": "review",
    "abstract": "Since their inception Generative Adversarial Networks (GANs) have been popular generative models for various data types, including images, audio, video, and tabular data. One promising application of generative models like GANs is to share restricted or sensitive data with third parties through the creation of synthetic data or the model itself, rather than sharing the underlying data. However, recent research on diffusion models has highlighted privacy vulnerabilities in this approach -- namely that the models memorize significant quantities of the training data, and that existing membership inference attacks can identify generated samples as training points. This paper investigates the privacy implications of using GANs in black-box settings, where adversaries only have access to samples from the generator, rather than access to the discriminator as is often assumed in prior work. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our attacks on image GANs trained on the CIFAR10 dataset and tabular GANs trained on genomic data. Our most successful attack, called The Distinguisher, involve training a second network to score samples based on their likelihood of being generated by the GAN as opposed to a sample from the distribution. A key insight is that a network capable of distinguishing GAN-generated samples from true distribution samples can also distinguish training samples from the distribution. Our main findings indicate that across various GAN architectures and data types, adversaries can orchestrate non-trivial privacy attacks when provided with access to samples from the generator. However, the observed privacy leakage in GANs appears to be lower compared to other generative and discriminative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=qoHeuRAcSl": {
    "title": "Grounding Language Plans in Demonstrations Through Counter-Factual Perturbations",
    "volume": "review",
    "abstract": "Grounding the abstract knowledge captured by Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem. Whereas prior works have largely focused on leveraging LLMs for generating abstract plans in symbolic spaces, this work uses LLMs to guide the learning for structures and constraints in robot manipulation tasks. Specifically, we borrow from manipulation plan- ning literature the concept of mode families, defining specific types of motion constraints among sets of objects, to serve as an intermediate layer that connects high-level language representations with low-level physical trajectories. By lo- cally perturbing a small set of successful human demonstrations, we augment the dataset with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains neural network-based classifiers to differentiate success task executions from failures and as a by-product learns classifiers that ground low-level states into mode families without dense labeling. This further enables us to learn structured policies for the target task. Experimental validation in both 2D continuous-space and robotic manipulation environments demonstrates the robustness of our mode-based imitation methods under external perturbations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=A7t7z6g6tM": {
    "title": "Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty",
    "volume": "review",
    "abstract": "Deep neural networks (DNNs) have been shown to perform well on exclusive, multi-class classification tasks. However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them. When an image is ambiguous, such as a blurry one where an annotator can't distinguish between a husky and a wolf, it may be labeled with both classes: {husky, wolf}. This scenario necessitates the use of composite set labels. In this paper, we propose a novel framework called Hyper-Evidential Neural Network (HENN) that explicitly models predictive uncertainty caused by composite set labels in training data in the context of the belief theory called Subjective Logic (SL). By placing a Grouped Dirichlet distribution on the class probabilities, we treat predictions of a neural network as parameters of hyper-subjective opinions and learn the network that collects both single and composite evidence leading to these hyper-opinions by a deterministic DNN from data. We introduce a new uncertainty type called vagueness originally designed for hyper-opinions in SL to quantify composite classification uncertainty for DNNs. Our experiments prove that HENN outperforms its state-of-the-art counterparts based on four image datasets. The code and datasets are available at: https://shorturl.at/dhoqx",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5o9G4XF1LI": {
    "title": "Goodhart's Law in Reinforcement Learning",
    "volume": "review",
    "abstract": "Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a *proxy* for the true objective rather than as its definition. We study this phenomenon through the lens of *Goodhart's law*, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to *quantify* the magnitude of this effect and *show empirically* that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart's law for a wide range of environments and reward functions. We then provide a *geometric explanation* for why Goodhart's law occurs in Markov decision processes. We use these theoretical insights to propose an *optimal early stopping method* that provably avoids the aforementioned pitfall and derive theoretical *regret bounds* for this method. Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function. Finally, we evaluate our early stopping method experimentally. Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xCRr9DrolJ": {
    "title": "Score Regularized Policy Optimization through Diffusion Behavior",
    "volume": "review",
    "abstract": "Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=HKV45Y0rFz": {
    "title": "Conservative Prediction via Data-Driven Confidence Minimization",
    "volume": "review",
    "abstract": "In safety-critical applications of machine learning, it is often desirable for a model to be conservative, abstaining from making predictions on \"unknown\" inputs which are not well-represented in the training data. However, detecting unknown examples is challenging, as it is impossible to anticipate all potential inputs at test time. To address this, prior work (Hendrycks et al., 2018) minimizes model confidence on an auxiliary outlier dataset carefully curated to be disjoint from the training distribution. We theoretically analyze the choice of auxiliary dataset for confidence minimization, revealing two actionable insights: (1) if the auxiliary set contains unknown examples similar to those seen at test time, confidence minimization leads to provable detection of unknown test examples, and (2) if the first condition is satisfied, it is unnecessary to filter out known examples for out-of-distribution (OOD) detection. Motivated by these guidelines, we propose the Data-Driven Confidence Minimization (DCM) framework, which minimizes confidence on an uncertainty dataset. We apply DCM to two problem settings in which conservative prediction is paramount—selective classification and OOD detection—and provide a realistic way to gather uncertainty data for each setting. Our experiments show that DCM consistently outperforms existing selective classification approaches on 4 datasets when tested on unseen distributions and outperforms state-of-the-art OOD detection methods on 8 ID-OOD dataset pairs, reducing FPR (at TPR 95%) by 6.3% and 58.1% on CIFAR-10 and CIFAR-100 compared to Outlier Exposure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=gBV21wK07P": {
    "title": "3D Autoencoding Diffusion Model for Molecule Interpolation and Manipulation",
    "volume": "review",
    "abstract": "Manipulating known molecules and interpolating between them is useful for many applications in drug design and protein engineering, where exploration around the molecular templates is involved. Recent studies using equivariant diffusion models have made significant progress in the de novo generation of high-quality molecules, but using these models to directly manipulate a specified template remains less explored. This is mainly due to an intrinsic property of diffusion models: the lack of a latent semantic space that is easy to operate on. To address this issue, we propose the first semantics-guided equivariant diffusion model that leverages the \"semantic\" embedding of a 3D molecule, learned from an auxiliary encoder, to control the generative denoising process. By modifying the embedding, we can steer the generation towards another specified molecule or a desired molecular property. We show that our model can effectively manipulate basic chemical properties, outperforming several baselines. We further verify that our approach can achieve smoother interpolation between 3D molecular pairs compared to standard diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=qPloNoDJZn": {
    "title": "Robustifying and Boosting Training-Free Neural Architecture Search",
    "volume": "review",
    "abstract": "Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=9528xxcT7h": {
    "title": "Two Heads are Better than One: Towards Better Adversarial Robustness by Combining Transduction and Rejection",
    "volume": "review",
    "abstract": "Both transduction and rejection have emerged as important techniques for defending against adversarial perturbations. A recent work by Tramèr showed that, in the rejection-only case (no transduction), a strong rejection-solution can be turned into a strong (but computationally inefficient) non-rejection solution. This detector-to-classifier reduction has been mostly applied to give evidence that certain claims of strong selective-model solutions are susceptible, leaving the benefits of rejection unclear. On the other hand, a recent work by Goldwasser et al. showed that rejection combined with transduction can give provable guarantees (for certain problems) that cannot be achieved otherwise. Nevertheless, under recent strong adversarial attacks (GMSA, which has been shown to be much more effective than AutoAttack against transduction), Goldwasser et al.'s work was shown to have low performance in a practical deep-learning setting. In this paper, we take a step towards realizing the promise of transduction+rejection in more realistic scenarios. Theoretically, we show that a novel application of Tramèr's classifier-to-detector technique in the transductive setting can give significantly improved sample-complexity for robust generalization. While our theoretical construction is computationally inefficient, it guides us to identify an efficient transductive algorithm to learn a selective model. Extensive experiments using state of the art attacks (AutoAttack, GMSA) show that our solutions provide significantly better robust accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=L9U5MJJleF": {
    "title": "Concept Bottleneck Generative Models",
    "volume": "review",
    "abstract": "We introduce a generative model with an intrinsically interpretable layer---a concept bottleneck layer---that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines---in some cases, it is \\textit{10 times} more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vrS1zwekw": {
    "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following",
    "volume": "review",
    "abstract": "In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. Experimental results across four zero-shot benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes, reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=duBCwjb68o": {
    "title": "Latent Consistency Models: Synthesizing High-Resolution Images with Few-step Inference",
    "volume": "review",
    "abstract": "Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song2023consistency), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach2022high). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768$\\times$768 2$\\sim$4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=V01FPV3SNY": {
    "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
    "volume": "review",
    "abstract": "Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100\\% to around 10\\% or less",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Rriucj4UmC": {
    "title": "Reconstruction of Cortical Surfaces with Spherical Topology from Infant Brain MRI via Recurrent Deformation Learning",
    "volume": "review",
    "abstract": "Cortical surface reconstruction (CSR) from MRI is key to investigating brain structure and function. While recent deep learning approaches have significantly improved the speed of CSR, a substantial amount of runtime is still needed to map the cortex to a topologically-correct spherical manifold to facilitate downstream geometric analyses. Moreover, this mapping is possible only if the topology of the surface mesh is homotopic to a sphere. Here, we present a method for simultaneous CSR and spherical mapping efficiently within seconds. Our approach seamlessly connects two sub-networks for white and pial surface generation. Residual diffeomorphic deformations are learned iteratively to gradually warp a spherical template mesh to the white and pial surfaces while preserving mesh topology and uniformity. The one-to-one vertex correspondence between the template sphere and the cortical surfaces allows easy and direct mapping of geometric features like convexity and curvature to the sphere for visualization and downstream processing. We demonstrate the efficacy of our approach on infant brain MRI, which poses significant challenges to CSR due to tissue contrast changes associated with rapid brain development during the first postnatal year. Performance evaluation based on a dataset of infants from 0 to 12 months demonstrates that our method substantially enhances mesh regularity and reduces geometric errors, outperforming state-of-the-art deep learning approaches, all while maintaining high computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=YPOcBR9h2a": {
    "title": "DLCNet: Enabling Long-Range Convolution with Data Dependency",
    "volume": "review",
    "abstract": "In recent years, the Transformer architecture and self-attention mechanism have become the first choice for sequence modeling, but they encounter significant computational challenges when processing lengthy sequences. Long-range convolution has emerged as a promising alternative to self-attention, offering distinct advantages across various domains. However, current long-range convolution architectures still face several issues, such as excessive parameter usage and limited in-context learning capabilities. To tackle these challenges, we propose a Data-dependent Long-range Convolution Network (DLCNet) that introduces data dependency through three key modules: Layer-Wise Mapping, Rectify SideNet, and SWEAP Operator. DLCNet effectively facilitates in-context learning within a reasonable parameter scale. Extensive experiments have demonstrated that DLCNet surpasses the state-of-the-art baselines in processing lengthy sequences, even when trained with short sequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nT8ouPui8": {
    "title": "On Memorization in Diffusion Models",
    "volume": "review",
    "abstract": "Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training-data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kDoKXaucJV": {
    "title": "Sparse-Guard: Sparse Coding-Based Defense against Model Inversion Attacks",
    "volume": "review",
    "abstract": "In this paper, we study neural network architectures that are robust to model inversion attacks. It is well-known that standard network architectures are vulnerable to model inversion, where an adversary can reconstruct images or data used to train the network by inspecting the network's output or the intermediate outputs from a single hidden network layer. Surprisingly, very little is known about how a network's architecture contributes to its robustness (or vulnerability). Instead, recent work on mitigating such attacks has focused on injecting random noise into the network layers or augmenting the training dataset with synthetic data. Our main result is a novel sparse coding-based network architecture, $Sparse$-$Guard$, that is robust to model inversion attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to prevent privacy attacks because they allow us to control the amount of irrelevant private information encoded in a model's intermediate representations in a manner that can be computed efficiently during training, that adds little to the trained model's overall parameter complexity, and that is known to have little effect on classification accuracy. Specifically, we demonstrate that compared to networks trained with state-of-the-art noise-based or data augmentation-based defenses, $Sparse$-$Guard$ networks maintain comparable or higher classification accuracy while degrading state-of-the-art training data reconstructions by a factor of $1.2$ to $16.2$ across a variety of reconstruction quality metrics (PSNR, SSIM, FID) on standard datasets. We also show that $Sparse$-$Guard$ is equally robust to attacks regardless of whether the leaked layer is earlier or later, suggesting it is also an effective defense under novel security paradigms such as Federated Learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=1iKydVG6pL": {
    "title": "Discovering Mathematical Formulas from Data via LSTM-guided Monte Carlo Tree Search",
    "volume": "review",
    "abstract": "Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is commonly referred to as symbolic regression, which poses an NP-hard combinatorial optimization problem. Traditional symbolic regression algorithms typically rely on genetic algorithms; however, these approaches are sensitive to hyperparameters and often struggle to fully recover the target expression. To address these limitations, a novel symbolic regression algorithm based on Monte Carlo Tree Search (MCTS) was proposed this year. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, it still faces challenges when dealing with complex expressions due to the vast search space involved. Moreover, the lack of guidance during the MCTS expansion process severely hampers its search efficiency. In order to overcome these issues, we propose AlphaSymbol - a new symbolic regression algorithm that combines MCTS with a Long Short-Term Memory network (LSTM). By leveraging LSTM's ability to guide the MCTS expansion process effectively, we enhance the overall search efficiency of MCTS significantly. Next, we utilize the MCTS results to further refine the LSTM network, enhancing its capabilities and providing more accurate guidance for the MCTS process. MCTS and LSTM hand in hand advance together, win-win cooperation until the target expression is successfully determined. We conducted extensive evaluations of AlphaSymbol using 222 expressions sourced from over 10 different symbolic regression datasets. The experimental results demonstrate that AlphaSymbol outperforms existing state-of-the-art algorithms in accurately recovering symbolic expressions both with and without added noise",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=4UP387Adir": {
    "title": "Weakly Supervised Graph Contrastive Learning",
    "volume": "review",
    "abstract": "Graph Contrastive Learning (GCL) has recently gained popularity owing to its ability to learn efficient node representations in a self-supervised manner. These representations are typically used to train a downstream classifier. In several real-world datasets, it is difficult to acquire sufficient clean labels for classification and instead we have weak or noisy labels available. There is little known about the robustness of the node representations learnt by the current GCL methods in the presence of weak labels. Moreover, GCL has been successfully adapted to a supervised setting where class labels are used to contrast between pairs of nodes. Can weak labels similarly be leveraged to learn better node embeddings? In this paper, we first empirically study the robustness of current GCL node representations to weak supervision. Then, we introduce Weakly Supervised Graph Contrastive Learning, WSNet, a novel method that incorporates signals from weak labels for the contrastive learning objective. We evaluate WSNet on five benchmark graph datasets comparing its performance with state-of-the-art GCL and noisy-label learning methods. We show that WSNet outperforms all baselines particularly in the high noise setting. We conclude that although current GCL methods show great promise in the weak supervision paradigm, they are still limited in their capacity to deal with label noise and utilizing signals from weak labels is an effective way to improve their performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=2JF8mJRJ7M": {
    "title": "Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance",
    "volume": "review",
    "abstract": "Large-scale contrastive vision-language pre-trained models provide the zero-shot model achieving competitive performance across a range of image classification tasks without requiring training on downstream data. Recent works have confirmed that while additional fine-tuning of the zero-shot model on the reference data results in enhanced downstream performance, it compromises the model's robustness against distribution shifts. Our investigation begins by examining the conditions required to achieve the goals of robust fine-tuning, employing descriptions based on feature distortion theory and joint energy-based models. Subsequently, we propose a novel robust fine-tuning algorithm, Lipsum-FT, that effectively utilizes the language modeling aspect of the vision-language pre-trained models. Extensive experiments conducted on distribution shift scenarios in DomainNet and ImageNet confirm the superiority of our proposed Lipsum-FT approach over existing robust fine-tuning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=6uUmpPvqUU": {
    "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "volume": "review",
    "abstract": "Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit. In-context learning is one of the celebrated abilities of recent LLMs. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, in-context learning has been studied from a mathematical perspective with simplified linear self-attention without softmax unit. Based on a linear regression formulation $ \\min_x \\| Ax - b \\|_2 $, existing works show linear Transformers' capability of learning linear functions in context. The capability of Transformers with softmax unit approaching full Transformers, however, remains unexplored. In this work, we study the in-context learning based on a softmax regression formulation $ \\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b \\|_2 $. We show the upper bounds of the data transformations induced by a single self-attention layer with softmax unit and by gradient-descent on a $ \\ell_2 $ regression loss for softmax prediction function. Our theoretical results imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wure6HljpJ": {
    "title": "CoSDA: Continual Source-Free Domain Adaptation",
    "volume": "review",
    "abstract": "Without access to the source data, source-free domain adaptation (SFDA) transfers knowledge from a source-domain trained model to target domains. Recently, SFDA has gained popularity due to the need to protect the data privacy of the source domain, but it suffers from catastrophic forgetting on the source domain due to the lack of data. To systematically investigate the mechanism of catastrophic forgetting, we first reimplement previous SFDA approaches within a unified framework and evaluate them on four benchmarks. We observe that there is a trade-off between adaptation gain and forgetting loss, which motivates us to design a consistency regularization to mitigate forgetting. In particular, we propose a continual source-free domain adaptation approach named CoSDA, which employs a dual-speed optimized teacher-student model pair and is equipped with consistency learning capability. Our experiments demonstrate that CoSDA outperforms state-of-the-art approaches in continuous adaptation. Notably, our CoSDA can also be integrated with other SFDA methods to alleviate forgetting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=DFTHW0MyiW": {
    "title": "Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies",
    "volume": "review",
    "abstract": "In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond merely worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic difficulty in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aiming for efficient adaptation within a compact, finite policy class $\\tilde{\\Pi}$, which can resort to an adversarial bandit subroutine. In light of the importance of a finite and compact $\\tilde{\\Pi}$, we propose a novel training-time algorithm to iteratively discover \\textit{non-dominated policies}, forming a near-optimal and minimal $\\tilde{\\Pi}$, thereby ensuring both robustness and test-time efficiency. Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=eY7sLb0dVF": {
    "title": "Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs",
    "volume": "review",
    "abstract": "Generating realistic time series data is important for numerous engineering and scientific applications. Several existing works tackle this problem using generative adversarial networks, however, GANs are often unstable during training and suffer from mode collpase. While variational autoencoders (VAEs) are more robust to the above issues, surprisingly, they are considered less for time series generation. In this work, we introduce Koopman VAE (KVAE), a new generative framework that is based on a novel design for the model prior, and that can be optimized for either regular and irregular training data. Inspired by the Koopman theory, we represent the latent conditional prior dynamics using a linear map. Our approach enhances generative modeling with two desired features: (i) incorporating domain knowledge can be achieved by leverageing spectral tools that prescribe constraints on the eigenvalues of the linear map; and (ii) studying the qualitative behavior and stablity of the system can be performed using tools from dynamical systems theory. Our results show that KVAE outperforms state-of-the-art GAN and VAE methods across several challenging synthetic and real-world time series generation benchmarks. Whether trained on regular or irregular data, KVAE generates time series that improve both discriminative and predictive metrics. Further, we present visual evidence suggesting that KVAE learns probability density functions that better approximate the empirical ground truth distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=yxKZGQLzOP": {
    "title": "Generating Pragmatic Examples to Train Neural Program Synthesizers",
    "volume": "review",
    "abstract": "Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one's intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference is effective in choosing the user intended programs. However, these models requires counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.We then use the informative dataset to train models to improve the synthesizer's ability to disambiguate user-provided examples _without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, finding that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=HSKaGOi7Ar": {
    "title": "Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness",
    "volume": "review",
    "abstract": "Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a novel framework for quantitatively studying the expressiveness of GNN architectures, addressing the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide new insights into a series of previous work, bridge disparate subareas within the GNN community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNNs models align well with the proposed metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.5,
    "authors": []
  },
  "https://openreview.net/forum?id=svSWP21tdp": {
    "title": "Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias",
    "volume": "review",
    "abstract": "Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. We provide a taxonomy for MIDS and demonstrate that their long-run fairness effects lead to a lack of representation and performance on minoritized groups within a few generations. We improve upon this unfairness behavior by situating Algorithmic Reparation as an intentional MIDS with the goal of providing redress for historical discrimination and improving the fairness of models subject to other MIDS. Our work makes an important step towards identifying and mitigating the justification of unfair feedback loops via the algorithmic objectivity and idealism ascribed to autonomous systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pe2lo3QOvo": {
    "title": "Making RL with Preference-based Feedback Efficient via Randomization",
    "volume": "review",
    "abstract": "RL algorithms that learn from human feedback (RLHF) need to be efficient in terms of *statistical complexity, computational complexity, and query complexity*. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, by using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. Particularly, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by the idea of Thompson sampling. Our algorithm minimizes Bayesian regret bound and query complexity, again achieving a near-optimal tradeoff between these two quantities. Computation-wise, similar to the prior Thompson sampling algorithms under the regular RL setting, the main computation primitives of our algorithm are Bayesian supervised learning oracles which have been heavily investigated on the empirical side when applying Thompson sampling algorithms to RL benchmark problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=oxEER3kZ9M": {
    "title": "On the Possibilities of AI-Generated Text Detection: A Sample Complexity Analysis",
    "volume": "review",
    "abstract": "Our work addresses the critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text, a task essential for numerous applications. Despite ongoing debate about the feasibility of such differentiation, we present evidence supporting its consistent achievability, except when human and machine text distributions are indistinguishable across their entire support. Drawing from information theory, we argue that as machine-generated text approximates human-like quality, the sample size needed for detection increases. We establish precise sample complexity bounds for detecting AI-generated text, laying groundwork for future research aimed at developing advanced, multi-sample detectors. Our empirical evaluations across multiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the viability of enhanced detection methods. We test various state-of-the-art text generators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and Llama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector, GPTZero. Our findings align with OpenAI's empirical data related to sequence length, marking the first theoretical substantiation for these observations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=OlwW4ZG3Ta": {
    "title": "Reflective Policy Optimization",
    "volume": "review",
    "abstract": "On-policy reinforcement learning methods, such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), often require significant data to be collected at each update, giving rise to issues of sample inefficiency. This paper introduces a novel extension to on-policy methods called Reflective Policy Optimization (RPO). RPO's fundamental objective is amalgamating prior and subsequent state and action information from trajectory data to optimize the current policy. This approach empowers the agent to engage in introspection and introduce modifications to its actions within the current state to a certain degree. Furthermore, theoretical analyses substantiate that our proposed method not only upholds the crucial property of monotonically improving policy performance but also adeptly contracts the solution space of the optimized policy, consequently expediting the training procedure. We empirically demonstrate the feasibility and efficacy of our approach in reinforcement learning benchmarks, culminating in superior performance in terms of sample efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=EBUoTvVtMM": {
    "title": "User Inference Attacks on Large Language Models",
    "volume": "review",
    "abstract": "Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore several heuristics for mitigating privacy attacks. We find that interventions in the training algorithm, such as batch or per-example gradient clipping and early stopping fail to prevent user inference. However, limiting the number of fine-tuning samples from a single user can reduce attack effectiveness, albeit at the cost of reducing the total amount of fine-tuning data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=L7LwHpjMTQ": {
    "title": "CLIP as Multi-Task Multi-Kernel Learning",
    "volume": "review",
    "abstract": "Contrastive Language-Image Pretraining (CLIP) is a foundational model that learns a latent embedding space through an inner product-based objective. In this paper, we provide a theoretical interpretation of CLIP utilizing Reproducing Kernel Hilbert Space (RKHS) framework. Specifically, we reformulate the problem of estimating the infinite-dimensional mapping with a neural network as selecting an unknown RKHS using multiple kernel learning. Such connection motivates us to propose to estimate the CLIP embedding via the multi-task multi-kernel (MTMK) method: we reformulate the different labels in the CLIP training data as the multiple training tasks, and reformulate learning the unknown CLIP embedding as choosing an optimal kernel from a family of Reproducing Kernel Hilbert Spaces, which is computationally more efficient. Utilizing the MTMK interpretation of CLIP, we also show an optimal statistical rate of the MTMK classifier under the scenario that both the number of covariates and the number of candidate kernels can increase with the sample size. Besides the synthetic simulations, we apply the proposed method to align the medical imaging data with the clinical codes in electronic health records and illustrate that our approach can learn the proper kernel space aligning the imaging embedding with the text embeddings with high accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFWG9Cy3WK": {
    "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy",
    "volume": "review",
    "abstract": "Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like: ($a$) $\\textit{High Memory Usage,}$ due to duplication of the network layers into multiple copies as experts; and ($b$) $\\textit{Redundancy in Experts,}$ as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: ($1$) redundant information overshadows critical experts; ($2$) appropriate neuron permutation for each expert is missing to bring all of them in alignment. To address these challenges, we propose a novel merging algorithm for SMoE, $\\textit{i.e.}$, $\\texttt{M-SMoE}$, which leverages routing statistics to guide expert merging. Specifically, it starts with neuron permutation alignment for experts; then, dominant experts and their \"group members\" are formed based on routing policies; lastly, every expert group is merged into a single expert by utilizing each expert's activation frequency as their weight for merging, thus diminishing the impact of insignificant experts. Moreover, we draw an interesting observation that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression. Hence, our final method, $\\texttt{MC-SMoE}$ ($\\textit{i.e.}$, Merge, then Compress SMoE), further decomposes the merged experts into low-rank and structural sparse alternatives. Extensive experiments across $8$ benchmarks validate the effectiveness of our proposals. For instance, our $\\texttt{MC-SMoE}$ achieves up to $80\\%$ memory and a $20\\%$ FLOPs reduction, with virtually no loss in performance. Our code is provided as supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=AY9KyTGcnk": {
    "title": "Adaptive Regret for Bandits Made Possible: Two Queries Suffice",
    "volume": "review",
    "abstract": "Fast changing states or volatile environments pose a significant challenge to online optimization, which needs to perform rapid adaptation under limited observation. In this paper, we give query and regret optimal bandit algorithms under the strict notion of strongly adaptive regret, which measures the maximum regret over any contiguous interval $I$. Due to its worst-case nature, there is an almost-linear $\\Omega(|I|^{1-\\epsilon})$ regret lower bound, when only one query per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just two queries per round, we give Strongly Adaptive Bandit Learner (StABL) that achieves $\\widetilde{O}(\\sqrt{n|I|})$ adaptive regret for multi-armed bandits with $n$ arms. The bound is tight and cannot be improved in general. Our algorithm leverages a multiplicative update scheme of varying stepsizes and a carefully chosen observation distribution to control the variance. Furthermore, we extend our results and provide optimal algorithms in the bandit convex optimization setting. Finally, we empirically demonstrate the superior performance of our algorithms under volatile environments and for downstream tasks, such as algorithm selection for hyperparameter optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ekdurSMmbH": {
    "title": "Universal Off-Policy Selection for Human-Centric Systems via Participant Sub-grouping",
    "volume": "review",
    "abstract": "Human-centric tasks like healthcare and education are characterized by heterogeneity among patients and students, resulting in different disease trajectories and learning styles that require personalized treatments or instructional interventions for specific subgroups. When deploying reinforcement learning (RL) for such tasks, off-policy selection (OPS) is essential, since it it closes the loop by selecting and evaluating RL-induced policies offline, without the need for any online interaction with the participants. Many pre-existing OPS methods, however, do not consider the heterogeneity among the participants. In this work, we introduce a universal off-policy selection (UOPS) approach to address the issue of participant heterogeneity by taking a multi-step approach. Initially, it divides the participants into sub-groups, grouping together those who exhibit similar behaviors. Subsequently, it acquires OPS criteria tailored to each of these sub-groups. Consequently, when new participants come, they will receive policy recommendations based on the sub-groups they align with. This methodology enhances the adaptability and personalization of the RL system, ensuring that policy selections align more closely with the unique characteristics of each participant or group of participants. We evaluate UOPS' effectiveness through two applications: an intelligent tutor system that has been used in classrooms for over eight years, as well as a healthcare application for sepsis treatment and intervention. In both applications, UOPS shows significant improvements in students' learning and patient outcomes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=jw8EoY1FvF": {
    "title": "Delayed Local-SGD for Distributed Learning with Linear Speedup",
    "volume": "review",
    "abstract": "Local-SGD-based algorithms have gained much popularity in distributed learning to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with the central server. However, since all participating clients are required to initiate iterations from the latest global model in each round of Local-SGD, the overall training process can be slowed down due to the straggler effect. To address this issue, we propose a Delayed Local-SGD (DLSGD) framework for distributed and federated learning with partial client participation. In DLSGD, each client performs local training starting from outdated models, regardless of whether it participates in the global aggregation. We investigate two types of DLSGD methods applied to scenarios where clients have identical or different local objective functions. Theoretical analyses demonstrate that DLSGD achieves asymptotic convergence rates that are on par with the classic Local-SGD methods for solving nonconvex problems, and guarantees linear speedup with respect to the number of participating clients. Additionally, we carry out numerical experiments using real datasets to validate the efficiency and scalability of our approach when training neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xle26hcxHh": {
    "title": "AudoFormer: An Efficient Transformer with Consistent Auxiliary Domain for Source-free Domain Adaptation",
    "volume": "review",
    "abstract": "Source-free domain adaptation (SFDA), which tackles domain adaptation without accessing the source domain directly, has gradually gained widespread attention. However, due to the inaccessibility of source domain data, deterministic invariable features cannot be obtained. Current advanced methods mainly evaluate pseudo-labels or consistent neighbor labels for self-supervision, which are susceptible to hard samples and affected by domain bias. In this paper, we propose an efficient transFormer with a consistent Auxiliary domain for source-free domain adaptation, abbreviated as AudoFormer, which solves the invariable feature representation from a new perspective by domain consistency. Concretely, AudoFormer constructs an auxiliary domain module (ADM) block, which can achieve diversified representations from the global attention feature in the intermediate layers. Then based on the auxiliary domain and target domain, we distinguish invariable feature representation by exploiting multiple consistency strategies, i.e., dynamically evaluated consistent labels and consistent neighbors, which can divide the whole target samples into source-like easy samples and target-specific hard samples. Finally, we align the source-like with the target-specific samples by conditional guided multi-kernel max mean discrepancy (CMK-MMD), which guides the hard samples to align the corresponding easy samples. To verify the effectiveness, we conduct extensive experiments on three benchmark datasets (i.e., Office-31, Office-Home, and VISDA-C). Results show that our approach achieves significant performance among multiple domain adaptation benchmarks compared to the other state-of-the-art baselines. Code will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=SHUQtRK0eU": {
    "title": "Generalized Activation via Multivariate Projection",
    "volume": "review",
    "abstract": "Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from $\\mathbb{R}$ onto the nonnegative half-line $\\mathbb{R}_+$. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architectures further corroborate MPU's effectiveness against a broader range of existing activation functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=zkVm3JqJzs": {
    "title": "Conformal Prediction for Deep Classifier via Label Ranking",
    "volume": "review",
    "abstract": "Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. In this paper, we empirically and theoretically show that disregarding the probabilities' value will mitigate the undesirable effect of miscalibrated probability values. Then, we propose a novel algorithm named $\\textit{Sorted Adaptive prediction sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce sets of small size and communicate instance-wise uncertainty. Theoretically, we provide a finite-sample coverage guarantee of SAPS and show that the expected value of set size from SAPS is always smaller than APS. Extensive experiments validate that SAPS not only lessens the prediction sets but also broadly enhances the conditional coverage rate and adaptation of prediction sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=eS0qCQDrkG": {
    "title": "Towards Efficient Trace Estimation for Optimal Transport in Domain Adaptation",
    "volume": "review",
    "abstract": "We improve the efficiency of optimal transport problems with Laplacian regularization in domain adaptation for large-scale data by utilizing Hutchinson's trace estimator, a classical method for approximating the trace of a matrix which to the best of our knowledge has not been used in this context. This approach significantly streamlines the computational complexity of the Laplacian regularization term with respect to the sample size $n$, improving the time from $O(n^3)$ to $O(n^2)$ by converting large-scale matrix multiplications into more manageable matrix-vector multiplication queries. In our experiments, we employed Hutch++, a more efficient variant of Hutchinson's method. Empirical validations confirm our method's efficiency, achieving an average accuracy within 1% of the original algorithm with 80% of its computational time, and maintaining an average accuracy within 3.25% in only half the time. Moreover, the integrated stochastic perturbations mitigate overfitting, enhancing average accuracy under certain conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyH5ijgx9C": {
    "title": "Efficient Stagewise Pretraining via Progressive Subnetworks",
    "volume": "review",
    "abstract": "Recent developments in language models have sparked interest in developing efficient pretraining methods. A recent and effective paradigm is to perform stagewise training, where the depth of the model is gradually increased over the course of training starting from a shallow network (e.g. gradual stacking (Reddi et al., 2023)). While this is appealing since it yields resource and wall-time savings, it has limitations, particularly the inability to assess and evaluate the full model performance during earlier stages, and degradation in model quality due to smaller capacity of models in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We empirically focus on a simple instantiation of this framework - Random Path Training (RAPTR) - that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. We demonstrate that RAPTR achieves better pre-training loss for BERT and UL2 language models while requiring 20-33\\% fewer FLOPs compared to standard training, and is competitive or better than gradual stacking at similar FLOPs. Furthermore, RAPTR shows better downstream performance on UL2, improving multiple QA and SuperGLUE tasks by 1-5\\% compared to standard training and stacking. Finally, we provide theoretical basis of RAPTR for residual networks by characterizing their stability due to residual connections and layer norm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=NvSwR4IvLO": {
    "title": "Can AI-Generated Text be Reliably Detected?",
    "volume": "review",
    "abstract": "The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks, including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, we show that these detectors are not reliable in practical scenarios. In particular, we develop a recursive paraphrasing attack to apply on AI text, which can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. Our experiments include passages around 300 tokens in length, showing the sensitivity of the detectors even in the case of relatively long passages. We also observe that our recursive paraphrasing only degrades text quality slightly, measured via perplexity scores and MTurk human study. Additionally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks aimed to mislead detectors to classify human-written text as AI-generated, potentially causing reputational damages to the developers. In particular, we show that an adversary can infer hidden AI text signatures of the LLM outputs without having white-box access to the detection method. Finally, we provide a theoretical connection between the AUROC of the best possible detector and the Total Variation distance between human and AI text distributions that can be used to study the fundamental hardness of the reliable detection problem for advanced language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=iAW2EQXfwb": {
    "title": "Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation",
    "volume": "review",
    "abstract": "Deep reinforcement learning has recently been successfully applied to online procedural content generation in which a policy determines promising game-level segments. However, existing methods can hardly discover diverse level patterns, while the lack of diversity makes the gameplay boring. This paper proposes an ensemble reinforcement learning approach that uses multiple negatively correlated sub-policies to generate different alternative level segments, and stochastically selects one of them following a selector model. A novel policy regularisation technique is integrated into the approach to diversify the generated alternatives. In addition, we develop theorems to provide general methodologies for optimising policy regularisation in a Markov decision process. The proposed approach is compared with several state-of-the-art policy ensemble methods and classic methods on a well-known level generation benchmark, with two different reward functions expressing game-design goals from different perspectives. Results show that our approach boosts level diversity notably with competitive performance in terms of the reward. Furthermore, by varying the regularisation coefficient, the trained generators form a well-spread Pareto front, allowing explicit trade-offs between diversity and rewards of generated levels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MOmqfJovQ6": {
    "title": "Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping",
    "volume": "review",
    "abstract": "Reinforcement learning often needs to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces (often known as the curse of dimensionality). In this work, we address this issue by learning the inherent structure of action-wise similar MDP to appropriately balance the performance degradation versus sample/computational complexity. In particular, we partition the action spaces into multiple groups based on the similarity in transition distribution and reward function, and build a linear decomposition model to capture the difference between the intra-group transition kernel and the intra-group rewards. Both our theoretical analysis and experiments reveal a *surprising and counter-intuitive result*: while a more refined grouping strategy can reduce the approximation error caused by treating actions in the same group as identical, it also leads to increased estimation error when the size of samples or the computation resources is limited. This finding highlights the grouping strategy as a new degree of freedom that can be optimized to minimize the overall performance loss. To address this issue, we formulate a general optimization problem for determining the optimal grouping strategy, which strikes a balance between performance loss and sample/computational complexity. We further propose a computationally efficient method for selecting a nearly-optimal grouping strategy, which maintains its computational complexity independent of the size of the action space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=QgwAYFrh9t": {
    "title": "Learning Hierarchical Polynomials with Three-Layer Neural Networks",
    "volume": "review",
    "abstract": "We study the problem of learning hierarchical polynomials over the standard Gaussian distribution with three-layer neural networks. We specifically consider target functions of the form $h = g \\circ p$ where $p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a degree $k$ polynomial and $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ is a degree $q$ polynomial. This function class generalizes the single-index model, which corresponds to $k=1$, and is a natural class of functions possessing an underlying hierarchical structure. Our main result shows that for a large subclass of degree $k$ polynomials $p$, a three-layer neural network trained via layerwise gradient descent on the square loss learns the target $h$ up to vanishing test error in $\\widetilde O(d^k)$ samples and polynomial time. This is a strict improvement over kernel methods, which require $\\widetilde \\Theta(d^{kq})$ samples, as well as existing guarantees for two-layer networks, which require the target function to be low-rank. Our result also generalizes prior works on three-layer neural networks, which were restricted to the case of $p$ being a quadratic. When $p$ is indeed a quadratic, we achieve the information-theoretically optimal sample complexity $\\widetilde O(d^2)$, which is an improvement over prior work (Nichani et al., 2023) requiring a sample size of $\\widetilde\\Theta(d^4)$. Our proof proceeds by showing that during the first stage of training the network performs feature learning to recover the feature $p$ with $\\widetilde O(d^k)$ samples. This work demonstrates the ability of three-layer neural networks to learn complex features and as a result learn a broad class of hierarchical functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=oKGDfMrD4A": {
    "title": "Exploring Adversarial Robustness of Graph Neural Networks in Directed Graphs",
    "volume": "review",
    "abstract": "Existing research on robust Graph Neural Networks (GNNs) focuses predominantly on undirected graphs, neglecting the trustworthiness inherent in directed graphs. This work analyzes the limitations of existing approaches from both attack and defense perspectives, and we present an exploration of the adversarial robustness of GNNs in directed graphs. Specifically, we first introduce a new and more realistic directed graph attack setting to overcome the limitations of existing attacks. Then we propose a simple and effective message-passing framework as a plug-in layer to enhance the robustness of GNNs while avoiding a false sense of security. Our findings demonstrate that the profound trust implications offered by directed graphs can be harnessed to bolster the robustness and resilience of GNNs significantly. When coupled with existing defense strategies, this framework achieves outstanding clean accuracy and state-of-the-art robust performance against both transfer and adaptive attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=usmP3muXMI": {
    "title": "Minimizing Chebyshev Risk Magically Mitigates the Perils of Overfitting",
    "volume": "review",
    "abstract": "Since reducing overfitting in deep neural networks (DNNs) increases their test performance, many efforts have tried to mitigate it by adding regularization loss terms in one or more hidden layers of the network, including the convolutional layers. To build upon the canonical wisdom guiding these previous works, we analytically tried to understand how intra and inter-class feature relationships affect misclassification. Our analysis begins by assuming a DNN is the composition of a feature extractor and classifier, where the classifier is the last fully connected layer of the network and the feature layer is the input vector to the classifier. We assume that, corresponding to each class, there exists an ideal feature vector which we designate as a class prototype. The goal of the training method is then to reduce the probability that an example's features deviate significantly from its class prototype, which increases the risk of misclassification. Formally, this probability can be bound using a Chebyshev's inequality comprised of within-class covariance and between-class prototype distance. The terms in the inequality are added to our loss function for optimizing the feature layer, which implicitly optimizes the previous convolutional layers' parameter values. We observe from empirical results on multiple datasets and network architectures that our training algorithm reduces overfitting and improves upon previous approaches in an efficient manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=bZMyHBSnEI": {
    "title": "Deep Equilibrium Multimodal Fusion",
    "volume": "review",
    "abstract": "Multimodal fusion integrates the complementary information present in multiple modalities and has gained much attention recently. Existing fusion approaches exhibit three key elements for informative multimodal fusion, *i.e.*, stabilizing unimodal signals, capturing intra- and inter-modality interactions at multi-level, and perceiving modality importance in a dynamic manner. The current fusion methods mostly suffice only one of these conditions, without considering all three aspects simultaneously. Encapsulating these ideas, in this paper, we propose a novel deep equilibrium (DEQ) method for multimodal fusion via seeking a fixed point of the dynamic multimodal fusion process and modeling feature correlations in an adaptive and recursive manner, which naturally consolidates the three key ingredients for successful multimodal fusion. Our approach encodes and stabilizes rich information within and across modalities thoroughly from low level to high level and dynamically perceives modality importance for efficacious downstream multimodal learning, and is readily pluggable to various multimodal frameworks. Extensive experiments on four well-known multimodal benchmarks, namely, BRCA, MM-IMDB, CMU-MOSI, and VQA-v2, involving a vast variety of modalities, demonstrate the superiority and generalizability of our DEQ fusion. Remarkably, our DEQ fusion consistently achieves state-of-the-art performance on these benchmarks. The code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Y8DClN5ODu": {
    "title": "Demonstration Distillation for Efficient In-Context Learning",
    "volume": "review",
    "abstract": "In-context learning (ICL) substantially amplifies the predictive capability of large language models (LLMs), where the prompt typically contains a few question-answer pairs termed demonstrations, and a final question. Although lengthy and information-rich demonstrations can improve performance, they also inflate the computational burdens and financial costs, sometimes even breaching the context limit of LLMs. Existing solutions, such as prompt selection or context compression, frequently neglect the presence of superfluous information within these elongated prompts. To bridge the gap, this paper introduces demonstration distillation, a novel paradigm that targets excising the redundant content in the prompt without sacrificing ICL efficacy. We propose a distillation framework, Distillist-Generalist-Specialist (DGS), as an automated solution without additional model training. DGS iteratively refines the demonstration with the aid of three LLM-powered agents, eliminating superfluous information while maintaining valuable knowledge. Evaluations on three diverse datasets—GSM8K, BoolQ, and MultiRC—reveal the robustness and effectiveness of DGS. Particularly, DGS realizes $1.5-2$, $3-6$, and $1.5-3$ distillation ratios without compromising ICL performance on the three datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=70xhiS0AQS": {
    "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
    "volume": "review",
    "abstract": "Recently, the incredible progress of large language models (LLMs) has ignited the spark of task automation, which decomposes the complex tasks described by user instructions into sub-tasks, and invokes external tools to execute them, and plays a central role in autonomous agents. Therefore, there has been an urgent demand to formulate a systematic and standardized benchmark to foster the development of LLMs in task automation. To this end, we introduce TaskBench to evaluate task automation. Specifically, the process of task automation can be formulated as three critical stages (i.e., task decomposition, tool invocation, and parameter prediction) to fulfill user intent, that renders its data collection more challenging than common NLP tasks. Here, we introduce the concept of Tool Graph to represent the decomposed tasks in user intent, and adopt a back-instruct method to generate user instruction. Moreover, the mechanism of task automation also drives us to formulate more advanced metrics to measure the capability of LLMs. Therefore, we further propose TaskEval to evaluate the capability of LLMs in our curated datasets from different aspects, including task decomposition, tool invocation, and parameter prediction. Experimental results demonstrate that TaskBench can effectively be utilized to reflect the capability of LLMs in task automation. The code and datasets of TaskBench are available in the supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=B1VWS7ZRm6": {
    "title": "On Transferring Expert Knowledge from Tabular Data to Images",
    "volume": "review",
    "abstract": "Transferring knowledge across modalities has gained considerable attention in machine learning. Expert knowledge in fields like medicine is often represented in tabular form, and transferring this information can enhance the comprehensiveness and accuracy of image-based learning. Unlike general knowledge reuse scenarios, tabular data is divided into numerical and categorical variables, with each column having a unique semantic meaning. In addition, not all columns can be accurately represented in images, making it challenging to determine \"how to reuse\" and \"which subset to reuse\". To address this, we propose a novel method called CHannel tAbulaR alignment with optiMal tranSport (CHARMS) that automatically and effectively transfers relevant tabular knowledge. Specifically, by maximizing the mutual information between a group of channels and tabular features, our method modifies the visual embedding and captures the semantics of tabular knowledge. The alignment between channels and attributes helps select the subset of tabular data which contains knowledge to images. Experimental results demonstrate that CHARMS effectively reuses tabular knowledge to improve the performance and interpretability of visual classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=i2Phucne30": {
    "title": "On Bias-Variance Alignment in Deep Models",
    "volume": "review",
    "abstract": "Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \\emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \\emph{aligned} at a sample level, where squared bias is approximately \\emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDEWIMoiNK": {
    "title": "Mobile Object Rearrangement with Learned Localization Uncertainty",
    "volume": "review",
    "abstract": "Mobile object rearrangement (MOR) is a pivotal embodied AI task for a mobile agent to move objects to their target locations. While previous works rely on accurate pose information, we focus on scenarios where the agent needs to always localize both itself and the objects. This is challenging because accurate rearrangement depends on precise localization, yet localization in such a non-static environment is often disturbed by changes in the surroundings after rearrangement. To address this challenge, we first learn an effective representation for MOR only from sequential first-person view RGB images. It recurrently estimates agent and object poses, along with their associated uncertainties. With such uncertainty-aware localization as the input, we can then hierarchically train rearrangement policy networks for MOR. We develop and open source a simplified, yet challenging 3D MOR simulation environment to evaluate our method and relevant embodied AI baselines. Extensive comparisons reveal better performances of our method than baselines and the need for uncertainty estimation in our task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJ0Cfj4Ex9": {
    "title": "Learning Grounded Action Abstractions from Language",
    "volume": "review",
    "abstract": "Long-horizon planning is dauntingly hard -- it requires modeling relevant aspects of the environment and searching over large, complex action spaces. \\textit{Hierarchical planning} approaches make complex problems more tractable using temporal \\textit{action abstractions}, decomposing hard tasks into smaller abstract subproblems that can be solved modularly. However, actually learning useful action abstractions has long posed significant challenges without human expert knowledge. Here, we introduce a system that leverages background information in language to learn a \\textit{library of symbolic action abstractions and accompanying low-level policies} that can be composed to solve increasingly complex tasks. Our approach queries large language models (LLMs) as a prior for proposing useful symbolic action definitions, but integrates these proposals into a formal hierarchical planning system to ground and verify proposed actions. On two language-guided interactive planning domains (\\textit{Mini Minecraft} and the \\textit{ALFRED Household Tasks} benchmark), our approach far outperforms other baseline approaches that use LLMs in planning, enabling far more accurate planning and enable better generalization to more complex tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=BEH4mGo7zP": {
    "title": "Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning",
    "volume": "review",
    "abstract": "Proteins can be represented in various ways, including their sequences, 3D structures, and surfaces. While recent studies have successfully employed sequence- or structure-based representations to address multiple tasks in protein science, there has been significant oversight in incorporating protein surface information, a critical factor for protein function. In this paper, we present a pre-training strategy that incorporates information from protein sequences, 3D structures, and surfaces to improve protein representation learning. Specifically, we utilize Implicit Neural Representations (INRs) for learning surface characteristics, and name it ProteinINR. We confirm that ProteinINR successfully reconstructs protein surfaces, and integrate this surface learning into the existing pre-training strategy of sequences and structures. Our results demonstrate that our approach can enhance performance in various downstream tasks, thereby underscoring the importance of including surface attributes in protein representation learning. These findings underline the importance of understanding protein surfaces for generating effective protein representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ta2ctBXj1J": {
    "title": "CityGPT: Generative Transformer for City Layout of Arbitrary Building Shape",
    "volume": "review",
    "abstract": "City layout generation has gained substantial attention in the research community with applications in urban planning and gaming. We introduce CityGPT, the generative pre-trained transformers for modeling city layout distributions from large-scale layout datasets without requiring priors like satellite images, road networks, or layout graphs. Inspired by masked autoencoders (MAE), our key idea is to decompose this model into two conditional ones: first a distribution of buildings' center positions conditioned on unmasked layouts, and then a distribution of masked layouts conditioned on their sampled center positions and unmasked layouts. These two conditional models are learned sequentially as two transformer-based masked autoencoders. Moreover, by adding an autoregressive polygon model after the second autoencoder, CityGPT can generate city layouts with arbitrary building footprint shapes instead of boxes or predefined shape sets. CityGPT exhibits strong performance gains over baseline methods and supports a diverse range of generation tasks, including 2.5D city generation, city completion, infinite city generation, and conditional layout generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDxGthJkSD": {
    "title": "Hybrid Classification-Regression Adaptive Loss for Dense Object Detection",
    "volume": "review",
    "abstract": "For object detection detectors, enhancing model performance hinges on the ability to simultaneously consider inconsistencies across tasks and focus on difficult-to-train samples. Achieving this necessitates incorporating information from both the classification and regression tasks. However, prior work tends to either emphasize difficult-to-train samples within their respective tasks or simply compute classification scores with IoU, often leading to suboptimal model performance. In this paper, we propose a Hybrid Classification-Regression Adaptive Loss, termed as HCRAL. Specifically, we introduce the Residual of Classification and IoU (RCI) module for cross-task supervision, addressing task inconsistencies, and the Conditioning Factor (CF) to focus on difficult-to-train samples within each task. Furthermore, we introduce a new strategy named Expanded Adaptive Training Sample Selection (EATSS) to provide additional samples that exhibit classification and regression inconsistencies. To validate the effectiveness of the proposed method, we conduct extensive experiments on COCO test-dev. Experimental evaluations demonstrate the superiority of our approachs. Additionally, we designed experiments by separately combining the classification and regression loss with regular loss functions in popular one-stage models, demonstrating improved performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=viC3cpWFTN": {
    "title": "Clip21: Error Feedback for Gradient Clipping",
    "volume": "review",
    "abstract": "Motivated by the increasing importance of deep neural network training, we study distributed gradient methods with gradient clipping, i.e., clipping applied to the gradients computed from local information at the nodes. While gradient clipping enforces the convergence of gradient-based methods that minimize rapidly growing functions, it also induces bias which causes serious convergence issues specific to the distributed setting. Inspired by recent progress in the error-feedback literature which is focused on taming the bias/error introduced by communication compression operators such as Top-$k$ (Richtárik et al, 2021), and mathematical similarities between the clipping operator and contractive compression operators, we design Clip21 -- the first provably effective and practically useful error feedback mechanism for distributed methods with gradient clipping. We prove that our method converges at the same $\\mathcal{O}({1}/{K})$ rate as distributed gradient descent in the smooth nonconvex regime, which improves the previous best $\\mathcal{O}({1}/{\\sqrt{K}})$ rate which was obtained under significantly stronger assumptions. Our method converges significantly faster in practice than competing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=qH8ADnIVII": {
    "title": "Dynamic Demonstrations Controller for In-Context Learning",
    "volume": "review",
    "abstract": "In-Context Learning (ICL) is a new paradigm for natural language processing (NLP), where a large language model (LLM) observes a small number of demonstrations and a test instance as its input, and directly makes predictions without updating model parameters. Previous studies have revealed that ICL is sensitive to the selection and the order of demonstrations. However, there are few studies regarding the impact of the demonstration number on the ICL performance within a limited input length of LLM, because it is commonly believed that the number of demonstrations is positively correlated with model performance. In this paper, we find this conclusion does not always hold true. Through pilot experiments, we discover that increasing the number of demonstrations does not necessarily lead to improved performance. Building upon this insight, we propose a $\\textbf{D}$ynamic $\\textbf{D}$emonstrations $\\textbf{Controller}$ $({\\textbf{D$^2$Controller}})$, which can improve the ICL performance by adjusting the number of demonstrations dynamically. The experimental results show that D$^2$Controller yields a 5.4\\% relative improvement on eight different sizes of LLMs across ten datasets. Moreover, we also extend our method to previous ICL models and achieve competitive results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=H396R79GiQ": {
    "title": "A unique M-pattern for micro-expreesion spotting in long videos",
    "volume": "review",
    "abstract": "Micro-expression spotting (MES) is challenging since the small magnitude of micro-expression (ME) makes them susceptible to global movements like head rotation. However, the unique movement pattern and inherent characteristics of ME allow them to be distinguished from other movements. Existing MES methods based on fixed reference frame degrade optical flow accuracy and are overly dependent on facial alignment. In this paper, we propose a skip-$k$-frame block-wise main directional mean optical flow (MDMO) feature for MES based on unfixed reference frame. By employing skip-$k$-frame strategy, we substantiate the existence of a distinct and exclusive movement pattern in ME, called M-pattern due to its feature curve resembling the letter `M'. Based on M-pattern and characteristics of ME, we then provide a novel spotting rules to precisely locate ME intervals. Block-wise MDMO feature is capable of removing global movements without compromising complete ME movements in the early feature extraction stage. Besides, A novel pixelmatch-based facial alignment algorithm with dynamic update of reference frame is proposed to better align facial images and reduce jitter between frames. Experimental results on CAS(ME)$^2$, SAMM-LV and CASME II validate the proposed methods are superior to the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=atQqW27RMQ": {
    "title": "GENIU: A Restricted Data Access Unlearning for Imbalanced Data",
    "volume": "review",
    "abstract": "With the increasing emphasis on data privacy, the significance of machine unlearning has grown substantially. Class unlearning, which involves enabling a trained model to forget data belonging to a specific class learned before, is important as classification tasks account for the majority of today's machine learning as a service (MLaaS). Retraining the model on the original data, excluding the data to be forgotten (also known as forgetting data), is a common approach to class unlearning. However, the availability of original data during the unlearning phase is not always guaranteed, leading to the exploration of class unlearning with restricted data access, which has attracted considerable attention. While current unlearning methods with restricted data access usually generate proxy sample via the trained neural network classifier, they typically focus on training and forgetting balanced data. However, the imbalanced original data can cause trouble for these proxies and unlearning, particularly when the forgetting data consists predominantly of the majority class. To address this issue, we propose the GENerative Imbalanced Unlearning (GENIU) framework. GENIU utilizes a Variational Autoencoder (VAE) to concurrently train a proxy generator alongside the original model. These generated proxies accurately represent each class and are leveraged in the unlearning phase, eliminating the reliance on the original training data. To further mitigate the performance degradation resulting from forgetting the majority class, we introduce an ``in-batch tuning'' strategy which works with the generated proxies. GENIU is the first practical framework for class unlearning in imbalanced data settings and restricted data access, ensuring the preservation of essential information for future unlearning. Experimental results confirm the superiority of GENIU over existing methods, establishing its effectiveness in empirical scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=k2lkeCCfRK": {
    "title": "GFLOWNET TRAINING BY POLICY GRADIENTS",
    "volume": "review",
    "abstract": "Generative Flow Networks (GFlowNets) have been shown with an attractive capability to generate combinatorial objects with desired properties. In this paper, we propose a policy-dependent reward that bridges the flow balance in GFlowNet training to optimizing the expected accumulated reward in traditional Reinforcement-Learning (RL). This allows us to derive policy-based GFlowNet training strategies. It is known that the training efficiency is affected by the design of backward policies in GFlowNets. We propose a coupled training strategy that can jointly solve the GFlowNet training and backward policy design. Performance analysis is provided with a theoretical guarantee of our proposed methods. We further conduct experiments on both simulated and real-world datasets to verify that our policy-based strategy outperforms the existing GFlowNet training strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=SirD4KYNRr": {
    "title": "Invariant Attention: Provable Clustering Under Transformations",
    "volume": "review",
    "abstract": "Attention mechanisms play a crucial role in state-of-the-art vision architectures, enabling them to rapidly identify relationships between distant image patches. Conventional attention mechanisms do not incorporate other structural properties of images, such as invariance to geometric transformations, instead learning these properties from data. In this paper, we introduce a novel mechanism, Invariant Attention, which, like standard attention, captures image similarity, but with the additional guarantee of being agnostic to geometric transformations. We provide theoretical assurance and empirical verification that invariant attention is far more successful than standard kernel attention on multi-class, transformed vision data, and illustrate its potential to correctly cluster transformed data with intra-class variation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=KUNzEQMWU7": {
    "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
    "volume": "review",
    "abstract": "Although Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive skills in various domains, their ability for mathematical reasoning within visual contexts has not been formally examined. Equipping LLMs and LMMs with this capability is vital for general-purpose AI assistants and showcases promising potential in education, data analysis, and scientific discovery. To bridge this gap, we present MathVista, a benchmark designed to amalgamate challenges from diverse mathematical and visual tasks. We first taxonomize the key task types, reasoning skills, and visual contexts from the literature to guide our selection from 28 existing math-focused and visual question answering datasets. Then, we construct three new datasets, IQTest, FunctionQA, and PaperQA, to accommodate for missing types of visual contexts. The problems featured often require deep visual understanding beyond OCR or image captioning, and compositional reasoning with rich domain-specific tools, thus posing a notable challenge to existing models. We conduct a comprehensive evaluation of 11 prominent open-source and proprietary foundation models (LLMs, LLMs augmented with tools, and LMMs). The best-performing model, Multimodal Bard, achieves only 58\\% of human performance (34.8\\% vs 60.3\\%), indicating ample room for further improvement. Given this significant gap, MathVista fuels future research in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=tj4a1JY03u": {
    "title": "Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
    "volume": "review",
    "abstract": "Instruction tuning enhances the capability of Large Language Models (LLMs) to interact with humans. Furthermore, recent instruction-following datasets include images as visual input, collecting responses for image-based instructions. However, current visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first used publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Furthermore, we prompt text-only GPT-4 with recognized text and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multimodal instruction-following data, our model, LLaVAR, substantially improves the capability of the LLaVA model on text-based VQA datasets (up to 20\\% accuracy improvement). The GPT-4-based instruction-following evaluation also demonstrates the improvement of our model on both natural images and text-rich images. Through qualitative analysis, LLaVAR shows promising interaction skills (e.g., reasoning, writing, and elaboration) with humans based on the latest real-world online content that combines text and images. We make our code/data/models publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=YcM6ofShwY": {
    "title": "BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference",
    "volume": "review",
    "abstract": "Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=kxpswbhr1r": {
    "title": "In-context Convergence of Transformers",
    "volume": "review",
    "abstract": "Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on $\\textit{linear}$ transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with $\\textit{softmax}$ attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3cE6NKYy8x": {
    "title": "Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation",
    "volume": "review",
    "abstract": "The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while ensuring fairness and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. Fairness in graphs is particularly crucial in anomaly detection areas such as misinformation detection, where decision outcomes can significantly affect individuals. Despite this need, existing works lack realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes for research in FairGAD. To bridge this gap, we present two novel graph datasets constructed from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400 thousand edges associated with 9 thousand and 47 thousand nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels. We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used by the research community. These new datasets offer significant values for FairGAD by providing realistic data that captures the intricacies of social networks. Using our datasets, we investigate the performance-fairness trade-off in three existing GAD methods on five state-of-the-art fairness methods, which sheds light on their effectiveness and limitations in addressing the FairGAD problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=J9wzKfgZVK": {
    "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization",
    "volume": "review",
    "abstract": "In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned within language models? (b) What are suitable performance metrics to evaluate ICL accurately and what are the error rates? (c) How does the transformer architecture enable ICL? To answer (a), we take a Bayesian view and demonstrate that ICL implicitly implements the Bayesian model averaging algorithm. This Bayesian model averaging algorithm is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a regret bound $\\mathcal{O}(1/T)$, where $T$ is the ICL input sequence length. To address (c), in addition to the encoded Bayesian model averaging algorithm in attention, we show that during pertaining, the total variation distance between the learned model and the nominal model is bounded by a sum of an approximation error and a generalization error of $\\tilde{\\mathcal{O}}(1/\\sqrt{N_{\\mathrm{p}}T_{\\mathrm{p}}})$, where $N_{\\mathrm{p}}$ and $T_{\\mathrm{p}}$ are the number of token sequences and the length of each sequence in pretraining, respectively. Our results provide a unified understanding of the transformer and its ICL ability with bounds on ICL regret, approximation, and generalization, which deepens our knowledge of these essential aspects of modern language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=thbtoAkCe9": {
    "title": "$\\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning",
    "volume": "review",
    "abstract": "In recent years, data quality has emerged as an important factor for training massive models. Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing *data diversity* in the coreset, and (2) functions that assign *difficulty scores* to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. In this work, we represent a dataset as an undirected graph and propose a novel pruning algorithm, $\\mathbb{D}^2$ Pruning, that uses message passing over this dataset graph for coreset selection. $\\mathbb{D}^2$ Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and NLP datasets. Results show that $\\mathbb{D}^2$ Pruning improves coreset selection over previous state-of-the-art methods at low-to-medium pruning rates. Additionally, we find that using $\\mathbb{D}^2$ Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models. Our work shows that $\\mathbb{D}^2$ Pruning is a versatile framework for understanding and processing datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=tVTN7Zs0ml": {
    "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
    "volume": "review",
    "abstract": "Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6% and 6.6% for mortality and readmission, and F1-score by 7.9% and 10.8% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=i5da6iedW8": {
    "title": "FedBiOT: a solution for federated large language model fine-tuning with intellectual property protection",
    "volume": "review",
    "abstract": "Due to data and information privacy concerns, data owners are not willing to share the data with others, but each of them may not have sufficient data to fine-tune a satisfactory large language model (LLM) individually. Parallelly, the LLM owners may not be willing to disclose the LLMs' details, including their architectures and parameters. Therefore, this leads to the challenge of fine-tuning an LLM on a federated learning task where the clients with task-specific data cannot obtain the complete LLM. To solve the challenge, this paper introduces FedBiOT, a method that guarantees the clients' data privacy and avoids the disclosure of an LLM. Specifically, we formulate and solve a bi-level optimization problem to ensure that the emulator distilled on the public dataset by the LLM owner can help the adaptors' local fine-tuning on clients' private datasets, regardless of the distribution drift between those datasets. Different clients' adapters are synchronized in a federated learning style, and the full model composed with the final derived adapter can achieve better performance on downstream tasks. We conduct extensive experiments on LLaMA-7B training for various federated learning tasks and witness significant improvements over existing baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=s5hSp7EdL3": {
    "title": "The Human-AI Substitution game: active learning from a strategic labeler",
    "volume": "review",
    "abstract": "The standard active learning setting assumes a willing labeler, who provides labels on informative examples to speed up learning. However, if the labeler wishes to be compensated for as many labels as possible before learning finishes, the labeler may benefit from actually slowing down learning. This incentive arises for instance if the labeler is to be replaced by the ML model, once it is learned. In this paper, we initiate the study of learning from a strategic labeler, who selectively abstains from labeling to slow down learning. We first prove that strategic abstention can prolong learning, and propose novel complexity measures to analyze the query cost of the learning game. Next, we develop a near-optimal deterministic algorithm, prove its robustness to strategic labeling, and contrast it with other active learning algorithms. We also provide extensions that encompass other learning setups/goals. Finally, we characterize the query cost of multi-task active learning, with and without abstention. Our first exploration of strategic labeling aims to add to our theoretical understanding of the imitative nature of ML in human-AI interaction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UfBIxpTK10": {
    "title": "The Discovery of Binding Modes Requires Rethinking Docking Generalization",
    "volume": "review",
    "abstract": "Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, it is critical that docking methods generalize well across the proteome. However, existing benchmarks fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that machine learning-based docking models have very weak generalization abilities even when combined with various data augmentation strategies. Instead, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between a diffusion and a confidence model. Unlike previous self-training methods from other domains, we directly exploit the multi-resolution generation process of diffusion models using rollouts and confidence scores to reduce the generalization gap. We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqVgBbNCm9": {
    "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding",
    "volume": "review",
    "abstract": "This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and further underscores the potential of pushing LLMs to think more like a human for answer quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=YGTSLDAPqb": {
    "title": "Connect Later: Improving Fine-Tuning for Robustness with Targeted Augmentations",
    "volume": "review",
    "abstract": "Models trained on a labeled source domain (e.g., bright, nearby astronomical objects) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., faint, distant objects). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over just supervised learning on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations to learn good representations within the source and target domains, fine-tune with targeted augmentations designed with knowledge of the distribution shift to better connect the domains. Connect Later improves average OOD error over standard fine-tuning and supervised learning with targeted augmentations on 3 real-world datasets: astronomical time-series classification (AstroClassification) by 12%, redshift prediction for astronomical time-series (Redshifts) by 0.03 RMSE (11% relative), and wildlife species identification (iWildCam-WILDS) by 0.9%, achieving the state-of-the-art on AstroClassification and on iWildCam-WILDS with ResNet-50",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=gEUN4FCCrS": {
    "title": "Value Bonuses using Ensemble Errors for Exploration in Reinforcement Learning",
    "volume": "review",
    "abstract": "Optimistic value estimates provide one mechanism for directed exploration in reinforcement learning (RL). The agent acts greedily with respect to an estimate of the value plus what can be seen as a \\emph{value bonus}. The value bonus can be learned by estimating a value function on \\emph{reward bonuses}, propagating local uncertainties around rewards. This approach, however, only increases the value bonus for an action retroactively, after seeing a higher reward bonus from that state and action. Such an approach does not encourage the agent to visit a state and action for the first time. In this work, we introduce an algorithm for exploration called Value Bonuses with Ensemble errors (VBE), that maintains an ensemble of random action-value functions (RQFs). VBE uses the errors in the estimation of these RQFs for designing value bonuses that provide first-visit optimism and deep exploration. The key idea is to design the rewards for these RQFs in such a way that the value bonus can decrease to zero. We show that VBE outperforms Bootstrap DQN and two reward bonus approaches (RND and ACB) on several classic environments used to test exploration and provide demonstrative experiments that it learns faster in several Atari environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=gzqrANCF4g": {
    "title": "Language Model Beats Diffusion - Tokenizer is key to visual generation",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce \\modelname{}, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3oTPsORaDH": {
    "title": "Improving Generalization in Equivariant Graph Neural Networks with Physical Inductive Biases",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. To incorporate these inductive biases, we propose the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically, we show how the second-order continuity can be incorporated into GNNs while maintaining the equivariant property. Furthermore, we offer theoretical insights into SEGNO, highlighting that it can learn a unique trajectory between adjacent states, which is crucial for model generalization. Additionally, we prove that the discrepancy between this learned trajectory of SEGNO and the true trajectory is bounded. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that our model yields a significant improvement over the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=N2WchST43h": {
    "title": "A Sublinear Adversarial Training Algorithm",
    "volume": "review",
    "abstract": "Adversarial training is a widely used strategy for making neural networks resistant to adversarial perturbations. For a neural network of width $m$, $n$ input training data in $d$ dimension, it takes $\\Omega(mnd)$ time cost per training iteration for the forward and backward computation. In this paper we analyze the convergence guarantee of adversarial training procedure on a two-layer neural network with shifted ReLU activation, and shows that only $o(m)$ neurons will be activated for each input data per iteration. Furthermore, we develop an algorithm for adversarial training with time cost $o(m n d)$ per iteration by applying half-space reporting data structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=7gLfQT52Nn": {
    "title": "Proper Laplacian Representation Learning",
    "volume": "review",
    "abstract": "The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems, by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning techniques. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees of our method and we also show that those results translate empirically into robust learning across multiple environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vSBB2nRaoj": {
    "title": "Bi-Directional Goal-Conditioning on Single Policy Function for State Space Search",
    "volume": "review",
    "abstract": "State space search problems have a binary (found/not found) reward system. However, in the real world, these problems often have a vast number of states compared to only a limited number of goal states. This makes the rewards very sparse for the search task. On the other hand, Goal-Conditioned Reinforcement Learning (GCRL) can be used to train an agent to solve multiple related tasks. In our work, we assume the ability to sample goal states and use the same to define a forward task (τ ∗) and a reverse task (τ inv) derived from the original state space search task to ensure more useful and learnable samples. We adopt the Universal Value Function Approximator (UVFA) setting with a GCRL agent to learn from these samples. We incorporate hindsight relabelling with goal-conditioning in the forward task to reach goals sampled from τ ∗, and similarly define ‘Foresight' for the backward task. We also use the agent's ability (from the policy function) to reach intermediate states and use these states as goals for new sub-tasks. Further, to tackle the problem of reverse transitions from the backward trajectories, we spawn new instances of the agent from states in these trajectories to collect forward transitions which are then used to train for the main task τ ∗. We consolidate these tasks and sample generation strategies into a three-part system called Scrambler-Resolver-Explorer (SRE). We also propose the ‘SRE-DQN' agent that combines our exploration module with the popular DQN algorithm. Finally, we demonstrate the advantages of bi-directional goal-conditioning and knowledge of the goal state by evaluating our framework on classical goal-reaching tasks, and comparing with existing concepts extended to our bi-directional setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=poFAoivHQk": {
    "title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
    "volume": "review",
    "abstract": "Transformers, renowned for their self-attention mechanism, have achieved the state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Trg9qb0d5U": {
    "title": "Fantastic DNN-Classifier Identification without Testing Dataset",
    "volume": "review",
    "abstract": "Deep Neural Networks (DNNs) are trained, validated, and tested with an example dataset. For a given example dataset, several models for different architectures are trained and then using the validation dataset a model is selected. If the models have hyper-parameters, their good values are selected using validation datasets as well. Finally, performance of the selected DNN is tested using a test dataset. This testing method treats the DNN as a black-box and doesn't attempt to understand its characteristics. On the other hand, many theoretical and empirical studies have used complexity measures for estimating generalization phenomena using the training dataset, with rare exceptions. To the best of our knowledge, no method exists to estimate test accuracy (not generalization) without any testing dataset. We propose a method for estimating test accuracy of a given DNN without any test dataset. Assuming that a DNN is the composition of a feature extractor and a classifier, we propose and evaluate a method for estimating their qualities. The first step of the proposed method is generation of one (input) prototype vector for each class. Then using these seed prototypes, (k − 1) core prototypes are generated for each class. These prototypes are our data for evaluating the qualities of the feature extractor and classifier as well as estimating test accuracy of the given DNN. We have empirically evaluated the proposed method for DNNs trained with CIFAR10, and CIFAR100",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=OeQE9zsztS": {
    "title": "Spectrally Transformed Kernel Regression",
    "volume": "review",
    "abstract": "Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose on the predictor a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work establishes a unified theory for learning with unlabeled data and a base kernel, by revisiting the classical idea of spectrally transformed kernel regression (STKR). First, by characterizing a universal type of \"target smoothness\", we show that any sufficiently smooth target function can be learned by STKR, so the theory in this work is valid for a broad class of methods, including various semi-supervised, self-supervised and representation learning algorithms. Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting. Third, we derive (near-)tight statistical guarantees for STKR in two scenarios: STKR with a known analytic transformation, and STKR with kernel PCA when the transformation is unknown. Overall, we believe that this work helps deepen our understanding of how to work with unlabeled data, and its generality and broad scope make it easier to inspire new methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=vJGKYWC8j8": {
    "title": "Continual Traffic Forecasting via Mixture of Experts",
    "volume": "review",
    "abstract": "The real-world traffic networks undergo expansion through the installation of new sensors, implying that the traffic patterns continually evolve over time. Incrementally training a model on the newly added sensors would make the model forget the past knowledge, i.e., catastrophic forgetting, while retraining the model on the entire network to capture these changes is highly inefficient. To address these challenges, we propose a novel Traffic Forecasting Mixture of Experts (\\proposed) for traffic forecasting under evolving networks. The main idea is to segment the traffic flow into multiple homogeneous groups, and assign an expert model responsible for a specific group. This allows each expert model to concentrate on learning and adapting to a specific set of patterns, while minimizing interference between the experts during training, thereby preventing the dilution or replacement of prior knowledge, which is a major cause of catastrophic forgetting. Through extensive experiments on a real-world long-term streaming network dataset, PEMSD3-Stream, we demonstrate the effectiveness and efficiency of~\\proposed. Our results showcase superior performance and resilience in the face of catastrophic forgetting, underscoring the effectiveness of our approach in dealing with continual learning for traffic flow forecasting in long-term streaming networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xw29VvOMmU": {
    "title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
    "volume": "review",
    "abstract": "We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component, which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Across standard NLP benchmarks, our low-rank plus quantized matrix decomposition approach (LQ-LoRA) is found to perform well against strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MNwXif6AWA": {
    "title": "Periodic Set Transformer: Material Property Prediction from Continuous Isometry Invariants",
    "volume": "review",
    "abstract": "Material or crystal property prediction using machine learning has grown popular in recent years as it provides an accurate and computationally efficient replacement to classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a fixed number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous isometry invariant for periodic point sets, as a representation for our learning algorithm. While the PDD is effective in distinguishing periodic point sets up to isometry, there is no consideration for the composition of the underlying material. We develop a transformer model with a modified self-attention mechanism that can utilize the PDD and incorporate compositional information via a spatial encoding method. This model is tested thoroughly with and without the use of compositional information on a variety of crystal datasets including the commonly used crystals of the Materials Project",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=YxvmODVWny": {
    "title": "RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches",
    "volume": "review",
    "abstract": "Natural language and images are commonly used as goal representations in goal-conditioned imitation learning (IL). However, natural language can be ambiguous and images can be over-specified. In this work, we propose hand-drawn sketches as a modality for goal specification in visual imitation learning. Sketches are easy for users to provide on the fly like language, but similar to images they can also help a downstream policy to be spatially-aware and even go beyond images to disambiguate task-relevant from task-irrelevant objects. We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions. We train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches. We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop. Experimentally we find that RT-Sketch is able to perform on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present. Additionally, we show that RT-Sketch has the capacity to interpret and act upon sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings. For supplementary material and videos, please refer to our website: http://rt-sketch-anon.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ViNe1fjGME": {
    "title": "Deep Temporal Graph Clustering",
    "volume": "review",
    "abstract": "Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the proposed framework TGC, we conduct extensive experiments. The experimental results show that temporal graph clustering enables more flexibility in finding a balance between time and space requirements, and our framework can effectively improve the performance of existing temporal graph learning methods. Our code is included in the supplementary material and will be released after publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=rOpK0ToM3o": {
    "title": "V-Former: Offline RL with Temporally-Extended Actions",
    "volume": "review",
    "abstract": "In this paper, we propose an offline reinforcement learning (RL) method that learns to take temporally extended actions, can handle narrow data distributions such as those produced by mixtures of multi-task demonstrations, and can train on data with different control frequencies. This combination of properties makes our proposed method especially well-suited for robotic offline RL, where datasets might consist of (narrow) demonstration data mixed with (broader) suboptimal data, and control frequencies can present a particularly significant challenge. We derive our method starting from a continuous time formulation of RL, and show that offline RL with temporally extended \"action chunks\" can be performed efficiently by extending the implicit Q-learning (IQL) approach, in combination with expressive Transformer-based policies for representing temporally extended open-loop action sequences. Our experiments show that our method both improves over prior approaches on simulated robotic demonstration data and outperforms prior works that aim to learn from data at multiple frequencies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=MEztAJjcYZ": {
    "title": "Enhancing Clinical Note Summarization: Iterative Reflexions with Small-model Supervision and Error2Correct Demonstrations",
    "volume": "review",
    "abstract": "Generating clinical notes from doctor-patient dialogues is an important task in medical artificial intelligence. Mainstream methods currently employ large language models with few-shot demonstrations to tackle this challenge. However, the absence of domain knowledge supervision in these models often results in issues like missing key information, irregular writing standards, and non-compliant language styles. To this end, in this paper, we propose a novel iterative reflexion framework with small-model supervision and Error2Correct demonstrations for clinical note summarization. In this framework, we leverage a large model to produce clinical notes and design a small model trained on domain-specific data to evaluate the generated content. To enhance the quality of the generated clinical notes, we further propose Error2Correct demonstrations, which consist of error examples, error analysis, and corresponding correct examples, to help the large model identify and rectify errors effectively. To evaluate the effectiveness of our proposed method, we conduct extensive experiments on both Chinese and English datasets. The results demonstrate that our method achieves state-of-the-art performance on both datasets for the clinical note summarization task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=PHGxChm1l5": {
    "title": "Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
    "volume": "review",
    "abstract": "A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make \"infinite use of finite means\". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their ``bag-of-words\" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose Compositional VLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=cHy00K3Och": {
    "title": "GRADSIMCORE: GRADIENT SIMILARITY BASED REPRESENTATIVE INSTANCES AS CORESET",
    "volume": "review",
    "abstract": "The rise in size and complexity of modern datasets and deep learning models have resulted in the usage of extensive computational resources and a rise in training time and effort. It also has increased the carbon footprint of training and fine-tuning models. One way to reduce the computational requirement is to extract the most representative subset (referred to as $\\textit{coreset}$) that can substitute for the larger dataset. Coresets can thus replace huge datasets to train models and tune hyperparameters, especially in the early stages of training. This will result in a significant reduction of computational resource requirement and reduce carbon footprint. We propose a simple and novel framework based on the similarity of loss gradients for identifying the representative training instances as a coreset. Our method, dubbed as $\\textit{GradSimCore}$, outperforms the state-of-the-art coreset selection algorithms on popular benchmark datasets ranging from MNIST to ImageNet. Because of its simplicity and effectiveness, our method is an important baseline for evaluating the effectiveness of the coreset selection algorithms. Anonymized codes for the proposed baseline are provided at https://anonymous.4open.science/r/GradSimCore-8884",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oOGqJ6Z1sA": {
    "title": "Treatment Effects Estimation By Uniform Transformer",
    "volume": "review",
    "abstract": "In observational studies, balancing covariates in different treatment groups is essential to estimate treatment effects. One of the most commonly used methods for such purposes is weighting. The performance of this class of methods usually depends on strong regularity conditions for the underlying model, which might not hold in practice. In this paper, we investigate weighting methods from a functional estimation perspective and argue that the weights needed for covariate balancing could differ from those needed for treatment effects estimation under low regularity conditions. Motivated by this observation, we introduce a new framework of weighting that directly targets the treatment effects estimation. Unlike existing methods, the resulting estimator for a treatment effect under this new framework is a simple kernel-based $U$-statistic after applying a data-driven transformation to the observed covariates. We characterize the theoretical properties of the new estimators of treatment effects under a nonparametric setting and show that they are able to work robustly under low regularity conditions. The new framework is also applied to several numerical examples to demonstrate its practical merits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=RDSj6S8WJe": {
    "title": "Demystifying Linear MDPs and Novel Dynamics Aggregation Framework",
    "volume": "review",
    "abstract": "In this paper, we first challenge the common premise that linear MDPs always induce performance guarantees independent of the state space. We prove that, in linear MDPs, the feature dimension $d$ is lower bounded by $S/U$ in order to aptly represent transition probabilities, where $S$ is the size of the state space and $U$ is the maximum size of directly reachable states. Hence, $d$ can still scale with $S$ depending on the direct reachability of the environment. To address this limitation of linear MDPs, we propose a novel structural aggregation framework based on dynamics, named as the *dynamics aggregation*. For this newly proposed framework, we design a provably efficient hierarchical reinforcement learning algorithm in linear function approximation that leverages aggregated sub-structures. Our proposed algorithm exhibits statistical efficiency, achieving a regret of $\\tilde{O} \\big( d_{\\psi}^{3/2} H^{3/2}\\sqrt{ NT} \\big)$, where $d_{\\psi}$ represents the feature dimension of *aggregated subMDPs* and $N$ signifies the number of aggregated subMDPs. We establish that the condition $d_{\\psi}^3 N \\ll d^{3}$ is readily met in most real-world environments with hierarchical structures, enabling a substantial improvement in the regret bound compared to LSVI-UCB, which enjoys a regret of $\\tilde{O}(d^{3/2} H^{3/2} \\sqrt{ T})$. To the best of our knowledge, this work presents the first HRL algorithm with linear function approximation that offers provable guarantees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=rDIqMB4mMg": {
    "title": "PostRainBench: A Comprehensive Benchmark and A New Model for Precipitation Forecasting",
    "volume": "review",
    "abstract": "Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention Enhanced Multi-task Learning framework with a specially designed weighted loss function. Its flexible design allows for easy plug-and-play integration with various backbones. Extensive experimental results on the proposed benchmark show that our method outperforms state-of-the-art methods by 6.3\\%, 4.7\\%, and 26.8\\% in rain CSI on the three datasets respectively. Most notably, our model is the first deep learning-based method to outperform traditional Numerical Weather Prediction (NWP) approaches in extreme precipitation conditions. It shows improvements of 15.6\\%, 17.4\\%, and 31.8\\% over NWP predictions in heavy rain CSI on respective datasets. These results highlight the potential impact of our model in reducing the severe consequences of extreme weather events",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mjDROBU93g": {
    "title": "DISTA: DENOISING SPIKING TRANSFORMER WITH INTRINSIC PLASTICITY AND SPATIOTEMPORAL ATTENTION",
    "volume": "review",
    "abstract": "Among the array of neural network architectures, the Vision Transformer (ViT) stands out as a prominent choice, acclaimed for its exceptional expressiveness and consistent high performance in various vision applications. Recently, the emerging Spiking ViT approach has endeavored to harness spiking neurons, paving the way for a more brain-inspired transformer architecture that thrives in ultra-low power operations on dedicated neuromorphic hardware. Nevertheless, this approach remains confined to spatial self-attention and doesn't fully unlock the potential of spiking neural networks. We introduce DISTA, a Denoising Spiking Transformer with Intrinsic Plasticity and SpatioTemporal Attention, designed to maximize the spatiotemporal computational prowess of spiking neurons, particularly for vision applications. DISTA explores two types of spatiotemporal attentions: intrinsic neuron-level attention and network-level attention with explicit memory. Additionally, DISTA incorporates an efficient nonlinear denoising mechanism to quell the noise inherent in computed spatiotemporal attention maps, thereby resulting in further performance gains. Our DISTA transformer undergoes joint training involving synaptic plasticity (i.e., weight tuning) and intrinsic plasticity (i.e., membrane time constant tuning) and delivers state-of-the-art performances across several static image and dynamic neuromorphic datasets. With only 6 time steps, DISTA achieves remarkable top-1 accuracy on CIFAR10 (96.26\\%) and CIFAR100 (79.15\\%), as well as 79.1\\% on CIFAR10-DVS using 10 time steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=W2HJKGnb5y": {
    "title": "POPULATION DESCENT: A NATURAL-SELECTION BASED HYPER-PARAMETER TUNING FRAMEWORK",
    "volume": "review",
    "abstract": "First-order gradient descent has been the base of the most successful optimization algorithms ever implemented. On supervised learning problems with very high dimensionality, such as neural network optimization, it is almost always the algorithm of choice, mainly due to its memory and computational efficiency. However, it is a classical result in optimization that gradient descent converges to local minima on non-convex functions. Even more importantly, in certain high-dimensional cases, escaping the plateaus of large saddle points becomes intractable. On the other hand, black-box optimization methods are not sensitive to the local structure of a loss function's landscape but suffer the curse of dimensionality. Instead, memetic algorithms aim to combine the benefits of both. Inspired by this, we present Population Descent, a memetic algorithm focused on hyperparameter optimization. We show that an adaptive $m$-elitist selection approach combined with a normalized-fitness-based randomization scheme outperforms more complex state-of-the-art algorithms by up to 13\\% on common benchmark tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=kJ0qp9Xdsh": {
    "title": "Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints",
    "volume": "review",
    "abstract": "Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (*e.g.*, document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models. In this work, we propose the **LA**yout **C**onstraint diffusion mod**E**l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of continuous aesthetic constraint functions in training more naturally. For conditional generation, we propose injecting layout conditions in the form of masks or gradient guidance during inference. Empirical results show that LACE produces high-quality layouts and outperforms existing state-of-the-art baselines. We will release our source code and model checkpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=L76lvHZqeS": {
    "title": "A Unified Framework of Theoretically Robust Contrastive Loss against Label Noise",
    "volume": "review",
    "abstract": "Learning from noisy labels is a critical challenge in machine learning, with vast implications for numerous real-world scenarios. While supervised contrastive learning has recently emerged as a powerful tool for navigating label noise, many existing solutions remain heuristic, often devoid of a systematic theoretical foundation for crafting robust supervised contrastive losses. To address the gap, in this paper, we propose a unified theoretical framework for robust losses under the pairwise contrastive paradigm. In particular, we for the first time derive a general robust condition for arbitrary contrastive losses, which serves as a criterion to verify the theoretical robustness of a supervised contrastive loss against label noise. This framework is not only holistic -- encompassing prior techniques such as nearest-neighbor (NN) sample selection and robust contrastive loss -- but also instrumental, guiding us to develop a robust version of the popular InfoNCE loss, termed Symmetric InfoNCE (SymNCE). Extensive experiments on benchmark datasets demonstrate the superiority of SymNCE against label noise",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=u4FiXrH09F": {
    "title": "Implicit Neural Network on Dynamic Graphs",
    "volume": "review",
    "abstract": "Recent works have demonstrated that graph convolution neural networks fail either to capture long-range dependencies in the network or suffer from over-smoothing issues. Several recent works have proposed implicit graph neural networks to remedy the issues. However, despite these issues being magnified in dynamic graphs, where the feature aggregation occurs through both the graph neighborhood and across time stamps, no prior work has developed implicit models to overcome these issues. Here we present IDGNN, a novel implicit neural network for dynamic graphs. We demonstrate that IDGNN is well-posed, i.e., it has a unique fixed-point solution. However, the standard iterative algorithm often used to train implicit models is computationally expensive in our setting and cannot be used to train IDGNN efficiently. To overcome this, we pose an equivalent bi-level optimization problem and propose a single-loop training algorithm. We conduct extensive experiments on real-world datasets on both classification and regression tasks to demonstrate the superiority of our approach over the state-of-the-art baseline approaches. We also demonstrate that our bi-level optimization framework maintains the performance of the standard iterative algorithm while obtaining up to 1600x speed-up",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=SMZGQu6lld": {
    "title": "LLM-Prop: Predicting Physical And Electronic Properties of Crystalline Solids From Their Text Descriptions",
    "volume": "review",
    "abstract": "The prediction of crystal properties plays a crucial role in the crystal design process. Current methods for predicting crystal properties focus on modeling crystal structures using graph neural networks (GNNs). Although GNNs are powerful, accurately modeling the complex interactions between atoms and molecules within a crystal remains a challenge. Surprisingly, predicting crystal properties from crystal text descriptions is understudied, despite the rich information and expressiveness that text data offer. One of the main reasons is the lack of publicly available data for this task. In this paper, we develop and make public a benchmark dataset (TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. LLM-Prop outperforms the current state-of-the-art GNN-based crystal property predictor by about 4% on predicting band gap, 3% on classifying whether the band gap is direct or indirect, and 66% on predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ndRkLsoQ1Q": {
    "title": "Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels",
    "volume": "review",
    "abstract": "In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data. These algorithms often incorporate sophisticated techniques, such as noise modeling, label correction, and co-training. In this study, we demonstrate that a simple baseline using cross-entropy loss, combined with widely used regularization strategies like learning rate decay, model weights average, and data augmentations, can outperform state-of-the-art methods. Our findings suggest that employing a combination of regularization strategies can be more effective than intricate algorithms in tackling the challenges of learning with noisy labels. While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored. Our results encourage a reevaluation of benchmarks for learning with noisy labels and prompt reconsideration of the role of specialized learning algorithms designed for training with noisy labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuZujQ9LJV": {
    "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) exhibit broad utility in diverse applications but remain vulnerable to jailbreak attacks, including hand-crafted and automated adversarial attacks, which can compromise their safety measures. However, patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block, while automated adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we propose an automatic and interpretable adversarial attack, AutoDAN, that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable, exhibiting strategies commonly used in manual jailbreak attacks. Moreover, these interpretable prompts transfer better than their non-readable counterparts, especially when using limited data and a single proxy model. Beyond eliciting harmful content, we also customize the objective of AutoDAN to leak system prompts, demonstrating its versatility. Our work underscores the seemingly intrinsic vulnerability of LLMs to interpretable adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=3GunDQNKFJ": {
    "title": "Learning-Retrieval-Revision For Large Language Model Domain Adaptation",
    "volume": "review",
    "abstract": "While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data. This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an adapt-retrieve-revise process. The initial step is to adapt an affordable 7B LLM to the target domain by continuing learning on public in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answers will be used to retrieve supporting evidence candidates from an external in-domain knowledge base. Finally, the draft answer and retrieved evidence are concatenated into a whole prompt to let GPT-4 assess the evidence and revise the draft answer to generate the final answer. Our proposal combines the advantages of the efficiency of adapting a smaller 7B model with the evidence-assessing capability of GPT-4 and effectively prevents GPT-4 from generating hallucinatory content. In the zero-shot setting of four Chinese legal tasks, our method improves accuracy by 33.3% compared to the direct generation by GPT-4. When compared to two stronger retrieval-based baselines, our method outperforms them by 15.4% and 23.9%. Our code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WrEFIbrVg9": {
    "title": "Non-asymptotic Analysis of Stochastic Gradient Descent under Local Differential Privacy Guarantee",
    "volume": "review",
    "abstract": "In private machine learning algorithms, Differentially Private Stochastic Gradient Descent (DP-SGD) plays an important role. Despite this, there have been few studies that have explored the theoretical analysis that can be derived from DP-SGD, particularly in a more challenging scenario where individual users retain the autonomy to specify their differential privacy budgets. In this work, we conduct a comprehensive non-asymptotic analysis of the convergence of the DP-SGD algorithm as well as its variants. This will allow individual users to assign different privacy guarantees when releasing models trained by DP-SGD. Most importantly, we provide readers with practical guidelines regarding the effect of various hyperparameters, such as step size, parameter dimensions, and privacy budgets, on convergence rates. The problem we consider includes the most commonly used loss functions in standard machine learning algorithms. For strongly convex loss functions, we establish an upper bound on the expected distance between the estimators and the global optimum. In the case of non-strongly convex functions, we analyze the upper bound difference between the loss incurred by the estimators and the optimal loss. Our proposed estimators are validated in the theoretical and practical realms by rigorous mathematical derivation and numerous numerical tests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=OyIzNLAQfE": {
    "title": "Adaptive Continual Learning: Rapid Adaptation and Knowledge Refinement",
    "volume": "review",
    "abstract": "Continual learning (CL) is an emerging research area aiming to emulate human learning throughout a lifetime. Most existing CL approaches primarily focus on mitigating catastrophic forgetting, a phenomenon where performance on old tasks declines while learning new ones. However, human learning involves not only retaining knowledge but also quickly recognizing the current environment, recalling related knowledge, and refining it for improved performance. In this work, we introduce a new problem setting, Adaptive CL, which captures these aspects in an online, possibly recurring task environment without explicit task boundaries or identities. We propose the LEARN algorithm to efficiently explore, recall, and refine knowledge in such environments. We provide theoretical guarantees from two perspectives: online prediction with tight regret bounds and asymptotic consistency of knowledge. Additionally, we present a scalable implementation that requires only first-order gradients for training deep learning models. Our experiments demonstrate that the LEARN algorithm is highly effective in exploring, recalling, and refining knowledge in adaptive CL environments, resulting in superior performance compared to competing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4olqbTBt1Y": {
    "title": "DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption",
    "volume": "review",
    "abstract": "Recently, numerous graph neural network methods have been developed to tackle domain shifts in graph data. However, these methods presuppose that unlabeled target graphs belong to categories previously seen in the source domain. This assumption could not hold true for in-the-wild target graphs. In this paper, we delve deeper to explore a more realistic problem open-set graph domain adaptation. Our objective is to not only identify target graphs from new categories but also accurately classify remaining target graphs into their respective categories under domain shift and label scarcity. To address this challenging problem, we introduce a novel method named Dual Structured Exploration with Mixup (DREAM). DREAM incorporates a graph-level representation learning branch as well as a subgraph-enhanced branch, which jointly explores graph topological structures from both global and local viewpoints. To maximize the use of unlabeled target graphs, we train these two branches simultaneously using posterior regularization to enhance their inter-module consistency. To accommodate the open-set setting, we amalgamate dissimilar samples to generate virtual unknown samples belonging to novel classes. Moreover, to alleviate domain shift, we establish a k nearest neighbor-based graph-of-graphs and blend multiple neighbors of each sample to produce cross-domain virtual samples for inter-domain consistency learning. Extensive experiments validate the effectiveness of our proposed DREAM compared with various state-of-the-art approaches in different settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=sRyGgkdQ47": {
    "title": "Making Batch Normalization Great in Federated Deep Learning",
    "volume": "review",
    "abstract": "Batch Normalization (BN) is commonly used in modern deep learning to improve stability and speed up convergence in centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. In this paper, we identify a more fundamental issue of BN in FL that makes BN inferior even with high-frequency communication between clients and servers. We then propose a frustratingly simple treatment, which significantly improves BN and makes it outperform GN across a wide range of FL settings. Along with this study, we also reveal an unreasonable behavior of BN in FL. We find it quite robust in the low-frequency communication regime where FL is commonly believed to degrade drastically. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=nFI3wFM9yN": {
    "title": "Communication-Efficient Federated Non-Linear Bandit Optimization",
    "volume": "review",
    "abstract": "Federated optimization studies the problem of collaborative function optimization among multiple clients (e.g. mobile devices or organizations) under the coordination of a central server. Since the data is collected separately by each client and always remains decentralized, federated optimization preserves data privacy and allows for large-scale computing, which makes it a promising decentralized machine learning paradigm. Though it is often deployed for tasks that are online in nature, e.g., next-word prediction on keyboard apps, most works formulate it as an offline problem. The few exceptions that consider federated bandit optimization are limited to very simplistic function classes, e.g., linear, generalized linear, or non-parametric function class with bounded RKHS norm, which severely hinders its practical usage. In this paper, we propose a new algorithm, named Fed-GO-UCB, for federated bandit optimization with generic non-linear objective function. Under some mild conditions, we rigorously prove that Fed-GO-UCB is able to achieve sub-linear rate for both cumulative regret and communication cost. At the heart of our theoretical analysis are distributed regression oracle and individual confidence set construction, which can be of independent interests. Empirical evaluations also demonstrate the effectiveness of the proposed algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Nn2BLV7SB": {
    "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
    "volume": "review",
    "abstract": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our findings reveal that PandaLM-7B offers a performance comparable to both GPT-3.5 and GPT-4. Impressively, PandaLM-70B surpasses their performance. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KbetDM33YG": {
    "title": "Online GNN Evaluation Under Test-time Graph Distribution Shifts",
    "volume": "review",
    "abstract": "Evaluating the performance of a well-trained GNN model on real-world graphs is a pivotal step for reliable GNN online deployment and serving. Due to a lack of test node labels and unknown potential training-test graph data distribution shifts, conventional model evaluation encounters limitations in calculating performance metrics (e.g., test error) and measuring graph data-level discrepancies, particularly when the training graph used for developing GNNs remains unobserved during test time. In this paper, we study a new research problem, online GNN evaluation, which aims to provide valuable insights into the well-trained GNNs's ability to effectively generalize to real-world unlabeled graphs under the test-time graph distribution shifts. Concretely, we develop an effective learning behavior discrepancy score, dubbed LeBeD, to estimate the test-time generalization errors of well-trained GNN models. Through a novel GNN re-training strategy with a parameter-free optimality criterion, the proposed LeBeD comprehensively integrates learning behavior discrepancies from both node prediction and structure reconstruction perspectives. This enables the effective evaluation of the well-trained GNNs' ability to capture test node semantics and structural representations, making it an expressive metric for estimating the generalization error in online GNN evaluation. Extensive experiments on real-world test graphs under diverse graph distribution shifts could verify the effectiveness of the proposed method, revealing its strong correlation with ground-truth test errors on various well-trained GNN models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BAX3NXJ6vU": {
    "title": "Escaping Saddle Point Efficiently in Minimax and Bilevel Optimizations",
    "volume": "review",
    "abstract": "Hierarchical optimization (including minimax optimization and bilevel optimization) is attracting significant attentions as it can be broadly applied to many machine learning tasks such as adversarial training, policy optimization, meta-learning and hyperparameter optimization. Recently, many algorithms have been studied to improve the theoretical analysis results of minimax and bilevel optimizations. Among these works, one of the most crucial issues is to escape saddle point and find local minimum, which is also of importance in conventional nonconvex optimization. In this paper, thus, we focus on investigating the methods to achieve second-order stationary point for nonconvex-strongly-concave minimax optimization and nonconvex-strongly-convex bilevel optimization. Specifically, we propose a new algorithm named PRGDA via perturbed stochastic gradient which does not require the computation of second order derivatives. In stochastic nonconvex-strongly-concave minimax optimization, we prove that our algorithm can find an $O(\\epsilon, \\sqrt{\\rho_{\\Phi} \\epsilon})$ second-order stationary point within gradient complexity of $\\tilde{O} (\\kappa^3 \\epsilon^{-3})$, which matches state-of-the-art to find first-order stationary point. To our best knowledge, our algorithm is the first stochastic algorithm that is guaranteed to obtain the second-order stationary point for nonconvex minimax problems. Besides, in stochastic nonconvex-strongly-convex bilevel optimization, our method also achieves better gradient complexity of $Gc(f, \\epsilon) = \\tilde{O}(\\kappa^3 \\epsilon^{-3})$ and $Gc(g, \\epsilon) = \\tilde{O}(\\kappa^7 \\epsilon^{-3})$ to find local minimum. Finally, we conduct a numerical experiment to validate the performance of our new method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=NltzxpG0nz": {
    "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
    "volume": "review",
    "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model designed to address this limitation. Steve-Eye integrates the LLM with a visual encoder which enables it to process visual-text inputs and generate multimodal feedback. In addition, we use a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, empowering our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out extensive experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. Codes and datasets will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=C371MUzjBl": {
    "title": "DAG-Based Column Generation for Adversarial Team Games",
    "volume": "review",
    "abstract": "Many works recently have focused on computing optimal solutions for the ex ante coordination of a team for solving sequential adversarial team games, where a team of players coordinate against an opponent (or a team of players) in a zero-sum extensive-form game. However, it is challenging to directly compute such an optimal solution because the team's coordinated strategy space is exponential in the size of the game tree due to the asymmetric information of team members. Column Generation (CG) algorithms have been proposed to overcome this challenge by iteratively expanding the team's coordinated strategy space via a Best Response Oracle (BRO). More recently, more compact representations (particularly, the Team Belief Directed Acyclic Graph (TB-DAG)) of the team's coordinated strategy space have been proposed, but the TB-DAG-based algorithms only outperform the CG-based algorithms in games with a small TB-DAG. Unfortunately, it is inefficient to directly apply CG to the TB-DAG because the size of the TB-DAG is still exponential in the size of the game tree and then makes the BRO unscalable. To this end, we develop our novel TB-DAG CG (DCG) algorithm framework by computing a coordinated best response in the original game first and then transforming this strategy into the TB-DAG form. To further improve the scalability, we propose a more suitable BRO for DCG to reduce the cost of the transformation at each iteration. We theoretically show that our algorithm converges exponentially faster than the state-of-the-art CG algorithms, and experimental results show that our algorithm is at least two orders of magnitude faster than the state-of-the-art baselines and solves games that were previously unsolvable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=4aywmeb97I": {
    "title": "Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration",
    "volume": "review",
    "abstract": "Asynchronous federated learning, which enables local clients to send their model update asynchronously to the server without waiting for others, has recently emerged for its improved efficiency and scalability over traditional synchronized federated learning. In this paper, we study how the asynchronous delay affects the convergence of asynchronous federated learning under non-i.i.d. distributed data across clients. Through the theoretical convergence analysis of one representative asynchronous federated learning algorithm under standard nonconvex stochastic settings, we show that the asynchronous delay can largely slow down the convergence, especially with high data heterogeneity. To further improve the convergence of asynchronous federated learning under heterogeneous data distributions, we propose a novel asynchronous federated learning method with a cached update calibration. Specifically, we let the server cache the latest update for each client and reuse these variables for calibrating the global update at each round. We theoretically prove the convergence acceleration for our proposed method under nonconvex stochastic settings. Extensive experiments on several vision and language tasks demonstrate our superior performances compared to other asynchronous federated learning baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=lgvOSEMEQS": {
    "title": "Lightweight Unsupervised Federated Learning with Pretrained Vision Language Model",
    "volume": "review",
    "abstract": "Federated learning aims to tackle the ``isolated data island\" problem, where it trains a collective model from physically isolated clients while safeguarding the privacy of users' data. However, supervised federated learning necessitates that each client labels their data for training, which can be both time-consuming and resource-intensive, and may even be impractical for edge devices. Moreover, the training and transmission of deep models present challenges to the computation and communication capabilities of the clients. To address these two inherent challenges in supervised federated learning, we propose a novel lightweight unsupervised federated learning approach that leverages unlabeled data on each client to perform lightweight model training and communication by harnessing pretrained vision-language models, such as CLIP. By capitalizing on the zero-shot prediction capability and the well-trained image encoder of the pre-trained CLIP model, we have carefully crafted an efficient and resilient self-training approach. This method refines the initial zero-shot predicted pseudo-labels of unlabeled instances through the sole training of a linear classifier on top of the fixed image encoder. Additionally, to address data heterogeneity within each client, we propose a class-balanced text feature sampling strategy for generating synthetic instances in the feature space to support local training. Experiments are conducted on multiple benchmark datasets. The experimental results demonstrate that our proposed method greatly enhances model performance in comparison to CLIP's zero-shot predictions and even outperforms supervised federated learning benchmark methods given limited computational and communication overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=J0qgRZQJYX": {
    "title": "An Axiomatic Approach to Model-Agnostic Concept Explanations",
    "volume": "review",
    "abstract": "Concept explanation is a popular approach for examining how human-interpretable concepts impact the predictions of a model. However, most existing methods for concept explanations are tailored to specific models. To address this issue, this paper focuses on model-agnostic measures. Specifically, we propose an approach to concept explanations that satisfy three natural axioms: linearity, recursivity, and similarity. We then establish connections with previous concept explanation methods, offering insight into their varying semantic meanings. Experimentally, we demonstrate the utility of the new method by applying it in different scenarios: for model selection, optimizer selection, and model improvement using a kind of prompt editing for zero-shot vision language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOgLmcJxxF": {
    "title": "Sample-Efficient Training for Score-Based Diffusion",
    "volume": "review",
    "abstract": "Score-based diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. Recently, a number of theoretical works \\citep{chen2022, Chen2022ImprovedAO, chen2023probability, benton2023linear} have shown that diffusion models can efficiently sample, assuming $L^2$-accurate score estimates. The score-matching objective naturally approximates the true score in $L^2$, but the sample complexity of existing bounds depends \\emph{polynomially} on the data radius and desired Wasserstein accuracy. By contrast, the time complexity of sampling is only logarithmic in these parameters. We show that estimating the score in $L^2$ \\emph{requires} this polynomial dependence, but that polylogarithmic samples actually do suffice for sampling. We show that with a polylogarithmic number of samples, the ERM of the score-matching objective is $L^2$ accurate on all but a probability $\\delta$ fraction of the true distribution, and that this weaker guarantee is sufficient for efficient sampling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xx05gm7oQw": {
    "title": "Debias your VLM with Counterfactuals: A Unified Approach",
    "volume": "review",
    "abstract": "Recent advances in vision-language research have produced numerous foundation models that excel in tasks such as image classification, image-text retrieval, and image captioning. However, these models are shown to exploit spurious correlations in biased training data, raising fairness concerns for discrimination against underprivileged groups. In this work, we propose CVLD, a unified framework for quantifying and mitigating vision-language biases in a task and domain-agnostic setting. By defining a causal intervention module that produces counterfactual image-text pairs, we apply causal fairness metrics to capture the discrepancy between model predictions on original and counterfactual distributions. Building on the universal fairness notion, we propose a set of bias-free adaptation techniques to mitigate the bias of pre-trained VL models by optimizing their robustness to interventions on the protected attribute, requiring minimal modification to the naive training pipeline. CVLD demonstrates robust debiasing results on image classification, retrieval and captioning using adaptation datasets of varying sizes, validating the importance of counterfactual data in studying vision-language bias",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjhUtloBZU": {
    "title": "Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks",
    "volume": "review",
    "abstract": "Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Bl8u7ZRlbM": {
    "title": "(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild",
    "volume": "review",
    "abstract": "Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual, opt-in for anonymous collection of their chat transcripts. From this, we compiled (InThe)WildChat, a corpus of 570K user-ChatGPT conversations, which consists of over 1.5 million interaction turns. We compare WildChat with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In particular, in WildChat we find that a majority of the potentially unsafe use is produced by users attempting to \"jailbreak\" the model using prompts posted on online platforms; these are successful more than 70% of the time for ChatGPT. Finally, because it captures a broad range of use cases, we demonstrate the dataset's potential utility in fine-tuning state-of-the-art instruction following models. WildLlama, a chatbot fine-tuned on WildChat, outperforms the latest Vicuna model of the same size on MT-Bench, which shows that WildChat has a high utility in addition to being a source for toxicity study. We will release WildChat and WildLlama with a license that emphasizes on accountability, collaboration, and transparency. The clean portion of WildChat will be publicly available, and the portion that contains potentially unsafe content will be made available upon request with a justification for AI safety research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=vE8Vn6DM0y": {
    "title": "Aligning Brains into a Shared Space Improves Their Alignment to Large Language Model",
    "volume": "review",
    "abstract": "The ability of Large Language Models (LLM) to perform remarkably well on various language processing tasks provides a computational modeling framework for studying the neural basis of human language. Recent studies show that the hidden states of the transformer layers of LLM, called contextual embeddings, can predict brain responses through linear encoding models. In this paper, we analyze the neural responses of 8 subjects while they listened to the same 30 minute podcast episode. We use a shared response model to compute the shared information space across subjects and show that LLM-based encoding models achieve significantly better performance in predicting the shared information features than the original brain responses. We also show that we can use this shared space to denoise the individual brain responses by projecting back to the neural space and this process achieves a mean 38% improvement in encoding performance across the subjects. A detailed inspection of this improvement in different brain areas reveals that the improvements are the most prominent in brain areas specialized for language comprehension, specifically in superior temporal gyrus (STG) and inferior frontal gyrus (IFG). Our analysis also shows that the shared space calculated from a group of subjects is generalizable to a new subject. This suggests that the LLM model can be used as a shared linguistic model for how information is shared across brains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=IRcv4yFX6z": {
    "title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition",
    "volume": "review",
    "abstract": "Image segmentation and recognition occur simultaneously, with recognition relying on the underlying segmentation to form a continuous visual grouping hierarchy. For example, the same object can be parsed into different part-to-whole structures, resulting in varying recognitions. Despite this, most prior works treated segmentation and recognition as separate tasks. In this paper, we aim to devise a learning framework that involves segmentation in the recognition process, utilizing hierarchical segmentation for recognition, which is learned by recognition. Specifically, we propose CAST, which realizes this concept through designs inspired by vision transformers, enabling concurrent segmentation and recognition with a single model. The core idea of CAST is to employ adaptive segment tokens that group the finest pixels into coarser segments, using the latest embedding to represent the entire image for recognition. Trained solely on image recognition objectives, CAST automatically discovers the hierarchy of segments. Our experiments demonstrate that CAST provides consistent hierarchical segmentation and recognition, which is impossible with state-of-the-art segmentation methods such as SAM. Additionally, CAST offers several advantages over the standard ViT, including improved semantic segmentation, computational efficiency, and object-centric attention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oZf2SlXEY": {
    "title": "Distribution Calibration For Few-Shot Learning by Bayesian Relation Inference",
    "volume": "review",
    "abstract": "Learning from a limited number of samples is difficult as a small number of samples cannot cover all the information in their category. It is worth noting that categories with scarce samples may be distributed in a way that is related to categories that contain sufficient data. Therefore it is possible to calibrate the distribution of a sample-poor category by using categories with a large amount of data. Existing methods of distribution calibration usually use artificially set distances to calculate the association between two categories, which may ignore deeper relations between categories. In this paper, we propose a distribution calibration method based on Bayesian relation inference. For the input few-sample classes, it can automatically infer their relation with the categories with sufficient data and adaptively generate a large amount of fused feature data that can represent the few-sample classes. The results show that a simple logistic regression classifier trained by using the large amount of data generated by our method, exceeds state-of-the-art accuracy for skin disease classification issue. Through visual analysis, we demonstrate that the relationship graph generated by this Bayesian relationship inference method has a degree of interpretability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=TilcG5C8bN": {
    "title": "Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning",
    "volume": "review",
    "abstract": "Deep Neural Networks (DNNs), essential for diverse applications such as visual recognition and eldercare, often require a large amount of labeled data for training, making widespread deployment of DNNs a challenging task. Self-supervised learning (SSL) emerges as a promising approach, which leverages inherent patterns within data through diverse augmentations to train models without explicit labels. However, while SSL has shown notable advancements in accuracy, its high computation costs remain a daunting impediment, particularly for resource-constrained platforms. To address this problem, we introduce SimWnW, a similarity-based efficient self-supervised learning framework. By strategically removing less important regions in augmented images and feature maps, SimWnW not only reduces computation costs but also eliminates irrelevant features that might slow down the learning process, thereby accelerating model convergence. The experimental results show that SimWnW effectively reduces the amount of computation costs in self-supervised model training without compromising accuracy. Specifically, SimWnW yields up to 54\\% and 51\\% computation savings in training from scratch and transfer learning tasks, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=x2rZGCbRRd": {
    "title": "Extracting Post-Treatment Covariates for Heterogeneous Treatment Effect Estimation",
    "volume": "review",
    "abstract": "The exploration of causal relationships between treatments and outcomes, and the estimation of causal effects from observational data, have garnered considerable interest in the scientific community in recent years. However, traditional causal inference methods implicitly assume that all covariates are measured prior to treatment assignment, while in many real-world scenarios, some covariates are affected by the treatment and collected post-treatment. In this paper, we demonstrate how ignoring or mishandling post-treatment covariates can lead to biased estimates of heterogeneous treatment effects, referred to as the \"post-treatment bias\" problem. We discuss the possible cases in which post-treatment bias may appear and the negative impact it can have on causal effect estimation. Methodologically, we propose a novel variable decomposition approach to account for post-treatment covariates and eliminate post-treatment bias, based on a newly proposed causal graph for post-treatment causal inference analyses. Extensive experiments on synthetic, semi-synthetic, and real-world data demonstrate the superiority of our proposed method over state-of-the-art models for heterogeneous treatment effect estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bDZCBjVgKW": {
    "title": "Fast Post-training Analysis of NeRFs Using A Simple Visibility Prediction Network",
    "volume": "review",
    "abstract": "Exercising NeRFs on real-world data taught us that their novel view rendering capability varies across different views and rendering of regions that are visible in more input images often produces more reliable results. However, efficient quantitative tools haven't been developed in this regard to facilitate the post-training analysis of NeRF rendered images. In this paper, we introduce a simple visibility prediction network that efficiently predicts the visibility of \\textit{any} point in space from \\textit{any} of the input cameras. We further introduce a visibility scoring function that characterizes the reliability of the rendered points, which assists the evaluation of NeRF rendering quality in the absence of ground truth. Utilizing this tool, we also empirically demonstrate two downstream post-training analysis tasks. The first task is to reduce rendering artifacts via modified volumetric rendering which skips unreliable near-range points. We achieve an average PSNR improvement of 0.6 dB in novel view rendering without changing the network parameters of the pre-trained base NeRF on a benchmark composed of 62 scenes. The second task is to select additional training images to re-train a NeRF and enhance its rendering quality. By re-training the base NeRF with a handful of additional views selected using the proposed visibility score, we achieve better rendering quality compared to random selection. Our method is rudimentary, yet efficient and simple to implement making it a suitable drop-in tool for various post-training tasks beyond the studies shown in this paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJHUYWviZ6": {
    "title": "On Socially Fair Regression and Low-Rank Approximation",
    "volume": "review",
    "abstract": "Regression and low-rank approximation are two fundamental problems that are applied across a wealth of machine learning applications. In this paper, we study the question of socially fair regression and socially fair low-rank approximation, where the goal is to minimize the loss over all sub-populations of the data. We show that surprisingly, socially fair regression and socially fair low-rank approximation exhibit drastically different complexities. Specifically, we show that while fair regression can be solved up to arbitrary accuracy in polynomial time for a wide variety of loss functions, even constant-factor approximation to fair low-rank approximation requires exponential time under certain standard complexity hypotheses. On the positive side, we give an algorithm for fair low-rank approximation that, for a constant number of groups and constant-factor accuracy, runs in $2^{\\text{poly}(k)}$ rather than the na\\\"{i}ve $n^{\\text{poly}(k)}$, which is a substantial improvement when the dataset has a large number $n$ of observations. Finally, we show that there exists a bicriteria approximation algorithm for fair low-rank approximation that runs in polynomial time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=UqEI76CKgO": {
    "title": "Amphibian: A Meta-Learner for Rehearsal-Free Fast Online Continual Learning",
    "volume": "review",
    "abstract": "Online continual learning is challenging as it requires fast adaptation over a stream of data in a non-stationary environment without forgetting the knowledge acquired in the past. To address this challenge, in this paper, we introduce Amphibian - a gradient-based meta-learner that learns to scale the direction of gradient descent to achieve the desired balance between fast learning and continual learning. For this purpose, using only the current batch of data, Amphibian minimizes a meta-objective that encourages alignments of gradients among given data samples along selected basis directions in the gradient space. From this objective, it learns a diagonal scale matrix in each layer that accumulates the history of such gradient alignments. Using these scale matrices Amphibian updates the model online only in the directions having positive cumulative gradient alignments among the data observed for far. With evaluation on standard continual image classification benchmarks, we show that such meta-learned scaled gradient descent in Amphibian achieves state-of-the-art accuracy in online continual learning while enabling fast learning with less data and few-shot knowledge transfer to new tasks. Finally, with loss landscape visualizations, we show such gradient updates incur minimum loss to the old task enabling fast continual learning in Amphibian",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=I09JonzQJV": {
    "title": "Counterfactual Fairness With the Human in the Loop",
    "volume": "review",
    "abstract": "Machine learning models have been increasingly used in human-related applications such as healthcare, lending, and college admissions. As a result, there are growing concerns about potential biases against certain demographic groups. To address the unfairness issue, various fairness notions have been introduced in the literature to measure and mitigate such biases. Among them, Counterfactual Fairness (CF) (Kusner $\\textit{et al.}$) is a notion defined based on an underlying causal graph that requires the prediction perceived by an individual in the real world to remain the same as it would be in a counterfactual world, in which the individual belongs to a different demographic group. Unlike Kusner $\\textit{et al.}$, this work studies the long-term impact of machine learning decisions using a causal inference framework where the individuals' future status may change based on the current predictions. We observe that imposing the original counterfactual fairness may not lead to a fair future outcome for the individuals. We thus introduce a fairness notion called $\\textit{lookahead counterfactual fairness}$ (LCF), which accounts for the downstream effects of ML models and requires the individual $\\textit{future status}$ to be counterfactually fair. We theoretically identify conditions under which LCF can be improved and propose an algorithm based on our theoretical results. Experiments on both synthetic and real data show the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vqHTUTod9": {
    "title": "Can Language Models be Instructed to Protect Personal Information?",
    "volume": "review",
    "abstract": "Large multimodal language models have proven transformative in numerous applications. However, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. While data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. In this paper, we introduce PrivQA --- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. We also propose a technique to iteratively self-moderate responses, which significantly improves privacy. However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. We believe PrivQA has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. We release the entire PrivQA dataset at [URL removed for review]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRhRDpsCO2": {
    "title": "MATT: Random Local Implicit Purification for Defending Query-based Attacks",
    "volume": "review",
    "abstract": "Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, MATT, that employs random patch-wise purifications with an ensemble of lightweight purification models. These models leverage the local implicit function and rebuild the natural image manifold with low inference latency. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks while preserving the average robustness improvement by combining randomness and purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defense mechanism, demonstrating significant improvements in classifier robustness against query-based attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=CfXh93NDgH": {
    "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions",
    "volume": "review",
    "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Both automatic and human evaluations consistently indicate that WizardLM outperforms baselines such as Alpaca (trained from Self-Instruct) and Vicuna (trained from human-created instructions). The experimental results demonstrate that the quality of instruction-following dataset crafted by Evol-Instruct can significantly improve the performance of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=t84UBRhhvp": {
    "title": "Text Descriptions are Compressive and Invariant Representations for Visual Learning",
    "volume": "review",
    "abstract": "Modern image classification is based upon directly predicting classes via large discriminative networks, which do not directly contain information about the intuitive visual features that may constitute a classification decision. Recently, work in vision-language models (VLM) such as CLIP has provided ways to specify natural language descriptions of image classes, but typically focuses on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, in line with humans' understanding of multiple visual features per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we introduce a novel method, \\textit{SLR-AVD (Sparse Logistic Regression using Augmented Visual Descriptors)}. This method first automatically generates multiple visual descriptions of each class via a large language model (LLM), then uses a VLM to translate these descriptions to a set of visual feature embeddings of each image, and finally uses sparse logistic regression to select a relevant subset of these features to classify each image. Core to our approach is the fact that, information-theoretically, these descriptive features are more invariant to domain shift than traditional image embeddings, even though the VLM training process is not explicitly designed for invariant representation learning. These invariant descriptive features also compose a better input compression scheme. When combined with finetuning, we show that SLR-AVD is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=7ipjMIHVJt": {
    "title": "DASFormer: Self-supervised Pretraining for Earthquake Monitoring",
    "volume": "review",
    "abstract": "Earthquake monitoring is a fundamental task to unravel the underlying physics of earthquakes and mitigate associated hazards for public safety. Distributed acoustic sensing, or DAS, which transforms pre-existing telecommunication cables into ultra-dense seismic networks, offers a cost-effective and scalable solution for next-generation earthquake monitoring. However, current approaches for earthquake monitoring primarily rely on supervised learning, while manually labeled DAS data is quite limited and it is difficult to obtain more annotated datasets. In this paper, we present DASFormer, a novel self-supervised pretraining technique on DAS data with a coarse-to-fine framework that models spatial-temporal signal correlation. Given the pretrained DASFormer, we treat earthquake monitoring as an anomaly detection task and demonstrate that the pretrained DASFormer can be successfully utilized as a seismic phase detector. Experimental results demonstrate that DASFormer is effective in terms of several evaluation metrics and outperforms state-of-the-art time-series forecasting, anomaly detection, and foundation models on several datasets in the seismic detection tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=PQStRgYfuJ": {
    "title": "Topology-aware Embedding Memory for Learning on Expanding Graphs",
    "volume": "review",
    "abstract": "Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\\mathcal{O}(nd^L)$ to $\\mathcal{O}(n)$ ($n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field), but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via Topology-aware Embeddings (TEs), which compress ego-subgraphs into compact vectors (i.e., TEs) to reduce the memory consumption. Based on this framework, we discover a unique \\textit{pseudo-training effect} in continual learning on expanding graphs and this effect motivates us to develop a novel coverage maximization sampling strategy that can enhance the performance with a tight memory budget. Thorough empirical studies demonstrate that, by tackling the memory explosion problem and incorporating topological information into memory replay, PDGNNs with TEM significantly outperform state-of-the-art techniques, especially in the challenging class-incremental setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=wabp68RoSP": {
    "title": "Active Prompting with Chain-of-Thought for Large Language Models",
    "volume": "review",
    "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answering tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving superior performance on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationships demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=usrChqw6yK": {
    "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors",
    "volume": "review",
    "abstract": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text descriptions of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=DQCZiKb3Uy": {
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "volume": "review",
    "abstract": "Intelligent beings have the ability to quickly learn new behaviors and tasks by leveraging background world knowledge. This stands in contrast to most agents trained with reinforcement learning (RL), which typically learn behaviors from scratch. Therefore, we would like to endow RL agents with a similar ability to leverage contextual prior information. To this end, we propose a novel approach that uses the vast amounts of general-purpose, diverse, and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data to generate text in response to images and prompts. We initialize RL policies with VLMs by using such models as sources of \\textit{promptable representations}: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex RL tasks in Minecraft. We find that policies trained on promptable embeddings significantly outperform equivalent policies trained on generic, non-promptable image encoder features. Moreover, we show that promptable representations extracted from general-purpose VLMs outperform both domain-specific representations and instruction-following methods. In ablations, we find that VLM promptability and text generation both are important in yielding good representations for RL. Finally, we give a simple method for evaluating and optimizing prompts used by our approach for a given task without running expensive RL trials, ensuring that it extracts task-relevant semantic features from the VLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=iQIQT88prm": {
    "title": "Adversarial Machine Unlearning: A Stackelberg Game Approach",
    "volume": "review",
    "abstract": "This paper focuses on the challenge of machine unlearning, aiming to remove the influence of specific training data on machine learning models. Traditionally, the development of unlearning algorithms runs parallel with that of membership inference attacks, a type of privacy threat to determine whether a data instance was used for training. Recognizing this interplay, we propose a game-theoretic framework that integrates the attacks into the design of unlearning algorithms. We model the unlearning problem as a Stackelberg game, introducing a two-player dynamic: a defender striving to unlearn specific training data from a model, and an attacker employing membership inference attacks to detect the traces of the data. Adopting this adversarial perspective allows the utilization of new attack advancements, facilitating the design of unlearning algorithms. Our framework stands out in two ways. First, it enables the exact implementation of advanced membership inference attacks, providing verification for the effectiveness of unlearning. Second, it enables differentiation through optimization problems of attacks, making the framework readily integrable into end-to-end learning pipelines. We present extensive experimental results to validate the efficacy of the proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7vzyqs8UbA": {
    "title": "LMCC-MBC: Metric-Constrained Model-Based Clustering with Wasserstein-2 Distance of Gaussian Markov Random Fields",
    "volume": "review",
    "abstract": "A wide range of temporal (1D) and spatial (2D) data analysis problems can be formulated as model-based clustering problems given metric constraints. For example, subsequence clustering of multivariate time series is constrained by 1D temporal continuity, while urban functional area identification is constrained by the spatial proximity in the 2D space. Existing works model such metric constraints independent of the model estimation process, failing to leverage the correlation between adjacent estimated models and their locations in the metric space. To solve this problem we propose a novel metric-constrained model-based clustering algorithm LMCC-MBC that softly requires the Wasserstein-2 distance between estimated model parameters (such as those of Gaussian Markov Random Fields) to be a locally monotonic continuous function of the metric distance. We theoretically prove that satisfaction of this requirement guarantees intra-cluster cohesion and inter-cluster separation. Moreover, without explicitly optimizing log-likelihood LMCC-MBC voids the expensive EM-step that is needed by previous approaches (e.g., TICC and STICC), and enables faster and more stable clustering. Experiments on both 1D and 2D synthetic as well as real-world datasets demonstrate that our algorithm successfully captures the latent correlation between the estimated models and the metric constraints, and outperforms strong baselines by a margin up to 14.3% in ARI (Adjusted Rand Index) and 32.1% in NMI (Normalized Mutual Information)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Mhu9iNGKqP": {
    "title": "Optimizing Layerwise Polynomial Approximation for Efficient Private Inference on Fully Homomorphically Encryption: A Dynamic Programming Approach",
    "volume": "review",
    "abstract": "Recent research has explored the implementation of privacy-preserving deep neural networks solely using fully homomorphic encryption. However, its practicality has been limited because of prolonged inference times. When using a pre-trained model without retraining, a major factor contributing to these prolonged inference times is the high-degree polynomial approximation of activation functions such as the ReLU function. The high-degree approximation consumes a substantial amount of homomorphic computational resources, resulting in slower inference. Unlike the previous works approximating activation functions uniformly and conservatively, this paper presents a \\emph{layerwise} degree optimization of activation functions to aggressively reduce the inference time while maintaining classification accuracy by taking into account the characteristics of each layer. Instead of the minimax approximation commonly used in state-of-the-art private inference models, we employ the weighted least squares approximation method with the input distributions of activation functions. Then we obtain the layerwise optimized degrees for activation functions through the \\emph{dynamic programming} algorithm considering how each layer's approximation error affects the classification accuracy of the deep neural network. Furthermore, we propose modulating the ciphertext moduli-chain layerwise to reduce the inference time. By these proposed layerwise optimization, we can reduce inference times for the ResNet-20 model and the ResNet-32 model by 3.44 times and 3.16 times, respectively, in comparison to the prior implementations employing uniform degree polynomials and a consistent ciphertext modulus",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SWRFC2EupO": {
    "title": "Language Reward Modulation for Pretraining Reinforcement Learning",
    "volume": "review",
    "abstract": "Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\\textbf{LA}$nguage Reward $\\textbf{M}$odulated $\\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=lKxL5zkssv": {
    "title": "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic",
    "volume": "review",
    "abstract": "The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method comprises a Transformer-based feature extractor that models global neural representations and two learnable subject-specific tokens representing different neural response patterns. These tokens enable the model to aggregate multi-subject data without a linear increase in the number of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=plmBsXHxgR": {
    "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
    "volume": "review",
    "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2GJm8yT2jN": {
    "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
    "volume": "review",
    "abstract": "Unsupervised representation learning has seen tremendous progress but is constrained by its reliance on data modality specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, human vision processes visual signals derived from irregular and non-stationary sampling lattices yet accurately perceives the geometry of the world. We introduce a novel framework that learns from high-dimensional data lacking stationarity and topology. Our model combines spectral clustering, and masked autoencoders and a learnable self-organizing layer. We evaluate its effectiveness on simulated biological vision data, neural recordings from the primary visual cortex, and gene expression datasets. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without depending on stationarity or topology. It also outperforms other methods not dependent on these factors, setting a new benchmark in the field. This work represents a step toward unsupervised learning methods that can generalize across diverse high dimensional data modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=zMPHKOmQNb": {
    "title": "Protein Discovery with Discrete Walk-Jump Sampling",
    "volume": "review",
    "abstract": "We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our $\\textit{Discrete Walk-Jump Sampling}$ formalism combines the contrastive divergence training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the $\\textit{distributional conformity score}$ to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100\\% of generated samples are successfully expressed and purified and 70\\% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a single round of laboratory experiments. We also report the first demonstration of long-run fast-mixing MCMC chains where diverse antibody protein classes are visited in a single MCMC chain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Te5v4EcFGL": {
    "title": "PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting",
    "volume": "review",
    "abstract": "Although the Transformer has been the dominant architecture for time series forecasting tasks in recent years, a fundamental challenge remains: the permutation-invariant self-attention mechanism within Transformers leads to a loss of temporal information. To tackle these challenges, we propose PatchMixer, a novel CNN-based model. It introduces a permutation-variant convolutional structure to preserve temporal information. Diverging from conventional CNNs in this field, which often employ multiple scales or numerous branches, our method relies exclusively on depthwise separable convolutions. This allows us to extract both local features and global correlations using a single-scale architecture. Furthermore, we employ dual forecasting heads that encompass both linear and nonlinear components to better model future curve trends and details. Our experimental results on seven time-series forecasting benchmarks indicate that compared with the state-of-the-art method and the best-performing CNN, PatchMixer yields 3.9\\% and 21.2\\% relative improvements, respectively, while being 2-3x faster than the most advanced method. We will release our code and model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HjfvnxaU5k": {
    "title": "Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties",
    "volume": "review",
    "abstract": "Experimental (design) optimization is a key driver in designing and discovering new products and processes. Bayesian Optimization (BO) is an effective tool for optimizing expensive and black-box experimental design processes. While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable). In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO. We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments. We discuss the convergence details of our proposed framework. The empirical results show the efficacy of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=71kocBuhNO": {
    "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
    "volume": "review",
    "abstract": "Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really \"reason\" over the natural language? This question has been receiving significant research attention and a number of reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. To enable systematic evaluation of logical reasoning, we introduce LogicBench, a natural language question-answering dataset encompassing 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. Key steps of our dataset construction consist of (1) controlled generation of sentences and their negations containing different ontologies, (2) (context, question, answer) triplets creation using heuristically designed templates, and (3) semantic variations of triplets adding more diversity. We present a comprehensive evaluation with a range of LLMs such as GPT-4, GPT-3, ChatGPT, and FLAN-T5 using chain-of-thought prompting in both zero-shot and few-shot settings. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle on instances requiring complex reasoning steps. Furthermore, we also show that LLMs trained using our data exhibit a better understanding of logical reasoning leading to performance improvements on several existing logical reasoning datasets such as LogicNLI, FOLIO, LogiQA, and ReClor",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=xVlcbh0poD": {
    "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
    "volume": "review",
    "abstract": "Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such \"in-the-wild\" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that are aligned with human preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0Y26tFG3WF": {
    "title": "Inducing Precision in Lagrangian Neural Networks : Proof of concept application on Chaotic systems",
    "volume": "review",
    "abstract": "Solutions of dynamic systems that exhibit chaotic behavior are particularly sensitive to errors in initial/intermediate state estimates when long term dynamics is of interest. Lagrangian Neural Networks (LNN) are a class of physics induced learning methods that seamlessly integrate physical conservation laws into functional solutions, by forming a parametric Lagrangian for the system of interest. However it has been seen that the function approximation error associated with the parametric Lagrangian modelling could prove to be catastrophic for the prediction of long term dynamics of chaotic systems. This makes improving the precision of the parametric Lagrangian particularly crucial. Considering the same in this work a modified Lagrangian Neural Network approach is proposed, where a customized neural network architecture is designed to directly emphasize the relative importance of each significant bit in the Lagrangian estimates produced. We evaluate our method on two dynamic systems that are well known in the literature in exhibiting deterministic chaos, namely the double pendulum and Henon-Helies systems. Further, we compare the obtained solutions with those estimated by Finite Element solvers (under optimal conditions) to validate the relative accuracy. We observe that the trajectory deviations as a result of chaotic behavior can be significantly reduced by the process of explicitly enforcing the precision requirement for the parametric Lagrangian, as modelled using the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=GURqUuTebY": {
    "title": "DreamFlow: High-quality text-to-3D generation by Approximating Probability Flow",
    "volume": "review",
    "abstract": "Recent progress in text-to-3D generation has been achieved through the utilization of score distillation methods: they make use of the pre-trained text-to-image (T2I) diffusion models by distilling via the diffusion model training objective. However, such an approach inevitably results in the use of random timesteps at each update, which increases the variance of the gradient and ultimately prolongs the optimization process. In this paper, we propose to enhance the text-to-3D optimization by leveraging the T2I diffusion prior in the generative sampling process with a predetermined timestep schedule. To this end, we interpret text-to-3D optimization as a multi-view image-to-image translation problem, and propose a solution by approximating the probability flow. By leveraging the proposed novel optimization algorithm, we design DreamFlow, a practical three-stage coarse-to-fine text-to-3D optimization framework that enables fast generation of high-quality and high-resolution (i.e., 1024×1024) 3D contents. For example, we demonstrate that DreamFlow is 5 times faster than the existing state-of-the-art text-to-3D method, while producing more photorealistic 3D contents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=oTRwljRgiv": {
    "title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
    "volume": "review",
    "abstract": "When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. When used with Transformer models trained from scratch, ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines. Finally, we use our benchmarks to demonstrate that LLMs struggle to compositionally generalize when asked to do programming-by-example in a few-shot setting, but an ExeDec-style prompting approach can improve the generalization ability and overall performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=98g9NdJPxm": {
    "title": "Theoretically Understanding Data Reconstruction Leakage in Federated Learning",
    "volume": "review",
    "abstract": "Federated learning is an emerging collaborative learning paradigm that aims to protect data privacy. Unfortunately, recent works show that federated learning algorithms are vulnerable to data reconstruction attacks, and a series of follow-up works are proposed to enhance the attack effectiveness. However, existing works lack of a theoretical understanding on to what extent the devices' data can be reconstructed and the effectiveness of these attacks cannot be compared theoretically. To address it, we propose a theoretical framework to understand data reconstruction attacks to FL. Our framework involves bounding the data reconstruction error and an attack's error bound reflects its inherent attack effectiveness. Under the framework, we can theoretically compare the effectiveness of existing attacks. For instance, our experimental results on multiple datasets validate that the iDLG data reconstruction attack inherently outperforms the DLG attack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=v8jdwkUNXb": {
    "title": "Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning",
    "volume": "review",
    "abstract": "Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9Xb6fADe4": {
    "title": "Towards Greener and Sustainable Airside Operations: A Deep Reinforcement Learning Approach to Pushback Rate Control for Mixed-Mode Runways",
    "volume": "review",
    "abstract": "Airside taxi delays have adverse consequences for airports and airlines globally, leading to airside congestion, increased Air Traffic Controller/Pilot workloads, missed passenger connections, and adverse environmental impact due to excessive fuel consumption. Effectively addressing taxi delays necessitates the synchronization of stochastic and uncertain airside operations, encompassing aircraft pushbacks, taxiway movements, and runway take-offs. With the implementation of mixed-mode runway operations (arrivals-departures on the same runway) to accommodate projected traffic growth, complexity of airside operations is expected to increase significantly. To manage airside congestion under increased traffic demand, development of efficient pushback control, also known as Departure Metering (DM), policies is a challenging problem. DM is an airside congestion management procedure that controls departure pushback timings, aiming to reduce taxi delays by transferring taxiway waiting times to gates. Under mixed-mode runway operations, however, DM must additionally maintain sufficient runway pressure---departure queues near runway for take-offs---to utilize available departure slots within incoming arrival aircraft steams. While a high pushback rate may result in extended departure queues, leading to increased taxi-out delays, a low pushback rate can result in empty slots between incoming arrival streams, leading to reduced runway throughput. This study introduces a Deep Reinforcement Learning (DRL) based DM approach for mixed-mode runway operations. We cast the DM problem in a markov decision process framework and use Singapore Changi Airport surface movement data to simulate airside operations and evaluate different DM policies. Predictive airside hotspots are identified using a spatial-temporal event graph, serving as the observation to the DRL agent. Our DRL based DM approach utilizes pushback rate as agent's action and reward shaping to dynamically regulate pushback rates for improved runway utilization and taxi delay management under uncertainties. Benchmarking the learnt DRL based DM policy against other baselines demonstrates the superior performance of our method, especially in high traffic density scenarios. Results, on a typical day of operations at Singapore Changi Airport, demonstrate that DRL based DM can reduce peak taxi times (1-3 minutes, on average); save approximately 27\\% in fuel consumption and overall better manage the airside traffic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=rO8QOHrCeA": {
    "title": "Grounding Code Generation with Input-Output Specifications",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated significant potential in code generation. However, the code generated by these models occasionally deviates from the user's intended outcome, resulting in executable but incorrect code. To mitigate this issue, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs specifically tailored for code generation. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program input-output specifications, is provided to the LLM to facilitate fine-tuning. We evaluated our approach on two challenging data science benchmarks, Arcade and DS-1000. Our results suggest that the method enhances the LLM's alignment with user intentions, considerably reducing the incidence of executable but incorrect outputs. Consequently, this leads to a marked improvement in the quality of generated code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=iX1RjVQODj": {
    "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning",
    "volume": "review",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. In contrast to prior work, this enables CPL to elegantly scale to high-dimensional and sequential RLHF problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HxHrRUHMOD": {
    "title": "Accurate Differential Operators for Neural Fields",
    "volume": "review",
    "abstract": "Neural fields have become widely used in various fields, from shape representation to neural rendering, and for solving partial differential equations (PDEs). With the advent of hybrid neural field representations like Instant NGP that leverage small MLPs and explicit representations, these models train quickly and can fit large scenes. Yet in many applications like rendering and simulation, hybrid neural fields can cause noticeable and unreasonable artifacts. This is because they do not yield accurate spatial derivatives needed for these downstream applications. In this work, we propose two ways to circumvent these challenges. Our first approach is a post hoc operator that uses local polynomial-fitting to obtain more accurate derivatives from pre-trained hybrid neural fields. Additionally, we also propose a self-supervised fine-tuning approach that refines the neural field to yield accurate derivatives directly while preserving the initial signal. We show the application of our method on rendering, collision simulation, and solving PDEs. We observe that using our approach yields more accurate derivatives, reducing artifacts and leading to more accurate simulations in downstream applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JDp3AQ2elP": {
    "title": "Revisiting Familiar Places in an Infinite World: Continuing RL in Unbounded State Spaces",
    "volume": "review",
    "abstract": "Deep reinforcement learning (RL) algorithms have been successfully applied to train neural network control policies for many sequential decision-making tasks. However, prior work has shown that neural networks are poor extrapolators and deep RL algorithms perform poorly with weakly informative cost signals. In this paper we show that these challenges are particularly problematic in real-world settings in which the state-space is unbounded and learning must be done without regular episodic resets. For instance, in stochastic queueing problems, the state space and cost can be unbounded and the agent may have to learn online without the system ever being reset to states the agent has seen before. In such settings, we show that deep RL agents can diverge into unseen states from which they can never recover, especially in highly stochastic environments. Towards overcoming this divergence, we introduce a Lyapunov-inspired reward shaping approach that encourages the agent to first learn to be stable (i.e. to achieve bounded cost) and then to learn to be optimal. We theoretically show that our reward shaping technique reduces the rate of divergence of the agent and empirically find that it prevents it. We further combine our reward shaping approach with a weight annealing scheme that gradually introduces the pursuit of optimality and a log-transform of state inputs, and find that these techniques enable deep RL algorithms to learn performant policies when learning online in unbounded state space domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=h8GeqOxtd4": {
    "title": "Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization",
    "volume": "review",
    "abstract": "Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded inputs, vector-valued outputs, and an additional time variable, preventing existing techniques from being applied directly. In this paper, we show that with a properly designed neural network architecture, the score function can be accurately approximated by a reproducing kernel Hilbert space induced by neural tangent kernels. Furthermore, by applying an early-stopping rule for gradient descent and leveraging certain coupling arguments between neural network training and kernel regression, we establish the first generalization error (sample complexity) bounds for learning the score function despite the presence of noise in the observations. Our analysis is grounded in a novel parametric form of the neural network and an innovative connection between score matching and regression analysis, facilitating the application of advanced statistical and optimization techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=v675Iyu0ta": {
    "title": "Interpretability Illusions in the Generalization of Simplified Models",
    "volume": "review",
    "abstract": "A common method to study deep learning systems is to create simplified representations---for example, using singular value decomposition to visualize the model's hidden states in a lower dimensional space. This approach assumes that the simplified model is faithful to the original model. Here, we illustrate an important caveat to this assumption: even if a simplified representation of the model can accurately approximate the original model on the training set, it may fail to match its behavior out of distribution; the understanding developed from simplified representations may be an illusion. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits, focusing on the Dyck balanced-parenthesis languages. We simplify these models using tools like dimensionality-reduction and clustering, and find clear patterns in the resulting representations. We then explicitly test how these simplified proxy models match the original models behavior on various out-of-distribution test sets. Generally, the simplified proxies are less faithful out of distribution. For example, in cases where the original model generalizes to novel structures or deeper depths, the simplified model may fail to generalize, or may generalize too well. We then show the generality of these results: even model simplifications that do not directly use data can be less faithful out of distribution, and other tasks can also yield generalization gaps. Our experiments raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=b9aCXHhdbv": {
    "title": "Pipeline Parallelism Optimization with Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "It has been widely observed that larger neural networks perform better in many real-world applications. While this scaling trend affirms the need to train a giant model across multiple devices, it is challenging to partition a model with millions of parameters to run efficiently and effectively on various devices deployed in a cluster of accelerators, e.g., GPUs and TPUs. Recently, a novel approach to training deep neural network (DNN) models distributedly has been proposed, pipeline parallelism. Compared with data parallelism, the existing works achieved a significant speed-up ratio even with a naive partition scheme. This paper presents a deep reinforcement learning (DRL)-based pipeline parallelism framework, DRL-PP, that learns to optimize the pipeline schedule for training large DNN models across multiple accelerators. The core of DRL-PP is a DRL agent consisting of a graph encoder, describing the semantics of an operator in the computational graph, followed by a recurrent model partitioner and a pipeline scheduler that learns to partition and place operations on various GPU devices automatically. In particular, by generating placement in a recurrent way, DRL-PP can partition DNN models in a more flexible and balanced manner, which improves accelerator utilization and speeds up DNN training. We deployed and extensively evaluated DRL-PP on various benchmarks. Compared with the state-of-the-art, DRL-PP can speed up the distributed training of benchmark models up to 6.8 and 1.3 over data parallelism and PipeDream, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=w4abltTZ2f": {
    "title": "Batched Low-Rank Adaptation of Foundation Models",
    "volume": "review",
    "abstract": "Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While \\lora/ offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To address this, we introduce FLORA (Fast LoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that \\flora/ retains the performance merits of \\lora/, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTddgL0lTQ": {
    "title": "ToolTalk: Evaluating Tool Usage in a Conversational Setting",
    "volume": "review",
    "abstract": "Large language models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Many recent works seek to augment LLM-based assistants with external tools so they can access private or up-to-date information and carry out actions on behalf of users. To better measure the performance of these assistants, this paper introduces ToolTalk, a benchmark consisting of complex user intents requiring multi-step tool usage specified through dialogue. ToolTalk contains 28 tools grouped into 7 plugins, and includes a complete simulated implementation of each tool, allowing for fully automated evaluation of assistants that rely on execution feedback. ToolTalk also emphasizes tools that externally affect the world rather than only tools for referencing or searching information. We evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and 50% respectively. Our analysis of the errors reveals three major categories and suggests some future directions for improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=SJZL5w4Iez": {
    "title": "Investigating the effective dimensionality of a model using a thermodynamic learning capacity",
    "volume": "review",
    "abstract": "We use a formal correspondence between thermodynamics and inference, where the number of samples can be thought of as the inverse temperature, to study a quantity called ``learning capacity'' which is a measure of the effective dimensionality of a model. We show that the learning capacity is a useful notion of the complexity because (a) it is a tiny fraction of the number of parameters for many deep networks trained on typical datasets and correlates well with the test loss, (b) it depends upon the number of samples used for training, (c) it is numerically consistent with notions of capacity obtained from PAC-Bayes generalization bounds, and (d) the test loss as a function of the learning capacity does not exhibit double descent. We show that the learning capacity saturates at very small and very large sample sizes; the threshold that characterizes the transition between these two regimes provides guidelines as to when one should procure more data and when one should search for a different architecture to improve performance. We show how the learning capacity can be used to provide a quantitative notion of capacity even for non-parametric models such as random forests and nearest neighbor classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=84Hk01tFKq": {
    "title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text",
    "volume": "review",
    "abstract": "We introduce HyperFields, a method for generating text-conditioned NeRFs with a single forward pass and (optionally) some finetuning. Key to our approach are: (i) a dynamic hypernetwork, which learns a smooth mapping from text token embeddings to the space of Neural Radiance Fields (NeRFs); (ii) NeRF distillation training, which distills scenes encoded in individual NeRFs into one dynamic hypernetwork. These techniques enable a single network to fit over a hundred unique scenes. We further demonstrate that HyperFields learns a more general map between text and NeRFs, and consequently is capable of predicting novel in-distribution and out-of-distribution scenes --- either zero-shot or with a few finetuning steps. Finetuning HyperFields benefits from accelerated convergence thanks to the learned general map, and is capable of synthesizing novel scenes 5 to 10 times faster than existing neural optimization-based methods. Our ablation experiments show that both the dynamic architecture and NeRF distillation are critical to the expressivity of HyperFields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xTFgpfIMOt": {
    "title": "Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment",
    "volume": "review",
    "abstract": "To succeed in the real world, robots must cope with situations that differ from those seen during training. We study the problem of adapting on-the-fly to such novel scenarios during deployment, by drawing upon a diverse repertoire of previously- learned behaviors. Our approach, RObust Autonomous Modulation (ROAM), introduces a mechanism based on the perceived value of pre-trained behaviors to select and adapt pre-trained behaviors to the situation at hand. Crucially, this adaptation process all happens within a single episode at test time, without any human supervision. We provide theoretical analysis of our selection mechanism and demonstrate that ROAM enables a robot to adapt rapidly to changes in dynamics both in simulation and on a real Go1 quadruped, even successfully moving forward with roller skates on its feet. Our approach adapts over 2x as efficiently compared to existing methods when facing a variety of out-of-distribution situations during deployment by effectively choosing and adapting relevant behaviors on-the-fly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=cI7WAadODh": {
    "title": "An Invex Relaxation Approach for Minimizing Polarization from Fully and Partially Observed Initial Opinions",
    "volume": "review",
    "abstract": "This paper investigates the problem of minimizing polarization within a network, operating under the foundational assumption that the evolution of underlying opinions adheres to the most prevalent model, the Friedkin-Johnson (FJ) model. We show that this optimization problem under integrality constraints is $\\mathcal{NP}$-Hard. Furthermore, we establish that the objective function fits into a specialized category of nonconvex functions called invex, where every local minimum is a global minimum. We extend this characterization to encompass a comprehensive class of matrix functions, including those pertinent to polarization and multiperiod polarization, even when addressing scenarios involving stubborn actors. We propose a novel nonconvex framework for this class of matrix functions with theoretical guarantees and demonstrate its practical efficacy for minimizing polarization without getting stuck at local minima. Through empirical assessments conducted in real-world network scenarios, our proposed approach consistently outperforms existing state-of-the-art methodologies. Moreover, we extend our work to encompass a novel problem setting that has not been previously studied, wherein the observer possesses access solely to a subset of initial opinions. Within this agnostic framework, we introduce a nonconvex relaxation methodology, which provides similar theoretical guarantees as outlined earlier and effectively mitigates polarization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMsmo01TaI": {
    "title": "The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning",
    "volume": "review",
    "abstract": "Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We consider simulations provided of both visual and tactile observations, namely, a robotic insertion environment, a door opening task, and dexterous in-hand manipulation, demonstrating the benefits of learning a multimodal policy. Videos of the experiments are available at https://m3l.site. Code will be released upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=5zwrpqYIK5": {
    "title": "Outlier-Robust Orthogonal Regression on Manifolds",
    "volume": "review",
    "abstract": "Motivated by machine learning and computer vision applications, we formulate the problem of Outlier-Robust Orthogonal Regression to find a point in a manifold that satisfies as many linear equations as possible. Existing approaches addressing special cases of our formulation either lack theoretical support, are computationally costly, or somewhat ignore the manifold constraint; the latter two limit them from many applications. In this paper, we propose a unified approach based on solving a non-convex and non-smooth $\\ell^1$ optimization problem over the manifold. We give conditions on the geometry of the input data, the manifold, and their interplay, under which the minimizers recover the ground truth; notably the conditions can hold even when the inliers are skewed within the true hyperplane. We provide a Riemannian subgradient method and an iteratively reweighted least squares method, suiting different computational oracles, and prove their linear/sub-linear convergence to minimizers/critical points. Experiments demonstrate that respecting the manifold constraints increases robustness against outliers in robust essential matrix estimation and robust rotation search",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=m0x0rv6Iwm": {
    "title": "Time-Varying Propensity Score to Bridge the Gap between the Past and Present",
    "volume": "review",
    "abstract": "Real-world deployment of machine learning models is challenging because data evolves over time. While no model can work when data evolves in an arbitrary fashion, if there is some pattern to these changes, we might be able to design methods to address it. This paper addresses situations when data evolves gradually. We introduce a time-varying propensity score that can detect gradual shifts in the distribution of data which allows us to selectively sample past data to update the model---not just similar data from the past like that of a standard propensity score but also data that evolved in a similar fashion in the past. The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Yr4RgiZ7P5": {
    "title": "Does resistance to style-transfer equal Shape Bias? Evaluating shape bias by distorted shape",
    "volume": "review",
    "abstract": "Deep learning models are known to exhibit a strong texture bias, while human tends to rely heavily on global shape for object recognition. The current benchmark for evaluating a model's shape bias is a set of style-transferred images with the assumption that resistance to the attack of style transfer is related to the development of shape sensitivity in the model. In this work, we show that networks trained with style-transfer images indeed learn to ignore style, but its shape bias arises primarily from local shapes. We provide a $\\textbf{Distorted Shape Testbench(DiST)}$ as an alternative measurement of global shape sensitivity. Our test includes 2400 original images from ImageNet-1K, each of which is accompanied by two images with the global shapes of the original image distorted while preserving its texture via the texture synthesis program. We found that (1) models that performed well on the previous shape bias evaluation do not fare well in the proposed DiST; (2) the widely adopted ViT models do not show significant advantages over Convolutional Neural Networks (CNNs) on this benchmark despite that ViTs rank higher on the previous shape bias tests. (3) training with DiST images bridges the significant gap between human and existing SOTA models' performance while preserving the model's accuracy on standard image classification tasks; training with DiST images and style-transferred images are complementary, and can be combined to train network together to enhance both the global and local shape sensitivity of the network. Our code will be host in the anonymous github: \\url{https://anonymous.4open.science/r/ICLR2024-DiST/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=XVhm3X8Fum": {
    "title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns",
    "volume": "review",
    "abstract": "Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=J7AwIJvR3d": {
    "title": "Discovering Divergences between Language Models and Human Brains",
    "volume": "review",
    "abstract": "Do machines and humans process language in similar ways? A recent line of research has hinted in the affirmative, demonstrating that human brain signals can be effectively predicted using the internal representations of language models (LMs). This is thought to reflect shared computational principles between LMs and human language processing. However, there are also clear differences in how LMs and humans acquire and use language, even if the final task they are performing is the same. Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative. In doing so we identify three phenomena that, in prior work, LMs have been found to not capture well: emotional understanding, figurative language processing, and physical commonsense. We further fine-tune models on datasets related to these three phenomena, and find that LMs fine-tuned on tasks related to emotion and figurative language show improved alignment with brain responses. We emphasize the importance of understanding not just similarities between human and machine language processing, but also differences. Our work takes the first steps toward this goal in the context of narrative reading",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=QHzzAU7Qf9": {
    "title": "Soft Merging of Experts with Adaptive Routing",
    "volume": "review",
    "abstract": "Neural networks that learn to route their inputs through different \"expert\" subnetworks provide a form of modularity that standard dense models lack. Despite their possible benefits, modular models with learned routing often underperform their parameter-matched dense counterparts as well as models that use non-learned heuristic routing strategies. In this paper, we hypothesize that these shortcomings stem from the gradient estimation techniques used to train modular models that use non-differentiable discrete routing decisions. To address this issue, we introduce $\\textbf{S}$oft $\\textbf{M}$erging of $\\textbf{E}$xperts with $\\textbf{A}$daptive $\\textbf{R}$outing (SMEAR), which avoids discrete routing by using a single \"merged\" expert constructed via a weighted average of all of the experts' parameters. By routing activations through a single merged expert, SMEAR does not incur a significant increase in computational costs and enables standard gradient-based training. We empirically validate that models using SMEAR outperform models that route based on metadata or learn routing through gradient estimation. Furthermore, we provide qualitative analysis demonstrating that the experts learned via SMEAR exhibit a significant amount of specialization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=jLIUfrAcMQ": {
    "title": "Debiasing Attention Mechanism in Transformer without Demographics",
    "volume": "review",
    "abstract": "Although transformers demonstrate impressive capabilities in a variety of tasks, the fairness issue remains a significant concern when deploying these models. Existing works to address fairness issues in transformers require sensitive labels (such as age, gender, etc.), which can raise privacy concerns or violate legal regulations. An alternative way is through fairness without demographics. However, existing works that improve Rawlsian Max-Min fairness may impose overly restrictive constraints. Other methods that use auxiliary networks could be parameter inefficient. In this paper, we present a new approach to debiasing transformers by leveraging their inherent structure. By reconsidering the roles of important components (queries, keys, and values) in the attention mechanism, we introduce a simple yet effective debiasing strategy from two perspectives: 1) Grounded in theoretical analysis, we normalize and apply absolute value operations to queries and keys to minimize the bias in attention weight allocation; 2) We reduce the bias within values through local alignment via contrastive learning. Throughout the entire process, our approach does not require any sensitive labels. Furthermore, to enhance memory efficiency in the training phase, we propose a strategy that debias only the last encoder to improve fairness in pre-trained models. We conduct experiments in computer vision and natural language processing tasks and show that our method is comparable and even outperforms the state-of-the-art method with substantially lower energy consumption",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=22OTbutug9": {
    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
    "volume": "review",
    "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=YH3tFtwuzb": {
    "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models",
    "volume": "review",
    "abstract": "We study the problem of differentially private (DP) fine-tuning of large pre-trained models — a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2 - 30X faster and uses 2 - 8X less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=BjG6McP5nA": {
    "title": "Improving Gradient-guided Nested Sampling for Posterior Inference",
    "volume": "review",
    "abstract": "We present a performant, general-purpose gradient-guided nested sampling (GGNS) algorithm, combining the state of the art in differentiable programming, Hamiltonian slice sampling, clustering, mode separation, dynamic nested sampling, and parallelization. This unique combination allows GGNS to scale well with dimensionality and perform competitively on a variety of synthetic and real-world problems. We also show the potential of combining nested sampling with generative flow networks to obtain large amounts of high-quality samples from the posterior distribution. This combination leads to faster mode discovery and more accurate estimates of the partition function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=mM7VurbA4r": {
    "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
    "volume": "review",
    "abstract": "*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rM9VJPB20F": {
    "title": "Like Oil and Water: Group Robustness Methods and Poisoning Defenses Don't Mix",
    "volume": "review",
    "abstract": "Group robustness has become a major concern in machine learning (ML) as conventional training paradigms were found to produce high error on minority groups. Without explicit group annotations, proposed solutions rely on heuristics that aim to identify and then amplify the minority samples during training. In our work, we first uncover a critical shortcoming of these methods: an inability to distinguish legitimate minority samples from poison samples in the training set. By amplifying poison samples as well, group robustness methods inadvertently boost the success rate of an adversary---e.g., from 0\\% without amplification to over 97\\% with it. Notably, we supplement our empirical evidence with an impossibility result proving this inability of a standard heuristic under some assumptions. Moreover, scrutinizing recent poisoning defenses both in centralized and federated learning, we observe that they rely on similar heuristics to identify which samples should be eliminated as poisons. In consequence, minority samples are eliminated along with poisons, which damages group robustness---e.g., from 55\\% without the removal of the minority samples to 41\\% with it. Finally, as they pursue opposing goals using similar heuristics, our attempt to alleviate the trade-off by combining group robustness methods and poisoning defenses falls short. By exposing this tension, we also hope to highlight how benchmark-driven ML scholarship can obscure the trade-offs among different metrics with potentially detrimental consequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=IYxDy2jDFL": {
    "title": "Improved Active Learning via Dependent Leverage Score Sampling",
    "volume": "review",
    "abstract": "We show how to obtain improved active learning methods in the agnostic (adversarial noise) setting by combining marginal leverage score sampling with non-independent sampling strategies that promote spatial coverage. In particular, we propose an easily implemented method based on the \\emph{pivotal sampling algorithm}, which we test on problems motivated by learning-based methods for parametric PDEs and uncertainty quantification. In comparison to independent sampling, our method reduces the number of samples needed to reach a given target accuracy by up to $50\\%$. We support our findings with two theoretical results. First, we show that any non-independent leverage score sampling method that obeys a weak \\emph{one-sided $\\ell_{\\infty}$ independence condition} (which includes pivotal sampling) can actively learn $d$ dimensional linear functions with $O(d\\log d)$ samples, matching independent sampling. This result extends recent work on matrix Chernoff bounds under $\\ell_{\\infty}$ independence, and may be of interest for analyzing other sampling strategies beyond pivotal sampling. Second, we show that, for the important case of polynomial regression, our pivotal method obtains an improved bound of $O(d)$ samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=WyEdX2R4er": {
    "title": "Visual Data-Type Understanding does not emerge from scaling Vision-Language Models",
    "volume": "review",
    "abstract": "Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domains pecific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic data-types, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. We will make our code available online upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNrFpDPMyo": {
    "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
    "volume": "review",
    "abstract": "In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=I0gwsdSgsk": {
    "title": "Memory Efficient Neural Processes via Constant Memory Attention Block",
    "volume": "review",
    "abstract": "Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that (1) is permutation invariant, (2) computes its output in constant memory, and (3) performs updates in constant computation. Building on CMAB, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires **constant** memory. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks (meta-regression and image completion) while being significantly more memory efficient than prior methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=z8Wva86JLB": {
    "title": "GEOFFair: a GEOmetric Framework for Fairness",
    "volume": "review",
    "abstract": "Fairness is a critical concern in Machine Learning, impacting its applications across domains. Existing fairness analyses often rely on complex mathematics, lacking of intuitive understanding. In this study, we introduce \\emph{GEOFFair}, a Geometric Framework for Fairness. It represents Machine Learning elements as vectors and sets, offering a more intuitive understanding of fairness related concepts. GEOFFair visualizes fairness mitigation techniques as vector projections, it provides a solid base to investigate the bias injection, aiding in constructing proofs, and it enables the study of fairness properties by means of geometric considerations. The main contribution of the work is to highlight GEOFFair's effectiveness in fairness studies, demonstrating that solely maximizing accuracy based on observed labels may not always be optimal for fairness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=FvfhHucpLd": {
    "title": "DIVERSITY OF THOUGHT IMPROVES REASONING ABILITIES OF LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break the problem into smaller reasoning steps (Wei et al., 2022b), or ensembling various generations through decoding alterations (Wang et al., 2023) boosts performance. Current approaches assume the input prompt is fixed and expect the decoding strategies introduce diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a methodology to automatically improve prompt diversity by soliciting feedback from the LLM. In our new prompting approach, DIV-SE (DIVerse reasoning path Self-Ensemble), we use these diverse prompts as part of an ensemble across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a single inference call; we call this IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Under a fixed generation budget, DIVSE and IDIV-SE generate more accurate results than the previously discussed baselines using both GPT-3.5 and GPT-4 on several reasoning benchmarks, without modifying the decoding process. Additionally, DIV-SE advances state-of-the-art performance on recent planning benchmarks (Valmeekam et al., 2022), exceeding the highest previously reported accuracy by at least 29.6 percentage points on the most challenging 4/5 blocks task in the Blocksworld problem. Our results shed light on how to enforce prompt diversity towards LLM reasoning without increasing the generation budget",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ORUiqcLpV6": {
    "title": "CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding",
    "volume": "review",
    "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned on an input utterance. Most existing methods devote the referring head to localize the referred object directly. However, this approach will fail in complex scenarios and not illustrate how and why the network reaches the final decision. In this paper, we address this question \"Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\". To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then utilizing them to pre- dict the final target. Following the chain of thoughts approach enables us to decom- pose the referring task into interpretable intermediate steps, which in turn, boosts the performance and makes our framework extremely data-efficient. Interpretabil- ity not only improves the overall performance but also helps us identify failure cases. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D and Sr3D benchmarks and show consistent performance gains compared to existing methods without requiring any manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8jKuUHsndT": {
    "title": "Re-evaluating Retrosynthesis Algorithms with Syntheseus",
    "volume": "review",
    "abstract": "The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=z6n1fKMMC1": {
    "title": "An Efficient Tester-Learner for Halfspaces",
    "volume": "review",
    "abstract": "We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan [2022]. In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution must pass the test. This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold. We consider the setting where the target distribution is the standard Gaussian in $d$ dimensions and the label noise is either Massart or adversarial (agnostic). For Massart noise, our tester-learner runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error $\\mathrm{opt}+\\epsilon$ (and extends to any fixed strongly log-concave target distribution). For adversarial noise, our tester-learner obtains error $O(\\mathrm{opt})+\\epsilon$ in polynomial time. Prior work on testable learning ignores the labels in the training set and checks that the empirical moments of the covariates are close to the moments of the base distribution. Here we develop new tests of independent interest that make critical use of the labels and combine them with the moment-matching approach of Gollakota et al. [2022]. This enables us to implement a testable variant of the algorithm of Diakonikolas et al. [2020a, 2020b] for learning noisy halfspaces using nonconvex SGD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=NGVljI6HkR": {
    "title": "Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces",
    "volume": "review",
    "abstract": "Recent works have introduced LEAPS and HPRL, systems that learn latent spaces of domain-specific languages, which are used to define programmatic policies for partially observable Markov decision processes (POMDPs). These systems induce a latent space while optimizing losses such as the behavior loss, which aim to achieve locality in program behavior, meaning that vectors close in the latent space should correspond to similarly behaving programs. In this paper, we show that the programmatic space, induced by the domain-specific language and requiring no training, presents values for the behavior loss similar to those observed in latent spaces presented in previous work. Moreover, algorithms searching in the programmatic space significantly outperform those in LEAPS and HPRL. To explain our results, we measured the ``friendliness'' of the two spaces to local search algorithms. We discovered that algorithms are more likely to stop at local maxima when searching in the latent space than when searching in the programmatic space. This implies that the optimization topology of the programmatic space, induced by the reward function in conjunction with the neighborhood function, is more conducive to search than that of the latent space. This result provides an explanation for the superior performance in the programmatic space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=fe8CzLTMG1": {
    "title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-Temporal Reasoning",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\\textbf{P}$ath $\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage ($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating \"path planning\" tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieved impressive results on in-distribution reasoning tasks, they struggled to generalize to larger environments or environments with more obstacles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MCjVArCAZ1": {
    "title": "Is Pre-training Truly Better Than Meta-Learning?",
    "volume": "review",
    "abstract": "n the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is high, MAML beats PT on average. The caveat is that the magnitude of the average difference between a PT vs. MAML using the effect size is low (according to classical statistical thresholds) -- less than 0.2. Nevertheless, this observation is contrary to the currently held belief that a pre-trained model is always better than a meta-learning model. Our extensive experiments consider 21 few-shot learning benchmarks, including the large-scale few-shot learning dataset Meta-Data set. We also show no significant difference between a MAML model vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a pre-trained model does not always beat a meta-learned model and that the formal diversity of a dataset is a driving factor",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ESSqkWnApz": {
    "title": "Fast and Reliable Generation of EHR Time Series via Diffusion Models",
    "volume": "review",
    "abstract": "Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time-series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=fmoknhh7CH": {
    "title": "Harmonic Prior Flow Matching for Multi-Ligand Docking and Binding Site Design",
    "volume": "review",
    "abstract": "A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon the state-of-the-art generative processes for docking in simplicity, generality, and performance. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches and provides the first general solution for binding site design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=506Sxc0Adp": {
    "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
    "volume": "review",
    "abstract": "Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to understand formal aspects of data \\textit{quality} that go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable and conjecture it can be used to build useful diverse datasets for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAEBTnTGo3": {
    "title": "JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning",
    "volume": "review",
    "abstract": "Join order selection (JOS) is the problem of ordering join operations to minimize total query execution cost and it is the core NP-hard combinatorial optimization problem of query optimization. In this paper, we present JoinGym, a lightweight and easy-to-use query optimization environment for reinforcement learning (RL) that captures both the left-deep and bushy variants of the JOS problem. Compared to existing query optimization environments, the key advantages of JoinGym are usability and significantly higher throughput which we accomplish by simulating query executions entirely offline. Under the hood, JoinGym simulates a query plan's cost by looking up intermediate result cardinalities from a pre-computed dataset. We release a novel cardinality dataset for $3300$ SQL queries based on real IMDb workloads which may be of independent interest, e.g., for cardinality estimation. Finally, we extensively benchmark four RL algorithms and find that their cost distributions are heavy-tailed, which motivates future work in risk-sensitive RL. In sum, JoinGym enables users to rapidly prototype RL algorithms on realistic database problems without needing to setup and run live systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=EpVe8jAjdx": {
    "title": "Privileged Sensing Scaffolds Reinforcement Learning",
    "volume": "review",
    "abstract": "We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon \"sensory scaffolding\": observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose _Scaffolder_, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new \"S3\" suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. _Scaffolder_ easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://sites.google.com/view/sensory-scaffolding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.5,
    "authors": []
  },
  "https://openreview.net/forum?id=w9tc699w3Z": {
    "title": "Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment",
    "volume": "review",
    "abstract": "We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language. Specifically, we train an image encoder for remote sensing images to align with the image encoder of CLIP using a large amount of paired internet and satellite images. Our unsupervised approach enables the training of a first-of-its-kind large scale VLM for remote sensing images at two different resolutions. We show that these VLMs enable zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question answering for satellite images. On each of these tasks, our VLM trained without textual annotations outperforms existing VLMs trained with supervision, with gains of up to 20\\% for classification and 80\\% for segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZWdgOvmAA": {
    "title": "LumiNet: The Bright Side of Perceptual Knowledge Distillation",
    "volume": "review",
    "abstract": "In knowledge distillation research, feature-based methods have dominated due to their ability to effectively tap into extensive teacher models. In contrast, logit-based approaches are considered to be less adept at extracting hidden 'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel knowledge-transfer algorithm designed to enhance logit-based distillation. We introduce a perception matrix that aims to recalibrate logits through adjustments based on the model's representation capability. By meticulously analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class relationships, enabling the student model to learn a richer breadth of knowledge. Both teacher and student models are mapped onto this refined matrix, with the student's goal being to minimize representational discrepancies. Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO) attests to LumiNet's efficacy, revealing its competitive edge over leading feature-based methods. Moreover, in exploring the realm of transfer learning, we assess how effectively the student model, trained using our method, adapts to downstream tasks. Notably, when applied to Tiny ImageNet, the transferred features exhibit remarkable performance, further underscoring LumiNet's versatility and robustness in diverse settings. With LumiNet, we hope to steer the research discourse towards a renewed interest in the latent capabilities of logit-based knowledge distillation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=C36v8541Ns": {
    "title": "The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing",
    "volume": "review",
    "abstract": "Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernstein's concentration inequality, along with an enhanced Lipschitz bound. Experimental results show a significant improvement in certified accuracy compared to current state-of-the-art methods. Our novel certification procedure allows us to use pre-trained models that are used with randomized smoothing, effectively improving the current certification radius in a zero-shot manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uf5EAGmkrN": {
    "title": "Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition",
    "volume": "review",
    "abstract": "We investigate phase transitions in a Toy Model of Superposition (TMS) \\citep{elhage2022superposition} using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=xnhvVtZtLD": {
    "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
    "volume": "review",
    "abstract": "In the field of algorithmic fairness, significant attention has been put on group fairness criteria, such as Demographic Parity and Equalized Odds. Nevertheless, these objectives, measured as global averages, have raised concerns about persistent local disparities between sensitive groups. In this work, we address the problem of local fairness, which ensures that the predictor is unbiased not only in terms of expectations over the whole population, but also within any subregion of the feature space, unknown at training time. To enforce this objective, we introduce ROAD, a novel approach that leverages the Distributionally Robust Optimization (DRO) framework within a fair adversarial learning objective, where an adversary tries to infer the sensitive attribute from the predictions. Using an instance-level re-weighting strategy, ROAD is designed to prioritize inputs that are likely to be locally unfair, i.e. where the adversary faces the least difficulty in reconstructing the sensitive attribute. Numerical experiments demonstrate the effectiveness of our method: it achieves Pareto dominance with respect to local fairness and accuracy for a given global fairness level across three standard datasets, and also enhances fairness generalization under distribution shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=1XReHUSUp9": {
    "title": "Monsters in the Dark: Sanitizing Hidden Threats with Diffusion Models",
    "volume": "review",
    "abstract": "Steganography is the art of hiding information in plain sight. This form of covert communication can be used by bad actors to propagate malware, exfiltrate victim data, and communicate with other bad actors. Current image steganography defenses rely upon steganalysis, or the detection of hidden messages. These methods, however, are non-blind as they require information about known steganography techniques and are easily bypassed. Recent work has instead focused on a defense mechanism known as sanitization, which eliminates hidden information from images. In this work, we introduce a novel blind deep learning steganography sanitization method that utilizes a diffusion model framework to sanitize universal and dependent steganography (DM-SUDS), which both sanitizes and preserves image quality. We evaluate this approach against state-of-the-art deep learning sanitization frameworks and provide further detailed analysis through an ablation study. DM-SUDS outperforms previous sanitization methods and improves image preservation MSE by 71.32\\%, PSNR by 22.43\\% and SSIM by 17.30\\%. This is the first blind deep learning image sanitization framework to meet these image quality results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=JAKcnjzQI3": {
    "title": "MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective",
    "volume": "review",
    "abstract": "The growing richness of large-scale datasets has been a crucial driving force behind the rapid advancement and wide adoption of machine learning technologies. The massive collection and usage of data, however, pose an increasing risk for people's private and sensitive information due to either inadvertent mishandling or malicious exploitation. Besides legislative solutions, many technical approaches have been proposed towards data privacy protection. However, they bear various limitations such as leading to degraded data availability and utility, or relying on heuristics and lacking solid theoretical bases. To overcome these limitations, we propose a formal information-theoretic definition for this utility-preserving privacy protection problem, and design a data-driven learnable data transformation framework that is capable of selectively suppressing sensitive attributes from target datasets while preserving the other useful attributes, regardless of whether or not they are known in advance or explicitly annotated for preservation. We provide rigorous theoretical analyses on the operational bounds for our framework, and carry out comprehensive experimental evaluations using datasets of a variety of modalities, including facial images, voice audio clips, and human activity motion sensor signals. Results demonstrate the effectiveness and generalizability of our method on different tasks and configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0BqyZSWfzo": {
    "title": "One-shot Empirical Privacy Estimation for Federated Learning",
    "volume": "review",
    "abstract": "Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss insettings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowl-edge of intermediate model iterates or the training data distribution), are tailored to specific tasks, model architectures, or DP algorithm, and/or require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federatedsettings where model training can take days or weeks. In this work, we present a novel \"one-shot\" approach that can systematically address these challenges, al-lowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring anyaprioriknowledge about the model architecture, task, or DP algorithm. We show that our method provides provably correct estimates for the privacy loss under the Gaussian mechanism, and we demonstrate its performance on a well-established FL benchmark dataset under several adversarial threat models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3UWuFoksGb": {
    "title": "Learning Planning Abstractions from Language",
    "volume": "review",
    "abstract": "This paper presents a framework for learning state and action abstractions in sequential decision-making domains. Our framework, planning abstraction from language (PARL), utilizes language-annotated demonstrations to automatically discover a symbolic and abstract action space and induce a latent state abstraction based on it. PARL consists of three stages: 1) recovering object-level and action concepts, 2) learning state abstractions, abstract action feasibility, and transition models, and 3) applying low-level policies for abstract actions. During inference, given the task description, PARL first makes abstract action plans using the latent transition and feasibility functions, then refines the high-level plan using low-level policies. PARL generalizes across scenarios involving novel object instances and environments, unseen concept compositions, and tasks that require longer planning horizons than settings it is trained on",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=y3qpL2Ioys": {
    "title": "Towards Neural Architecture Search through Hierarchical Generative Modeling",
    "volume": "review",
    "abstract": "Neural Architecture Search (NAS) is gaining popularity in automating designing deep neural networks for various tasks. A typical NAS pipeline begins with a manually designed search space which is methodically explored during the process, aiding the discovery of high-performance models. Although NAS has shown impressive results in many cases, the strong performance remains largely dependent on, among other things, the prior knowledge about good designs which is implicitly incorporated into the process by carefully designing search spaces. In general, this dependency is undesired, as it limits the applicability of NAS to less-studied tasks and/or results in an explosion of the cost needed to obtain strong results. In this work, our aim is to address this limitation by leaning on the recent advances in generative modelling -- we propose a method that can navigate an extremely large, general-purpose search space efficiently, by training a two-level hierarchy of generative models. The first level focuses on micro-cell design and leverages Conditional Continuous Normalizing Flow (CCNF) and the subsequent level uses a transformer-based sequence generator to produce macro architectures for a given task and architectural constraints. To make the process computationally feasible, we perform task-agnostic pretraining of the generative models using a metric space of graphs and their zero-cost (ZC) similarity. We evaluate our method on typical tasks, including CIFAR-10, CIFAR-100 and ImageNet models, where we show state-of-the-art performance compared to other low-cost NAS approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=rvUq3cxpDF": {
    "title": "Learning to Act without Actions",
    "volume": "review",
    "abstract": "Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in several domains, including language and vision. However, this paradigm has not yet taken hold in deep reinforcement learning (RL). This gap is due to the fact that the most abundant form of embodied behavioral data on the web consists of videos, which do not include the action labels required by existing methods for training policies from offline data. We introduce Latent Action Policies from Observation (LAPO), a method to infer latent actions and, consequently, latent-action policies purely from action-free demonstrations. Our experiments on challenging procedurally-generated environments show that LAPO can act as an effective pre-training method to obtain RL policies that can then be rapidly fine-tuned to expert-level performance. Our approach serves as a key stepping stone to enabling the pre-training of powerful, generalist RL models on the vast amounts of action-free demonstrations readily available on the web",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oaTkYHPINY": {
    "title": "Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new --but often individual-- downstream tasks. Thus, how one would expand prompt tuning to handle --concomitantly-- heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of Mixture of Prompts, or MoPs, associated with smart gating functionality: the latter --whose design is one of the contributions of this paper-- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied --for efficiency reasons-- as well as instruction data source and task composition. In practice, MoPs can simultaneously mitigate prompt training \"interference'' in multi-task, multi-source scenarios (e.g., task and data heterogeneity across sources), as well as possible implications from model approximations. As a highlight, MoPs manage to decrease final perplexity from $\\sim20$% up to $\\sim70$%, as compared to baselines, in the federated scenario, and from $\\sim 3$% up to $\\sim30$% in the centralized scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OQccFglTb5": {
    "title": "FT-SHIELD: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "Text-to-image generative models based on latent diffusion models (LDM) have demonstrated their outstanding ability in generating high-quality and high-resolution images according to language prompt. Based on these powerful latent diffusion models, various fine-tuning methods have been proposed to achieve the personalization of text-to-image diffusion models such as artistic style adaptation and human face transfer. However, the unauthorized usage of data for model personalization has emerged as a prevalent concern in relation to copyright violations. For example, a malicious user may use the fine-tuning technique to generate images which mimic the style of a painter without his/her permission. In light of this concern, we have proposed FT-Shield, a watermarking approach specifically designed for the fine-tuning of text-to-image diffusion models to aid in detecting instances of infringement. We develop a novel algorithm for the generation of the watermark to ensure that the watermark on the training images can be quickly and accurately transferred to the generated images of text-to-image diffusion models. A watermark will be detected on an image by a binary watermark detector if the image is generated by a model that has been fine-tuned using the protected watermarked images. Comprehensive experiments were conducted to validate the effectiveness of FT-Shield",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=t8eO0CiZJV": {
    "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
    "volume": "review",
    "abstract": "Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (∼200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on three difficult question-answering datasets StrategyQA, QuaRel and OpenBookQA show that not only does MaRio improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised fine-tuning (SFT) baseline. Extensive human evaluations confirm that MaRio rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=EnXJfQqy0K": {
    "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
    "volume": "review",
    "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a **Co**operative **E**mbodied **L**anguage **A**gent *CoELA*, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that *CoELA* driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a *CoLLAMA* with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that *CoELA* communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://llm-co.github.io/CoELA/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TTonmgTT9X": {
    "title": "Fast Hyperboloid Decision Tree Algorithms",
    "volume": "review",
    "abstract": "Hyperbolic geometry is gaining traction in machine learning due to its capacity to effectively capture hierarchical structures in real-world data. Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial advantages and have consistently delivered state-of-the-art results across diverse applications. However, hyperbolic classifiers often grapple with computational challenges. Methods reliant on Riemannian optimization frequently exhibit sluggishness, stemming from the increased computational demands of operations on Riemannian manifolds. In response to these challenges, we present HyperDT, a novel extension of decision tree algorithms into hyperbolic space. Crucially, HyperDT eliminates the need for computationally intensive Riemannian optimization, numerically unstable exponential and logarithmic maps, or pairwise comparisons between points by leveraging inner products to adapt Euclidean decision tree algorithms to hyperbolic space. Our approach is conceptually straightforward and maintains constant-time decision complexity while mitigating the scalability issues inherent in high-dimensional Euclidean spaces. Building upon HyperDT, we introduce HyperRF, a hyperbolic random forest model. Extensive benchmarking across diverse datasets underscores the superior performance of these models, providing a swift, precise, accurate, and user-friendly toolkit for hyperbolic data analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.6,
    "authors": []
  },
  "https://openreview.net/forum?id=IzqZbNMZ0M": {
    "title": "Private Zeroth-Order Nonsmooth Nonconvex Optimization",
    "volume": "review",
    "abstract": "We introduce a new zeroth-order algorithm for private stochastic optimization on nonconvex and nonsmooth objectives. Given a dataset of size $M$, our algorithm ensures $(\\alpha,\\alpha\\rho^2/2)$-Renyi differential privacy and finds a $(\\delta,\\epsilon)$-stationary point so long as $M=\\tilde\\Omega(\\frac{d}{\\delta\\epsilon^3} + \\frac{d^{3/2}}{\\rho\\delta\\epsilon^2})$. This matches the optimal complexity found in its non-private zeroth-order analog. Notably, although the objective is not smooth, we have privacy ``for free'' when $\\rho \\ge \\sqrt{d}\\epsilon$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=EsiU7bNabf": {
    "title": "Approximate Clustering for Extracting Task Relationships in Multi-Instruction Tuning",
    "volume": "review",
    "abstract": "The development of language models involves the evaluation of a broad range of learning tasks. Recent work has shown that by using carefully designed instructions to teach a large transformer model, they can be fine-tuned on a wide range of downstream tasks. However, when the number of instructions increases, they can negatively interfere with each other if trained together. Existing works have relied on domain expertise and manual inspection to construct multi-instruction sets, which can be time-consuming and difficult to scale. To address this challenge, this paper develops a clustering algorithm to find groups of similar tasks based on a given set of task affinity scores. This is an NP-hard problem, and conventional algorithms such as spectral and Llyod's clustering are sensitive to variations in the scale of task losses. Our algorithm instead uses a semidefinite relaxation to maximize the average density of clusters and then rounds the solution with a threshold. We adaptively build the clusters by gradually adding tasks so that the affinities only need to be computed in the existing clusters. Then, we construct an evaluation benchmark to assess task grouping algorithms with verified group structures. The evaluation set includes 63 cases, spanning multitask instruction tuning, multi-instruction tuning, and in-context learning of multiple functions. We validate our algorithm on this evaluation set by showing that it recovers the group structure found by an exhaustive search. We also show that our approach improves performance over multi-instruction and soft-prompt tuning by up to 6\\% on several sentence classification and structure-to-text generative tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pB9XVRGVu0": {
    "title": "GeRA: Label-Efficient Geometrically Regularized Alignment",
    "volume": "review",
    "abstract": "Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process —potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs — we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our method in the domains of speech-text and image-text alignment. Our experiments demonstrate significant improvement in alignment quality compared to a variaty of leading baselines, especially with a small amount of paired data, using our proposed geometric regularization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2FAPahXyVh": {
    "title": "OptiMUS: Optimization Modeling Using mip Solvers and large language models",
    "volume": "review",
    "abstract": "Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MLIP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS is able to solve 67\\% more problems compared to a basic LLM prompting strategy. The code OptiMUS and the data for NLP4LP are available at \\href{https://anonymous.4open.science/r/nlp4lp-8F62/README.md}{https://anonymous.4open.science/r/nlp4lp-8F62/README.md}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=68k0KcHFrW": {
    "title": "Stochastic Unrolled Federated Learning",
    "volume": "review",
    "abstract": "Algorithm unrolling has emerged as a learning-based optimization paradigm that unfolds truncated iterative algorithms in trainable neural-network optimizers. We introduce Stochastic UnRolled Federated learning (SURF), a method that expands algorithm unrolling to a federated learning scenario. Our proposed method tackles two challenges of this expansion, namely the need to feed whole datasets to the unrolled optimizers to find a descent direction and the decentralized nature of federated learning. We circumvent the former challenge by feeding stochastic mini-batches to each unrolled layer and imposing descent constraints to mitigate the randomness induced by using mini-batches. We address the latter challenge by unfolding the distributed gradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolled architecture, which preserves the decentralized nature of training in federated learning. We theoretically prove that our proposed unrolled optimizer converges to a near-optimal region infinitely often. Through extensive numerical experiments, we also demonstrate the effectiveness of the proposed framework in collaborative training of image classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=WNSjteBJd9": {
    "title": "Who Leaked the Model? Tracking IP Infringers in Accountable Federated Learning",
    "volume": "review",
    "abstract": "Federated learning (FL) emerges as an effective collaborative learning framework to coordinate data and computation resources from massive and distributed clients in training. Such collaboration results in non-trivial intellectual property (IP) represented by the model parameters that should be protected and shared by the whole party rather than an individual user. Meanwhile, the distributed nature of FL endorses a malicious client the convenience to compromise IP through illegal model leakage to unauthorized third parties. To block such IP leakage, it is essential to make the IP identifiable in the shared model and locate the anonymous infringer who first leaks it. The collective challenges call for accountable federated learning, which requires verifiable ownership of the model and is capable of revealing the infringer's identity upon leakage. In this paper, we propose Decodable Unique Watermarking (DUW) for complying with the requirements of accountable FL. Specifically, before a global model is sent to a client in an FL round, DUW encodes a client-unique key into the model by leveraging a backdoor-based watermark injection. To identify the infringer of a leaked model, DUW examines the model and checks if the triggers can be decoded as the corresponding keys. Extensive empirical results show that DUW is highly effective and robust, achieving over 99% watermark success rate for Digits, CIFAR-10, and CIFAR-100 datasets under heterogeneous FL settings, and identifying the IP infringer with 100% accuracy even after common watermark removal attempts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=gZRfDWLlGY": {
    "title": "Exact Path Kernels Naturally Decompose Model Predictions",
    "volume": "review",
    "abstract": "This paper proposes a generalized exact path kernel gEPK which naturally decomposes model predictions into localized input gradients or parameter gradients. Many cutting edge out-of-distribution (OOD) detection methods are in effect projections onto a reduced representation of the gEPK parameter gradient subspace. This decomposition is also shown to map the significant modes of variation that define how model predictions depend on training input gradients at arbitrary test points. These local features are independent of architecture and can be directly compared between models. Furthermore this method also allows measurement of signal manifold dimension and can inform theoretically principled methods for OOD detection on pre-trained models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=3xHDeA8Noi": {
    "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training",
    "volume": "review",
    "abstract": "Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50\\% fewer steps, less total compute, and reduced wall-clock time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=yRrPfKyJQ2": {
    "title": "Conversational Drug Editing Using Retrieval and Domain Feedback",
    "volume": "review",
    "abstract": "Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=iHcTLIor0m": {
    "title": "Poly-View Contrastive Learning",
    "volume": "review",
    "abstract": "Contrastive learning typically matches pairs of related views among a number of unrelated negatives. These two related be generated (e.g. by augmentations) or occur naturally. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Jhu4dQv5rY": {
    "title": "Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm",
    "volume": "review",
    "abstract": "Contextual biasing refers to the problem of biasing the automatic speech recognition (ASR) systems towards rare entities that are relevant to the specific user or application scenarios. We propose algorithms for contextual biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During beam search, we boost the score of a token extension if it extends matching into a set of biasing phrases. Our method simulates the classical approaches often implemented in the weighted finite state transducer (WFST) framework, but avoids the FST language altogether, with careful considerations on memory footprint and efficiency on tensor processing units (TPUs) by vectorization. Without introducing additional model parameters, our method achieves significant word error rate (WER) reductions on biasing test sets by itself, and yields further performance gain when combined with a model-based biasing method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=MVe2dnWPCu": {
    "title": "A Probabilistic Framework for Modular Continual Learning",
    "volume": "review",
    "abstract": "Modular approaches that use a different composition of modules for each problem are a promising direction in continual learning (CL). However, searching through the large, discrete space of module compositions is challenging, especially because evaluating a composition's performance requires a round of neural network training. We address this challenge through a modular CL framework, PICLE, that uses a probabilistic model to cheaply compute the fitness of each composition, allowing PICLE to achieve both perceptual, few-shot and latent transfer. The model combines prior knowledge about good module compositions with dataset-specific information. We evaluate PICLE using two benchmark suites designed to assess different desiderata of CL techniques. Comparing to a wide range of approaches, we show that PICLE is the first modular CL algorithm to achieve perceptual, few-shot and latent transfer while scaling well to large search spaces, outperforming previous state-of-the-art modular CL approaches on long problem sequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bLpUtGyf9g": {
    "title": "Boundary Denoising for Video Activity Localization",
    "volume": "review",
    "abstract": "Video activity localization aims at understanding the semantic content in long, untrimmed videos and retrieving actions of interest. The retrieved action with its start and end locations can be used for highlight generation, temporal action detection, etc. Unfortunately, learning the exact boundary location of activities is highly challenging because temporal activities are continuous in time, and there are often no clear-cut transitions between actions. Moreover, the definition of the start and end of events is subjective, which may confuse the model. To alleviate the boundary ambiguity, we propose to study the video activity localization problem from a denoising perspective. Specifically, we propose an encoder-decoder model named DenosieLoc. During training, a set of action spans is randomly generated from the ground truth with a controlled noise scale. Then, we attempt to reverse this process by boundary denoising, allowing the localizer to predict activities with precise boundaries and resulting in faster convergence speed. Experiments show that DenosieLoc advances several video activity understanding tasks. For example, we observe a gain of +12.36% average mAP on the QV-Highlights dataset. Moreover, DenosieLoc achieves state-of-the-art performance on the MAD dataset but with much fewer predictions than others",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=cWiEN1plhJ": {
    "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
    "volume": "review",
    "abstract": "The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for such detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that generated the text to be detected at inference or detection time, which is often impractical. In light of these challenge, we pursue a fundamentally different approach not relying on samples from language models of concern at training time. Instead, we propose to leverage representations of writing style estimated from human-authored text. Indeed, we find that features effective at distinguishing among human authors are also effective at distinguishing human from machine authors, including state of the art large language models like Llama 2, ChatGPT, and GPT-4. Furthermore, given handfuls of examples composed by each of several specific language models of interest, our approach affords the ability to predict which model specifically generated a given document",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=PCm1oT8pZI": {
    "title": "Safe and Robust Watermark Injection with a Single OoD Image",
    "volume": "review",
    "abstract": "Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=L6L1CJQ2PE": {
    "title": "Massive Editing for Large Language Model via Meta Learning",
    "volume": "review",
    "abstract": "While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount (Hase et al., 2023b; Huang et al., 2023). For instance, Mitchell et al. (2022) mimics gradient accumulation to sum the parameter shifts together, which lacks statistical significance and is prone to cancellation effect. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameter using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2, and GPT-J (6B), across various knowledge-intensive NLP tasks, i.e., closed book fact-checking and question answering. Remarkably, MALMEN is capable of editing hundreds of times more facts than MEND (Mitchell et al., 2022) with the identical hyper-network architecture and outperforms editor specifically designed for GPT, i.e., MEMIT (Meng et al., 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RRKggDJxo2": {
    "title": "Real-time learning of decay trajectory of Higgs boson using reservoir-in-reservoir architecture",
    "volume": "review",
    "abstract": "Real-time learning of the decay trajectory in Higgs bosons as they interact in the Higgs Field is the key to understanding and furthering of the mass providing mechanism and particle interaction mechanism beyond the Standard model in particle physics. We propose a novel machine learning architecture called reservoir-in-reservoir, to learn this complex high dimensional weak and electromagnetic interaction model involving a large number of arbitrary parameters whose full understanding remains elusive to physicists, making it harder to handcraft features or represent in a closed-form equation. Reservoir-in-reservoir is a reservoir computing (RC) approach, where we built a large reservoir using a pool of small reservoirs that are individually specialized to learn patterns from discrete time samples of decay trajectory without any prior knowledge. Each small reservoir consists of a paired primary and secondary reservoir of recurrently-connected neurons, known as learner and generator, respectively, with a readout connected to the head. During the training phase, we activate the learner-generator pairs within the pool. Then we excite each learners with an unit impulse and individual time windows of the incoming system. We train the internal recurrent connections and readouts using a recursive least squares-based First-Order and Reduced Control Error (FORCE) algorithm. To enhance adaptability and performance, we implement a time-varying forgetting factor optimization during training. This optimization helps control the fading and adaptation of the covariance matrix based on variations in the incoming decay trajectory and patterns. This comprehensive training strategy aims to guarantee that the entire reservoir pool evolves in harmony with the desired output dynamics. We optimize hyper-parameters such as the number of learner-generator pairs within the pool, their network sizes, batch sizes, and the number of training trials. During testing, we excite the generators in the pool, with only an unit impulse, to mimic the dynamic system. We facilitate real-time learning by re-triggering the training process involving learner-generator pairs whenever the error rate exceeds a predefined threshold. We evaluate our reservoir-in-reservoir architecture using Higgs boson decay trajectories as detected in the Compact Muon Solenoid (CMS) detector of CERN's Large Hadron Collider (LHC). The reservoir pool is used to model the dynamics of momentum components (and transverse momentum) as Higgs boson decays into photons and leptons (electrons and muons) with invariant masses between 120-130 GeV. Our results indicate that reservoir-in-reservoir architecture is a well suited machine learning paradigm in learning dynamical systems such as Higgs boson decay",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RwI7ZEfR27": {
    "title": "BrainLM: A foundation model for brain activity recordings",
    "volume": "review",
    "abstract": "We introduce the Brain Language Model (BrainLM), a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings. Utilizing self-supervised masked-prediction training, BrainLM demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for the accurate prediction of clinical variables like age, anxiety, and PTSD as well as forecasting of future brain states. Critically, the model generalizes well to entirely new external cohorts not seen during training. In zero-shot inference mode, BrainLM can identify intrinsic functional networks directly from raw fMRI data without any network-based supervision during training. The model also generates interpretable latent representations that reveal relationships between brain activity patterns and cognitive states. Overall, BrainLM offers a versatile and interpretable framework for elucidating the complex spatiotemporal dynamics of human brain activity. It serves as a powerful \"lens\" through which massive repositories of fMRI data can be analyzed in new ways, enabling more effective interpretation and utilization at scale. The work demonstrates the potential of foundation models to advance computational neuroscience research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOb0xFwdpr": {
    "title": "On Sarcasm Detection with OpenAI GPT-based Models",
    "volume": "review",
    "abstract": "Sarcasm is a form of irony that requires readers or listeners to interpret its intended meaning by considering context and social cues. Machine learning classification models have long had difficulty detecting sarcasm due to its social complexity and contradictory nature. This paper explores the applications of the Generative Pretrained Transformer (GPT) models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4, in detecting sarcasm in natural language. It assesses the differences in sarcasm detection between GPT models with and without domain context, and tests fine-tuned and zero-shot models of different sizes. The GPT models were tested on the political and balanced (pol-bal) portion of the popular Self-Annotated Reddit Corpus (SARC 2.0) sarcasm dataset. In the fine-tuning case, the largest fine-tuned GPT-3 model achieves accuracy and $F_1$-score of 0.81, outperforming prior models. In the zero-shot case, the latest GPT-4 model yields an accuracy of 0.71 and $F_1$-score of 0.75. Other models score lower. Moreover, domain context does not enhance fine-tuning and reduce zero-shot performance. Additionally, a model's performance may improve or deteriorate with each release, highlighting the need to reassess performance after each release",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=VTF8yNQM66": {
    "title": "SWE-bench: Can Language Models Resolve Real-world Github Issues?",
    "volume": "review",
    "abstract": "Language models (LMs) have been improving rapidly, and today we lack benchmarks that are hard to solve but easy to evaluate. Coding is such a desired task, but existing coding benchmarks only feature self-contained problems solvable within tens of lines. Inspired by how real-world programmers code to fix bugs or ship new features, we introduce SWE-bench, a benchmark with 2,294 GitHub issues sourced from 12 popular Python repositories. Given a codebase and an issue description, an LM is tasked with editing the codebase to resolve the issue and pass all related tests. Our experiments show that both state-of-the-art proprietary LMs and our fine-tuned LM, SWE-Llama, can resolve only the simplest issues. For example, Claude 2 and GPT-4 solve a mere 3.6% and 1.3% of tasks respectively, even when provided with an oracle retriever. Through systematic analysis, we identify various factors underlying LM performances, such as the retrieval setup, codebase size, and issue complexity. We also identify key challenges for LMs to solve real-world software engineering problems, including understanding cross-file dependencies, localizing edit locations, and generating long and well-formatted patch files. SWE-bench shows that real-world software engineering is a diverse, challenging and sustainable testbed for evaluating a wide range of language model abilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=CtiFwPRMZX": {
    "title": "A simple connection from loss flatness to compressed representations in neural networks",
    "volume": "review",
    "abstract": "Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on the prior work of Ma and Ying, which shows how flatness (i.e., small eigenvalues of the loss Hessian) develops in late phases of learning and leads robustness to perturbations in network inputs. Moreover, we show there is no similarly direct connection between local dimensionality and sharpness, suggesting that this property may be controlled by different mechanisms than volume and hence may play a complementary role in neural representations. Overall, we advance a dual perspective on generalization in neural networks in both parameter and feature space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=XMaPp8CIXq": {
    "title": "Always-Sparse Training with Guided Stochastic Exploration",
    "volume": "review",
    "abstract": "The excessive computational requirements of modern artificial neural networks (ANNs) are posing limitations on the machines that can run them. Sparsification of ANNs is often motivated by time, memory and energy savings only during model inference, yielding no benefits during training. A growing body of work is now focusing on providing the benefits of model sparsification also during training. While these methods greatly improve the training efficiency, the training algorithms yielding the most accurate models still materialize the dense weights, or compute dense gradients during training. We propose an efficient, always-sparse training algorithm which improves the accuracy over previous methods. Additionally, our method has excellent scaling to larger and sparser models, supported by its linear time complexity with respect to the model width during training and inference. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG, and ViT models, and compare it against a range of sparsification methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1NHgmKqOzZ": {
    "title": "Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality",
    "volume": "review",
    "abstract": "Dataset distillation aims to reduce the time and memory requirement of training deep networks on large datasets by synthesizing a small number of synthetic images that can provide a similar generalization performance to that of the full data. Despite the recent efforts, existing dataset distillation methods suffer from a significant performance gap compared to training on the original data. In this work, we argue that distilling the entire data into one synthetic subset cannot achieve a superior generalization performance. This is because the training dynamics of deep networks drastically change during the training. Hence, multiple synthetic subsets are required to capture the training dynamics at different phases of training. To improve the distillation performance, we propose progressive dataset distillation (PDD), which synthesizes multiple small sets of synthetic images conditioned on the previous ones and trains the model on the union of the subsets generated so far. Our extensive experiments show that PDD can effectively improve the performance of existing dataset distillation methods by up to 4.3%. In addition, our method for the first time enable generating considerably larger synthetic datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=r9FsiXZxZt": {
    "title": "Object centric architectures enable efficient causal representation learning",
    "volume": "review",
    "abstract": "Causal representation learning has showed a variety of settings in which we can disentangle latent variables with identifiability guarantees (up to some reasonable equivalence class). Common to all of these approaches is the assumption that (1) the latent variables are represented as $d$-dimensional vectors, and (2) that the observations are the output of some injective generative function of these latent variables. While these assumptions appear benign, we show that when the observations are of multiple objects, the generative function is no longer injective and disentanglement fails in practice. We can address this failure by combining recent developments in object-centric learning and causal representation learning. By modifying the Slot Attention architecture (Locatello et al., 2020), we develop an object-centric architecture that leverages weak supervision from sparse perturbations to disentangle each object's properties. This approach is more data-efficient in the sense that it requires significantly fewer perturbations than a comparable approach that encodes to a Euclidean space and we show that this approach successfully disentangles the properties of a set of objects in a series of simple image-based disentanglement experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=tI3eqOV6Yt": {
    "title": "Adaptivity and Modularity for Efficient Generalization Over Task Complexity",
    "volume": "review",
    "abstract": "Can transformers generalize efficiently on problems that require dealing with examples with different levels of difficulty? We introduce a new task tailored to assess generalization over different complexities and present results that indicate that standard transformers face challenges in solving these tasks. These tasks are variations of pointer value retrieval previously introduced by Zhang et al. (2021). We investigate how the use of a mechanism for adaptive and modular computation in transformers facilitates the learning of tasks that demand generalization over the number of sequential computation steps (i.e., the depth of the computation graph). Based on our observations, we propose a transformer-based architecture called Hyper-UT, which combines dynamic function generation from hyper networks with adaptive depth from Universal Transformers. This model demonstrates higher accuracy and a fairer allocation of computational resources when generalizing to higher numbers of computation steps. We conclude that mechanisms for adaptive depth and modularity complement each other in improving efficient generalization concerning example complexity. Additionally, to emphasize the broad applicability of our findings, we illustrate that in a standard image recognition task, Hyper-UT's performance matches that of a ViT model but with considerably reduced computational demands (achieving over 70\\% average savings by effectively using fewer layers)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=760br3YEtY": {
    "title": "($\\texttt{PEEP}$) $\\textbf{P}$redicting $\\textbf{E}$nzym$\\textbf{e}$ $\\textbf{P}$romiscuity with its Molecule Mate – an Attentive Metric Learning Solution",
    "volume": "review",
    "abstract": "Annotating the functions of proteins (e.g., enzymes) is a fundamental challenge, due to their diverse functionalities and rapidly increased number of protein sequences in databases. Traditional approaches have limited capability and suffer from false positive predictions. Recent machine learning (ML) methods reach satisfactory prediction accuracy but still fail to generalize, especially for less-studied proteins and those with previously uncharacterized functions or promiscuity. To address these pain points, we propose a novel ML algorithm, PEEP, to predict enzyme promiscuity, which integrates biology priors of protein functionality to regularize the model learning. To be specific, at the input level, PEEP fuses the corresponding molecule into protein embeddings to gain their reaction information; at the model level, a tailored self-attention is leveraged to capture importance residues which we found are aligned with the active site in protein pocket structure; at the objective level, we embed functionality label hierarchy into metric learning objectives by imposing larger distance margin between proteins that have less functionality in common. PEEP is extensively validated on three public benchmarks, achieving up to 4.6%,3.1%,3.7% improvements on F-1 scores compared to existing methods. Moreover, it demonstrates impressive generalization to unseen protein sequences with unseen functionalities. Codes are included in the supplement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=J1djqLAa6N": {
    "title": "Efficient Score Matching with Deep Equilibrium Layers",
    "volume": "review",
    "abstract": "Score matching methods -- estimate probability densities without computing the normalization constant -- are particularly useful in deep learning. However, computational and memory costs of score matching methods can be prohibitive for high-dimensional data or complex models, particularly due to the derivatives or Hessians of the log density function appearing in the objective function. Some existing approaches modify the objective function to reduce the quadratic computational complexity for Hessian computation. However, the memory bottleneck of score matching methods remains for deep learning. This study improves the memory efficiency of score matching by leveraging deep equilibrium models. We provide a theoretical analysis of deep equilibrium models for scoring matching and applying implicit differentiation to higher-order derivatives. Empirical evaluations demonstrate that our approach enables the development of deep and expressive models with improved performance and comparable computational and memory costs over shallow architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ONhLaNbxVV": {
    "title": "Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining",
    "volume": "review",
    "abstract": "In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the \\textit{prototypical part network} (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, it often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the \\textit{reweighed, reselected, and retrained prototypical part network} (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The first two steps are reward-based reweighting and reselection, which align prototypes with human feedback. The final step is retraining to realign the model's features with the updated prototypes. We find that R3-ProtoPNet improves the overall consistency and meaningfulness of the prototypes, and maintains or improves individual model performance. When multiple trained R3-ProtoPNets are incorporated into an ensemble, we find an increase in interpretability and an increase in predictive performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=jYsowwcXV1": {
    "title": "A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization",
    "volume": "review",
    "abstract": "Large text-to-image models have revolutionized the ability to generate imagery using natural language. However, particularly unique or personal visual concepts, such as your pet, an object in your house, etc., will not be captured by the original model. This has led to interest in how to inject new visual concepts, bound to a new text token, using as few as 4-6 examples. Despite significant progress, this task remains a formidable challenge, particularly in preserving the subject's identity. While most researchers attempt to to address this issue by modifying model architectures, our approach takes a data-centric perspective, advocating the modification of data rather than the model itself. We introduce a novel regularization dataset generation strategy on both the text and image level; demonstrating the importance of a rich and structured regularization dataset (automatically generated) to prevent losing text coherence and better identity preservation. The better quality is enabled by allowing up to 5x more fine-tuning iterations without overfitting and degeneration. The generated renditions of the desired subject preserve even fine details such as text and logos; all while maintaining the ability to generate diverse samples that follow the input text prompt. Since our method focuses on data augmentation, rather than adjusting the model architecture, it is complementary and can be combined with prior work. We show on established benchmarks that our data-centric approach forms the new state of the art in terms of image quality, with the best trade-off between identity preservation, diversity, and text alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=zMvMwNvs4R": {
    "title": "Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models",
    "volume": "review",
    "abstract": "Text generation models are notoriously vulnerable to errors in the training data. With the wide-spread availability of massive amounts of web-crawled data becoming more commonplace, how can we enhance the robustness of models trained on a massive amount of noisy web-crawled text? In our work, we propose Error Norm Truncation (ENT), a robust enhancement method to the standard training objective that truncates noisy data. Compared to methods that only uses the negative log-likelihood loss to estimate data quality, our method provides a more accurate estimation by considering the distribution of non-target tokens, which is often overlooked by previous work. Through comprehensive experiments across language modeling, machine translation, and text summarization, we show that equipping text generation models with ENT improves generation quality over standard training and previous soft and hard truncation methods. Furthermore, we show that our method improves the robustness of models against two of the most detrimental types of noise in machine translation, resulting in an increase of more than 2 BLEU points over the MLE baseline when up to 50\\% of noise is added to the data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=97Dl82avFs": {
    "title": "Alt-Text with Context: Improving Accessibility for Images on Twitter",
    "volume": "review",
    "abstract": "In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative. We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks. We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation. We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=X2gjYmy77l": {
    "title": "Taming AI Bots: Controllability of Neural States in Large Language Models",
    "volume": "review",
    "abstract": "We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. We view large language models (LLMs) and their corresponding conversational interfaces (AI bots) as discrete-time dynamical systems evolving in the embedding space of (sub-)word tokens, where they are trivially controllable. However, we are not interested in controlling AI Bots to produce individual words but rather sequences, or sentences, that convey certain ''meanings''. To tackle the question of controllability in the space of meanings, we first describe how meanings are represented in an LLM: after pre-training, the LLM is a deterministic map from incomplete sequences of discrete tokens to an inner product space of discriminant vectors (''embeddings'') of the next token; after fine-tuning and reinforcement, the same LLM maps complete sequences to a vector space. Since no token follows the special end-of-sequence token during pre-training, that vector space can be co-opted to represent meanings and align them with human supervision during fine-tuning. Accordingly, ''meanings'' in trained LLMs can be viewed simply as equivalence classes of complete trajectories of tokens. Although rudimentary, this characterization of meanings is compatible with so-called deflationary theories in epistemology. More importantly, defining meanings as equivalence classes of sentences allows us to frame the key question as determining the controllability of a dynamical system evolving in the quotient space of discrete trajectories induced by the model itself, a problem that to the best of our knowledge has never been tackled before. To do so, we characterize a ``well trained LLM'' through conditions that are largely met by today's LLMs and show that, when restricted to the space of meanings, a well-trained AI bot is controllable under verifiable conditions. More precisely, we introduce a functional characterization of AI bots, and derive necessary and sufficient conditions for controllability. The fact that AI bots are controllable means that they can be designed to counteract adverse actions and avoid reaching undesirable states before their boundary is crossed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YPfmglNRU": {
    "title": "Defining Expertise: Applications to Treatment Effect Estimation",
    "volume": "review",
    "abstract": "Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and \"expertise\" is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise—particularly the type of expertise the decision-makers of a domain are likely to have—can be informative in designing and selecting methods for treatment effect estimation. We formally define two types of expertise, predictive and prognostic, and demonstrate empirically that: (i) the prominent type of expertise in a domain significantly influences the performance of different methods in treatment effect estimation, and (ii) it is possible to predict the type of expertise present in a dataset, which can provide a quantitative basis for model selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=BxPqibGUPR": {
    "title": "VibeSpace: Automatic vector embedding creation for arbitrary domains and mapping between them using large language models",
    "volume": "review",
    "abstract": "We present VibeSpace; a method for the fully unsupervised construction of interpretable embedding spaces applicable to arbitrary domain areas. By leveraging knowledge contained within large language models, our method automates otherwise costly data acquisition processes and assesses the similarity of entities, allowing for meaningful and interpretable positioning within vector spaces. Our approach is also capable of learning intelligent mappings between vector space representations of non-overlapping domains, allowing for a novel form of cross-domain similarity analysis. First, we demonstrate that our data collection methodology yields comprehensive and rich datasets across multiple domains, including songs, books, and movies. Second, we show that our method yields single-domain embedding spaces which are separable by various domain specific features. These representations provide a solid foundation upon which we can develop classifiers and initialise recommender systems, demonstrating our method's utility as a data-free solution to the cold-start problem. Further, these spaces can be interactively queried to obtain semantic information about different regions in embedding spaces. Lastly, we argue that by exploiting the unique capabilities of current state-of-the-art large language models, we produce cross-domain mappings which capture contextual relationships between heterogeneous entities which may not be attainable through traditional methods. The presented method facilitates the creation of embedding spaces of any domain which circumvents the need for collection and calibration of sensitive user data, as well as providing deeper insights and better interpretations of multi-domain data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBpSkFGVQU": {
    "title": "Depth-Guided Self-Supervised Learning: Seeing the World in 3D",
    "volume": "review",
    "abstract": "Self-Supervised Learning (SSL) methods operate on unlabeled data to learn robust representations useful for downstream tasks. Most SSL methods rely on augmentations obtained by transforming the 2D image pixel map. These augmentations ignore the fact that biological vision takes place in an immersive three-dimensional, temporally contiguous environment, and that low-level biological vision relies heavily on depth cues. Using a signal provided by a pretrained state-of-the-art monocular RGB-to-depth model (the Depth Prediction Transformer, Ranftl et al., 2021), we explore two distinct approaches to incorporating depth signals into the SSL framework. First, we evaluate self-supervised learning using an RGB+depth input representation. Second, we use the depth signal to generate novel views from slightly different camera positions, thereby producing a 3D augmentation for self-supervised learning. We also examine the combination of the two approaches. We evaluate the approaches on three different SSL methods---BYOL, SimSiam, and SwAV---using ImageNette (10 class subset of ImageNet), ImageNet-100 and ImageNet-1k datasets. We find that both approaches to incorporating depth signals improve the robustness and generalization of the baseline SSL methods, and the two approaches are complementary because the combination of depth and 3D views performs the best in most settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSD3MloKe6": {
    "title": "Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps",
    "volume": "review",
    "abstract": "Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the synthesis of high-quality images. However, their inference process characteristically requires numerous, potentially hundreds, of iterative steps, which could exaggerate the problem of exposure bias due to the training and inference discrepancy. Previous work has attempted to mitigate this issue by perturbing inputs during training, which consequently mandates the retraining of the DPM. In this work, we conduct a systematic study of exposure bias in DPM and, intriguingly, we find that the exposure bias could be alleviated with a novel sampling method that we propose, without retraining the model. We empirically and theoretically show that, during inference, for each backward time step $t$ and corresponding state $\\hat{x}_t$, there might exist another time step $t_s$ which exhibits superior coupling with $\\hat{x}_t$. Based on this finding, we introduce a sampling method named Time-Shift Sampler. Our framework can be seamlessly integrated to existing sampling algorithms, such as DDPM, DDIM and other high-order solvers, inducing merely minimal additional computations. Experimental results show our method brings significant and consistent improvements in FID scores on different datasets and sampling methods. For example, integrating Time-Shift Sampler to F-PNDM yields a FID=3.88, achieving 44.49\\% improvements as compared to F-PNDM, on CIFAR-10 with 10 sampling steps, which is more performant than the vanilla DDIM with 100 sampling steps. We will release the code upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=sb0ojNl7F6": {
    "title": "End-Effector-Elbow: A New Action Space for Robot Learning",
    "volume": "review",
    "abstract": "Joint control and end-effector control are the two most dominant control methods for robot arms within the robot learning literature. Joint control, while precise, often suffers from inefficient training; end-effector control boasts data-efficient training but sacrifices the ability to perform tasks in confined spaces due to limited control over the robot joint configuration. This paper introduces a novel action space formulation: End-Effector-Elbow (E3), which addresses the limitations of existing control paradigms by allowing the control of both the end-effector and elbow of the robot. E3 combines the advantages of both joint and end-effector control, offering fine-grained comprehensive control with overactuated robot arms whilst achieving highly efficient robot learning. E3 systematically outperforms other action spaces, when precise control over the robot configuration is required, both in simulated and real environments. Project website: https://doubleblind-repos.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGHJAyR8w0": {
    "title": "Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks",
    "volume": "review",
    "abstract": "Theoretical and empirical comparisons have been made to assess the expressive power and performance of invariant and equivariant GNNs. However, there is currently no theoretical result comparing the expressive power of $k$-hop invariant GNNs and equivariant GNNs. Additionally, little is understood about whether the performance of equivariant GNNs, employing steerable features up to type-$L$, increases as $L$ grows -- especially when the feature dimension is held constant. In this study, we introduce a key lemma that allows us to analyze steerable features by examining their corresponding invariant features. The lemma facilitates us in understanding the limitations of $k$-hop invariant GNNs, which fail to capture the global geometric structure due to the loss of geometric information between local structures. Furthermore, we investigate the invariant features associated with different types of steerable features and demonstrate that the expressiveness of steerable features is primarily determined by their dimension -- independent of their irreducible decomposition. This suggests that when the feature dimension is constant, increasing $L$ does not lead to essentially improved performance in equivariant GNNs employing steerable features up to type-$L$. We substantiate our theoretical insights with numerical evidence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=dALYqPm9gW": {
    "title": "Recurrent Linear Transformers",
    "volume": "review",
    "abstract": "The self-attention mechanism in the transformer architecture is capable of capturing long-range dependencies and it is the main reason behind its effectiveness in processing sequential data. Nevertheless, despite their success, transformers have two significant drawbacks that still limit their broader applicability: (1) In order to remember past information, the self-attention mechanism requires access to the whole history to be provided as context. (2) The inference cost in transformers is expensive. In this paper we introduce recurrent alternatives to the transformer self-attention mechanism that offer a context-independent inference cost, leverage long-range dependencies effectively, and perform well in practice. We evaluate our approaches in reinforcement learning problems where the aforementioned computational limitations make the application of transformers nearly infeasible. We quantify the impact of the different components of our architecture in a diagnostic environment and assess performance gains in 2D and 3D pixel-based partially-observable environments. When compared to a state-of-the-art architecture, GTrXL, inference in our approach is at least 40\\% cheaper while reducing memory use in more than 50\\%. Our approach either performs similarly or better than GTrXL, improving more than 37\\% upon GTrXL performance on harder tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=SZErAetdMu": {
    "title": "Time Series Modeling at Scale: A Universal Representation Across Tasks and Domains",
    "volume": "review",
    "abstract": "Time series are ubiquitous, capturing real-world phenomena ranging from human neuronal firing and tectonic activity to atmospheric conditions. However, they are challenging to analyze due to domain-specific timescales (e.g., sub-second for brain activity and years for weather phenomena), complex multivariate relations, and disparate modeling objectives. Prior works model time series by targeting specific tasks, like forecasting, or distinct domains, like neural recordings. We introduce a universal approach for scalable time series modeling across many tasks and domains, which we call TOTEM: Tokenized Time Series Embeddings. We propose a task-agnostic embedding that projects a continuous time series of any length onto a discrete set of learned tokens. This embedding is derived by optimizing a self-supervised objective formulated as a task-independent convolution-based vector quantized variational autoencoder. Drawing inspiration from the recent successes of Large Language Models, these discrete token sequences are then used to learn downstream models with the powerful Transformer architecture. We show that TOTEM matches or achieves SOTA performance on forecasting, classification, and translation tasks with data drawn from a myriad of domains: neuroscience, seismology, meteorology, power grids, and urban traffic. We further demonstrate TOTEM's scalability by introducing and evaluating it on new datasets, the largest being ∼14× larger than existing benchmarks. Finally, we illustrate TOTEM's dominant zero-shot generalization capabilities across all of our downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5eLgTLusaR": {
    "title": "Loco3D: Indoor Multiuser Locomotion 3D Dataset",
    "volume": "review",
    "abstract": "In the context of human-AI interaction, modeling human actions is a critical and challenging endeavor, with locomotion being a particularly fundamental behavior for AI agents to understand. Modeling human trajectories in complex indoor scenes, such as the home environment, requires an understanding of how humans interact with their surroundings and other humans. These interactions are influenced by a range of factors, including the geometry and semantics of the scene, the socio-cultural context, and the task each human needs to perform. Previous research has shared datasets containing human motion and scene structure in indoor scenes, but these datasets are limited in scale due to the difficulty and time required to collect data at different locations. To solve the scale problem, we propose to use a virtual reality (VR) system to build a human motion dataset. Specifically, we present Loco3D, a dataset of multi-person interactions in over 100 different indoor VR scenes, including 3D body pose data and highly accurate spatial information. The dataset can be used for building AI agents that operate in indoor environments, such as home robots, or to create virtual avatars for games or animations that mimic human movement and posture. With an initial evaluation, we demonstrate that models trained with our dataset have improved multi-person trajectory synthesis performance on real-world data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=z3L59iGALM": {
    "title": "Massively Scalable Inverse Reinforcement Learning for Route Optimization",
    "volume": "review",
    "abstract": "Optimizing for humans' latent preferences remains a grand challenge in route recommendation. Prior research has provided increasingly general methods based on inverse reinforcement learning (IRL), yet no approach has successfully addressed planetary-scale routing problems with hundreds of millions of states and demonstration trajectories. In this paper, we introduce scaling techniques based on graph compression, spatial parallelization, and improved initialization conditions inspired by a connection to eigenvector algorithms. We revisit classic algorithms in the routing context, and make the key observation that there exists a trade-off between the use of cheap, deterministic planners and expensive yet robust stochastic policies. This insight is leveraged in Receding Horizon Inverse Planning (RHIP), a new generalization of classic IRL algorithms that provides fine-grained control over performance trade-offs via its planning horizon. Our contributions culminate in a policy that achieves a 16-24% improvement in route quality at a global scale, and to the best of our knowledge, represents the largest published benchmark of IRL algorithms in a real-world setting to date. We conclude by conducting an ablation study of key components, presenting negative results from alternative eigenvalue solvers, and identifying opportunities to further improve scalability via IRL-specific batching strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=O1lR4vSw5x": {
    "title": "RECURSIVE NEURAL ORDINARY DIFFERENTIAL EQUATIONS FOR PARTIALLY OBSERVED SYSTEM",
    "volume": "review",
    "abstract": "Identifying spatiotemporal dynamics is a difficult task, especially in scenarios where latent states are partially observed and/or represent physical quantities. In this context, first-principle ordinary differential equation (ODE) systems are often designed to describe the system's dynamics. In this work, we address the problem of learning parts of the spatiotemporal dynamics with neural networks when only partial information about the system's state is available. Taking inspiration from recursive state estimation and Neural ODEs, we outline a general framework in which complex dynamics generated by differential equations with distinguishable states can be learned in a principled way. We demonstrate the performance of the proposed approach leveraging both numerical simulations and a real dataset extracted from an electro-mechanical positioning system. We show how the underlying equations fit into our formalism and demonstrate the improved performance of the proposed method when compared with standard baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rzF0R6GOd4": {
    "title": "Neural SDF Flow for 3D Reconstruction of Dynamic Scenes",
    "volume": "review",
    "abstract": "In this paper, we tackle the problem of 3D reconstruction of dynamic scenes from multi-view videos. Previous works attempt to model the motion of 3D points in space, which either constrains them to handle a single articulated object or requires extra efforts to handle topology changes. By contrast, we propose to directly estimate the change of Signed Distance Function (SDF), namely SDF flow, of the dynamic scene. We show that the SDF flow captures the evolution of the scene surface and handles topology changes naturally. We further derive the mathematical relation between the SDF flow and the scene flow, which allows us to calculate the scene flow from the SDF flow analytically by solving linear equations. Our experiments on real-world multi-view video datasets show that our reconstructions are better than those of the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mliQ2huFrZ": {
    "title": "Class Probability Matching with Calibrated Networks for Label Shift Adaption",
    "volume": "review",
    "abstract": "We consider the domain adaptation problem in the context of label shift, where the label distributions between source and target domain differ, but the conditional distributions of features given the label are the same. To solve the label shift adaption problem, we develop a novel matching framework named \\textit{class probability matching} (\\textit{CPM}). It is inspired by a new understanding of the source domain's class probability, as well as a specific relationship between class probability ratios and feature probability ratios between the source and target domains. CPM is able to maintain the same theoretical guarantee with the existing feature probability matching framework, while significantly improving the computational efficiency due to directly matching the probabilities of the label variable. Within the CPM framework, we propose an algorithm named \\textit{class probability matching with calibrated networks} (\\textit{CPMCN}) for target domain classification. From the theoretical perspective, we establish the generalization bound of the CPMCN method in order to explain the benefits of introducing calibrated networks. From the experimental perspective, real data comparisons show that CPMCN outperforms existing matching-based and EM-based algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=eJHnSg783t": {
    "title": "DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation",
    "volume": "review",
    "abstract": "We introduce DIFFTACTILE, a physics-based and fully differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically-accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM) -based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, plastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap, and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. The source codes of DIFFTACTILE will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=VLFhbOCz5D": {
    "title": "Tangent Transformers for Composition,Privacy and Removal",
    "volume": "review",
    "abstract": "We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yV6wwEbtkR": {
    "title": "Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information",
    "volume": "review",
    "abstract": "It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. In fact, maximizing the teacher's CMI value ensures that the teacher can effectively capture the contextual information within the images, and for visualizing this information, we deploy Eigen-CAM. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-the-art KD frameworks, the student's classification accuracy consistently increases, with the gain of up to 3.32\\%. This suggests that the teacher's BCPD estimate provided by MCMI method is more accurate than that provided by MLL method. In addition, we show that such improvements in the student's accuracy are more drastic in zero-shot and few-shot settings. Notably, the student's accuracy increases with the gain of up to 5.72\\% when 5\\% of the training samples are available to student (few-shot), and increases from 0\\% to as high as 84\\% for an omitted class (zero-shot)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=RIu5lyNXjT": {
    "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
    "volume": "review",
    "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=wUaOVNv94O": {
    "title": "AUTOMATIC NEURAL SPATIAL INTEGRATION",
    "volume": "review",
    "abstract": "Spatial integration is essential for a number of scientific computing applications, such as solving Partial Differential Equations. Numerically computing a spatial integration is usually done via Monte Carlo methods, which produce accurate and unbiased results. However, they can be slow since it require evaluating the integration many times to achieve accurate low-variance results. Recently, researchers have proposed to use neural networks to approximate integration results. While networks are very fast to infer in test-time, they can only approximate the integration results and thus produce biased estimations. In this paper, we propose to combine these two complementary classes of methods to create a fast and unbiased estimator. The key idea is instead of relying on the neural network's approximate output directly, we use the network as a control variate for the Monte Carlo estimator. We propose a principal way to construct such estimators and derive a training object that can minimize its variance. We also provide preliminary results showing our proposed estimator can both reduce the variance of Monte Carlo PDE solvers and produce unbiased results in solving Laplace and Poisson equations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzpWBbnwiJ": {
    "title": "Universal Guidance for Diffusion Models",
    "volume": "review",
    "abstract": "Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, style guidance and classifier signals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=osoWxY8q2E": {
    "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Io0Q37X5fP": {
    "title": "Counterfactual Generative Models for Time-Varying Treatments",
    "volume": "review",
    "abstract": "Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability weighting. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KsUh8MMFKQ": {
    "title": "Thin-Shell Object Manipulations With Differentiable Physics Simulations",
    "volume": "review",
    "abstract": "In this work, we aim to teach robots to manipulate various thin-shell materials. Prior works studying thin-shell object manipulation mostly rely on heuristic policies or learn policies from real-world video demonstrations, and only focus on limited material types and tasks (e.g., cloth unfolding). However, these approaches face significant challenges when extended to a wider variety of thin-shell materials and a diverse range of tasks. On the other hand, while virtual simulations are shown to be effective in diverse robot skill learning and evaluation, prior thin-shell simulation environments only support a subset of thin-shell materials, which also limits their supported range of tasks. To fill in this gap, we introduce ThinShellLab - a fully differentiable simulation platform tailored for robotic interactions with diverse thin-shell materials possessing varying material properties, enabling flexible thin-shell manipulation skill learning and evaluation. Building on top of our developed simulation engine, we design a diverse set of manipulation tasks centered around different thin-shell objects. Our experiments suggest that manipulating thin-shell objects presents several unique challenges: 1) thin-shell manipulation relies heavily on frictional forces due to the objects' co-dimensional nature, 2) the materials being manipulated are highly sensitive to minimal variations in interaction actions, and 3) the constant and frequent alteration in contact pairs makes trajectory optimization methods susceptible to local optima, and neither standard reinforcement learning algorithms nor trajectory optimization methods (either gradient-based or gradient-free) are able to solve the tasks alone. To overcome these challenges, we present an optimization scheme that couples sampling-based trajectory optimization and gradient-based optimization, boosting both learning efficiency and converged performance across various proposed tasks. In addition, the differentiable nature of our platform facilitates a smooth sim-to-real transition. By tuning simulation parameters with a minimal set of real-world data, we demonstrate successful deployment of the learned skills to real-robot settings. ThinShellLab will be publicly available. Video demonstration and more information can be found on the project website https://thinshelllab.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=L4nOxziGf9": {
    "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
    "volume": "review",
    "abstract": "An increasing number of vision-language tasks can be handled with little to no training (i.e., in a zero and few-shot manner) by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides (e.g., not requiring training data or custom architectures), how an input is presented to a LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. To this end, we present **Rep**hrase, **A**ugment and **Re**ason (RepARe), a gradient-free framework, which extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on two visual question answering tasks, we show that RepARe can result in an 3.85 percentage point (absolute) increase in zero-shot performance on VQAv2 and a 6.41 point increase on A-OKVQA. Additionally, we find that using gold answers for oracle selection of question candidates achieves an impressive gain in VQA accuracy by up to 14.41 percentage points. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity and better utilize the frozen language model in LVLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=eojWsJQ2fe": {
    "title": "Prompt Engineering a Prompt Engineer",
    "volume": "review",
    "abstract": "Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models (LLMs). It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task clearly to the LLM. While recent works indicate that LLMs can be meta-prompted to perform automatic prompt engineering, their potentials are not fully unlocked as the meta-prompts may not offer sufficient guidance to elicit complex reasoning capabilities in LLMs. In this work, we investigate the problem of \"prompt engineering a prompt engineer\"---constructing a meta-prompt that more effectively guides LLMs to perform prompt engineering. We introduce and analyze key components, such as a step-by-step reasoning template and context specification, which leads to improved performance on automatic prompt engineering. The resulting method, named PE2, finds a prompt that outperforms ``let's think step by step'' by 6.3\\% on the MultiArith dataset and 3.1\\% on the GSM8K dataset. To demonstrate its versatility, we apply PE2 to the Instruction Induction benchmark, a suite of counterfactual tasks, and a real-world industrial prompt. In these settings, PE2 achieves strong performance and outperforms prior automatic prompt engineering baselines. Further, we show that PE2 makes meaningful and targeted prompt edits, amends erroneous or incomplete prompts, and presents non-trivial counterfactual reasoning abilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=LWuYsSD94h": {
    "title": "A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\\widetilde{O}\\left(\\Delta^{1/4}T^{3/4}\\right)$ regret when the degree of nonstationarity, as measured by total variation $\\Delta$, is known, and $\\widetilde{O}\\left(\\Delta^{1/5}T^{4/5}\\right)$ regret when $\\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, our algorithm inherits the favorable dependence on number of agents from the oracles. As a side contribution that may be independent of interest, we show how to test for various types of equilibria by a black-box reduction to single-agent learning, which includes Nash equilibria, correlated equilibria, and coarse correlated equilibria",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ssOs9BBxa": {
    "title": "A Competition Winning Deep Reinforcement Learning Agent in microRTS",
    "volume": "review",
    "abstract": "Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. \\agentName\\ is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, \\agentName\\ regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to \\agentName's winning performance. These strategies can be used in economically training future DRL agents. Further work in Imitation Learning using Behavior Cloning and fine-tuning these models with DRL has proven promising as an efficient way to bootstrap models with novel behaviors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=EHKS0oXuku": {
    "title": "Jensen-Shannon Divergence Based Novel Loss Functions for Bayesian Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i4eDGZFcva": {
    "title": "Reward Centering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zap3nZhRIQ": {
    "title": "Three ways that non-differentiability affects neural network training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VKEJKKLvr": {
    "title": "A GRAPH-BASED REPRESENTATION LEARNING APPROACH FOR BREAST CANCER RISK PREDICTION USING GENOTYPE DATA",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jnZtTUdWyi": {
    "title": "Adaptive Invariant Representation Learning for Non-stationary Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REKRLIXtQG": {
    "title": "Supermodular Rank: Set Function Decomposition and Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tGGWOijvq": {
    "title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0kaVlC5ue": {
    "title": "Spectral Neural Networks: Approximation Theory and Optimization Landscape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fZZ4ubttru": {
    "title": "GenBot: Generative Simulation Empowers Automated Robotic Skill Learning at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=smy4DsUbBo": {
    "title": "Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LyNsMNNLjY": {
    "title": "Large Language Model Routing with Benchmark Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0kvrymILfy": {
    "title": "Making Predictors More Reliable with Selective Recalibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g0mlwqs8pi": {
    "title": "Adaptive Federated Learning with Auto-Tuned Clients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=27YiINkhw3": {
    "title": "ToolDec: Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71mqtQdKB9": {
    "title": "Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7erlRDoaV8": {
    "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6tazBqPem3": {
    "title": "Capacity Analysis of Vector Symbolic Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GOiEdLIgVF": {
    "title": "Saliency-Guided Hidden Associative Replay for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SznHfMwmjG": {
    "title": "Measuring Feature Sparsity in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uavy4DLrXR": {
    "title": "($\\texttt{PASS}$) Visual Prompt Locates Good Structure Sparisty through a Recurent HyperNetwork",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8CtXin7mZ": {
    "title": "A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2gwo9cjOEz": {
    "title": "Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zamGHHs2u8": {
    "title": "If there is no underfitting, there is no Cold Posterior Effect",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vaf4sIrRUC": {
    "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g16vmAtJ8x": {
    "title": "On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against ``Truly Anonymous Synthetic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHmmnNvU2U": {
    "title": "Weighted Risk Invariance for Density-Aware Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ky2JYPKkml": {
    "title": "Towards Explainable and Efficient Multi-Modality Learning: Domain-Agnostic Concept Space Paired with Domain-Specific Projection Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DQTxr8JtPX": {
    "title": "Detecting Influence Structures in Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zeobgjmUCc": {
    "title": "Using Machine Learning Models to Predict Genitourinary Involvement Among Gastrointestinal Stromal Tumour Patients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8gZtt8nrpI": {
    "title": "Diffusion Models With Learned Adaptive Noise Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dsd04MYKax": {
    "title": "Sum-of-Parts Models: Faithful Attributions for Groups of Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCfz492fM8": {
    "title": "CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HCCkCjClO0": {
    "title": "Online Weight Approximation for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIt0sJsZw9": {
    "title": "Clustering Entity Specific Embeddings Towards a Prescribed Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aiPcdCFmYy": {
    "title": "Sinkhorn Distributional Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LY3ukUANko": {
    "title": "On input-dependence and recall in convolutional language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYyu3jpk8z": {
    "title": "Open-Domain Text Evaluation via Contrastive Distribution Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ta26LtNq2r": {
    "title": "Learning to Reject for Balanced Error and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzZjyYee6L": {
    "title": "Don't Reinvent the Steering Wheel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2kvDzdC5rh": {
    "title": "IntentGPT: Few-Shot Intent Discovery with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tj3xLVuE9f": {
    "title": "On the Foundations of Shortcut Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sehRvaIPQQ": {
    "title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFfMsKjqaH": {
    "title": "Interpreting Categorical Distributional Reinforcement Learning: An Implicit Risk-Sensitive Regularization Effect",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xhCZD9hiiA": {
    "title": "Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ze7DOLi394": {
    "title": "On the Joint Interaction of Models, Data, and Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DASh78rJ7g": {
    "title": "Plugin estimators for selective classification with out-of-distribution detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kOBkxFRKTA": {
    "title": "Dynamic Sparse Training with Structured Sparsity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fkrYDQaHOJ": {
    "title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ROxsH4rMe4": {
    "title": "Systolic Array Acceleration of Spiking Neural Networks with Application-Independent Split-Time Temporal Coding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=53gU1BASrd": {
    "title": "Evaluating and Finetuning Models For Financial Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tcFcKyJgRM": {
    "title": "HeaP: Hierarchical Policies for Web Actions using LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvRZ68ObgW": {
    "title": "Controlling language over-optimization by targeting reward distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aKivEaIbN2": {
    "title": "Graph is All You Need? Lightweight Data-agnostic Neural Architecture Search without Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MFCjgEOLJT": {
    "title": "Learning interpretable control inputs and dynamics underlying animal locomotion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUBLhhVM1l": {
    "title": "Tight Rates in Supervised Outlier Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gTWaUlxxWi": {
    "title": "On the Effectiveness of One-Shot Federated Ensembles in Heterogeneous Cross-Silo Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMVFYTgj0H": {
    "title": "Continual Reinforcement Learning by Reweighting Bellman Targets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ut9aUpFZFr": {
    "title": "COINs: Model-based Accelerated Inference for Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oAMArMMQxb": {
    "title": "Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xh0XzueyCJ": {
    "title": "Plug-And-Play Controllable Graph Generation With Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GicZtgSlJW": {
    "title": "Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dl34rOnbqJ": {
    "title": "Actions-to-Action: Inductive Attention for Egocentric Video Action Anticipation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UB03wcP8RH": {
    "title": "Multitask Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itNHdOzZig": {
    "title": "Encodings for Prediction-based Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KqkizXgXU": {
    "title": "Curiosity-driven Red-teaming for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkETBJRKH7": {
    "title": "A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a745RnSFLT": {
    "title": "Understanding prompt engineering may not require rethinking generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Naiy1jf8UA": {
    "title": "MGDC-UNet: Multi-group Deformable Convolution for Medical Image Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GwBTlCIGs5": {
    "title": "Addressing Sample Inefficiency in Multi-View Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YikB42Oyaw": {
    "title": "MoReDrop: Dropout Without Dropping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dLrhRIMVmB": {
    "title": "Topological data analysis on noisy quantum computers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DuQkqSe9en": {
    "title": "Adversarial Imitation Learning via Boosting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2uHTuvDkLZ": {
    "title": "Physics-aware Causal Graph Network for Spatiotemporal Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6pHf8AiO7": {
    "title": "Pruning neural networks using FishLeg estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xXtD9P2lvH": {
    "title": "Directed Graph Generation with Heat Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mjsP8RYAw": {
    "title": "Unsupervised Fact Verification by Language Model Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0w42S2Gp70": {
    "title": "LipSim: A Provably Robust Perceptual Similarity Metric",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhY1XSYqO4": {
    "title": "Deep Variational Multivariate Information Bottleneck - A Framework for Variational Losses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d98CzL5h0i": {
    "title": "Learning to Generate Better than your Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rhaQbS3K3R": {
    "title": "Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtOydkE1Ku": {
    "title": "TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbOSJ3CATY": {
    "title": "A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4SrzKsJocx": {
    "title": "Simultaneous Dimensionality Reduction: A Data Efficient Approach for Multimodal Representations Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4mwbPjOwb": {
    "title": "Simple-TTS: End-to-End Text-to-Speech Synthesis with Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNtcoAM5Gy": {
    "title": "BaFTA: Backprop-Free Test-Time Adaptation for Zero-shot Vision Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AMDKqZcZbi": {
    "title": "Rapid Learning without Catastrophic Forgetting in the Morris Water Maze",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q20O1J9ujh": {
    "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsztjXcvUf": {
    "title": "A Primal-Dual Approach to Solving Variational Inequalities with General Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLADT8Wrhn": {
    "title": "TiC-CLIP: Continual Training of CLIP Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dLoAdIKENc": {
    "title": "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rNvyMAV8Aw": {
    "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XK7kyCVjqr": {
    "title": "Alignment-Enhancing Parallel Code Generation for Semi-Supervised Code Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DayPQKXaQk": {
    "title": "Constrained Decoding for Cross-lingual Label Projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hSyW5go0v8": {
    "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=piWvNRR0Ym": {
    "title": "Towards Minimal Targeted Updates of Language Models with Targeted Negative Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x5txICnnjC": {
    "title": "Synaptic Weight Distributions Depend on the Geometry of Plasticity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CK5Hfb5hBG": {
    "title": "Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ijK5hyxs0n": {
    "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SFCHv2G33F": {
    "title": "Protein Language Models Enable Accurate Cryptic Ligand Binding Pocket Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=06lrITXVAx": {
    "title": "Dropout Enhanced Bilevel Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dgmcE0RsTi": {
    "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n3kFlvVhJM": {
    "title": "Adder: Adapted Dense Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljVCPV7jK3": {
    "title": "Fairness Under Demographic Scarce Regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VoLDkQ6yR3": {
    "title": "Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tfyLS1cB5W": {
    "title": "Encoding Ontologies with Holographic Reduced Representations for Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUzWmFdglP": {
    "title": "Privacy Amplification for Matrix Mechanisms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsxeNvYqCj": {
    "title": "Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yol6nUVIJD": {
    "title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iIT02bAKzv": {
    "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qup9xD8mW4": {
    "title": "Behaviour Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLPzL6HWNl": {
    "title": "Improving LoRA in Privacy-preserving Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZXwP3H5t6": {
    "title": "Faster and Accurate Neural Networks with Semantic Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hB7SlfEmze": {
    "title": "PhyloGFN: Phylogenetic inference with generative flow networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TskzCtpMEO": {
    "title": "Training Bayesian Neural Networks with Sparse Subspace Variational Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eoB6JmdmVf": {
    "title": "Speech language models lack important brain-relevant semantics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Ua4hKiAJX": {
    "title": "Locality-Aware Graph Rewiring in GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nz6xYntfJ": {
    "title": "AlignDiff: Aligning Diffusion Models for General Few-Shot Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QeX0YFt4iW": {
    "title": "Multi-modality Adversarial Attacks on Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4FUa5dxiiA": {
    "title": "Risk-Sensitive Variational Model-Based Policy Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnVtsfyvap": {
    "title": "Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C6zFUEvgiU": {
    "title": "Feedback-guided Data Synthesis for Imbalanced Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSaN4gxuEf": {
    "title": "Adapting to Distribution Shift by Visual Domain Prompt Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S7j1sNVIm9": {
    "title": "Locally Adaptive Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Je5SHCKpPa": {
    "title": "Multimodal Patient Representation Learning with Missing Modalities and Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h4pNROsO06": {
    "title": "Improved sampling via learned diffusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fj5SqqXfn1": {
    "title": "Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OLi39lZS9Y": {
    "title": "Learning to Solve New sequential decision-making Tasks with In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1tZbq88f27": {
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yAcLwJu9qs": {
    "title": "Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkBg5D2lgT": {
    "title": "Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SIojR1ruNQ": {
    "title": "TIGERScore: Building Explainable Metric for All Text Generation Task",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZyhUXpEee": {
    "title": "SpaFL: Communication-Efficient Federated Learning with Sparse Models and Low Computational Overhead",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUZ2S0JVJP": {
    "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AweVGJeW47": {
    "title": "Smoothing for exponential family dynamical systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HE9eUQlAvo": {
    "title": "What Data Benefits My Classifier?\" Enhancing Model Performance and Interpretability through Influence-Based Data Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQt0MwXA81": {
    "title": "Do LLMs exhibit human-like response biases? A case study in survey design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6tqgL8VluV": {
    "title": "Towards Establishing Guaranteed Error for Learned Database Operations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTHfNGI3zT": {
    "title": "Quantifying the Plausibility of Context Reliance in Neural Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p79lnC36CO": {
    "title": "Automatic Calibration Diagnosis: Interpreting Probability Integral Transform (PIT) Histograms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9j1RD9LlWH": {
    "title": "Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4CxQmp9wc": {
    "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nqlymMx42E": {
    "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MO632iPq3I": {
    "title": "Differentiable Euler Characteristic Transforms for Shape Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hp4yOjhwTs": {
    "title": "Causally Aligned Curriculum Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RG98EkuHdT": {
    "title": "Transforming Transformers for Resilient Lifelong Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6EQbYM0CIX": {
    "title": "Conditional Generative Modeling for High-dimensional Marked Temporal Point Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L7KDMsqWl9": {
    "title": "HHD-Ethiopic: A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YGWGhdik6O": {
    "title": "Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTl1ABwM4n": {
    "title": "Improving length generalization in transformers via task hinting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KBGbEncHZF": {
    "title": "ARE YOU CERTAIN THAT IT IS A DEEPFAKE? DETECTION, GENERATION, AND SOURCE DETECTION FROM AN UNCERTAINTY PERSPECTIVE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gWHiS8Z867": {
    "title": "Routing with Rich Text Queries via Next-Vertex Prediction Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zgHamUBuuO": {
    "title": "Sparling: Learning Latent Representations With Extremely Sparse Activations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YUefWMfPoc": {
    "title": "How to fix a broken confidence estimator: Evaluating post-hoc methods for selective classification with deep neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v63GWletn8": {
    "title": "Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lr0byX2aNO": {
    "title": "Counterfactual Fairness on Graphs: Augmentations, Hidden Confounders, and Identifiability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v1VvCWJAL8": {
    "title": "Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNLVvdHyAw": {
    "title": "Detecting Language Model Attacks With Perplexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vt5mnLVIVo": {
    "title": "Grokking as the transition from lazy to rich training dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t5LXyWbs5p": {
    "title": "Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RLC0J2N9n": {
    "title": "SynBench: Evaluating Pretrained Representations for Image Classification using Synthetic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qcigbR1UYA": {
    "title": "Performance Bounds for Active Binary Testing with Information Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GE0N1htnu": {
    "title": "RINGER: Conformer Ensemble Generation of Macrocyclic Peptides with Sequence-Conditioned Internal Coordinate Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M0xK8nPGvt": {
    "title": "Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KgaBScZ4VI": {
    "title": "Language Model Cascades: Token-Level Uncertainty And Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3JjJezzVkT": {
    "title": "The Marginal Value of Momentum for Small Learning Rate SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mnWvUZIXt": {
    "title": "Towards Principled Representation Learning from Videos for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w49jlMWDSA": {
    "title": "GIST: Generating Image-Specific Text for Fine-grained Object Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duLr8BIzro": {
    "title": "A Fast and Effective Alternative to Graph Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3tjTJeXyA7": {
    "title": "Revitalizing Channel-dimension Fourier Transform for Image Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gyfXuRfxW2": {
    "title": "Learning Polynomial Problems with $SL(2, \\mathbb{R})$-Equivariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wYvuY60SdD": {
    "title": "Mixture of Weak and Strong Experts on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zz61cEY84L": {
    "title": "Meta-Learning Strategies through Value Maximization in Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oNzf7u5lT": {
    "title": "Pylic: Leveraging Source Code for Planning in Structured Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nji0ztL5rP": {
    "title": "Best Arm Identification for Stochastic Rising Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NU9AYHJvYe": {
    "title": "Optimal Sample Complexity of Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uS85FzjNDR": {
    "title": "A Unified Framework for Heterogeneous Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6K81ILDnuv": {
    "title": "Learning from Integral Losses in Physics Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLkj91HIZU": {
    "title": "Transformers can optimally learn regression mixture models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S83ldgJZLh": {
    "title": "A Structured Pruning Algorithm for Model-based Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iCNOK45Csv": {
    "title": "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l9GaXJnMJ8": {
    "title": "Fast Stochastic Kernel Approximation by Dual Wasserstein Distance Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6pPYRXKPpw": {
    "title": "Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyXWIJ99nh": {
    "title": "Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=odY3PkI5VB": {
    "title": "Reconciling Spatial and Temporal Abstractions for Goal Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6NO5UVWvo6": {
    "title": "Annotation by Clicks: A Point-Supervised Contrastive Variance Method for Medical Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oju2Qu9jvn": {
    "title": "Estimating Conditional Mutual Information for Dynamic Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jjA4O1vJRz": {
    "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PvJnX3dwsD": {
    "title": "Quadratic models for understanding neural network dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TdyfmCM8iR": {
    "title": "Latent Concept-based Explanation of NLP Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxPTSDp9xK": {
    "title": "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fud9JxIiEq": {
    "title": "Improved DDIM Sampling with Moment Matching Gaussian Mixtures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTYuRVrdK3": {
    "title": "Evaluating Representation Learning on the Protein Structure Universe",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aemqy6Hjdj": {
    "title": "Enhancing Compositional Generalization via Compositional Feature Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ViPtjIVzUw": {
    "title": "T-MARS: Improving Visual Representations by Circumventing Text Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUtxNAKpdV": {
    "title": "Nougat: Neural Optical Understanding for Academic Documents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vVoWRFV5Y4": {
    "title": "Solving the Quadratic Assignment Problem With Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6aRMQVlPVE": {
    "title": "Rank-adaptive spectral pruning of convolutional layers during training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3OzQhhPLyW": {
    "title": "Meta-Value Learning: a General Framework for Learning with Learning Awareness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STUGfUz8ob": {
    "title": "When can transformers reason with abstract symbols?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dLMPOY0HW": {
    "title": "When Do MLPs Excel in Node Classification? An Information-Theoretic Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IcR1OOFzxm": {
    "title": "Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=79FVDdfoSR": {
    "title": "A Characterization Theorem for Equivariant Networks with Point-wise Activations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiKHRTqaUD": {
    "title": "Handling Cost and Constraints with Off-Policy Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2qLSkTuqrb": {
    "title": "Translating cognitive models into neural and statistical descriptions of real-world multi-agent foraging behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAkjVCtRO2": {
    "title": "Variational quantization for state space models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iShM3YolRY": {
    "title": "On the Tool Manipulation Capability of Open-sourced Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ysue5S6cVS": {
    "title": "Confidence-driven Sampling for Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fvse7bMkAs": {
    "title": "Risk Assessment and Statistical Significance in the Age of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LVFoynuAQn": {
    "title": "A universal metric of dataset similarity for multi-source learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ph04CRkPdC": {
    "title": "Think before you speak: Training Language Models With Pause Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EWcybWr3MR": {
    "title": "Unlocking Tuning-free Generalization: Minimizing the PAC-Bayes Bound with Trainable Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IuXR1CCrSi": {
    "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Scc7Nl7lg": {
    "title": "Revealing Vision-Language Integration in the Brain with Multimodal Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaFrlUcAn3": {
    "title": "Debiasing Language Models Using Energy-Guided Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FM5xfcaR2Y": {
    "title": "Post-hoc bias scoring is optimal for fair classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWlSNtViSA": {
    "title": "Coupling Fairness and Pruning in a Single Run: a Bi-level Optimization Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxITGFPVWh": {
    "title": "Sharpness-Aware Data Poisoning Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pK7V0glCdj": {
    "title": "BOtied: Multi-objective Bayesian optimization with tied multivariate ranks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3d0OmYTNui": {
    "title": "Privately Aligning Language Models with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHBfxhZu1u": {
    "title": "YaRN: Efficient Context Window Extension of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wk77w7DG1N": {
    "title": "Evaluating and Improving Generation Consistency of Large Language Models via A Divide-Conquer-Reasoning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EE75tyB5Ay": {
    "title": "On the Generalization of Training-based ChatGPT Detection Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybiwT2yP1c": {
    "title": "BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aJl5aK9n7e": {
    "title": "What Improves the Generalization of Graph Transformer? A Theoretical Dive into Self-attention and Positional Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLfdJEuXkR": {
    "title": "UGSL: A Unified Framework for Benchmarking Graph Structure Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kuj5gVp5GQ": {
    "title": "Accelerating Sinkhorn algorithm with sparse Newton iterations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EmUVpfrXWN": {
    "title": "Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cDInj7WMQm": {
    "title": "UGC: UNIVERSAL GRAPH COARSENING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rR03qFesqk": {
    "title": "Functional Interpolation for Relative Positions improves Long Context Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLOaCQfZe9": {
    "title": "Dream to Adapt: Meta Reinforcement Learning by Latent Context Imagination and MDP Imagination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfEqSWpMfj": {
    "title": "Word Importance Explains How Prompts Affect Language Model Outputs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLOgB6oDnd": {
    "title": "KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4Hcegjzph": {
    "title": "Pre-training with Random Orthogonal Projection Image Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPpkFqMX6V": {
    "title": "Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkJiNn2QDF": {
    "title": "FeatUp: A Model-Agnostic Framework for Features at Any Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Giwj9cgAIl": {
    "title": "Mechanistic Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=12zKEh2APn": {
    "title": "PROSE: Predicting Operators and Symbolic Expressions using Multimodal Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q53QLftNkA": {
    "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nshk5YpdWE": {
    "title": "Lagrangian Flow Networks for Conservation Laws",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J2TZgj3Tac": {
    "title": "Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDuQNUQN6q": {
    "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F76bwRSLeK": {
    "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w7LU2s14kE": {
    "title": "Linearity of Relation Decoding in Transformer Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Gvs64deOj": {
    "title": "Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3RfGSbXUt8": {
    "title": "Option Boosting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUtGjQEDd4": {
    "title": "Generative Modeling with Phase Stochastic Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbuyDzZTPt": {
    "title": "OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yiMB2DOjsR": {
    "title": "Chain of Log-Concave Markov Chains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zww4Xqmk38": {
    "title": "Tree-based Ensemble Learning for Out-of-distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YTKShuSOhI": {
    "title": "Demonstrating the capacity of a Path-Based variational inference formulation for robust hidden Markov modelling of complex and noisy binary trees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxHgpC6FNv": {
    "title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lkIRFglmTp": {
    "title": "Resolving Partial Observability in Decision Processes via the Lambda Discrepancy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RxhOEngX8s": {
    "title": "Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=unE3TZSAVZ": {
    "title": "Breaking Neural Network Scaling Laws with Modularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20oxNYWQl9": {
    "title": "Sensitivity Sampling for Coreset-Based Data Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zRMXQMyyM8": {
    "title": "DISCRET: a self-interpretable framework for treatment effect estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EArTDUmILF": {
    "title": "VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kKxvFpvV04": {
    "title": "Towards Exact Computation of Inductive Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJ1w6MzVZw": {
    "title": "Large Pre-trained time series models for cross-domain Time series analysis tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qyp3Rni2g1": {
    "title": "Efficiency Pentathlon: A Standardized Benchmark for Efficiency Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qTui9aQ3VW": {
    "title": "How Robust Are Energy-Based Models Trained With Equilibrium Propagation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXpH3D0TVP": {
    "title": "The Journey, Not the Destination: How Data Guides Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNQjN5HzXt": {
    "title": "AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qUVP6IDc5J": {
    "title": "Eliciting Attributions from LLMs with Minimal Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owokKCrGYr": {
    "title": "Quality-Diversity through AI Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WM0OogPTx": {
    "title": "Learning from Sparse Offline Datasets via Conservative Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ivokwVKY4o": {
    "title": "Formal Verification for Neural Networks with General Nonlinearities via Branch-and-Bound",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qqu5mMgIBV": {
    "title": "Castor: Causal Temporal Regime Structure Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DL7JWbdGr3": {
    "title": "PEMs: Pre-trained Epidemic Time-Series Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJDFhkwQg6": {
    "title": "Federated contrastive GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JigPd5Pm5": {
    "title": "Informed weight initialization of Graph Neural Networks and its effect on Oversmoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5EtSvYUU0v": {
    "title": "Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vg7dECgAw2": {
    "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qr4ECbGcSj": {
    "title": "On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auUngos7eR": {
    "title": "Implicit Maximum a Posteriori Filtering via Adaptive Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHXCByvrLd": {
    "title": "Rethinking Optimal Transport in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=93LoCyww8o": {
    "title": "The HIM Solution for Legged Locomotion: Minimal Sensors, Efficient Learning, and Substantial Agility",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rAHcTCMaLc": {
    "title": "S$2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qhkEOCcVX9": {
    "title": "A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oDGkq0AleM": {
    "title": "Anomaly Detection with Variance Stabilized Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9cumTvvlHG": {
    "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xhEN0kJh4q": {
    "title": "Robust Model-Based Optimization for Challenging Fitness Landscapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4AEBLHuA6": {
    "title": "Solving High Frequency and Multi-Scale PDEs with Gaussian Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yjX303Smre": {
    "title": "Reinforcement Learning of Diverse Skills using Mixture of Deep Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XMJBrvRDI8": {
    "title": "Hierarchically branched diffusion models leverage dataset structure for class-conditional generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKHmjlpViu": {
    "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7QAX3zRh0": {
    "title": "Towards guarantees for parameter isolation in continual learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nf4Lm6fXN8": {
    "title": "Replay across Experiments: A Natural Extension of Off-Policy RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzE7EG7S4i": {
    "title": "High-Dimensional Geometric Streaming for Nearly Low Rank Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dapU3n7yfp": {
    "title": "Automatically Eliciting Toxic Outputs from Pre-trained Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2SuA42Mq1c": {
    "title": "BMAD: Benchmarks for Medical Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YkCjojDG3l": {
    "title": "PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EcDO5EXFdH": {
    "title": "SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nrctFaenIZ": {
    "title": "GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S5yOuNfSA0": {
    "title": "Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQrFaQC3kj": {
    "title": "Dataset Fairness: Achievable Fairness On Your Data With Utility Guarantees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fx8AJDQRVB": {
    "title": "Image Super-Resolution via Latent Diffusion: A Sampling-Space Mixture of Experts and Frequency-Augmented Decoder Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YOKnEkIuoi": {
    "title": "Conditional Variational Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CH6DQGcI3a": {
    "title": "Revisiting DeepFool: generalization and improvement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hj9ZuNimRl": {
    "title": "Better Neural PDE Solvers Through Data-Free Mesh Movers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sq5gkjC9jv": {
    "title": "Topological Expressive Power of ReLU Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wIFvdh1QKi": {
    "title": "Metric Space Magnitude for Evaluating Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUz8QXAgFV": {
    "title": "Bridging Autoregressive and Masked Modeling for Enhanced Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MXI8aSgl53": {
    "title": "NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ydlfehfvge": {
    "title": "Mitigating Estimation Errors By Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JsRZEGZ7L": {
    "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xIHi5nxu9P": {
    "title": "Subtractive Mixture Models via Squaring: Representation and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PbGs8PGoCn": {
    "title": "Stateless Mean-Field Games: A Framework for Independent Learning with Large Populations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZWHYyfPwY": {
    "title": "Don't trust your eyes: on the (un)reliability of feature visualizations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CBGdLyJXBW": {
    "title": "Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W0zgCR6FIE": {
    "title": "Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKALcMvCdm": {
    "title": "Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ipWSxcmgsx": {
    "title": "Optimizing the trade-off between utility and performance in interpretable sleep classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r0BcyqWAcj": {
    "title": "Loci-Segmented: Improving Scene Segmentation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3rjenIOfx": {
    "title": "A Conceptual Framework for Analyzing Social Representation in Unstructured Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqTipMg9CZ": {
    "title": "Contextual Molecule Representation Learning from Chemical Reaction Knowledge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wCRTEOIdmf": {
    "title": "Towards Subgraph Isomorphism Counting with Graph Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5XUlfPcQnG": {
    "title": "A Calibrated Simulation for Offline Training of Reinforcement Learning Agents to Optimize Energy and Emission in Office Buildings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmXXKxQpOR": {
    "title": "On the Provable Advantage of Unsupervised Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uKB4cFNQFg": {
    "title": "BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1IIiQnLRe8": {
    "title": "Diversity Modeling for Semantic Shift Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QxItoEAVMb": {
    "title": "TorchRL: A data-driven decision-making library for PyTorch",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dm4qrBuFKH": {
    "title": "Training Binary Neural Networks in a Binary Weight Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdOaaDzDD6": {
    "title": "Bandits with Ranking Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqYjAQ5diD": {
    "title": "Factorized Neural Radiance Field with Depth Covariance Function for Dense RGB Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AIbQ3HDDHU": {
    "title": "Training and inference of large language models using 8-bit floating point",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uJPWeZffgl": {
    "title": "Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sysX9XMGdF": {
    "title": "Understanding and Tackling Over-Dilution in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7pVIFJW2Hp": {
    "title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N7rEyHTZO9": {
    "title": "SSC Layer - A replacement for convolutional layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64t9er38Zs": {
    "title": "Learning Deep O($n$)-Equivariant Hyperspheres",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ledQ1BCrwc": {
    "title": "GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VNyIVrKrqv": {
    "title": "Constrained Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Piod76RSrx": {
    "title": "Slicing Mutual Information Generalization Bounds for Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5hAMmCU0bK": {
    "title": "Towards Robust Offline Reinforcement Learning under Diverse Data Corruption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T16M4SzH1v": {
    "title": "Distributional Bellman Operators over Mean Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GdTOzdAX5A": {
    "title": "On the Identifiability of Switching Dynamical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KknWbD5j95": {
    "title": "SoundStorm: Efficient Parallel Audio Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYp2KwjCWx": {
    "title": "Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r125wFo0L3": {
    "title": "Large Trajectory Models are Scalable Motion Predictors and Planners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUy4mGPmUy": {
    "title": "Optimization Framework of Transfer Learning and its Feasibility",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=up6hr4hIQH": {
    "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39HaKNXpsu": {
    "title": "Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sx7BIiPzys": {
    "title": "Variational Bayesian Last Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hOMVq57Ce0": {
    "title": "Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jc0FssXh2R": {
    "title": "Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIJFG7wW2d": {
    "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gIiz7tBtYZ": {
    "title": "Neural Optimal Transport with General Cost Functionals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7gUrYE50Rb": {
    "title": "EQA-MX: Embodied Question Answering using Multimodal Expression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnAeyjtMFM": {
    "title": "When Witnesses Defend: A Witness Graph Topological Layer for Adversarial Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Re5KnZcXhf": {
    "title": "Constrained Variational Generation for Generalizable Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEDTZxwJjT": {
    "title": "DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZDEwhAlCO": {
    "title": "ILPO-NET: convolution network for the recognition of arbitrary volumetric patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apA6SSXx2e": {
    "title": "A Topological Perspective on Demystifying GNN-Based Link Prediction Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5liV2xUdJL": {
    "title": "Time-Efficient Reinforcement Learning with Stochastic Stateful Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=whFQe4MRIY": {
    "title": "MI-NeRF: Learning a Single Face NeRF from Multiple Identities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0y0yOpI4wx": {
    "title": "General-Purpose In-Context Learning by Meta-Learning Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZlQRiFmq7Y": {
    "title": "Retrieval-based Disentangled Representation Learning with Natural Language Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6eA8Srlqd": {
    "title": "Scalable Long Range Propagation on Continuous-Time Dynamic Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PN0SuVRMxa": {
    "title": "Structured Packing in LLM Training Improves Long Context Utilization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2Kdq5biZx": {
    "title": "Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mnipav175N": {
    "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy8upuD6Bw": {
    "title": "Emergent Communication with Conversational Repair",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGUyAuuTYZ": {
    "title": "Bridging the Gap between Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lROh08eK6n": {
    "title": "Efficient Network Embedding in the Exponentially Large Quantum Hilbert Space: A High-Dimensional Perspective on Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UyPmWupphV": {
    "title": "Hyperion: Fused Multi-Trial and Gradient Descent for Joint Hyperparameter and Neural Architecture Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=czpx02orl7": {
    "title": "Learning Abstract World Models for Value-preserving Planning with Options",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D96juYQ2NW": {
    "title": "Coresets for Clustering with Noisy Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CW2aryHm95": {
    "title": "Policy Learning For Video Streaming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pzir15nPfc": {
    "title": "Contextual Vision Transformers for Robust Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lOWCkhr4g": {
    "title": "Unsupervised ASR via Cross-Lingual Pseudo-Labeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XdSYtriYfI": {
    "title": "Federated Ensemble-Directed Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bH6T0Jjw5y": {
    "title": "Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RPhoFFj0jg": {
    "title": "ResBit: Residual Bit Vector for Categorical Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j511LaqEeP": {
    "title": "Non-Exchangeable Conformal Risk Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pqgDqYinDZ": {
    "title": "Learning From Multi-Expert Demonstrations: A Multi-Objective Inverse Reinforcement Learning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kn7tWhuetn": {
    "title": "On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jId5PXbBbX": {
    "title": "Provably Efficient UCB-type Algorithms For Learning Predictive State Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kBNIx4Biq4": {
    "title": "Lifting Architectural Constraints of Injective Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UH4HinPK9d": {
    "title": "Provably Accurate ODE Forecasting Through Explicit Trajectory Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxBoUKhcBJ": {
    "title": "LM-Switch: Transforming Word Embedding Space for Flexible Language Model Steering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxebDHZ7b7": {
    "title": "TRAM: Bridging Trust Regions and Sharpness Aware Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ptCIlV24YZ": {
    "title": "Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=khAE1sTMdX": {
    "title": "Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iyMixbK9M2": {
    "title": "The Extrapolation Power of Implicit Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lIYxAcxY1B": {
    "title": "Understanding Sparse Feature Updates in Deep Networks using Iterative Linearisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkJOCga62u": {
    "title": "Orbit-Equivariant Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aBUidW4Nkd": {
    "title": "Object-Centric Learning with Slot Mixture Module",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhanPLSHRt": {
    "title": "EXCOST: Semi-Supervised Classification with Exemplar-Contrastive Self-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhPUSofMgr": {
    "title": "Text-Aware Diffusion Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lm7MRcsFiS": {
    "title": "Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jD1sU2vLOn": {
    "title": "Learning Counterfactually Invariant Predictors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oYjW8QxuC": {
    "title": "Pi-DUAL: Using privileged information to distinguish clean from noisy labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xlQrAm3LE4": {
    "title": "DiffSim: Aligning Diffusion Model and Molecular Dynamics Simulation for Accurate Blind Docking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Oiee202rd": {
    "title": "More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSvOIT5Ai2": {
    "title": "Interpretable Concept Discovery and Learning from Pretrained Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rr4OccbgJi": {
    "title": "A Lennard-Jones Layer for Distribution Normalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8SPSIfR2e0": {
    "title": "Dissecting Language Models: Machine Unlearning via Selective Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjNfLdxr3A": {
    "title": "ELoRA: Efficient Low-Rank Adaptation with Random Matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v9Pguuamfp": {
    "title": "Explaining Emergent In-Context Learning as Kernel Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4Yd9i5FFm": {
    "title": "Asymmetric Momentum: A Rethinking of Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Zbg38nA0J": {
    "title": "Explaining grokking through circuit efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WKDwIaF7y": {
    "title": "Lookahead Sharpness-Aware Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wLbL3lJNTL": {
    "title": "Joint Representations for Reinforcement Learning with Multiple Sensors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0sbIEkIutN": {
    "title": "From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bb21JPnhhr": {
    "title": "AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DGez4B2a6Y": {
    "title": "A Plug-and-Play Image Registration Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZTkLDRmkg": {
    "title": "BENO: Boundary-embedded Neural Operators for Elliptic PDEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8aD5pUcVX": {
    "title": "What Makes for Good Visual Tokenizers for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUeYSTIhpE": {
    "title": "DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwO71a8wET": {
    "title": "Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4GfEOQlBoc": {
    "title": "Disentangling the Link Between Image Statistics and Human Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdwrCm5Msr": {
    "title": "MapLearn: Indoor Mapping using Audio",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nTNElfN4O5": {
    "title": "3D Interacting Hands Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQqZVayz07": {
    "title": "Aligning Agents like Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TPAj63ax4Y": {
    "title": "Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NddKiWtdUm": {
    "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vE1e1mLJ0U": {
    "title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rrCF6WasY8": {
    "title": "Distributed DPHelmet: Differentially Private Non-interactive Convex Blind Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykEixGIJYb": {
    "title": "Incentivized Truthful Communication for Federated Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ep9y5OrFmS": {
    "title": "What Apples Tell About Oranges: Connecting Pruning Masks and Hessian Eigenspaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UDNJdVjhyg": {
    "title": "Learning Graph Representations via Graph Entropy Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pC3WJHf51j": {
    "title": "Large-scale training of foundation models for wearable biosignals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aFMiKm9Qcx": {
    "title": "The Central Spanning Tree Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oq5EF8parZ": {
    "title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OIsahq1UYC": {
    "title": "Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfAqPxPsAj": {
    "title": "Language Conditioned Equivariant Grasp",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j20nMRUWK9": {
    "title": "Adaptive Knowledge Transfer for Generalized Category Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7FHrZuKogW": {
    "title": "Contractive Systems Improve Graph Neural Networks Against Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eMNN0wIyVw": {
    "title": "On Trajectory Augmentations for Off-Policy Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ylHLVq0psd": {
    "title": "Rethinking the Noise Schedule of Diffusion-Based Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JSlTXa6WE6": {
    "title": "Efficient Certification of Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yz0Strbex6": {
    "title": "A Note on Some Statistical Properties of Signature Transform Under Stochastic Integrals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rsg1mvUahT": {
    "title": "Federated Wasserstein Distance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gENfMmUIkT": {
    "title": "A Pipeline-Based Approach for Object Detection on Resource Constrained Internet of Things Devices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WoP9veDwUp": {
    "title": "Variance-Reduced Meta-Learning via Laplace Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UulwvAU1W0": {
    "title": "Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WR9M6AA4LT": {
    "title": "Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Diffusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oashk4fDD9": {
    "title": "Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kA7vZQG34x": {
    "title": "Adversarial Imitation Learning from Visual Observations using Latent Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=17BA0Tl2Id": {
    "title": "Meta-Referential Games to Learn Compositional Learning Behaviours",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GY1fKFXG5i": {
    "title": "Non-Vacuous Generalization Bounds for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zz594UBNOH": {
    "title": "Clifford Group Equivariant Simplicial Message Passing Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5HpZZbgdeK": {
    "title": "Efficient calibration as a binary top-versus-all problem for classifiers with many classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NxoFmGgWC9": {
    "title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hsf2pDv2Qw": {
    "title": "RL Simplex: Bringing Computational Efficiency in Linear Programming via Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TkP2RtR4hr": {
    "title": "Regulating the level of manipulation in text augmentation with systematic adjustment and advanced sentence-embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ohamFnX14": {
    "title": "The (co)limit of metabeliefs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jFox1iMWUa": {
    "title": "CAUSAL NEURAL NETWORKS FOR CONTINUOUS TREATMENT EFFECT ESTIMATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6iBe17Y16": {
    "title": "On Using Admissible Bounds for Learning Forward Search Heuristics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWHf7lg8zM": {
    "title": "MultiContrievers: Analysis of Dense Retrieval Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EDPxCjXzSb": {
    "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wYmvN3sQpG": {
    "title": "Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ltZ9ianMth": {
    "title": "RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyuCmLLluu": {
    "title": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8vJSIsLhC": {
    "title": "SMPE: A Framework for Multi-Dimensional Permutation Equivariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VNHsZPZ5rJ": {
    "title": "Targeted Model Inversion: Distilling Style Encoded in Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PXD3FAVHJT": {
    "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rzBskAEmoc": {
    "title": "CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h5lqXPd9JN": {
    "title": "Model-Decoupling-Based Federated Learning with Consistency via Knowledge Distillation Using Conditional Generator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q00CO1Tm6M": {
    "title": "Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATEawsFUj4": {
    "title": "GAIA: Data-driven Zero-shot Talking Avatar Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KbvKjpqYQR": {
    "title": "Equivariant Quantum Graph Neural Network for Mixed-Integer Linear Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKPzWyaOsK": {
    "title": "Are machines automating morality?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUiEgloner": {
    "title": "Adaptive Learning of Quantum Hamiltonians",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MnMWa94t12": {
    "title": "DyST: Towards Dynamic Neural Scene Representations on Real-World Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89l6VLPrin": {
    "title": "Graph layouts and graph contrastive learning via neighbour embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VIEbRFp6s3": {
    "title": "Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skcTCdJz0f": {
    "title": "Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djmLZkEw1L": {
    "title": "IMPLICIT STACKED AUTOREGRESSIVE MODEL FOR WEATHER FORECASTING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bdJaYLiOxi": {
    "title": "Radar Spectra-language Model for Automotive Scene Parsing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zvB14IF6D": {
    "title": "DORSal: Diffusion for Object-centric Representations of Scenes $\\textit{et al.}$",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GsNp4ob8BY": {
    "title": "Mark My Words: Repurposing LLMs for Specialized Domains via Ability Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ulMXGO1fdH": {
    "title": "Estimating Post-Synaptic Effects for Online Training of Feed-Forward SNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s5ZAs0UkRr": {
    "title": "ODEdit: Blind Face Restoration through Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PoBB8n52oi": {
    "title": "SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WEoyWdsI9f": {
    "title": "Quantifying and Defending against the Privacy Risk in Logit-based Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8Qg1IIMaR": {
    "title": "Fool Your Large (Vision and) Language Models with Embarrassingly Simple Permutations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bIHyMpzeuI": {
    "title": "Sparse MoE as a New Treatment: Addressing Forgetting, Fitting, Learning Issues in Multi-Modal Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HsJzGWvg7K": {
    "title": "Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MACKSU3xed": {
    "title": "PeriodNet:Lightweight And Efficient Time Series Prediction Model Based On Periodic Characteristics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYyzypZrgU": {
    "title": "DOMAIN-GROUNDING OF NEURAL NETWORKS FOR SPATIOTEMPORAL REASONING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvq4Nh8eZB": {
    "title": "Protecting Sensitive Data through Federated Co-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3y2TfP966N": {
    "title": "T-Rep: Representation Learning for Time Series using Time-Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gk75gOjtQh": {
    "title": "Variational Inference with Singularity-Free Planar Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vy5aRVSbNo": {
    "title": "Looping LOCI: Developing Object Permanence from Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7P2mK3x3o": {
    "title": "Computing high-dimensional optimal transport by flow neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KiK4MNkuiQ": {
    "title": "Clustering with Geometric Modularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8S14xeFQAY": {
    "title": "Segmenting the Unknown: Discrete Diffusion Models for Non-Deterministic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x36mCqVHnk": {
    "title": "Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5juXkyorf": {
    "title": "Closed-Form Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDMyhAxok3": {
    "title": "MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible and Diverse Neuronal Morphology Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpWRCiw8g5": {
    "title": "JOSENet: A Joint Stream Embedding Network for Violence Detection in Surveillance Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5vXDQ65dzH": {
    "title": "ParFam - Symbolic Regression Based on Continuous Global Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2dVrgLpsF": {
    "title": "On partial prototype collapse in clustering-based self-supervised learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LqRGsGWOTX": {
    "title": "Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IB1HqbA2Pn": {
    "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8ibi56aM6": {
    "title": "SINGLE-IMAGE COHERENT RECONSTRUCTION OF OBJECTS AND HUMANS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WdvT2UgsTK": {
    "title": "Enhancing the Cross-Size Generalization for Solving Vehicle Routing Problems via Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lf8QQ2KMgv": {
    "title": "Is Memorization Actually Necessary for Generalization?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1VeQ6VBbev": {
    "title": "Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DT8ipHAAVz": {
    "title": "End-to-End Training of Unsupervised Trees: KAURI and DOUGLAS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATQSDgYwqA": {
    "title": "Diffusion Random Feature Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mOTiVzTgF2": {
    "title": "ResiDual: Transformer with Dual Residual Connections",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7LZjuA4AB2": {
    "title": "Ask Your Distribution Shift if Pre-Training is Right for You",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vXxardq6db": {
    "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B37UmlxsaP": {
    "title": "Revealing The Intrinsic Ability of Generative Text Summarizers for Outlier Paragraph Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O04DqGdAqQ": {
    "title": "Ada-Instruct: Adapting Instruction Generators For Complex Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MloaGA6WwX": {
    "title": "Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g4I3Wzv3fw": {
    "title": "Revisiting the Static Model in Robust Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ee4QXtVDVm": {
    "title": "Subword embedding from bytes against embedding-based attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d3xKPQVjSc": {
    "title": "Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1oqedRt6Z7": {
    "title": "Convolutional Deep Kernel Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RJDjSXNuAZ": {
    "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IL9o1meezQ": {
    "title": "Random Walk Diffusion For Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HobyL1B9CZ": {
    "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DEJIDCmWOz": {
    "title": "On the Reliability of Watermarks for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q0IZQMojwv": {
    "title": "Objectives Are All You Need: Solving Deceptive Problems Without Explicit Diversity Maintenance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDaLmkdSKU": {
    "title": "Near-Optimal Solutions of Constrained Learning Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lUYY2qsRTI": {
    "title": "Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9F0xInGNBF": {
    "title": "VIDEOPROMPTER: AN ENSEMBLE OF FOUNDATIONAL MODELS FOR ZERO-SHOT VIDEO UNDERSTANDING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b7bilXYHgG": {
    "title": "Counterfactual Fairness for Predictions using Generative Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CCo8ElCT7v": {
    "title": "Comprehensive Comparison between Vision Transformers and Convolutional Neural Networks for Face Recognition Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RNfX0ah0K": {
    "title": "Leave-one-out Distinguishability in Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBEQIxXDDD": {
    "title": "TopoFormer: Topology-aware Transformer for Reactive Motion Prediction in Close Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pj3ErOxlLo": {
    "title": "NaviFormer: A Deep Reinforcement Learning Transformer-like Model to Holistically Solve the Navigation Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7cuUZxwFS": {
    "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESq3U7z6FD": {
    "title": "EHI: End-to-end learning of Hierarchical Index for Efficient Dense Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3IUunLy8s": {
    "title": "Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3y1K6buO8c": {
    "title": "Brain decoding: toward real-time reconstruction of visual perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nM2AHzqUj": {
    "title": "Linear Log-Normal Attention with Unbiased Concentration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pSf8rrn49H": {
    "title": "Copyright Plug-in Market for The Text-to-Image Copyright Protection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3J7foqnJkA": {
    "title": "Understanding Parameter Saliency via Extreme Value Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2wFXD2upSQ": {
    "title": "A Demon at Work: Leveraging Neuron Death for Efficient Neural Network Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nil8G449BI": {
    "title": "Block-local learning with probabilistic latent representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7OB810eaP": {
    "title": "Non-ergodicity in reinforcement learning: robustness via ergodic transformations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MvHiijJL3": {
    "title": "Model Explanation Disparities as a Fairness Diagnostic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wiYV0KDAE6": {
    "title": "Diffusion Models for Tabular Data Imputation and Synthetic Data Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JlSyXwCEIQ": {
    "title": "CodeIt: Abstract Reasoning with Iterative Policy-Guided Program Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sY5N0zY5Od": {
    "title": "DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=le1UUMd45T": {
    "title": "Solving Multiobjective Combinatorial Optimization via Learn to Improve Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3QR230r11w": {
    "title": "Multi-Fidelity Active Learning with GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=25VG15SnkH": {
    "title": "United We Train, Divided We Fail! Representation Learning for Time Series by Pretraining from 75 Datasets at Once",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJEd8PkdNz": {
    "title": "Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0MyyXyGfn": {
    "title": "Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDgw3yX2aO": {
    "title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6tUsZeVs7": {
    "title": "Energy-guided Entropic Neural Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FPpLTTvzR0": {
    "title": "IDEA: Invariant Causal Defense for Graph Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RN2lIjrtSR": {
    "title": "ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CwAY8b8i97": {
    "title": "Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9WD9KwssyT": {
    "title": "Zipformer: A faster and better encoder for automatic speech recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TWVMVPx2wO": {
    "title": "Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DFQCJmHPoe": {
    "title": "Adversarial latent representation for positive unlabeled learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Akd36hG9z": {
    "title": "Enhancing Offline Reinforcement Learning with an Optimal Supported Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vN9fpfqoP1": {
    "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WOiOzHG2zD": {
    "title": "TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UMwn5l37gU": {
    "title": "Non-uniform Noise Injection For Enhancing DNN Adversarial Robustness And Efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKPh4CLmYp": {
    "title": "Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m5m3nugttY": {
    "title": "UniVis: A Universal Framework for Computer Vision Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PbpJnyewVM": {
    "title": "Zero-shot Cross-task Preference Alignment for Offline RL via Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qODvxQ8TXW": {
    "title": "Masks, Signs, And Learning Rate Rewinding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cUSNs8nGaV": {
    "title": "GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gDlsMWost9": {
    "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GYAvwLviup": {
    "title": "Aligning brain functions boosts the decoding of videos in novel subjects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XyrB1Ay44j": {
    "title": "Quantifying and Enhancing Multi-modal Robustness with Modality Preference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xi7UoErFRt": {
    "title": "FedGP: Buffer-based Gradient Projection for Continual Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dxJKLozjQl": {
    "title": "Data Distribution Valuation with Incentive Compatibility",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgrZluxFC7": {
    "title": "Adversarial Machine Learning in Latent Representations of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVuZa1bgOs": {
    "title": "Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=otU31x3fus": {
    "title": "Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AcSChDWL6V": {
    "title": "Transformers vs. Message Passing GNNs: Distinguished in Uniform",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvSMIsztka": {
    "title": "Faster Approximation of Probabilistic and Distributional Values via Least Squares",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1pTlvxIfuV": {
    "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yisfNWUEsD": {
    "title": "SCALE: Synergized Collaboration of Asymmetric Language Translation Engines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zgQ0PHeGnL": {
    "title": "Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SqNi6Se1NT": {
    "title": "A Bayesian Framework for Clustered Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fd8MBEOirN": {
    "title": "OpenPatch: a 3D patchwork for Out-Of-Distribution detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pA4s793lcB": {
    "title": "Improved Algorithms for Replicable Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mavWQw7DnC": {
    "title": "Explaining recommendation systems through contrapositive perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTTZFKrlGV": {
    "title": "Gradual Domain Adaptation via Gradient Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhINOCrrqI": {
    "title": "AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YzJT0Y67Go": {
    "title": "HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YmQyEdLIkU": {
    "title": "Adversarial Attacks as Near-Zero Eigenvalues in The Empirical Kernel of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a4O528mek9": {
    "title": "Learning Multi-modal Representations Under Incomplete Data Via Dual Level Alignments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nliDYxirqq": {
    "title": "EduGym: An Environment Suite for Reinforcement Learning Education",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=akKNGGWegr": {
    "title": "Spatio-Temporal Graph Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlY7WQ2hWS": {
    "title": "Incentive-Aware Federated Learning with Training-Time Model Rewards",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7PmO5boQ9": {
    "title": "DynaEval: A Dynamic Interaction-based Evaluation Framework for Assessing LLMs in Real-world Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AwX6ON5A0V": {
    "title": "On Gaussian Mixture Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7TOs9gjAg1": {
    "title": "Removing Biases from Molecular Representations via Information Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXjXeTsqgW": {
    "title": "Sequential Condition Evolved Interaction Knowledge Graph for Traditional Chinese Medicine Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JtTPYBKqt": {
    "title": "Neural Architecture Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QibPzdVrRu": {
    "title": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pf2hEdu8B": {
    "title": "Rethinking the Uniformity Metric in Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmqOhBC4a5": {
    "title": "Maximum Entropy Heterogeneous-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFqVifIr6E": {
    "title": "Rethinking Semantic Few-Shot Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VWGyUZ9dOX": {
    "title": "Data augmentation guided Decouple Knowledge Distillation for low-resolution fine-grained image classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x17qiTPDy5": {
    "title": "DiffFlow: A Unified SDE for Score-Based Diffusion Models and Generative Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AKZtQO81GQ": {
    "title": "Evaluating model bias requires characterizing model mistakes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBSmr51fCR": {
    "title": "URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99tKiMVJhY": {
    "title": "Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Xx0mKoCMd": {
    "title": "ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1wlmtPGLq": {
    "title": "TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oc4ji1iCjQ": {
    "title": "Catch the Shadow: Automatic Shadow Variables Generation for Treatment Effect Estimation under Collider Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Jer2DQt9V": {
    "title": "The Unreasonable Effectiveness of Pretraining in Graph OOD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a4DBEeGfQq": {
    "title": "StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mxCX2bSV0Z": {
    "title": "Using Forwards-Backwards Models to Approximate MDP Homomorphisms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BBD6KXIGJL": {
    "title": "Hybrid Directional Graph Neural Network for Molecules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=816T4ab9Z5": {
    "title": "Perfect Alignment May be Poisonous to Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oJ1tx3fXDA": {
    "title": "Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkH1l00p6u": {
    "title": "When Treatment Effect Estimation Meets Collider Bias: A Dual Counterfactual Generative Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KqbCvIFBY7": {
    "title": "Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pE6gWrASQm": {
    "title": "On Adversarial Training without Perturbing all Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KBo7Z5aTV0": {
    "title": "Diving Segmentation Model into Pixels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rBu7dR7rm": {
    "title": "Unified Long-Term Time-Series Forecasting Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AfhNyr73Ma": {
    "title": "General Stability Analysis for Zeroth-Order Optimization Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yVJd8lKyVX": {
    "title": "Hybrid Sharing for Multi-Label Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e1vqloonRy": {
    "title": "Symmetric Single Index Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdPvGRvoBC": {
    "title": "An improved analysis of per-sample and per-update clipping in federated learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ey3GhWXQ97": {
    "title": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nkCWKkSLyb": {
    "title": "Benchmarking Diffusion Based Text-Guided Image Editing Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VtmBAGCN7o": {
    "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FddFxi08J3": {
    "title": "On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yMMIWHbjWS": {
    "title": "On convex decision regions in deep network representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWVC5FVidc": {
    "title": "Unbiased Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EriR6Ec69a": {
    "title": "Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dt3rcTC8Sw": {
    "title": "Enhancing Mutual Information Estimation in Self-Interpretable Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VB2WkqvFwF": {
    "title": "The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4vPVBh3fhz": {
    "title": "PAC Prediction Sets Under Label Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PtB6l1vNtk": {
    "title": "PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Unz9zYdjTt": {
    "title": "FedNovel: Federated Novel Class Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X1p0eNzTGH": {
    "title": "How the Level Sampling Process impacts Zero-Shot Generalisation in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JWVWUlobv": {
    "title": "4D Tensor Multi-task Continual Learning for Disease Dynamic Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KSjPaXtxP8": {
    "title": "Memorization in Self-Supervised Learning Improves Downstream Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M3QXCOTTk4": {
    "title": "The Curse of Diversity in Ensemble-Based Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUcdXA8hAa": {
    "title": "Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sK2A7Ve2co": {
    "title": "Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5rEkR8OgU": {
    "title": "Implicit Intermediate Supervision for Learning Complex Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bbCL5aRjUx": {
    "title": "Multilinear Operator Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zhHVyLY4K": {
    "title": "Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r65xfUb76p": {
    "title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W8S8SxS9Ng": {
    "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vy42bYs1Wo": {
    "title": "Off-Policy Primal-Dual Safe Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EwMhfwiAuv": {
    "title": "Localized Text-to-Image Generation For Free via Cross Attention Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T8RiH35Hy6": {
    "title": "Understanding Community Bias Amplification in Graph Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vyGp9Mty2t": {
    "title": "Implicit Neural Representations for Joint Sparse-View CT Reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KkrDUGIASk": {
    "title": "An Extensible Framework for Open Heterogeneous Collaborative Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q6HYM1EMu8": {
    "title": "LARG2, Language-based Automatic Reward and Goal Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R4gqcDRJ9l": {
    "title": "TopoFR: A Closer Look at Topology Alignment on Face Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsJGd0xfgv": {
    "title": "Quantum Architecture Search with Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=StkLULT1i1": {
    "title": "Learning a Diffusion Model Policy from Rewards via Q-Score Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4flIscNE6": {
    "title": "Meta-Collaboration in Distillation: Pooled Learning from Multiple Students",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V1GM9xDvIY": {
    "title": "Neural structure learning with stochastic differential equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OTMPdMH9JL": {
    "title": "Neural Eigenfunctions Are Structured Representation Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wPhbtwlCDa": {
    "title": "STARC: A General Framework For Quantifying Differences Between Reward Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fibxvahvs3": {
    "title": "GAIA: a benchmark for General AI Assistants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYMeXb6PAw": {
    "title": "Adaptive Compression of the Latent Space in Variational Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KC2MViQASx": {
    "title": "Mutual Information Estimation via $f$-Divergence and Data Derangement Based Learning Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AU2gS9ut61": {
    "title": "BrainPy: a differentiable brain simulator bridging brain simulation and brain-inspired computing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbRWdSxTPY": {
    "title": "SQS: Speech Quality Assessment in the Data Annotation Context",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LGzTtvisL3": {
    "title": "FLea: Improving federated learning on scarce and label-skewed data via privacy-preserving feature augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvbeD9Ttkx": {
    "title": "FOSI: Hybrid First and Second Order Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jMJ9IRWmH9": {
    "title": "Privacy Preserving API Fine-tuning for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0zIKlb0prF": {
    "title": "MPPN: Multi-Resolution Periodic Pattern Network For Long-Term Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KP4xJQcG3H": {
    "title": "Lagrangian Proximal Gradient Descent for Learning Convex Optimization Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VQ7Q6qdp0P": {
    "title": "Fine-tuning can cripple foundation models; preserving features may be the solution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3jATpVEGv": {
    "title": "GraphAgent: Exploiting Large Language Models for Interpretable Learning on Text-attributed Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B8FA2ixkPN": {
    "title": "GML-NeRF: Gate-guided Mutual Learning Framework for Neural Rendering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vb3O9jxTLc": {
    "title": "Lost in Translation: Conceptual Blind Spots in Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Va2IQ471GR": {
    "title": "Convergence of SVGD in KL divergence via approximate gradient flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tWOUmBHRv": {
    "title": "Offline Tracking with Object Permanence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwU9scoU4A": {
    "title": "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DjzvJCRsVf": {
    "title": "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ywGSgEmOYb": {
    "title": "Fine-Tuning Is All You Need to Mitigate Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vQqJJzL2Jf": {
    "title": "Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lvf7GnaLru": {
    "title": "Unraveling the Key Components of OOD Generalization via Diversification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzTpTRVtrP": {
    "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNgY6ODeMp": {
    "title": "Cross-modality Interpretable image classification via Concept Decomposition Vector of Visual Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5THrhXDV3": {
    "title": "Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AY6aM13gGF": {
    "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=87YOFayjcG": {
    "title": "JudgeLM : Fine-tuned Large Language Models are Scalable Judges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H03dW4TysQ": {
    "title": "Experts on Demand: Dynamic Routing for Personalized Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrBVFXwAmi": {
    "title": "Q-TAPE: A Task-Agnostic Pre-Trained Approach for Quantum Properties Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlH6VB5sJN": {
    "title": "A Parallel Multi-compartment Spiking Neuron For Multi-scale Sequential Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juStNETXI5": {
    "title": "Tiny-StyleWizard: Unleashing the Potential of Small Language Models in Complex Style Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0oIkKERYhH": {
    "title": "DOG: Discriminator-only Generation Beats GANs on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BIveOmD1Nh": {
    "title": "Learning Scalar Fields for Molecular Docking with Fast Fourier Transforms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8RPghUs3W": {
    "title": "Analytic DAG Constraints for Differentiable DAG Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5oJlyJXUxK": {
    "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fh8EYKFKns": {
    "title": "The Alignment Problem from a Deep Learning Perspective: A Position Paper",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9o7KuFcsps": {
    "title": "Unified Anomaly Detection via Multi-Scale Contrasted Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=79tJB1eTmb": {
    "title": "Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJJcs3zbmi": {
    "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YSJW69CFQ": {
    "title": "Enhancing Machine Learning System Reliability in Healthcare through Uncertainty Estimation and Multi-Modal Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WcOohbsF4H": {
    "title": "Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tBROYsEz9G": {
    "title": "How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s8cMuxI5gu": {
    "title": "Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3Foe1fDjh": {
    "title": "Expected Probabilistic Hierarchies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PFUrgJtfs0": {
    "title": "Lost in Transformation: Current roadblocks for Transformers in 3D medical image segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7QnIKlC1N": {
    "title": "GTMGC: Using Graph Transformer to Predict Molecule's Ground-State Conformation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=llXCyLhOY4": {
    "title": "Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhlwoC1XaN": {
    "title": "From Zero to Turbulence: Generative Modeling for 3D Flow Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YclZqtwf9e": {
    "title": "Slingshot Perturbation to Learning in Monotone Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fagTLzHFBs": {
    "title": "LDINet: Latent Decomposition and Interpolation for Single Image FMO Deblatting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vq75kRCYuY": {
    "title": "SOLO: Surrogate Online Learning at Once for Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4YK1e3Ehdy": {
    "title": "Understanding Deep Neural Networks as Dynamical Systems: Insights into Training and Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AlkANue4lm": {
    "title": "Non-Redundant Graph Neural Networks with Improved Expressiveness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oa758mIOcP": {
    "title": "A Structured Matrix Method for Nonequispaced Neural Operators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s25i99RTCg": {
    "title": "Multi-modal Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yvxDJ8eyBu": {
    "title": "MuseCoco: Generating Symbolic Music from Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q9R10ZKd8z": {
    "title": "PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tMzPZTvz2H": {
    "title": "Generalization of Deep ResNets in the Mean-Field Regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tIsYpRxMvr": {
    "title": "Good Better Best: Self-Motivated Imitation Learning For Noisy Demonstrations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pxI5IPeWgW": {
    "title": "ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6GySuKTJcd": {
    "title": "Energy-Guided Continuous Entropic Barycenter Estimation for General Costs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zj12nzlQbz": {
    "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5j6wtOO6Fk": {
    "title": "Hieros: Hierarchical Imagination on Structured State Space Sequence World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MZsKW0CraD": {
    "title": "Beyond Labeling Oracles: What does it mean to steal ML models?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExiBN1ZWJn": {
    "title": "Denoising Graph Dissipation Model Improves Graph Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlhjUkC7vH": {
    "title": "DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tD4NOxYTfg": {
    "title": "The Convergence of Variance Exploding Diffusion Models under the Manifold Hypothesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yx7TnC6AAp": {
    "title": "Towards Provably Efficient Learning of Extensive-Form Games with Imperfect Information and Linear Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlfTLt0zvd": {
    "title": "An Efficient Multi-Task Transformer for 3D Face Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zrxlSviRqC": {
    "title": "Learning energy-based models by self-normalising the likelihood",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WsHaBoucSG": {
    "title": "Emergent Language based Dialog for Collaborative Multi-agent Navigation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUveo5k1GF": {
    "title": "Improving equilibrium propagation without weight symmetry through Jacobian homeostasis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SGQ9aDvObu": {
    "title": "DIFAIR: Towards learning differenciated and interpretable representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WtNgFrPn8y": {
    "title": "Safe Online Bid Optimization with Return On Investment and Budget Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjCDNssXKU": {
    "title": "Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wcKGK0tRHD": {
    "title": "The Trifecta: Three simple techniques for training deeper Forward-Forward networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYXzKTljwx": {
    "title": "Uniform Localized Convergence and Sharper Generalization Bounds for Minimax Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3k6raldhEd": {
    "title": "A Best-of-Both-Worlds Algorithm for MDPs with Long-Term Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGQBpkIEuu": {
    "title": "Revisiting Data Augmentation in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HiClR4rwJf": {
    "title": "Value Factorization for Asynchronous Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKnzPdyeJu": {
    "title": "Structural Inference with Dynamics Encoding and Partial Correlation Coefficients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mFBR2ksIwY": {
    "title": "MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SCQfYpdoGE": {
    "title": "Prediction without Preclusion: Recourse Verification with Reachable Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Djw0XhjHZb": {
    "title": "Simplicial Representation Learning with Neural $k$-Forms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cVUOnF7iVp": {
    "title": "Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NzxCMe88HX": {
    "title": "Toward effective protection against diffusion-based mimicry through score distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KW3aAxkhE1": {
    "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LqaEEs3UxU": {
    "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IPayPEGwdE": {
    "title": "Learning Good Interventions in Causal Contextual Bandits with Adaptive Context",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wY67ZDQTE": {
    "title": "Cauchy-Schwarz Divergence Information Bottleneck for Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7sMR09VNKU": {
    "title": "Learning System Dynamics from Sensory Input under Optimal Control Principles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZTFUtldbC": {
    "title": "MeMo: Meaningful, Modular Controllers Via Information Bottlenecks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=14rn7HpKVk": {
    "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g8oaZRhDcf": {
    "title": "Copy Suppression: Comprehensively Understanding an Attention Head",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rc3RP9OoEJ": {
    "title": "In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzmKEpze8e": {
    "title": "Kalman Filter Online Learning from non-Stationary Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYjPk8mqAV": {
    "title": "Magnushammer: A Transformer-Based Approach to Premise Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bZHz9WYs9z": {
    "title": "Molecule Generation by Heterophilious Triple Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uLOFyiruin": {
    "title": "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojAc7y2P4K": {
    "title": "Dispatching Ambulances using Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49Tn5mfTy5": {
    "title": "Uncertainty Quantification Using a Codebook of Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ggu3cWldTy": {
    "title": "Unified Mirror Descent: Towards a Big Unification of Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u11Jwd3opH": {
    "title": "Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L8UNn7Llt4": {
    "title": "Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZtBP6DZah": {
    "title": "Contrastive Grouping-based Invariant Learning for Generalizable Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8cVivO5k5": {
    "title": "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZnnHDrBXD": {
    "title": "Tree-based Action-Manipulation Attack Against Continuous Reinforcement Learning with Provably Efficient Support",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HSUSo9p8X5": {
    "title": "Stochastic Subgoal Representation for Hierarchical Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CupHThqQl3": {
    "title": "It's About Time: Temporal References in Emergent Communication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HT2dAhh4uV": {
    "title": "Learning to Compose: Improving Object Centric Learning by Injecting Compositionality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CVldG5ohCy": {
    "title": "Adam through a Second-Order Lens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqsYs05Ri7": {
    "title": "Estimation of Concept Explanations Should be Uncertainty Aware",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8hc2UvwTaL": {
    "title": "FLAIM: AIM-based Synthetic Data Generation in the Federated Setting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8Lj9eoGl8": {
    "title": "Proximal Curriculum with Task Correlations for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRbnZs2WY4": {
    "title": "SelfClean: A Self-Supervised Data Cleaning Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LnxviiZ1xi": {
    "title": "MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wD8L86iCvD": {
    "title": "FINE-GRAINED AUDIO-VISUAL JOINT REPRESENTATIONS FOR MULTIMODAL LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rfSfDSFrRL": {
    "title": "Gated recurrent neural networks discover attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNzL1nglNB": {
    "title": "Label-encoding Risk Minimization under Label Insufficient Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R1crLHQ4kf": {
    "title": "Leveraging characteristics of the output distribution for identifying adversarial audio examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WhZoCLRWYJ": {
    "title": "Light Schrödinger Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKkiX32Zw1": {
    "title": "Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNMsieEiAc": {
    "title": "Prompt2Rec : Prompt based user and item Re-characterizing method for Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEoBvQWzHo": {
    "title": "Dirichlet-based Uncertainty Quantification for Personalized Federated Learning with Improved Posterior Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eCGpNGDeNu": {
    "title": "Reward-Free Curricula for Training Robust World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7UhxsmbdaQ": {
    "title": "Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uf4Hr5qU6L": {
    "title": "PreCoT: Problem Representation Enhances Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GaBg3pgXfX": {
    "title": "MusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical Representation of Symbolic Music",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bGJZXb26lo": {
    "title": "DITTO: Offline Imitation Learning with World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsNyDvNQTc": {
    "title": "Leveraging Uncertainty Estimates To Improve Classifier Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jzzEHTBFOT": {
    "title": "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GAXedKmbFZ": {
    "title": "Disco-Bench: A Context-Aware Evaluation Benchmark for Language Modelling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqCt76eiNt": {
    "title": "Advantage-Aware Policy Optimization for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FTpdQBoBd0": {
    "title": "Enhancing Fine-Tuning Performance of Large-Scale Text-to-Image Models on Specialized Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2UlHeyyC0": {
    "title": "Retrieval-augmented Vision-Language Representation for Fine-grained Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvXK8Xk9Jk": {
    "title": "DEEP NEURAL NETWORK INITIALIZATION WITH SPARSITY INDUCING ACTIVATIONS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=324zEJCo3a": {
    "title": "Local Vs. Global Interpretability: A Computational Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQUbpAHbIZ": {
    "title": "Post-Nonlinear Causal Relationship with Finite Samples: A Maximal Correlation Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ic1Z7Qe9xH": {
    "title": "Elastic Load Balancing for Dynamic LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jwzm44fsJ8": {
    "title": "Multilingual Code Retrieval Without Paired Data: New Datasets and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZnmofqLWMQ": {
    "title": "Zero-shot Image Restoration via Diffusion Inversion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrYiwlDRQO": {
    "title": "PeFLL: Personalized Federated Learning by Learning to Learn",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUe1YqEgd6": {
    "title": "Unsupervised motion segmentation in one go: Smooth long-term model over a video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FYKVPOHCpE": {
    "title": "Improving Non-Transferable Representation Learning by Harnessing Content and Style",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W4rbphLht": {
    "title": "A Semi-smooth, Self-shifting, and Singular Newton Method for Sparse Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ciBFYxzpBT": {
    "title": "PASTA: Pretrained Action-State Transformer Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HaLxcQ6OOm": {
    "title": "Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xx0ITyHp3u": {
    "title": "Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BEyEziZ4R6": {
    "title": "DP-SGD Without Clipping: The Lipschitz Neural Network Way",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3PWYAlAQxv": {
    "title": "Neural Networks Trained by Weight Permutation are Universal Approximators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LlG0jR7Yjh": {
    "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B21c9hT1D7": {
    "title": "High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nfIAEJFiBZ": {
    "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W3GLNImfS": {
    "title": "Human Feedback is not Gold Standard",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pLvh9DTyoE": {
    "title": "Integrating Visual Cues via Prompting for Low-Resource Multimodal Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vpJMJerXHU": {
    "title": "ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMkvcg1i1u": {
    "title": "Abstract Interpretation of ReLU Neural Networks with Optimizable Polynomial Relaxations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3FHMoKZcS": {
    "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NukRlEUICA": {
    "title": "AFFINE INVARIANCE IN CONTINUOUS-DOMAIN CONVOLUTIONAL NEURAL NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOFLn0pMoe": {
    "title": "High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YrXHEb2qMb": {
    "title": "Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IAWIgFT71j": {
    "title": "Assessing Large Language Models on Climate Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PORUmWsgBN": {
    "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MU6jInwj7p": {
    "title": "LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jU3zRzUBiD": {
    "title": "Compensating for Nonlinear Reduction with Linear Computations in Private Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=if2vRbS8Ew": {
    "title": "First-order ANIL provably learns representations despite overparametrisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jGuXGNcK6O": {
    "title": "The Fundamental Limits of Least-Privilege Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZGNWW7xZ6Q": {
    "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=567BjxgaTp": {
    "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1VcKvdYbUM": {
    "title": "APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CetUU9FSt": {
    "title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86zAUE80pP": {
    "title": "CPPO: Continual Learning for Reinforcement Learning with Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKuimaBj4I": {
    "title": "Learning Optimal Contracts: How to Exploit Small Action Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fIKRJeLH7W": {
    "title": "Proper Backward Connection Placement Boosts Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9cQtXpRshE": {
    "title": "AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=am7BPV3Cwo": {
    "title": "Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=17ZbByq95E": {
    "title": "Memory-Efficient Backpropagation through Large Linear Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uizIvVBY8P": {
    "title": "Continual Supervised Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuoeYIGaSW": {
    "title": "$\\texttt{PREMIER-TACO}$ is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BI1N3lTWtn": {
    "title": "A Multi-Level Framework for Accelerating Training Transformer Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7q7s5fXEpP": {
    "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pa4hecILrt": {
    "title": "Incremental Successive Halving for Hyperparameter Optimization with Budget Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tc3duzOHa8": {
    "title": "RODEO: Robust Out-of-Distribution Detection Via Exposing Adaptive Outliers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izO4mxI9nU": {
    "title": "HAICO-CN: Human-AI Collaboration By Cluster-wise Noisy-Label Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oQKKlzxV1o": {
    "title": "Online Information Acquisition: Hiring Multiple Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sZZ3R0lV9f": {
    "title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxmvbzw76R": {
    "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y8OaqdX5Xt": {
    "title": "Planning with Theory of Mind for Few-Shot Adaptation in Sequential Social Dilemmas",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IkIqzDI7ie": {
    "title": "M$^4$LE: A Multi-Ability Multi-Range Long Context Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MzjiMxlWab": {
    "title": "Learning Multi-Faceted Prototypical User Interests",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11oqo92x2Z": {
    "title": "Detection and Segmentation of Solar Farms in Satellite Imagery: A Study of Deep Neural Network Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0V311Uh8q1": {
    "title": "Algorithmic Stability Unleashed: Generalization Bounds with Unbounded Losses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c4QgNn9WeO": {
    "title": "LMEye: An Interactive Perception Network for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZWTHU7AsQ": {
    "title": "Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBIJRIYTqa": {
    "title": "Bandits with Replenishable Knapsacks: the Best of both Worlds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QFo2wxQEW6": {
    "title": "Autonomous Catheterization with Open-source Simulator and Expert Trajectory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YRXDl6I3j5": {
    "title": "Tall Tales at Different Scales: Evaluating Scaling Trends For Deception in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2xLkpkh0s": {
    "title": "FARSE-CNN: Fully Asynchronous, Recurrent and Sparse Event-Based CNN",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1jIKhMJ8y": {
    "title": "Learning Embeddings for Sequential Tasks Using Population of Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SvEMbCMgb5": {
    "title": "$R^2$: Range Regularization for Model Compression and Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gsZAtAdzkY": {
    "title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5qi6fnnw7": {
    "title": "Conservative World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwMfg9PfPs": {
    "title": "Out-of-Variable Generalisation for Discriminative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OWpp0TjdTt": {
    "title": "Semi-supervised batch learning from logged data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVsiTj9aOO": {
    "title": "Improved Variational Bayesian Phylogenetic Inference using Mixtures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39cPKijBed": {
    "title": "Training Unbiased Diffusion Models From Biased Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tPEwSYPtAC": {
    "title": "Towards Robust Out-of-Distribution Generalization Bounds via Sharpness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f43Kxj0FaW": {
    "title": "Meta-Prior: Meta learning for Adaptive Inverse Problem Solvers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JfqN3gu0i7": {
    "title": "The optimality of kernel classifiers in Sobolev space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jiQg5IvuYF": {
    "title": "Corgi$^2$: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dW7FRwi1eA": {
    "title": "Learning a Reusable Meta Denoiser for Learning with Noisy Labels on Multiple Target Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eOCvA8iwXH": {
    "title": "Neural Fourier Transform: A General Approach to Equivariant Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3GurO0kRue": {
    "title": "On Harmonizing Implicit Subpopulations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jdynlBj3b0": {
    "title": "Can Class-Priors Help Single-Positive Multi-Label Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pRpMAD3udW": {
    "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOwDQl8haC": {
    "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9k4Yvb75ED": {
    "title": "EquiAV: Single-modal Equivariance Promotes Audio-Visual Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2boLXjsHsB": {
    "title": "Multi-Objective Reinforcement Learning for Forward-Backward Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rKMQhP6iAv": {
    "title": "Personas as a way to Model Truthfulness in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RTLjdy6Ntk": {
    "title": "FL-GNN: A Fuzzy-logic Graph Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Y7r6xueJJ": {
    "title": "Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKhNBulNMh": {
    "title": "Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BURvGotSLz": {
    "title": "Is Training Necessary for Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8eWha27jw": {
    "title": "Accelerating Federated Learning with Quick Distributed Mean Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itGkF993gz": {
    "title": "MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUO1HXz4an": {
    "title": "Negative Label Guided OOD Detection with Pretrained Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgayrNSbri": {
    "title": "Close the Gap: Lightweight Image Captioning via Retrieval Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE6WxgrOWP": {
    "title": "Chain of Images for Intuitively Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IhOeYKqnfp": {
    "title": "Continual Memory Neurons",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jHkUDyEO9": {
    "title": "Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZrY38sUYWs": {
    "title": "Feature Map Matters in Out-of-distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wYmcfur889": {
    "title": "Data Prediction Denoising Models: The Pupil Outdoes the Master",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZRzlhfMqHt": {
    "title": "Periodic and Random Sparsity for Multivariate Long-Term Time-Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QI7tVrh2c": {
    "title": "Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tj6Wcx7gVk": {
    "title": "Probabilistically Rewired Message-Passing Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HhVns87e74": {
    "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6ajk22thA": {
    "title": "HGMD: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qIn2IgMWYg": {
    "title": "Iterative Search Attribution for Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duZANm2ABX": {
    "title": "BadEdit: Backdooring Large Language Models by Model Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUnFOgAy1D": {
    "title": "Efficient Differentiable Approximation of the Generalized Low-rank Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TBLe2BHBsr": {
    "title": "Dilated convolution neural operator for multiscale partial differential equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7ZEcoSdXQ": {
    "title": "Incentivizing Data Collection from Heterogeneous Clients in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qxLVaYbsSI": {
    "title": "Robust Training of Federated Models with Extremely Label Deficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D3JpYSn7dL": {
    "title": "An Instance-Level Framework for Multi-tasking Graph Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktJAF3lxbi": {
    "title": "On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHLVmV88Zy": {
    "title": "Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1ZEMXxSz1": {
    "title": "Segment Anything Model is a Good Teacher for Local Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmQAKu1wCe": {
    "title": "Temporal Flexibility in Spiking Neural Networks: A Novel Training Method for Enhanced Generalization Across Time Steps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qGaIMO8dqD": {
    "title": "Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sW95puhphh": {
    "title": "DECENTRALIZED MULTI-AGENT REINFORCEMENT LEARNING VIA ANTICIPATION SHARING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3w6xuXDOdY": {
    "title": "A Study of Generalization in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xwKt6bUkXj": {
    "title": "Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47hDbAMLbc": {
    "title": "OPTIMAL ROBUST MEMORIZATION WITH RELU NEURAL NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iAYIRHOYy8": {
    "title": "Neural Contractive Dynamical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpXGPCBOTX": {
    "title": "Sparsistency for inverse optimal transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yV6fD7LYkF": {
    "title": "ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HexshmBu0P": {
    "title": "A Recipe for Watermarking Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pEKJl5sflp": {
    "title": "Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dlIMcmlAdk": {
    "title": "Noise-free Score Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BO3aRwGzq0": {
    "title": "DINAR: Fine-Grained Privacy Preserving Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWS4iOkhXv": {
    "title": "Scalable Lipschitz Estimation for CNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z0ojN315Uf": {
    "title": "Differentially Private Principal Component Analysis for Vertically Partitioned Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VhQUwxIHER": {
    "title": "Small Variance, Big Fairness: A Path to Harmless Fairness without Demographics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pB1FeRSQxh": {
    "title": "Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sPUrdFGepF": {
    "title": "Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qe49ybvvPs": {
    "title": "Diverse Projection Ensembles for Distributional Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6r9t0HtqQ": {
    "title": "KEFI: Kernel-based Feature Identification for Generalizable Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESmvnmZ9fT": {
    "title": "SCoRF: Single-stage convolutional radiance fields for effective 3D scene representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVl1KO5K76": {
    "title": "Momentum-SAM: Sharpness Aware Minimization without Computational Overhead",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hy84B74XFt": {
    "title": "Towards Interpretable Controllability in Object-Centric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MK7TEe7SJ3": {
    "title": "USTAM: UNIFIED SPATIO-TEMPORAL ATTENTION MIXFORMER FOR VISUAL OBJECT TRACKING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nY9nITZQjc": {
    "title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iGHPVbttMs": {
    "title": "The Cyclical Chaos And Its Equilibrium",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vKViCoKGcB": {
    "title": "Intriguing Properties of Data Attribution on Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5jBLcVmhe": {
    "title": "SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree Expansion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H6XiAoyugv": {
    "title": "Robust Backdoor Attack with Visible, Semantic, Sample-specific and Compatible Triggers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekz1hN5QNh": {
    "title": "Fully Hyperbolic Convolutional Neural Networks for Computer Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKGpe1792U": {
    "title": "RGLA: Reverse Gradient Leakage Attack using Inverted Cross-Entropy Loss Function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FdUloEgBSE": {
    "title": "Text-guided Diffusion Model for 3D Molecule Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PCXvcULwiI": {
    "title": "Benchmarking Structural Inference Methods for Interacting Dynamical Systems with Synthetic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tzh6xAJSll": {
    "title": "Scaling Laws for Associative Memories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMZc3KqjEb": {
    "title": "Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjXjkxhSdE": {
    "title": "Enhancing One-Shot Pruned Generative Pre-training Language Models through Sparse-Dense-Sparse Mechanism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1BuWv9poWz": {
    "title": "Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fVxIEHGnVT": {
    "title": "An interpretable error correction method for enhancing code-to-code translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgCz44ebFe": {
    "title": "May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYsLU5TEEo": {
    "title": "Counterfactual Image Generation for adversarially robust and interpretable Classifiers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLLZhbBSOU": {
    "title": "RLIF: Interactive Imitation Learning as Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K13qUXDsTS": {
    "title": "Bidirectional-Reachable Hierarchical RL with Mutually Responsive Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyCPspuM5L": {
    "title": "PowerGraph: A power grid benchmark dataset for graph neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DyBcEiIs5J": {
    "title": "Boosting Adverse Weather Crowd Counting via Multi-queue Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6jFjYmahxu": {
    "title": "DiffSound: Differentiable Modal Sound Simulation for Inverse Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBAWiEjogY": {
    "title": "ProteiNexus: Illuminating Protein Pathways through Structural Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXjz7p4qha": {
    "title": "Rotation Invariant Quantization for Model Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MVmT6uQ3cQ": {
    "title": "The Need for Speed: Pruning Transformers with One Recipe",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWS4MueyzC": {
    "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xI4yNlkaqh": {
    "title": "Towards 3D Molecule-Text Interpretation in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CtOA9aN8fr": {
    "title": "Effective pruning of web-scale datasets based on complexity of concept clusters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngp5jzx5oK": {
    "title": "Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLTgo1dcIl": {
    "title": "Stochastic Extragradient with Flip-Flop Shuffling & Anchoring: Provable Improvements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FsVxd9CIlb": {
    "title": "AttEXplore: Attribution for Explanation with model parameters eXploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PAfnMGXief": {
    "title": "BRUSLEATTACK: QUERY-EFFICIENT SCORE-BASED SPARSE ADVERSARIAL ATTACK",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N23A4ybMJr": {
    "title": "Win-Win: Training High-Resolution Vision Transformers from Two Windows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bDkisS75zy": {
    "title": "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fn655mJ4bv": {
    "title": "SOInter: A Novel Deep Energy-Based Interpretation Method for Explaining Structured Output Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gMLQwKDY3N": {
    "title": "A Private Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUM39YTRxH": {
    "title": "Text2Reward: Dense Reward Generation with Language Models for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OzAGE2W9yz": {
    "title": "Improving Neural Program Induction by Reflecting on Failures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m52uU0dVbH": {
    "title": "Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8y5vlBuRll": {
    "title": "Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VPx3Jw2MSk": {
    "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cLIvvqf3Wk": {
    "title": "Attribute-Guided Diffusion for Unsupervised Few-Shot Font Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3tM1l5tSbv": {
    "title": "Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6tNkN6ate": {
    "title": "Interpretable Diffusion via Information Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuIiLSktO4": {
    "title": "Algorithms for Caching and MTS with reduced number of predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=crF9dk4poo": {
    "title": "Interpretable Deep Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T218mLyMHg": {
    "title": "Spectrum-guided Multi-view Graph Fusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0uzEweZB1": {
    "title": "FrAug: Frequency Domain Augmentation for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CIGhcJYJH": {
    "title": "Two-timescale Extragradient for Finding Local Minimax Points",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UJeIujVxMn": {
    "title": "FedEBA+: Towards Fair and Effective Federated Learning via Entropy-based Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JxQyat11M": {
    "title": "Zero-Shot Visual Classification with Guided Cropping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILYjDvUM6U": {
    "title": "Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BUSZQWbRaR": {
    "title": "Generalized Convergence Analysis of Tsetlin Machines: A Probabilistic Approach to Concept Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KzMMv0OygD": {
    "title": "TeG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Task Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lK0WxHeups": {
    "title": "Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GQGNLEHmdl": {
    "title": "AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QcMdPYBwTu": {
    "title": "Scalable and Effective Implicit Graph Neural Networks on Large Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EMVct15bl5": {
    "title": "A qualitative theory of dynamical systems for assessing stability in ResNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qofh48zW3T": {
    "title": "Distributional Distance Classifiers for Goal-Conditioned Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oXYZJXDdo7": {
    "title": "Retrieval is Accurate Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d18RgYF6Y7": {
    "title": "Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NdcQQ82mfy": {
    "title": "Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qwxe8WKSgy": {
    "title": "BTBS-LNS: A Binarized-Tightening, Branch and Search Approach of Learning Large Neighborhood Search Policies for MIP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DkhYlWZq84": {
    "title": "Protein Captioning: Bridging the Gap between Protein Sequences and Natural Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3IyC5lQTSi": {
    "title": "Fairness Through Matching for better group fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c85tdYOOju": {
    "title": "Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V3j5d0GQgH": {
    "title": "FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uSX6IbpGZ9": {
    "title": "Trend/Seasonality based Causal Structure for Time Series Counterfactual Outcome Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk7yW3vmSq": {
    "title": "Conceptual Graph Counterfactuals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5HGPR6fg2S": {
    "title": "Normalized Space Alignment: A Versatile Metric for Representation Space Discrepancy Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gK7HIepdn7": {
    "title": "MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKf6JtXtoy": {
    "title": "MAP IT to Visualize Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=070DFUdNh7": {
    "title": "GraphGPT: Graph Learning with Generative Pre-trained Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjwZHuQ3cm": {
    "title": "LIMANS: Linear Model of the Adversarial Noise Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=spvaV5LELF": {
    "title": "Measuring Vision-Language STEM Skills of Neural Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkpVgxHQ1S": {
    "title": "Latent Diffusion Counterfactual Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PqjQmLNuJt": {
    "title": "DUAL DENOISING LOGICAL REASONING FOR INDUC\u0002TIVE KNOWLEDGE GRAPH COMPLETION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wvHfsiWXUR": {
    "title": "Tell, Don't Show: Internalized Reasoning influences how LLMs generalize",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PaOuEBMvTG": {
    "title": "Multiple Object Stitching for Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dXRWP4n15q": {
    "title": "$\\sigma$-zero: Gradient-based Optimization of \\\\$\\ell_0$-norm Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RIbNmx984": {
    "title": "On Double-Descent in Reinforcement Learning with LSTD and Random Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GhYXocT75t": {
    "title": "Forward-Backward Reasoning in Large Language Models for Mathematical Verification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N5ID99rsUq": {
    "title": "Stability and Generalization in Free Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRmmIdqvZp": {
    "title": "Barycentric Alignment of Mutually Disentangled Modalities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zW1tyw3UFu": {
    "title": "Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MLShfiJ3CB": {
    "title": "Towards Reliable Backdoor Attacks on Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrtGfTGaGe": {
    "title": "The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHAr7ZA1OL": {
    "title": "Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tAmfM1sORP": {
    "title": "Large Language Models can Learn Rules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XZGklkaOsL": {
    "title": "Unified Medical Image Pre-training in Language-Guided Common Semantic Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycF7mKfVGO": {
    "title": "Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PCTqol2hvy": {
    "title": "Characterizing ResNet's Universal Approximation Capability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3P87ptzvTm": {
    "title": "Optimal Multiple Transport with Applications to Visual Matching, Model Fusion and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNiI3aGcE6": {
    "title": "Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2fSyBPBfBs": {
    "title": "Bilevel Optimization without Lower-Level Strong Convexity from the Hyper-Objective Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwjDyJfe3m": {
    "title": "Benchmarks for Reinforcement Learning with Biased Offline Data and Imperfect Simulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjeqMvQpen": {
    "title": "Transformer Fusion with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TkdMRKvZDJ": {
    "title": "Phrase Grounding-based Style Transfer for Single-Domain Generalized Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nkKWY5JjtZ": {
    "title": "Exact Mean Square Linear Stability Analysis for SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMoifbuxjB": {
    "title": "Towards Meta-Pruning via Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=libLqoInAd": {
    "title": "Reliable Classifications with Guaranteed Confidence using the Dempster-Shafer Theory of Evidence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWvKBCYh4S": {
    "title": "MoLE: Mixture of LoRA Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tth2qXY7RU": {
    "title": "Super Floating-Point (SuFP): Efficient To All. Multi-Region Piecewise Quantization using Scalable Bias with Hardware Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I8pdQLfR77": {
    "title": "Improving MLP Module in Vision Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NJ6nyv3XWH": {
    "title": "Leveraging Graph Neural Networks to Boost Fine-Grained Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T11rD8k578": {
    "title": "Calibrated on Average, but not Within Each Slice: Few-shot Calibration for All Slices of a Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=adSGeugiuj": {
    "title": "On the Posterior Distribution in Denoising: Application to Uncertainty Quantification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jeNWwtIX71": {
    "title": "Provable Domain Generalization via Information Theory Guided Distribution Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T2ToleSDk6": {
    "title": "Learning Constraints from Offline Dataset via Inverse Dual Values Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BLGQ3oqldb": {
    "title": "LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eiC4BKypf1": {
    "title": "Turning large language models into cognitive models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vI95kcLAoU": {
    "title": "Skip-Attention: Improving Vision Transformers by Paying Less Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=evk6pPJqMy": {
    "title": "Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1qDRwhe379": {
    "title": "Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phBS6YpTzC": {
    "title": "Benchmarking and Improving Generator-Validator Consistency of Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u7559ZMvwY": {
    "title": "Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a6SntIisgg": {
    "title": "LogoRA: Local-Global Representation Alignment for Robust Time Series Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MLBdiWu4Fw": {
    "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbFBh3LMKi": {
    "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gg7cXo3S8l": {
    "title": "Dictionary Contrastive Forward Learning via Adaptive Label Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXBsK3GsL6": {
    "title": "Soft iEP: On the Exploration Inefficacy of Gradient Based Strong Lottery Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2XfOm3RJa": {
    "title": "How Large Language Models Implement Chain-of-Thought?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmM4Ecm4HJ": {
    "title": "Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3wGi5m2YHY": {
    "title": "FlowHash: Accelerating Audio Search with Balanced Hashing via Normalizing Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hnrB5YHoYu": {
    "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFJVpeYcnv": {
    "title": "Bandit Learning in Matching: Unknown Preferences On Both Sides",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PnR1MNen7u": {
    "title": "Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mNYF0IHbRy": {
    "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzAJbTClAz": {
    "title": "FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4qWt62BdN": {
    "title": "DSparsE: Dynamic Sparse Embedding for Knowledge Graph Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tveiUXU2aa": {
    "title": "SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUtTtiNksb": {
    "title": "FFCA-Net: Stereo Image Compression via Fast Cascade Alignment of Side Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EWTFMkTdkT": {
    "title": "Invariance-based Learning of Latent Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=znjaiy1Z9q": {
    "title": "AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F1TKzG8LJO": {
    "title": "Robotic Task Generalization via Hindsight Trajectory Sketches",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=90yw2uM6J5": {
    "title": "Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYv3OMboTF": {
    "title": "ExcelFormer: Making Neural Network Excel in Small Tabular Data Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rL7xsg1aRn": {
    "title": "Masked Structural Growth for 2x Faster Language Model Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rc7dAwVL3v": {
    "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kq5avXrkpY": {
    "title": "Federated Optimization Algorithms with Random Reshuffling and Gradient Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JiIKjcwrr": {
    "title": "Robust Self-supervised Learning in Heterogeneous Graph Based on Feature-Topology Balancing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZVXqiaI4U": {
    "title": "Attribute Based Interpretable Evaluation Metrics for Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KojubHBr8": {
    "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jw0qHTfdhv": {
    "title": "Learning to Generate Predictor for Long-Term Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4iQuByhNie": {
    "title": "ContextNER: Contextual Phrase Generation at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=258EqEA05w": {
    "title": "A Simple Data Augmentation for Feature Distribution Skewed Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Tl99XWXC": {
    "title": "Efficient Transfer Learning in Diffusion Models via Adversarial Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXv0zQ1umU": {
    "title": "Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6Ux5OCGmW": {
    "title": "RSAM: Learning on Manifolds with Riemannian Sharpness-Aware Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xwZhyKynCB": {
    "title": "${\\rm EFO}_k$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cfnevfQDsx": {
    "title": "Converging and Stabilizing Generative Adversarial Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yeKQXf08Db": {
    "title": "Dynamic Continuous Hyperparameter Tuning for Generalized Linear Contextual Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXWTXXtHNl": {
    "title": "Label-Noise Robust Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PJwAkg0z7h": {
    "title": "EasyTPP: Towards Open Benchmarking Temporal Point Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ec2rYpP42y": {
    "title": "Solving Inverse Problem With Unspecified Forward Operator Using Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1pSL2cXWoz": {
    "title": "ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hI18CDyadM": {
    "title": "Adaptive Window Pruning for Efficient Local Motion Deblurring",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mY9aGiMn0": {
    "title": "Sparser, Better, Deeper, Stronger: Improving Sparse Training with Exact Orthogonal Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W2d3LZbhhI": {
    "title": "A unified sampling framework for solver searching of Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpTitXWGNO": {
    "title": "xCodeEval: An Execution based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sZACTXpSc6": {
    "title": "ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uw8xvFqVAE": {
    "title": "A representation-learning game for classes of prediction tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZIbUx5dzfZ": {
    "title": "ORBIS: Open Dataset Can Rescue You From Dataset Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NdbUfhttc1": {
    "title": "Learning to Optimize for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pGL4P2kg6V": {
    "title": "Towards Interpretable Continual Learning Through Controlling Concepts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1MRfyGLCcU": {
    "title": "Graph Representation Learning enhanced Semi-supervised Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdoD5OYHPJ": {
    "title": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8jH6rjw8c": {
    "title": "Fairness Improves Learning from Noisily Labeled Long-Tailed Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDuqo9KTzh": {
    "title": "Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxe0hQ5mxp": {
    "title": "Elephant Neural Networks: Born to Be a Continual Learner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ChNy95ovpF": {
    "title": "DebateGPT: Fine-tuning Large Language Models with Multi-agent Debate Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFjhxXrTlX": {
    "title": "Learning Unorthogonalized Matrices for Rotation Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vULHgaoASR": {
    "title": "MissDiff: Training Diffusion Models on Tabular Data with Missing Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bYQkOPvgDw": {
    "title": "Probabilistic Graphical Model for Robust Graph Neural Networks against Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=loYSzjSaAK": {
    "title": "Submodular Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7GkdjhupsV": {
    "title": "InfoAug: Mutual Information Informed Augmentation for Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hyjfjf8GA0": {
    "title": "A Case Study for the Behaviors of Generalists and Specialists in Competitive Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wwotGBxtC3": {
    "title": "Data-Efficient Molecular Generation with Hierarchical Textual Inversion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F6QqOdLzsx": {
    "title": "Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ouNI9pkf9g": {
    "title": "Quantifying Anonymity in Score-Based Generators with Adversarial Fingerprinting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kPrxk6tUcg": {
    "title": "Neuron-Enhanced AutoEncoder Matrix Completion: Theory and Practice",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIv15j44gR": {
    "title": "Estimating Heterogeneous Treatment Effect with Delayed Response",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=viJlKbTfbb": {
    "title": "What If You Were Not There? Learning Causally-Aware Representations of Multi-Agent Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OUeIBFhyem": {
    "title": "$\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1eynu9DVf": {
    "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l3s3HwJYDm": {
    "title": "Opponent Modeling based on Sub-Goal Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bpkhu2ExxU": {
    "title": "Stochastic Modified Equations and Dynamics of Dropout Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJLD9MquPH": {
    "title": "Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hDKLrK8AwP": {
    "title": "Towards Readable Scalable Vector Graphic Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uLCtVTzFhg": {
    "title": "Contrastive Positive Unlabeled Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7OwML7fwl8": {
    "title": "Fairness without Sensitive attributes via Noise and Uncertain Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=npoi2fr882": {
    "title": "3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jd5GokdySz": {
    "title": "Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWNJFD1l8M": {
    "title": "Transferring Learning Trajectories of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S7T0slMrTD": {
    "title": "Resolving Knowledge Conflicts in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2EN8MCHiz": {
    "title": "Understanding Vision and Language Representations under the Lens of Intrinsic Dimension",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lCLdLlXAvt": {
    "title": "Average Sensitivity of Hierarchical Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RNgZTA4CTP": {
    "title": "Best Possible Q-Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IxpTsFS7mh": {
    "title": "VQ-TR: Vector Quantized Attention for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BuFNoKBiMs": {
    "title": "Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1CK45cqkEh": {
    "title": "Unsupervised Order Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vEgLnT9avP": {
    "title": "ResolvNet: A Graph Convolutional Network with multi-scale Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mrBd4hyWlP": {
    "title": "CRL-NET: ACCELERATED MAGNETIC RESONANCE IMAGING RECONSTRUCTION THROUGH COIL REPRESENTATION LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anzIzGZuLi": {
    "title": "Making Pre-trained Language Models Great on Tabular Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YRJDZYGmAZ": {
    "title": "Domain Prompt Matters a Lot in Multi-Source Few-Shot Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ur25Xxvzsg": {
    "title": "Transferable Deep Clustering Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F4bmOrmUwc": {
    "title": "Fixed Non-negative Orthogonal Classifier: Inducing Zero-mean Neural Collapse with Feature Dimension Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rn3qJGOitY": {
    "title": "Linear Attention via Orthogonal Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6r0BOIb771": {
    "title": "Sequential Bayesian Continual Learning with Meta-Learned Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qgLyKwXVDs": {
    "title": "FreeLM: Fine-Tuning-Free Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ug8wDSimNK": {
    "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKDwsJmrDJ": {
    "title": "Collaborating Heterogeneous Natural Language Processing Tasks via Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJiOQw1fkF": {
    "title": "Enhanced Model-agnostic Training of Deep Tabular Generation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j8hdRqOUhN": {
    "title": "Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHVIxJGwr4": {
    "title": "Learning to Branch with Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pk0iUCNVPa": {
    "title": "Polynomial-based Self-Attention for Table Representation learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EFGwiZ2pAW": {
    "title": "SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tG5mpAM7ZK": {
    "title": "Extending to New Domains without Visual and Textual Oracles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kg0IDQF3wp": {
    "title": "LegoMT2: Non-Blocking Federated Learning for Massive Multilingual Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7x0XVlCpX": {
    "title": "MaGIC: Multi-modality Guided Image Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kz3yckpCN5": {
    "title": "The False Promise of Imitating Proprietary Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=psEswR8Jz4": {
    "title": "AmortizedPeriod: Attention-based Amortized Inference for Periodicity Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikwEDva1JZ": {
    "title": "How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycv2z8TYur": {
    "title": "EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NeKjkOWvwd": {
    "title": "Rethinking the OoD Generalization for Deep Neural Network: A Frequency Domain Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eMHn77ZKOp": {
    "title": "Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tr3fZocrI6": {
    "title": "Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkkHqoerLV": {
    "title": "Alice Benchmarks: Connecting Real World Object Re-Identification with the Synthetic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6c4gv0E9sF": {
    "title": "SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6imHU4Ebu": {
    "title": "Large Language Models as Generalizable Policies for Embodied Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jg8y1buQ3r": {
    "title": "LLM-driven Hateful Meme Detection via Cross-modal Memorizing and Self-rejection Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9pKtcJcMP3": {
    "title": "Video Language Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JuMFjSkpD": {
    "title": "Fair Attribute Classification via Distance Covariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mao3y822aM": {
    "title": "NanoLM: An Affordable LLM Study Benchmark via Accurate Loss Prediction Across Scales",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzxKLZNbrQ": {
    "title": "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ffTRtzXkIW": {
    "title": "PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QmZKc7UZCy": {
    "title": "LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rpH9FcCEV6": {
    "title": "An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PyHRUMxKbT": {
    "title": "InfoNet: An Efficient Feed-Forward Neural Estimator for Mutual Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tlsdsb6l9n": {
    "title": "Mol-Instructions - A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtT6o5tfHu": {
    "title": "Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iP31fDtrPR": {
    "title": "Learning Directed Graphical Models with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gCjeBKuDlc": {
    "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gTRtDFm8Di": {
    "title": "Reduce, Reuse, and Recycle: Navigating Test-Time Adaptation with OOD-Contaminated Streams",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByXWN19vWP": {
    "title": "Confident Sinkhorn Allocation for Pseudo-Labeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o83eu4H9Mb": {
    "title": "Information Retention via Learning Supplemental Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3NmO9lY4Jn": {
    "title": "Don't Play Favorites: Minority Guidance for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E60SIDItyT": {
    "title": "Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BMw4Cm0gGO": {
    "title": "C-MCTS: Safe Planning with Monte Carlo Tree Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1RqSVIf3c": {
    "title": "Bayesian Preference Elicitation for Personalized Prefactual Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U17KoLrXE8": {
    "title": "ObjectNet Captions: Models are not superhuman captioners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTwwtlEfTF": {
    "title": "Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n6mLhaBahJ": {
    "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zix86UbMGh": {
    "title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imuVEKaU3b": {
    "title": "Noise-guided Unsupervised Outlier Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n7Sr8SW4bn": {
    "title": "Mayfly: a Neural Data Structure for Graph Stream Summarization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhMEkBSdIV": {
    "title": "LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with Class Taxonomies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IrdbUQ1zTw": {
    "title": "Reinforcement Learning-based Layer-wise Aggregation for Personalized Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6jbcaCHqO": {
    "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wAsjsSe0U6": {
    "title": "Visual Semantic Learning via Early Stopping in Inverse Scale Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1SktNMHA7": {
    "title": "Rethinking Label Smoothing as a Tool for Embedding Perturbation Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11WAKGH8uv": {
    "title": "FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=776lhoaulC": {
    "title": "Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HqmpIud9Uq": {
    "title": "Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbWtOYIzIK": {
    "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HFtrXBfNru": {
    "title": "Temporal Generalization Estimation in Evolving Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yN4Wv17ss3": {
    "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUVvUf9SZB": {
    "title": "On Learning with a Concurrent Verifier: Convexity, Improving Bounds, and Complex Requirements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZgECfyHXF": {
    "title": "On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c5pwL0Soay": {
    "title": "METRA: Scalable Unsupervised RL with Metric-Aware Abstraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mXpNp8MMr5": {
    "title": "On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RTI6MLwWbs": {
    "title": "Physics-infused Intention Network for Crowd Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6N8TW504aa": {
    "title": "Graphical Multioutput Gaussian Process with Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQYpgReSRk": {
    "title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CRkvR8TJkk": {
    "title": "A Game-theoretic Approach to Personalized Federated Learning Based on Target Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lzpHNyhIbr": {
    "title": "CropNet: An Open Large-Scale Dataset with Multiple Modalities for Climate Change-aware Crop Yield Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AB077M4TY": {
    "title": "Dynamic Training Guided by Training Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsnWEcuymH": {
    "title": "Accelerating Simulation-Based Influence Maximization via Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pAsQSWlDUf": {
    "title": "Soft Contrastive Learning for Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qA4foxO5Gf": {
    "title": "Efficient Integrators for Diffusion Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSDszJ2uIV": {
    "title": "Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYVQHR5IAq": {
    "title": "Enhancing Personal Decentralized Federated Learning through Model Decoupling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0pMPCmEfN": {
    "title": "Splitted Wavelet Differential Inclusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H6pf70GZVU": {
    "title": "YoooP: You Only Optimize One Prototype per Class for Non-Exemplar Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V163iNHVi7": {
    "title": "Feynman-Kac Operator Expectation Estimator: An Innovative Method for Enhancing MCMC Efficiency and Reducing Variance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWAhEjXjeG": {
    "title": "Provably Robust Conformal Prediction with Improved Efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xlayxj2fWp": {
    "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=41CYtxM2jQ": {
    "title": "Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=giU9fYGTND": {
    "title": "FedImpro: Measuring and Improving Client Update in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=quf7D5agqa": {
    "title": "Deep Reinforcement Learning from Weak Hierarchical Preference Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8mGHjwFZV": {
    "title": "How to Guess a Gradient",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p4eG8rCa0b": {
    "title": "Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W9zRGhLq7": {
    "title": "A New Theoretical Perspective on Data Heterogeneity in Federated Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EODzbQ2Gy4": {
    "title": "Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMNkj4ER7V": {
    "title": "A Unified Framework for Bayesian Optimization under Contextual Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DS5qRs0tQz": {
    "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7zY781bMDO": {
    "title": "Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbv0sBIZh9": {
    "title": "Diffusion Models for Multi-Task Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oKn9c6ytLx": {
    "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HAJzPND6Xt": {
    "title": "Efficient Realistic Avatar Generation via Model Compression and Enhanced Rendering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZG3RaNIsO8": {
    "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yR5QbFv4Xb": {
    "title": "Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9t7MqZtCR": {
    "title": "Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdwVOREDZM": {
    "title": "WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFf9m4v7oC": {
    "title": "Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E1NxN5QMOE": {
    "title": "Enhancing Group Fairness in Online Settings Using Oblique Decision Forests",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CdjnzWsQax": {
    "title": "Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnsPXGPcgI": {
    "title": "MetaDist: An Infrastructure for Automatic Parallelism via ShardCombine Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Lp6qU9hzV": {
    "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXc5aXeoc8": {
    "title": "Diffusion Sampling with Momentum for Mitigating Divergence Artifacts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKIwtXHg4D": {
    "title": "ProGO: Probabilistic Global Optimizer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KSvRZFCy7s": {
    "title": "Differentially Private Low-dimensional Synthetic Data from High-dimensional Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKEuFKfCKA": {
    "title": "A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJ7XuW5JaH": {
    "title": "Posterior Probability-Based Label Recovery Attack in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVsXDLIt45": {
    "title": "Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgwgrxBYOI": {
    "title": "Deep PDE Solvers for Subgrid Modelling and Out-of-Distribution Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YHUGlwTzFB": {
    "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxfKIYfHyx": {
    "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjGJFkU3xL": {
    "title": "Doubly Robust Proximal Causal Learning for Continuous Treatments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MREQ0k6qvD": {
    "title": "One-hot Generalized Linear Model for Switching Brain State Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dUTwqiEked": {
    "title": "RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WA2mZrDTAH": {
    "title": "ZegOT: Zero-shot Segmentation Through Optimal Transport of Pixels to Text Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8htNAnMSyP": {
    "title": "Neural Auto-designer for Enhanced Quantum Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dr4qD9bzZd": {
    "title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sIcPMMhl9W": {
    "title": "The Phase Transition Phenomenon of Shuffled Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T8Rf1CRbHQ": {
    "title": "Error-Feedback Meets Stochastic Approximation with Two Time Scales",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g8sGBSQjYk": {
    "title": "On the Parameterization of Second-Order Optimization Effective towards the Infinite Width",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXMTK2eltf": {
    "title": "GPT-Driver: Learning to Drive with GPT",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aG3EARrrd1": {
    "title": "OSRT: An Online Sparse Approximation Model for Scattered Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNJEyP4Jv2": {
    "title": "Understanding and Improving Adversarial Attacks on Latent Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C9uv8qR7RX": {
    "title": "SiT: Symmetry-invariant Transformers for Generalisation in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKkFJgAdlW": {
    "title": "Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C53xlgEqVh": {
    "title": "Vec-Tok Speech: Speech Vectorization and Tokenization for Neural Speech Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cnn60wwTe1": {
    "title": "Which mode is better for federated learning? Centralized or Decentralized",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkYCuGM7E2": {
    "title": "Large Language Models as Decision Makers for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GEcwtMk1uA": {
    "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DesYwmUG00": {
    "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYhglxSj8j": {
    "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YM0aPHTDe8": {
    "title": "Finite-Time Analysis of Federated Temporal Difference Learning with Linear Function Approximation under Environment and Computation Heterogeneity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTf4DGAzus": {
    "title": "$\\textbf{\\textit{M}}^\\textbf{\\textit{3}}$: Towards Robust Multi-Modal Reasoning via Model Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4WWqhD9Mw": {
    "title": "Holmex: Human-Guided Spurious Correlation Detection and Black-box Model Fixing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQscjhKXIF": {
    "title": "Class-Incremental Continual Learning for Multi-View Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FZfWQFrdBT": {
    "title": "Split and Merge Proxy: pre-training protein inter-chain contact prediction by mining rich information from monomer data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZAlb8fX5y": {
    "title": "KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Z8rZlKpNT": {
    "title": "Normalizing Flows For Out of Distribution Detection via Latent Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ALVwQjZRS8": {
    "title": "Coeditor: Leveraging Repo-level Diffs for Code Auto-editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rsY6J3ZaTF": {
    "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCDT918ZkI": {
    "title": "Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJjHQS2DyE": {
    "title": "Conditional Support Alignment for Domain Adaptation with Label Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cB9bAFGFAA": {
    "title": "FedSRC: Federated Learning with Self-Regulating Clients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qAoxvePSlq": {
    "title": "DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3qo1pJHabg": {
    "title": "LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3bmjHYX42n": {
    "title": "Leveraging Human Revisions for Improving Text-to-Layout Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ll8PmgD0IB": {
    "title": "Divide and Orthogonalize: Efficient Continual Learning with Local Model Space Projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWcnvZ3qMb": {
    "title": "FITS: Modeling Time Series with $10k$ Parameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S6Dn3uyM2p": {
    "title": "Differentially Private One Permutation Hashing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXeSwrVgjN": {
    "title": "Benchmarking Deletion Metrics with the Principled Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x3LxHdZX0f": {
    "title": "PUMA: Secure Inference of LLaMA-7B in Five Minutes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=URCfZ2NgaR": {
    "title": "Structural Knowledge Informed Continual Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0EVNrBQh6": {
    "title": "Investigating Human-Identifiable Features Hidden in Adversarial Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o0oroLuPLZ": {
    "title": "Sp-R-IP: A Decision-Focused Learning Strategy for Linear Programs that Avoids Overfitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ooThrz2NvC": {
    "title": "CICD-Coder: Chinese EMRs Based ICD Coding With Multi-axial Supported Clinical Evidence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcWwhF8cTZ": {
    "title": "Gradient norm as a powerful proxy to out-of-distribution error estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bpheRCxzb4": {
    "title": "Measuring Information in Text Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ys3uPmZGOR": {
    "title": "Enhancing Neural Network Performance with Leader-Follower Architecture and Local Error Signals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3QLkwU40EE": {
    "title": "SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9NKRfhKgzI": {
    "title": "Adversarially Robust and Privacy-Preserving Representation Learning via Information Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XjlZJJFyla": {
    "title": "Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6tDPefQyvB": {
    "title": "Rotation-Equivariance and Position Encodings for Enhancing Local Descriptors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=09iOdaeOzp": {
    "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JnaN0Crlz": {
    "title": "Enhancing Adversarial Robustness on Categorical Data via Attribution Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KszBlT26pl": {
    "title": "FP-IRL: Fokker-Planck-based Inverse Reinforcement Learning --- A Physics-Constrained Approach to Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1BmveEMNbG": {
    "title": "Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbKcFZ6Dbp": {
    "title": "The Dark Side of the Hyperbolic Moon",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6vtGG0WMne": {
    "title": "Regulating Imbalanced Deep Models with User-Specified Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g1S72T3FGc": {
    "title": "Neural Active Learning Beyond Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ly10tMV6cD": {
    "title": "Structure-Rich Text Benchmark for Knowledge Inference Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9UGAUQjibp": {
    "title": "Quantized Local Independence Discovery for Fine-Grained Causal Dynamics Learning in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qwYKE3VB2h": {
    "title": "From Graphs to Hypergraphs: Hypergraph Projection and its Remediation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N8N0hgNDRt": {
    "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=COO51g41Q4": {
    "title": "Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UoBymIwPJR": {
    "title": "Query-Policy Misalignment in Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Klj7QG0NO": {
    "title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0SOhDO7xI0": {
    "title": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pVKEFtGkM6": {
    "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rFCGiFTVyY": {
    "title": "FedSKU: Defending Backdoors in Federated Learning Through Selective Knowledge Unlearning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v5PBcUSP2o": {
    "title": "Score-based Neural Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TS8HoIWAPQ": {
    "title": "Feature-aligned N-BEATS with Sinkhorn divergence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70PPJo3DwI": {
    "title": "Towards Out-of-federation Generalization in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wriKDQqiOQ": {
    "title": "On the Effect of Batch Size in Byzantine-Robust Distributed Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LebzzClHYw": {
    "title": "Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6okaSfANzh": {
    "title": "Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uQBW7ELXfO": {
    "title": "Unpaired Image-to-Image Translation via Neural Schrödinger Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jesfGs3fIc": {
    "title": "Set-based Neural Network Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0S0CgZEYxR": {
    "title": "Examining the Achilles' Heel of CLIP Models: The Worst-Performing Categories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zI6mMl7UmW": {
    "title": "Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4XNuQ5qft": {
    "title": "Optimum Shifting to Stabilize Training and Improve Generalization of Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q4FmJPQwuJ": {
    "title": "CrossTVR: Multi-Grained Re-Ranker for Text Video Retrieval with Frozen Image Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0aR1s9YxoL": {
    "title": "Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fW7DOHDQvF": {
    "title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=farT6XXntP": {
    "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M0MF4t3hE9": {
    "title": "Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTXaJmWXKu": {
    "title": "Continual Nonlinear ICA-Based Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r5sikTJ94y": {
    "title": "Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qbf1hy8b7m": {
    "title": "Scaling Supervised Local Learning with Augmented Auxiliary Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9DXXMXnIGm": {
    "title": "Elucidating the design space of classifier-guided diffusion generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qo21ZlfNu6": {
    "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SnlDQ5pL6L": {
    "title": "Spatial-Temporal Mutual Distillation for Lightweight Sleep Stage Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IL71c1z7et": {
    "title": "Fleet Policy Learning via Weight Merging and An Application to Robotic Tool-Use",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TAtAYUf1aq": {
    "title": "Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLgbjzKJkk": {
    "title": "CO-MOT: Boosting End-to-end Transformer-based Multi-Object Tracking via Coopetition Label Assignment and Shadow Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g6eCbercEc": {
    "title": "InfoCon: Concept Discovery with Generative and Discriminative Informativeness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eBTtShIjxu": {
    "title": "Prompt Tuning Is All We Need?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JCseZixaI7": {
    "title": "Meta Koopman Decomposition for Time Series Forecasting Under Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hbsvyhznr4": {
    "title": "AutoJoin: Efficient Adversarial Training against Gradient-Free Perturbations for Ro- bust Maneuvering via Denoising Autoencoder and Joint Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fq1wNrC2ai": {
    "title": "Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLO9VaAb3B": {
    "title": "Alphazero-like Tree-Search can guide large language model decoding and training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z3dfuRcGAK": {
    "title": "Revisit and Outstrip Entity Alignment: A Perspective of Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLR7ElwD51": {
    "title": "Learning Scalable Causal Discovery Policies with Adversarial Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oFNpRlPxyQ": {
    "title": "MSPipe: Minimal Staleness Pipeline for Efficient Temporal GNN Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6vF0ZJGor4": {
    "title": "ImplicitSLIM and How it Improves Embedding-based Collaborative Filtering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKlxDx2SoS": {
    "title": "Prompt Learning with Quaternion Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xThb6APBoG": {
    "title": "Adapting Retrieval Models to Task-Specific Goals using Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7etoNfU9uF": {
    "title": "SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PIl69UIAWL": {
    "title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A4YlfnbaSD": {
    "title": "Overcoming the Stability Gap in Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fWk5Qx0exc": {
    "title": "Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KmvYOALQnm": {
    "title": "Improving Sample Efficiency in Off-policy RL with Low-dimensional Policy Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MZzlUyU2ih": {
    "title": "DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xm8okHEC3G": {
    "title": "Boosting Dataset Distillation with the Assistance of Crucial Samples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=esh9JYzmTq": {
    "title": "Assessing the Impact of Distribution Shift on Reinforcement Learning Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDU6p4Fydz": {
    "title": "Enhancing Parameter Efficiency in Summarization via Expertise Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3Cu426njo": {
    "title": "Meta-Learning Priors Using Unrolled Proximal Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQDFsuG9HP": {
    "title": "Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZttUMTiPL": {
    "title": "Uncertainty Quantification via Stable Distribution Propagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jDy2Djjrge": {
    "title": "LAURAGPT: LISTEN, ATTEND, UNDERSTAND, AND REGENERATE AUDIO WITH GPT",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VBsoluxR2": {
    "title": "MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vMNpv5OBGb": {
    "title": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hAYHmV1gM8": {
    "title": "FedWon: Triumphing Multi-domain Federated Learning Without Normalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TpD2aG1h0D": {
    "title": "Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VVGO0kuuY": {
    "title": "Learning Causal Dynamics Models in Object-Oriented Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftKqt3Di3H": {
    "title": "Text-Free Federated Transformers Knowledge Distillation Without GAN",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVb0g26xyH": {
    "title": "Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tzpXhoNel1": {
    "title": "GRepsNet: A Simple Equivariant Network for Arbitrary Matrix Groups",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0bdvNsgcF": {
    "title": "A-Loc: Efficient Alternating Iterative Methods for Locating the $k$ Largest/Smallest Elements in a Factorized Tensor",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aaYBsuGRne": {
    "title": "Understanding In-context Learning with a Pelican Soup Hypothesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yC2waD70Vj": {
    "title": "Inverse Approximation Theory for Nonlinear Recurrent Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RRayv1ZPN3": {
    "title": "TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=McfYbKnpT8": {
    "title": "L2P-MIP: Learning to Presolve for Mixed Integer Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SiUhAbb3LH": {
    "title": "Continual Learning Knowledge Graph Embeddings for Dynamic Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EPNEazJoAg": {
    "title": "Exploring the cloud of feature interaction scores in a Rashomon set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSZ2GmGvUV": {
    "title": "EigenGuard: Backdoor Defense in Eigenspace",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QcgvtqxRhI": {
    "title": "BOSS: Diversity-Difficulty Balanced One-Shot Subset Selection for Data-Efficient Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fPEWP3we45": {
    "title": "AED: Adaptable Error Detection for Few-shot Imitation Policy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvDQtdMnOl": {
    "title": "Long-Short-Range Message-Passing: A Fragmentation-Based Framework to Capture Non-Local Atomistic Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZaEdLM4Gn": {
    "title": "TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtCROgWC2o": {
    "title": "Hierarchical Approach to Explaining Poisoned AI Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9QVqYBvCD8": {
    "title": "Asking Before Acting: Gather Information in Embodied Decision-Making with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SA19ijj44B": {
    "title": "A Study of Bayesian Neural Network Surrogates for Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=plebgsdiiV": {
    "title": "Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgDICX1h50": {
    "title": "Large Language Models as Analogical Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aGH43rjoe4": {
    "title": "Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ceATjGPTUD": {
    "title": "Large Language Models are Efficient Learners of Noise-Robust Speech Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H56qXKaNpF": {
    "title": "POSITION EMBEDDING INTERPOLATION IS ALL YOU NEED FOR EFFICIENT IMAGE-TO-IMAGE VIT",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5H8WGLQm5": {
    "title": "UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qvoe4wXWFi": {
    "title": "NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TMYxJIcdgS": {
    "title": "What Makes ImageNet Look Unlike LAION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExZ5gonvhs": {
    "title": "GPS-SSL: Guided Positive Sampling to Inject Prior into Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h9TTpQdGKJ": {
    "title": "Learning Transferable Robust Representations for Few-shot Learning via Multi-view Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6HwamHLDa6": {
    "title": "A Multi-In-Single-Out Network for Video Frame Interpolation without optical flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ArYyAmDGQ": {
    "title": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uFHDOSfuGS": {
    "title": "An Entropic Risk Measure for Robust Counterfactual Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eWLOoaShEH": {
    "title": "Learning to Model the World with Language",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eT6oLkm1cm": {
    "title": "Annealing Self-Distillation Rectification Improves Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLOxzCa6DS": {
    "title": "Differentially Private Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7iuFxx9Ccx": {
    "title": "Resource Efficient Test-Time Training with Slimmable Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VGLU5N1AD2": {
    "title": "Incentivized Black-Box Model Sharing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0gDQgwjoX0": {
    "title": "Stochastic Gradient Discrete Langevin Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBmMgvaEtO": {
    "title": "Stochastic Adaptive Sequential Black-box Optimization for Diffusion Targeted Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIcYTbpO38": {
    "title": "Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ASppt1L3hx": {
    "title": "Cooperative Minibatching in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Unb5CVPtae": {
    "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DmD1wboID9": {
    "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RMgqvQGTwH": {
    "title": "Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBP36xQhZl": {
    "title": "Forward Gradient Training of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PLoWVP7Mjc": {
    "title": "Embarrassingly Simple Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P1ANzoGg3W": {
    "title": "H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i5JfdnCob7": {
    "title": "Optimal Kernel Choice for Score Function-based Causal Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHF35XVjmm": {
    "title": "MADiff: Offline Multi-agent Learning with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0u9uvPdRgV": {
    "title": "Semi-supervised Diffusion Solver for Travelling Salesman Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhXymWfdcN": {
    "title": "Domain Generalization Deep Graph Transformation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MIuimtOu0T": {
    "title": "Towards Fair Knowledge Distillation using Student Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxLMnvnZv0": {
    "title": "CoMNet: Where Biology Meets ConvNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RemfXx7ebP": {
    "title": "Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bUgni8nH8Z": {
    "title": "Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sGVmr7KHfn": {
    "title": "Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikRVG8awyS": {
    "title": "RFold: RNA Secondary Structure Prediction with Decoupled Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qV83K9d5WB": {
    "title": "Large Language Models as Tool Makers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sx038qxjek": {
    "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ItPYVON0mI": {
    "title": "ACHIEVING DYNAMIC ACCURACY IN MACHINE-LEARNED CG POTENTIALS BY MODULATING POTENTIAL ENERGY LANDSCAPE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOtZlKkvdz": {
    "title": "Are Large Language Models Post Hoc Explainers?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JDud6zbpFv": {
    "title": "Sample-Efficient Quality-Diversity by Cooperative Coevolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPONXmVmZ6": {
    "title": "Multi-Agent Interpolated Policy Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3z60EWfh1p": {
    "title": "Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWF4KWeNgb": {
    "title": "Hierarchical Gaussian Mixture Normalizing Flows Modeling for Multi-Class Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yuGqe96t9O": {
    "title": "Stoichiometry Representation Learning with Polymorphic Crystal Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qL9gogRepu": {
    "title": "Zero and Few-shot Semantic Parsing with Ambiguous Inputs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIEW6M9YoV": {
    "title": "Graph Generation with $K^2$-trees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VtpANKeHeJ": {
    "title": "Strategic Classification with Unforeseeable Outcomes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yldBrD4nYB": {
    "title": "CI-VAE: a Generative Deep Learning Model for Class-Specific Data Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7jWiBAWG0b": {
    "title": "Learning Guarantees for Non-convex Pairwise SGD with Heavy Tails",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnYsWoe1ST": {
    "title": "Self-Supervision is Not All You Need: In Defense of Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2DldCIjAdX": {
    "title": "LayerNAS: Neural Architecture Search in Polynomial Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o3BxOLoxm1": {
    "title": "Manifold Preserving Guided Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sNFLN3itAd": {
    "title": "Neural Common Neighbor with Completion for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZu4gUGN13": {
    "title": "Latent Intuitive Physics: Learning to Transfer Hidden Physics from a 3D Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x4OPJ7lHVU": {
    "title": "Privacy-Preserving In-Context Learning for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=24CZaossxH": {
    "title": "PyTorch Geometric High Order: A Unified Library for High Order Graph Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itJj6p7ssr": {
    "title": "Hardware-Friendly Post-Training Quantization: Input- and Output-Channelwise Scale and Offset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUpC8KTvdV": {
    "title": "Masked Distillation Advances Self-Supervised Transformer Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WipsLtH77t": {
    "title": "Adaptive Self-training Framework for Fine-grained Scene Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WvVyG8qBCt": {
    "title": "DPFormer: Learning Differentially Private Transformer on Long-Tailed Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FAH0SgQzO": {
    "title": "FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKAUvMePUN": {
    "title": "Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVEoydFOl9": {
    "title": "Towards Foundation Models for Knowledge Graph Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MiRPBbQNHv": {
    "title": "COCO-Periph: Bridging the Gap Between Human and Machine Perception in the Periphery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9QV7Q9gKl9": {
    "title": "DIFUSCO-LNS: Diffusion-Guided Large Neighbourhood Search for Integer Linear Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VGEeER6W9": {
    "title": "Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OdpIjS0vkO": {
    "title": "More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJFt8ZRQ9a": {
    "title": "IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJbsmB8UMx": {
    "title": "SALMON: Self-Alignment with Principle-Following Reward Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSWGqY2qNJ": {
    "title": "Indeterminate Probability Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B6t5wy6g5a": {
    "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2sCcTMWPc2": {
    "title": "TimelyGPT: Recurrent Convolutional Transformer for Long Time-series Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wKNKnXjCfT": {
    "title": "Efficient Identification of Direct Causal Parents via Invariance and Minimum Error Testing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5a79AqFr0c": {
    "title": "ControlVideo: Training-free Controllable Text-to-video Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tfp4FxWCC8": {
    "title": "Topo-Diffusion: Topological Diffusion Model for Image and Point Cloud Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=23b9KSNQTX": {
    "title": "RETSim: Resilient and Efficient Text Similarity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mpqMVWgqjn": {
    "title": "KW-Design: Pushing the Limit of Protein Deign via Knowledge Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLJg4wgBPu": {
    "title": "GPT Is Becoming a Turing Machine: Here Are Some Ways to Program It",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruk0nyQPec": {
    "title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmqTuFMk68": {
    "title": "Trainable Transformer in Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i8bdPSmOwk": {
    "title": "Momentum-driven Noise-free Guided Conditional Sampling for Denoising Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0r0GphlIL": {
    "title": "Improving Convergence and Generalization Using Parameter Symmetries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gywQnORzJX": {
    "title": "NPEFF: Non-Negative Per-Example Fisher Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KFjCFxiGk4": {
    "title": "Certified Deductive Reasoning with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2uhY4pXrb": {
    "title": "ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6fFd8QaPVx": {
    "title": "OneBNet: Binarized Neural Networks using Decomposed 1-D Binarized Convolutions on Edge Device",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a1AMdN8pSD": {
    "title": "Neural implicit mapping via nested neighborhoods: real-time rendering of neural SDFs with textures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhMrGCMIRL": {
    "title": "Fusing Models with Complementary Expertise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=85Af6AcMo5": {
    "title": "SciRE-Solver: Accelerating Diffusion Models Sampling by Score-integrand Solver with Recursive Difference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y3dqBDnPay": {
    "title": "HyperRep: Hypergraph-Based Self-Supervised Multimodal Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fJNnerz6iH": {
    "title": "Magnitude Invariant Parametrizations Improve Hypernetwork Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=he4CPgU44D": {
    "title": "Active Continual Learning: On Balancing Knowledge Retention and Learnability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VAmVEghgoC": {
    "title": "Detecting Out-of-distribution with Insights from Neural Collapse",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FhQSGhBlqv": {
    "title": "A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g7ohDlTITL": {
    "title": "Flow Matching on General Geometries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KuPixIqPiq": {
    "title": "Teaching Large Language Models to Self-Debug",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pj52xO5ysY": {
    "title": "Interpretable word-level context-based sentiment analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KF3Q79t8B": {
    "title": "Learning An Efficient-And-Rigorous Neural Multigrid Solver",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y3BuJotSKl": {
    "title": "Adversarial Defense using Targeted Manifold Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O0FOVYV4yo": {
    "title": "A LOCAL POLYAK-ŁOJASIEWICZ AND DESCENT LEMMA OF GRADIENT DESCENT FOR OVERPARAMETERIZED LINEAR MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A2KKgcYYDB": {
    "title": "Global Convergence Rate of Deep Equilibrium Models with General Activations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xU7VWHIo1i": {
    "title": "Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RjYKTQ0L0W": {
    "title": "Achieving Human Parity in Content-Grounded Datasets Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xuxYaBMd9F": {
    "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YuYxoaL7YX": {
    "title": "Learning an Inventory Control Policy with General Inventory Arrival Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xkXdE81mOK": {
    "title": "Federated Recommendation with Additive Personalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ia9fKO1Vjq": {
    "title": "Identifiable Latent Polynomial Causal Models through the Lens of Change",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nKvGCUoiuW": {
    "title": "MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wNere1lelo": {
    "title": "Certifying LLM Safety against Adversarial Prompting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kIP0duasBb": {
    "title": "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iVaxLjVRQN": {
    "title": "Balancing Fairness and Accuracy in Data-Restricted Binary Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHZVrhQuO1": {
    "title": "Linking Finite-Time Lyapunov Exponents to RNN Gradient Subspaces and Input Sensitivity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LxruQOI93v": {
    "title": "Just How Flexible are Neural Networks in Practice?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rN3fh43D30": {
    "title": "Enhancing Length Extrapolation in Sequential Models with Pointer-Augmented Neural Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bRLed9prWC": {
    "title": "Future Language Modeling from Temporal Document History",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dnqPvUjyRI": {
    "title": "SemiReward: A General Reward Model for Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PbvbLyqT6": {
    "title": "Dynamic Discounted Counterfactual Regret Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ad87VjRqUw": {
    "title": "Ghost on the Shell: An Expressive Representation of General 3D Shapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2eIembMRQJ": {
    "title": "Active Teacher Selection for Reinforcement Learning from Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bnPALC6S4l": {
    "title": "Towards general neural surrogate PDE solvers with specialized neural accelerators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvyYFSxdgB": {
    "title": "DATS: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ltutP1Iwqq": {
    "title": "Investigating Feature Alignment Under An Infant-Inspired Visual Distribution Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKqMmKlmA7": {
    "title": "Modulate Your Spectrum in Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YbZxT0SON4": {
    "title": "Improving Intrinsic Exploration by Creating Stationary Objectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pev2ufTzMv": {
    "title": "Why Sanity Check for Saliency Metrics Fails?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mZn2Xyh9Ec": {
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gFBTNDNDUG": {
    "title": "A Computational Framework for Solving Wasserstein Lagrangian Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=228XQpErvW": {
    "title": "Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gU58d5QeGv": {
    "title": "Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUELOzcTk8": {
    "title": "Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZyAqjAjJW": {
    "title": "LDReg: Local Dimensionality Regularized Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lOsF9k1sxW": {
    "title": "Fisher Information Guided Backdoor Purification Via Naive Exploitation of Smoothness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GACjMj1MS1": {
    "title": "Empirical Likelihood for Fair Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7fwzPsn1lJ": {
    "title": "LLark: A Multimodal Foundation Model for Music",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AL1fq05o7H": {
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=88FcNOwNvM": {
    "title": "Compositional Image Decomposition with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3NnfJnbJT2": {
    "title": "GIO: Gradient Information Optimization for Training Dataset Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAwaaLJNCk": {
    "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vl3F3s8OMg": {
    "title": "Can Euclidean Symmetry Help in Reinforcement Learning and Planning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7nz6ljg9Y": {
    "title": "The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XwiA1nDahv": {
    "title": "Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pjtIEgscE3": {
    "title": "Probabilistic Adaptation of Black-Box Text-to-Video Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SdBApv9iT4": {
    "title": "Horizon-Free Regret for Linear Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kBTzlxM2J1": {
    "title": "Faithful Rule Extraction for Differentiable Rule Learning Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0yz1pmVfE": {
    "title": "A Cooperative-Game-Theoretical Model for Ad Hoc Teamwork",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5u72wkdH3": {
    "title": "SYRAC: Synthesize, Rank, and Count",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwlm6Po61I": {
    "title": "Delving into LLMs' visual understanding ability using SVG to bridge image and text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1XDG1Z5Nhk": {
    "title": "Sparse Backpropagation for MoE Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CY9f6G89Rv": {
    "title": "High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tkmO6bXT54": {
    "title": "Mining latent labels for imbalance classification: a regrouping perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wG12xUSqrI": {
    "title": "Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZSEgJGPxu": {
    "title": "SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wm4WlHoXpC": {
    "title": "Scalable Diffusion for Materials Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dm8e7gsH0d": {
    "title": "Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPFsIbF3V6": {
    "title": "Guess & Sketch: Language Model Guided Transpilation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hEl2HpiH3g": {
    "title": "FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QaODpeRaOK": {
    "title": "Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmBKIecDE9": {
    "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cuAxSHcsSX": {
    "title": "On Differentially Private Federated Linear Contextual Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6I7UsvlDPj": {
    "title": "LaMPP: Language Models as Probabilistic Priors for Perception and Action",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDZumshwym": {
    "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJzwUyryyl": {
    "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=daEqXJ0yZo": {
    "title": "Generative Human Motion Stylization in Latent Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2NpAw2QJBY": {
    "title": "Neural Neighborhood Search for Multi-agent Path Finding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IcbC9F9xJ7": {
    "title": "A General Single-Cell Analysis Framework via Conditional Diffusion Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwNj5TP9gm": {
    "title": "Evidential Conservative Q-Learning for Dynamic Recommendations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpw7V0P136": {
    "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mqukp3Lsnt": {
    "title": "Space-Time Attention with Shifted Non-Local Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lIwp1C1eSK": {
    "title": "Compositional Instruction Following with Language Models and Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2tVHNRZuCs": {
    "title": "Enable Lanuguage Models to Implicitly Learn Self-Improvement From Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AS7vaVU8d": {
    "title": "Learning Personalized Story Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5AbtYdHlr3": {
    "title": "Stochastic Safe Action Model Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m50eKHCttz": {
    "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aMfdN4ZQVx": {
    "title": "Training-free Deep Concept Injection Enables Language Models for Crossmodal Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLJznSp6X3": {
    "title": "ReLoRA: High-Rank Training Through Low-Rank Updates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4uaogMQgNL": {
    "title": "UpFusion: Novel View Diffusion from Unposed Sparse View Observations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPwQj4Mf3u": {
    "title": "Hopfield Encoding Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qi5Xa2cOZg": {
    "title": "Learning with Language-Guided State Abstractions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oM7Jbxdk6Z": {
    "title": "Multimodal Molecular Pretraining via Modality Blending",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o87xfYKQC1": {
    "title": "Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical Invariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZfatbNdLV": {
    "title": "Generative modeling for RNA splicing code predictions and design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVEu295o70": {
    "title": "Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DjeQ39QoLQ": {
    "title": "Robustifying State-space Models for Long Sequences via Approximate Diagonalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hwXUmwJAq5": {
    "title": "UGradSL: Machine Unlearning Using Gradient-based Smoothed Label",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U4RoAyYGJZ": {
    "title": "On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbJqRGNYCf": {
    "title": "JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YZrg56G0JV": {
    "title": "Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVMPfEGT2w": {
    "title": "Provable Offline Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pbLjYjjWqd": {
    "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmg7t8b4s0": {
    "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g1fE8fSOm5": {
    "title": "Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrjLHbqiYs": {
    "title": "Quantifying Interactions in Semi-supervised Multimodal Learning: Guarantees and Applications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTBXeXdbMf": {
    "title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1CPta0bfN2": {
    "title": "Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKKwQvepx0": {
    "title": "Explainable, Steerable Models with Natural Language Parameters and Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ozX92bu8VA": {
    "title": "The Truth Is In There: Improving Reasoning with Layer-Selective Rank Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LNTSrJjBe": {
    "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qBWhjsNPEY": {
    "title": "DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s6bKLlF4Pe": {
    "title": "Provable Knowledge Transfer using Successor Feature for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mmjnr0G8ZY": {
    "title": "Multi-Resolution Diffusion Models for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jg8yHRPU7l": {
    "title": "Mechanism of clean-priority learning in early stopped neural networks of infinite width",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NmaXXAiJJC": {
    "title": "COMPRESSION AND ACCELERATION OF DEEP NEURAL NETWORKS: A VECTOR QUANTIZATION APPROACH",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QDrG0ALevs": {
    "title": "Advantage-Conditioned Diffusion: Offline RL via Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wcka3bd7P4": {
    "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FIGXAxr9E4": {
    "title": "CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RtAct1E2zS": {
    "title": "On Error Propagation of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXGph215fL": {
    "title": "The Update Equivalence Framework for Decision-Time Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aaBnFAyW9O": {
    "title": "Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnYaF3vv3G": {
    "title": "LabelDP-Pro: Learning with Label Differential Privacy via Projections",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkKQjaS9GX": {
    "title": "SDM-RL: Steady-State Divergence Maximization for Robust Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tpk0p9QBM6": {
    "title": "Computing Low-Entropy Couplings for Large-Support Distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YgMdDQB09U": {
    "title": "AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mdCet7vVv": {
    "title": "Maestro: Uncovering Low-Rank Structures via Trainable Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mR5pknv2oP": {
    "title": "Chain-of-Thought Reasoning is a Policy Improvement Operator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SLA7VOqwwT": {
    "title": "Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQzdtBiKie": {
    "title": "Fractal Patterns May Unravel the Intelligence in Next-Token Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kce6LTZ5vY": {
    "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrWuDiW4Qm": {
    "title": "MetaPhysiCa: Improving OOD Robustness in Physics-informed Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3N5JJfqMX": {
    "title": "Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dsUB4bst9S": {
    "title": "Teaching Arithmetic to Small Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KI9NqjLVDT": {
    "title": "ReMasker: Imputing Tabular Data with Masked Autoencoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iStX5y0Ttg": {
    "title": "Towards Universal Robust Federated Learning via Meta Stackelberg Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lpyxWITF2c": {
    "title": "Topology Matters in Fair Graph Learning: a Theoretical Pilot Study",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ogxrdvFdx5": {
    "title": "ReLU soothes NTK conditioning and accelerates optimization for wide neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0i6Z9N5MLY": {
    "title": "Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6YZmkpivVH": {
    "title": "TpopT: Efficient Trainable Template Optimization on Low-Dimensional Manifolds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lrQlLqQase": {
    "title": "A Dynamical View of the Question of Why",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMFqEror1b": {
    "title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CZ50WgfCG": {
    "title": "Learning Reusable Dense Rewards for Multi-Stage Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTFPO9FHL3": {
    "title": "Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware Tokenization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvveGAbkVx": {
    "title": "Fair Classifiers that Abstain without Harm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3gOYtv1xV": {
    "title": "Carrying over Algorithm in Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ODSgo2m8aE": {
    "title": "Aligning Relational Learning with Lipschitz Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUoBaW8KH1": {
    "title": "Rethinking the Smoothness of Node Features Learned by Graph Convolutional Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBZBPXdJlC": {
    "title": "Listen, Think, and Understand",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAR9xu8WM6": {
    "title": "Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nh5tSrqTpe": {
    "title": "Don't Pre-train, Teach Your Small Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkF7NZGVr5": {
    "title": "Curvature Explains Loss of Plasticity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Q8TZWAHv4": {
    "title": "GOAt: Explaining Graph Neural Networks via Graph Output Attribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9kG7TwgLYu": {
    "title": "Time Fairness in Online Knapsack Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gdm87rRjep": {
    "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=okYdj8Ysru": {
    "title": "A Lie Group Approach to Riemannian Normalization for SPD Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q20kiEt1oW": {
    "title": "Strategies and impact of learning curve estimation for CNN-based image classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AEi2wyAMyb": {
    "title": "Bi-Level Optimization for Pseudo-Labeling Based Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NW31gAylIm": {
    "title": "Text-driven Prompt Generation for Vision-Language Models in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yJdj2QQCUB": {
    "title": "Graph Positional and Structural Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mutJBk3ILg": {
    "title": "Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DTwpuoaea4": {
    "title": "PAGER: A Framework for Failure Analysis of Deep Regression Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99hq9VMkbg": {
    "title": "Fisher-aware Quantization for DETR Detectors with Critical-category Objectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gLtHsY0zCC": {
    "title": "T-Measure: A Measure for Model Transferabilty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HodMKbJkl3": {
    "title": "SGD batch saturation for training wide neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fht65Wm5JC": {
    "title": "Borda Regret Minimization for Generalized Linear Dueling Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFWuX1Fhtj": {
    "title": "On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yCYnKMHX3u": {
    "title": "MultiLayerDiffusion: Composing Global Contexts and Local Details in Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JTBe1WG3Ws": {
    "title": "FLIRT: Feedback Loop In-context Red Teaming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ErllmwXym": {
    "title": "Interpreting and improving diffusion models using the Euclidean distance function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZEZ0CPmoSI": {
    "title": "Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyWKb7Ltcx": {
    "title": "Intrinsic Riemannian Classifiers on the Deformed SPD Manifolds: A Unified Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ti0kjqFx7D": {
    "title": "Editable Graph Neural Network for Node Classifications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtZ7vCt5QY": {
    "title": "Causal-StoNet: Causal Inference for High-Dimensional Complex Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K1mcPiDdOJ": {
    "title": "Conditional Information Bottleneck Approach for Time Series Imputation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2drC319yHQ": {
    "title": "RepoFusion: Training Code Models to Understand Your Repository",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ng7OYC3PT8": {
    "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljwoQ3cvQh": {
    "title": "The Optimal Constant Solution: Predictable Extrapolation in Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hJEMTDOwKx": {
    "title": "Language Models as Semantic Indexers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqDeICpLFo": {
    "title": "Global minima, recoverability thresholds, and higher-order structure in GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v3XXtxWKi6": {
    "title": "RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TVDUVpgu9s": {
    "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=flg9EB6ikY": {
    "title": "Selective Prediction via Training Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afQuNt3Ruh": {
    "title": "Entropy Coding of Unordered Data Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdYqGkSr9S": {
    "title": "Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KZpDGD4Nh": {
    "title": "Neurosymbolic Grounding for Compositional Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8pYNdmwGAO": {
    "title": "EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yF19SY1i8M": {
    "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BSePKWwTUj": {
    "title": "Multiobjective Stochastic Linear Bandits under Lexicographic Ordering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qa0ULgosc9": {
    "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AialDkY6y3": {
    "title": "Deep Graph Predictions using Dirac-Bianconi Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRO4PfCiwb": {
    "title": "OS-net: Orbitally Stable Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rV9cp7KRH": {
    "title": "Incentivized Collaborative Learning: Architectural Design and Insights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QFYVVwiAM8": {
    "title": "Adaptive Sharpness-Aware Pruning for Robust Sparse Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8RGmICUkJ": {
    "title": "SiBBlInGS: Similarity-driven Building Block Inference using Graphs across States",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojrVw6GFFD": {
    "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDRoonpLkD": {
    "title": "Revisiting GNNs for Boolean Satisfiability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bkNx3O0sND": {
    "title": "MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAs4LdaP9Y": {
    "title": "Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ShjMHfmPs0": {
    "title": "Self-Consuming Generative Models Go MAD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1m5uqUpO9": {
    "title": "A Local Graph Limits Perspective on Sampling-Based GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zt8bb6vC4m": {
    "title": "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=awWpHnEJDw": {
    "title": "The Hidden Language of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yC2jjvYVzb": {
    "title": "Confidence-Based Model Selection: When to Take Shortcuts in Spurious Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WO4BCqEyWc": {
    "title": "Augmentation-aware Self-Supervised Learning with Conditioned Projector",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tGQirjzddO": {
    "title": "Reasoning with Latent Diffusion in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=biNhA3jbHc": {
    "title": "Learning Sequence Attractors in Recurrent Networks with Hidden Neurons",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6CBQYxXvr": {
    "title": "Project and Probe: Sample-Efficient Adaptation by Interpolating Orthogonal Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P1aobHnjjj": {
    "title": "Implicit bias of SGD in $L_2$-regularized linear DNNs: One-way jumps from high to low rank",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0gT4A0jNV": {
    "title": "Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3jq8f8t3d": {
    "title": "Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dyG2oLJYyX": {
    "title": "DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y5Gseybzp": {
    "title": "Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7tJxajC3G": {
    "title": "Federated Causal Discovery from Heterogeneous Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKXvPDekud": {
    "title": "CellPLM: Pre-training of Cell Language Model Beyond Single Cells",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0H6DFoZZXZ": {
    "title": "Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7iiF79kI3": {
    "title": "CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZndRcfyNI": {
    "title": "Principled Architecture-aware Scaling of Hyperparameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wAyTOazvN0": {
    "title": "Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vc52HZwNwe": {
    "title": "Gradient-free Proxy for Efficient Language Model Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SsmT8aO45L": {
    "title": "Provable Robust Watermarking for AI-Generated Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yd7idEYzNv": {
    "title": "EGALA: Efficient Gradient Approximation for Large-scale Graph Adversarial Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4BikKsgmK": {
    "title": "Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LQ6LQ8f4y8": {
    "title": "CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R3Tf7LDdX4": {
    "title": "Memory-Consistent Neural Networks for Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwhHSOHMTM": {
    "title": "Learning dynamic representations of the functional connectome in neurobiological networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZFMiHfZwIf": {
    "title": "Skill or Luck? Return Decomposition via Advantage Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLqkCvjHRD": {
    "title": "Coarse-Tuning Models of Code with Reinforcement Learning Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7gDENzTzw1": {
    "title": "Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YHihO8Ka3O": {
    "title": "FedGT: Federated Node Classification with Scalable Graph Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sn7CYWyavh": {
    "title": "Whole-song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L7jtdGhWzT": {
    "title": "Toward $\\textbf{F}$aithfulness-guided $\\textbf{E}$nsemble $\\textbf{I}$nterpretation of Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2oTVrlcp3": {
    "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bM6LUC2lec": {
    "title": "MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g9diuvxN6D": {
    "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDhq1icpO8": {
    "title": "Conditional Instrumental Variable Regression with Representation Learning for Causal Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRgT26JlAx": {
    "title": "Learning with Temporal Label Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aOPTDchLBz": {
    "title": "ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ewIfVxCzbo": {
    "title": "DPO-Diff: On Discrete Prompt Optimization of Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jsQPjIaNNh": {
    "title": "Illuminating Protein Function Prediction through Inter-Protein Similarity Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzQSR56JZr": {
    "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jhPvuc7kxB": {
    "title": "Look, Remember and Reason: Grounded Reasoning in Videos with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1M8yDTa0Pp": {
    "title": "Cross-Model Semi-Supervised Prompt Learning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TyZhiK6fDf": {
    "title": "Co-Learning Empirical Games & World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkibCOnsEv": {
    "title": "Structured Inverse-Free Natural Gradient: Memory-Efficient & Numerically-Stable KFAC for Large Neural Nets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PXNrncg2DF": {
    "title": "SOHES: Self-supervised Open-world Hierarchical Entity Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vq8BCZYAdj": {
    "title": "Multi-fidelity Deep Symbolic Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d0BXudm2S4": {
    "title": "Natural Counterfactuals With Necessary Backtracking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PKUy1VJqwV": {
    "title": "Graph Representation Learning with Multi-granular Semantic Ensemble",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dwademPdV1": {
    "title": "Understanding Unfairness via Training Concept Influence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kYXZ4FT2b3": {
    "title": "Grid Cell-Inspired Fragmentation and Recall for Efficient Map Building",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7sASqAmGaO": {
    "title": "Augmenting Negative Representation for Continual Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZG2AiVMj1I": {
    "title": "IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mCOBKZmrzD": {
    "title": "EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sDlMJVXXeV": {
    "title": "Neural varifolds: an aggregate representation for quantifying geometry of point clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWSTBrmRqD": {
    "title": "DOMINO: A Dual-System for Multi-step Visual Language Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uV39mPKRGw": {
    "title": "Concept Matching: Clustering-based Federated Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4DoSULcfG6": {
    "title": "Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXTmAdGjlg": {
    "title": "Adaptive Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JfjduOxrTY": {
    "title": "Understanding Graph Transformers by Generalized Propagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zCJFTA19K4": {
    "title": "Token Alignment via Character Matching for Subword Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xLoxMvO695": {
    "title": "Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wqi85OBVLE": {
    "title": "Reward Adaptation Via Q-Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cdUpf6t6LZ": {
    "title": "Robust NAS benchmark under adversarial training: assessment, theory, and beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aq35gl2c1k": {
    "title": "Critical Learning Periods Emerge Even in Deep Linear Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eEslYpY6Yq": {
    "title": "On the Equivalence of Graph Convolution and Mixup",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzUhfQ74c5": {
    "title": "Conformal Language Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NialiwI2V6": {
    "title": "MOTOR: A Time-To-Event Foundation Model For Structured Medical Records",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3l0piOrGU": {
    "title": "Representation Deficiency in Masked Language Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OI3RoHoWAN": {
    "title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=chVYVLJIAh": {
    "title": "$\\lambda$-AC: Effective decision-aware reinforcement learning with latent models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE7PY7e4tr": {
    "title": "Neural Network Expressive Power Analysis Via Manifold Topology",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=34STseLBrQ": {
    "title": "Polynomial Width is Sufficient for Set Representation with High-dimensional Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XwBIcywWM": {
    "title": "Learning Variational Neighbor Labels for Test-Time Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tcx84iyqaC": {
    "title": "Reward Collapse in Aligning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIPLdFy3vp": {
    "title": "Parametric Augmentation for Time Series Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxcEqwl9es": {
    "title": "Microenvironment Probability Flows as Proficient Protein Engineers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQLDXQ3IG8": {
    "title": "Robustness Guarantees for Adversarial Training on Non-Separable Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DJZDgMOLXQ": {
    "title": "Prediction Error-based Classification for Class-Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CNL2bku4ra": {
    "title": "Test-Time Training on Nearest Neighbors for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ax2yRhCQr1": {
    "title": "Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bC50ZOyPQm": {
    "title": "READ: Recurrent Adaptation of Large Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ft1mr3WlGM": {
    "title": "Improved Probabilistic Image-Text Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Cw3yFqPDX": {
    "title": "Buffered Asynchronous Federated Learning with Local Differential Privacy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nR1EEDuov7": {
    "title": "Securing Deep Generative Models with Universal Adversarial Signature",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NoeLQU4J2O": {
    "title": "Soon Filter: Advancing Feed-Forward Neural Architectures for Inference at the Edge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iy0WQ0c75x": {
    "title": "Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYwYYwsbSo": {
    "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=abL5LJNZ49": {
    "title": "SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lpxcCD7WbQ": {
    "title": "Task adaptation by biologically inspired stochastic comodulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NOz4YbdHl9": {
    "title": "Confession Networks: Boosting Accuracy and Improving Confidence in Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yui55YzCao": {
    "title": "Shape-aware Graph Spectral Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IAkflJmNrC": {
    "title": "Polarity-Aware Semantic Retrieval with Fine-Tuned Sentence Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iH49a0yxgF": {
    "title": "DUDE: Deep Unsupervised Domain adaptation using variable nEighbors for physiological time series analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s56xikpD92": {
    "title": "BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSVtmmzeRB": {
    "title": "Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BYUdBlaNqk": {
    "title": "System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUGFpDCu3W": {
    "title": "What does GPT store in its MLP weights? A case study of long-range dependencies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MO5PiKHELW": {
    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LAEd3kHao9": {
    "title": "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y0GJXRungR": {
    "title": "Is Self-Repair a Silver Bullet for Code Generation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8OxL034uEr": {
    "title": "MgNO: Efficient Parameterization of Linear Operators via Multigrid",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ev10F9TWML": {
    "title": "Dissecting Neural Network Robustness Proofs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uFbWHyTlPn": {
    "title": "Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PoDkdFQIu3": {
    "title": "A Linear Algebraic Framework for Counterfactual Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMLQB4EZE1": {
    "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmXedvwrT1": {
    "title": "Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cXbnGtO0NZ": {
    "title": "Latent 3D Graph Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XLjlLQz2y2": {
    "title": "Spectral Greedy Coresets for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OUkZXbbwQr": {
    "title": "Reward Design for Justifiable Sequential Decision-Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rfz3K3yyU4": {
    "title": "Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5xKixQzhDE": {
    "title": "Calibrated Dataset Condensation for Faster Hyperparameter Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qL6brrBDk2": {
    "title": "SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnsLGUCynE": {
    "title": "3D Diffuser Actor: Multi-task 3D Robot Manipulation with Iterative Error Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TqYbAWKMIe": {
    "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tOzCcDdH9O": {
    "title": "Matryoshka Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9bCeJGUJi": {
    "title": "Curriculum Dynamic Graph Invariant Learning under Distribution Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LLbHdII8Pg": {
    "title": "Two Birds with One Stone: Protecting DNN Models Against Unauthorized Inference and Domain Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kJFIH23hXb": {
    "title": "SE(3)-Stochastic Flow Matching for Protein Backbone Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILtA2ebLYR": {
    "title": "Efficient Interactive Preference Learning in Evolutionary Algorithms: Active Dueling Bandits and Active Learning Integration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlZIXgfWPH": {
    "title": "On the Hyperparameter Loss Landscapes of Machine Learning Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LGXlMmDarK": {
    "title": "On the Stochasticity in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x9cVJnlX9n": {
    "title": "Risk-Controlling Model Selection via Guided Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4pnhzuRtJ2": {
    "title": "Optimized Tradeoffs for Private Majority Ensembling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4iPw1klFWa": {
    "title": "Scalable Neural Network Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D7KJmfEDQP": {
    "title": "Model Merging by Uncertainty-Based Gradient Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GRlKzhHl9Z": {
    "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cfi68cGzIt": {
    "title": "Conservative Reinforcement Learning by Q-function Disagreement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fHZ04oyEed": {
    "title": "Representation Learning from Interventional Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzcuXQq0aR": {
    "title": "PlugVFL: Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=12Acp6ZcRa": {
    "title": "Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3uITarEQ7p": {
    "title": "Differentially Private Model Compression via Selective Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ifz3IgsEPX": {
    "title": "DP-OPT: Make Large Language Model Your Differentially-Private Prompt Engineer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymR2bz0cEs": {
    "title": "Interaction-centric Hypersphere Reasoning for Multi-person Video HOI Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ptXo0epLQo": {
    "title": "$\\alpha$TC-VAE: On the relationship between Disentanglement and Diversity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pv2U1BeC5Z": {
    "title": "Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7d1qXEn1e": {
    "title": "A Restoration Network as an Implicit Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rhp5PDNOgf": {
    "title": "Spaced Scheduling Enhances Instruction-Prompted Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PudduufFLa": {
    "title": "Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3KDbIWT26J": {
    "title": "The Reasonableness Behind Unreasonable Translation Capability of Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZIOBA2oDU": {
    "title": "Fast Value Tracking for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HrRKc9ei7h": {
    "title": "Oracle Efficient Algorithms for Groupwise Regret",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NlBuWEJCug": {
    "title": "PcLast: Discovering Plannable Continuous Latent States",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WSzRdcOkEx": {
    "title": "GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=26XphugOcS": {
    "title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d8w0pmvXbZ": {
    "title": "Small-scale proxies for large-scale Transformer training instabilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dfEuojp0rX": {
    "title": "Variational Quantum Linear Solver enhanced Quantum Support Vector machine",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfrpYGKGPL": {
    "title": "The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qsAckNdySL": {
    "title": "Causality is Invariance Across Heterogeneous Units",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHfIe4chR5": {
    "title": "Long-distance Targeted Poisoning Attacks on Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXoq9EqR9e": {
    "title": "FairVLM: Mitigating Bias In Pre-Trained Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoismgeX7z": {
    "title": "Generalized Schrödinger Bridge Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QMQBza9BCx": {
    "title": "Persistent homology for high-dimensional data based on spectral methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N1TyUhkvjW": {
    "title": "Time Series Anomaly Detection using Reconstruction and RBF Similarity Scores",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dQVtTdsvZH": {
    "title": "Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjU0jtXcYn": {
    "title": "A General Framework for User-Guided Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1RE0H6mU7M": {
    "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXrUarMM20": {
    "title": "Efficient and Quantization-Friendly Ternary Fourier Convolution Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZJehvRKGD": {
    "title": "Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mzb7XD0O1Q": {
    "title": "CRAFT: Cross-Representation modeling on Audio waveForms and specTrograms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EaB7Ue1X9p": {
    "title": "High-Dimensional Safe Exploration via Optimistic Local Latent Safe Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kiwyQsZIGP": {
    "title": "Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5AjtSen6L": {
    "title": "ELEGANT: Certified Defense on the Fairness of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7YEXo5qUmN": {
    "title": "Organ-DETR: 3D Organ Detection Transfomer with Multiscale Attention and Dense Query Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ztuCObOc2i": {
    "title": "Neural Sinkhorn Gradient Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNrTYz1rXH": {
    "title": "Fairness-Aware Domain Generalization under Covariate and Dependence Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=88MalncLgU": {
    "title": "GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Networks Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LH2JNpfwdH": {
    "title": "Towards 4D Human Video Stylization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AAxIs3D2ZZ": {
    "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LdiVep1jpj": {
    "title": "RASP Quadratures: Efficient Numerical Integration for High-Dimensional Mean-Field Variational Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzElnMrgSD": {
    "title": "How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9BERij4Gbv": {
    "title": "Guided Evolution with Binary Discriminators for ML Program Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PyERBFX0wJ": {
    "title": "Reflected Schr\\\"odinger Bridge for Constrained Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BC4AUywMow": {
    "title": "Zero-Level-Set Encoder for Neural Distance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izdFGwDgvW": {
    "title": "ICE: Image-Caption Encoding for Improved Out-Of-Distribution Generalization In Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3eFMnZ3N4J": {
    "title": "Efficient-3Dim: Learning a Generalizable Single-image Novel-view Synthesizer in One Day",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ffJo4vtTY": {
    "title": "Robust multimodal models have outlier features and encode more concepts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hNhwSmtXRh": {
    "title": "Lemur: Harmonizing Natural Language and Code for Language Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfzRRjumpX": {
    "title": "CODE REPRESENTATION LEARNING AT SCALE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ezBaMwOqY": {
    "title": "Trading-off Multiple Properties for Molecular Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=de1218PoEl": {
    "title": "Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B9klVS7Ddk": {
    "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gl4AsqInti": {
    "title": "How Hessian structure explains mysteries in sharpness regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vM1xZ9kDUj": {
    "title": "Capture Concept through Comparison: Vision-and-Language Representation Learning with Intrinsic Information Mining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hiHZVUIYik": {
    "title": "A path-norm toolkit for modern networks: consequences, promises and challenges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S4YVoQ70b2": {
    "title": "Characterising Partial Identifiability in Inverse Reinforcement Learning For Agents With Non-Exponential Discounting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4SiDyYQbo": {
    "title": "An Investigation of Representation and Allocation Harms in Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efeBC1sQj9": {
    "title": "SEPT: Towards Efficient Scene Representation Learning for Motion Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XkTz7gdpc": {
    "title": "Efficient and Scalable Graph Generation by Spectrum Preserving Local Expansion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J4V3lW9hq6": {
    "title": "A Multi-Grained Group Symmetric Framework for Learning Protein-Ligand Binding Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4i4fgCOBDE": {
    "title": "Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eSO9quCgmz": {
    "title": "Rethinking pseudo-labeling: Data-centric insights improve semi-supervised learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H98CVcX1eh": {
    "title": "Discovering modular solutions that generalize compositionally",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOHVDPqoUJ": {
    "title": "Less is More: Selective Layer Finetuning with SubTuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CUfSCwcgqm": {
    "title": "Long-range Neural Atom Learning for Molecular Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ArpwmicoYW": {
    "title": "FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2RJAzSphy9": {
    "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j56A1HUTQS": {
    "title": "Bridging Indexing Structure and Graph Learning: Expressive and Scalable Graph Neural Network via Core-Fringe",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Glcsog6zOe": {
    "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUUjIWntkU": {
    "title": "Explainable medical image clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K1bv86Uvbp": {
    "title": "LARGE LANGUAGE MODELS FOR BIOMEDICAL KNOWLEDGE GRAPH CONSTRUCTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QqjFHyQwtF": {
    "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C5u71ph75Q": {
    "title": "Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9FXRHoQdc": {
    "title": "Best Response Shaping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDQF6A1s6M": {
    "title": "LOQA: Learning with Opponent Q-Learning Awareness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kuh5qgCGCp": {
    "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8J7Pw7hpj": {
    "title": "Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WceuzWff5": {
    "title": "Understanding the Transfer of High-Level Reinforcement Learning Skills Across Diverse Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BoLqnXEdSE": {
    "title": "Measuring Fairness Using Probable Segmentation for Continuous Sensitive Attributes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eRAXvtP0gA": {
    "title": "Unsupervised Cognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iJBQAAhqvY": {
    "title": "RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0A5o6dCKeK": {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkRD6GsswM": {
    "title": "CLA-RA: COLLABORATIVE ACTIVE LEARNING AMIDST RELABELING AMBIGUITY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dA4EWchlbn": {
    "title": "Advancing the Adversarial Robustness of Neural Networks from the Data Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jolYuxpVn1": {
    "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qYb0CANLGC": {
    "title": "Auto-Regressive Next-Token Predictors are Universal Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IryGDUHxDE": {
    "title": "Unsupervised open-vocabulary action recognition with an autoregressive model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BGkqypmGvm": {
    "title": "A 2-Dimensional State Space Layer for Spatial Inductive Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4rBEgZCubP": {
    "title": "Learning 3D Particle-based Simulators from RGB-D Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jxpsAj7ltE": {
    "title": "From Sparse to Soft Mixtures of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6kt50zAiG": {
    "title": "CAMBranch: Contrastive Learning with Augmented MILPs for Branching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NaxbdRi8Rv": {
    "title": "StyleAdapter: A Unified Stylized Image Generation Model without Test-Time Fine-Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xdnoULh5Sv": {
    "title": "CARSO: Blending Adversarial Training and Purification Improves Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxVBKhyfSo": {
    "title": "Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KKZaj2QS3G": {
    "title": "Enriching Time Series Representation: Integrating a Noise-Resilient Sampling Strategy with an Efficient Encoder Architecture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3TAhlGaMKD": {
    "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b1Hivmb86F": {
    "title": "MetaTST: Essential Transformer Components for Time Series Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cijOBlCxMa": {
    "title": "CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNOewRJLgQ": {
    "title": "Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OfXqQ5TRwp": {
    "title": "ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DqD59dQP37": {
    "title": "Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ALGFFPXWSi": {
    "title": "One Forward is Enough for Neural Network Training via Likelihood Ratio Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=etm456yoiq": {
    "title": "B$^{3}$CT: Three-branch Coordinated Training for Domain Adaptive Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBo7544jZx": {
    "title": "Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JU8TwFXGC": {
    "title": "LLM Performance Predictors are good initializers for Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NRIcs2TR7k": {
    "title": "Extending Multi-modal Contrastive Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZXlEFO3tZ": {
    "title": "Counterfactual Density Estimation using Kernel Stein Discrepancies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dnO3LLiJ1": {
    "title": "Vision Transformers Need Registers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8J0b9gNfG": {
    "title": "Multilingual Visual Speech Recognition with a Single Model using Visual Speech Unit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JG9PoF8o07": {
    "title": "Beyond Laplace and Gaussian: Exploring the Generalized Gaussian Mechanism for Private Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tsNLIBlG4p": {
    "title": "Analysis of a class of stochastic component-wise soft-clipping schemes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Feiz5HtCD0": {
    "title": "Does Writing with Language Models Reduce Content Diversity?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rt7ekFkSJZ": {
    "title": "Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCpayOuqBx": {
    "title": "DOS: Dreaming Outlier Semantics for Out-of-distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=homn1jOKI5": {
    "title": "Conformal Inductive Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BifeBRhikU": {
    "title": "PB-LLM: Partially Binarized Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6O3Q6AFUTu": {
    "title": "Beyond Linear Spherical Interpolation: Noise Correction for Image Interpolation with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0m0DdCCQ2": {
    "title": "Liteformer: Lightweight Evoformer for Protein Structure Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rBH7x87VfJ": {
    "title": "Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L7gQWBcFxK": {
    "title": "Efficient Gradient Estimation via Adaptive and Importance Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A18gWgc5mi": {
    "title": "Course Correcting Koopman Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AcRfzLS6se": {
    "title": "Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWHPW0sXFK": {
    "title": "PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=di52zR8xgf": {
    "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nq45xeghcL": {
    "title": "Intelligent Switching for Reset-Free RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EJPIzl7mgc": {
    "title": "Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvDeiLv7qc": {
    "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHROe7Mfcb": {
    "title": "Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrE2fqAInO": {
    "title": "Fixed-Budget Differentially Private Best Arm Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UXALv0lJZS": {
    "title": "Separate and Diffuse: Using a Pretrained Diffusion Model for Better Source Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zavLQJ1XjB": {
    "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4jBL79L5QS": {
    "title": "Beyond Shortest-Paths: A Benchmark for Reinforcement Learning on Traffic Engineering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oo5spZRpH6": {
    "title": "HAct: Out-of-Distribution Detection with Neural Net Activation Histograms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mu1UUl14cw": {
    "title": "Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNQJtoPZmz": {
    "title": "Simplicity Bias in Overparameterized Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irBktGvHmC": {
    "title": "Characterizing Exceptional Distributions with Neural Rule Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NRRHkJE03w": {
    "title": "Beyond Dynamics: Learning to Discover Conservation Principles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Ed7b52z53": {
    "title": "On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eNoiRal5xi": {
    "title": "Unknown Domain Inconsistency Minimization for Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5MquO1g7R": {
    "title": "Change Point Detection via Variational Time-Varying Hidden Markov Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDxeSZ1wdI": {
    "title": "Entity-Centric Reinforcement Learning for Object Manipulation from Pixels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLWqIWPMDH": {
    "title": "Memoization-Aware Bayesian Optimization for AI Pipelines with Unknown Costs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6rEcB9m9AI": {
    "title": "Promoting Exploration in Memory-Augmented Adam using Critical Momenta",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjeQKFxFpZ": {
    "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bu3JGyfz23": {
    "title": "A multi-view latent space learning framework via adaptive graph embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c9xsaASm9L": {
    "title": "Enhancing Neural Training via a Correlated Dynamics Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mXJ9o2DNx": {
    "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4A5D1nsdtj": {
    "title": "An Effective Universal Polynomial Basis for Spectral Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8YwPdoSlr": {
    "title": "Capturing Static, Short-Term, and Long-Term Dynamics Through Self-Supervised Time Series Learning: CHRONOS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iVqd2aXzvd": {
    "title": "Automata Learning for Neural Event ODEs: An Interpretable Model of Piecewise Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxnbPPVvOG": {
    "title": "Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuYvrjh2od": {
    "title": "ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UgTrngiN16": {
    "title": "LangProp: A code optimization framework using Language Models applied to driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tGOOP7DGxs": {
    "title": "Graph Transformers for Large Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nO344avRib": {
    "title": "A Simple and Scalable Representation for Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sBSC0OXEQG": {
    "title": "Correlated dense associative memories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hILVmJ4Uvu": {
    "title": "True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pijvVzhRdZ": {
    "title": "Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcqWJ8JgMR": {
    "title": "AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ldJXXxPE0L": {
    "title": "The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yfdtkYQesu": {
    "title": "Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxurbfWwRr": {
    "title": "Learning Time-Varying Convexifications of Multiple Fairness Measures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d1zLRzhalF": {
    "title": "Knowledge Graph Reasoning with Reinforcement Learning Agent guided by Multi-relational Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=01ep65umEr": {
    "title": "TeLLMe what you see: Using LLMs to Explain Neurons in Vision Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nTNgkEIfeb": {
    "title": "FedInverse: Evaluating Privacy Leakage in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GpGJg1gsjl": {
    "title": "Uncertainty for Active Learning on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K3tHTPjFBM": {
    "title": "Equivariant Protein Multi-task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3WzT2mrhB": {
    "title": "From Sparse to Dense: Learning to Construct 3D Human Meshes from WiFi",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJ5N8qrEPl": {
    "title": "Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5NJzNAXAmx": {
    "title": "Informed POMDP: Leveraging Additional Information in Model-Based RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iKd99CYwPX": {
    "title": "Deterministic Diffusion for Sequential Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sgvb61ZM2x": {
    "title": "Effective Learning by Node Perturbation in Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xriGRsoAza": {
    "title": "Inherently Interpretable Time Series Classification via Multiple Instance Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W3T9rql5eo": {
    "title": "Uniform as Glass: Gliding over the Pareto Front with Neural Adaptive Preferences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ZUYJpvIys": {
    "title": "TOSS: High-quality Text-guided Novel View Synthesis from a Single Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6KkyweyYh": {
    "title": "Biological Sequence Analysis Using B ́ezier Curve",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PPjf4wife": {
    "title": "Leveraging Large Language Models for Optimised Coordination in Textual Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbKThNNFxr": {
    "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hvoVD7x7f8": {
    "title": "Uncertainty-Aware Decision Transformer for Stochastic Driving Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=upVI6V81Qn": {
    "title": "Structural Adversarial Objectives For Self-Supervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JrmPG9ufKg": {
    "title": "A Mutual Information Perspective on Federated Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZtt0pRnOl": {
    "title": "Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mt5NPvTp5a": {
    "title": "Improved Operator Learning by Orthogonal Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGdEM131Ht": {
    "title": "GENERATIVE TIME SERIES LEARNING WITH TIME-FREQUENCY FUSED ENERGY-BASED MODEL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ROGsTX3IR": {
    "title": "Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ux2cgxw6O": {
    "title": "LOVECon: Text-driven Training-free Long Video Editing with ControlNet",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4zAFFyoXK": {
    "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFYbBOEOtv": {
    "title": "V-JEPA: Latent Video Prediction for Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ry1SZkcYbX": {
    "title": "Edge-Sampler: Efficient Importance Sampling for Neural Implicit Surfaces Reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3b8CgMO5ix": {
    "title": "Model guidance via explanations turns image classifiers into segmentation models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5Xkw9fpty": {
    "title": "Smooth Min-Max Monotonic Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xEJMoj1SpX": {
    "title": "Elucidating the Exposure Bias in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKnbIZefER": {
    "title": "Availability Attacks Need to Create Shortcuts for Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0tsJ7Nv5hk": {
    "title": "Harnessing Orthogonality to Train Low-Rank Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIqjp9yTDq": {
    "title": "Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XbLffB0T2z": {
    "title": "Transferable Availability Poisoning Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pjz3jkCAir": {
    "title": "CONFIDE: CONtextual FInite DifferencE modelling of PDEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dPHLbUqGbr": {
    "title": "Fast, Expressive $\\mathrm{SE}(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwbQ2YwLhD": {
    "title": "Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tusy7IlWWw": {
    "title": "SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mE52zURNGc": {
    "title": "An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kv5xE1p3jz": {
    "title": "JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LkB80Rw6Ap": {
    "title": "Curvature MPNNs : Improving Message Passing with Local Structural Properties",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7jhe7gr7C": {
    "title": "Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GZ6AcZwA8r": {
    "title": "MMD Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iqHh5Iuytv": {
    "title": "RNNS with gracefully degrading continuous attractors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kvcbV8KQsi": {
    "title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P15CHILQlg": {
    "title": "Learning Energy Decompositions for Partial Inference of GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bshfchPM9H": {
    "title": "RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kgy2swARws": {
    "title": "S\\(^{2}\\)-DMs: Skip-Step Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y6TIwYucUC": {
    "title": "SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tYTCyYI27": {
    "title": "Calibration Bottleneck: What Makes Neural Networks less Calibratable?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HDbKLu0bkn": {
    "title": "Heterogeneity of Regularization between adjacent periods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7t7AOseAa": {
    "title": "ZEST: ZEROSHOT SPARSE FINE-TUNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=huGECz8dPp": {
    "title": "Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SU8vFBJXJt": {
    "title": "Why not both? Combining Bellman losses in deep reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTf7mXhTVt": {
    "title": "Query Efficient Black-Box Adversarial Attack with Automatic Region Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u0INlprg3U": {
    "title": "LIFT: Efficient Layer-wise Fine-tuning for Large Model Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cc8h3I3V4E": {
    "title": "Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6jBNQ8nSxA": {
    "title": "Just-in-Time Security Patch Detection - LLM At the Rescue for Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aLiinaY3ua": {
    "title": "Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WX9cd9iII4": {
    "title": "Fair Off-Policy Learning from Observational Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gvHvwL6Ks4": {
    "title": "Privacy-Preserving Data Quality Evaluation in Federated Learning Using Influence Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gtMbnrsyLA": {
    "title": "SMAAT: Scalable Manifold-Aware Adversarial Training for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XgklTOdV4J": {
    "title": "DualAug: Exploiting Additional Heavy Augmentation with OOD Data Rejection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tWNHQq7gZX": {
    "title": "Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bfRDhzG3vn": {
    "title": "Continual Contrastive Spoken Language Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jiDsk12qcz": {
    "title": "Knowledge Fusion of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K98byXpOpU": {
    "title": "Double Momentum Method for Lower-Level Constrained Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9St5HsXMOr": {
    "title": "Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89AOrk05uy": {
    "title": "Understanding and addressing spurious correlation via Neural Tangent Kernels: A spectral bias perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgOJlxzB16": {
    "title": "SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gAnRV4UaUv": {
    "title": "ISCUTE: Instance Segmentation of Cables Using Text Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7zxGHwe7Vw": {
    "title": "FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mxaOpDHpCW": {
    "title": "Breadth First Exploration in Grid-based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQfCboYwDK": {
    "title": "Adiabatic replay for continual learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DRu8PMHgCh": {
    "title": "FedTrans: Client-Transparent Utility Estimation for Robust Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjfOL9z5Xr": {
    "title": "DyVal: Graph-informed Dynamic Evaluation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTzdVhbTEt": {
    "title": "Designing Long-term Group Fair Policies in Dynamical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rleZtn5OqJ": {
    "title": "Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n2nPeZ9VJ0": {
    "title": "Optimized Large Language Models Accurately Identify Recurrence of VT After Ablation from Complex Medical Notes: Will Chart Review Become Obsolete?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ff2g30cZxj": {
    "title": "From Posterior Sampling to Meaningful Diversity in Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikX6D1oM1c": {
    "title": "A Neural Framework for Generalized Causal Sensitivity Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yLgr02IsXY": {
    "title": "AMPipe: Accelerating MoE Model Training with Intra-Block Pipelining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=msuaCcTMQ2": {
    "title": "Active Automated Machine Learning with Self-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvQ4kzcRSL": {
    "title": "Graph Clustering with Masked AutoEncoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mngdhgi711": {
    "title": "OKR-Agent: An Object and Key Results Driven Agent System with Hierarchical Self-Collaboration and Self-Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qiduMcw3CU": {
    "title": "Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8DW3aSOnou": {
    "title": "Video Deblurring with Adaptive High-frequency Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ap1ByuwQrX": {
    "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQUXBoGiDU": {
    "title": "Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hvtSLkKeZ": {
    "title": "Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvwnYpesoD": {
    "title": "A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIaS66XkNA": {
    "title": "Idempotent Generative Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dnc3paMqDE": {
    "title": "DeepSPF: Spherical SO(3)-Equivariant Patches for Scan-to-CAD Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxL35zAxvT": {
    "title": "Test Time Adaptation with Auxiliary Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1AXvGjfF0V": {
    "title": "Evaluating Hallucinations in Chinese Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RofU5v2BvZ": {
    "title": "Efficient Human-AI Coordination via Preparatory Language-based Convention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x6CjUdI25z": {
    "title": "Suppressing Overestimation in Q-Learning through Adversarial Behaviors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgDrVM9Rpx": {
    "title": "P-MapNet: Far-seeing Map Constructer Enhanced by both SDMap and HDMap Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g0fHn95m3D": {
    "title": "Text-To-Energy: Accelerating Quantum Chemistry Calculations through Enhanced Text-to-Vector Encoding and Orbital-Aware Multilayer Perceptron",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HHWlwxDeRn": {
    "title": "SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LXnTFMvn8A": {
    "title": "A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0elDO9v31": {
    "title": "Intrinsic Mesh CNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JtKGkz9fAe": {
    "title": "Improving Natural Language Understanding with Computation-Efficient Retrieval Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=59nCKifDtm": {
    "title": "Improve Temporal Consistency In Diffusion Models through Noise Correlations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O0dW800ukz": {
    "title": "Multimodal Distillation of Protein Sequence, Structure, and Function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wDE3clrYWR": {
    "title": "Combinatorial Optimization via Memory Metropolis: Template Networks for Proposal Distributions in Simulated Annealing applied to Nanophotonic Inverse Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nbPGqeH3lt": {
    "title": "FedCDA: Federated Learning with Cross-rounds Divergence-aware Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7C04OET3V": {
    "title": "Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=drovOv7IKB": {
    "title": "Divide-and-Conquer Time Series Forecasting with Auto-Frequency-Correlation via Cross-Channel Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fTiU8HhdBD": {
    "title": "A Unified Framework for Reinforcement Learning under Policy and Dynamic Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F5dhGCdyYh": {
    "title": "Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k0nlUXYKhX": {
    "title": "A Fault Forecasting Approach Using Two-Dimensional Optimization (TDO)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6crLU7MIE": {
    "title": "Who to imitate: Imitating desired behavior from divserse multi-agent datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8OOlBjhkU": {
    "title": "Optimization over Sparse Restricted Convex Sets via Two Steps Projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89ZekEEsSJ": {
    "title": "Stealthy Targeted Backdoor Attack Against Image Captioning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nJsfYo3HDy": {
    "title": "Why are Modern GANs Poor Density Models?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GOt2kP383R": {
    "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7VW3KBm34": {
    "title": "Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWzxhtl1HP": {
    "title": "Exploring Diffusion Time-steps for Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ff5srKUefm": {
    "title": "Entropy Voting Between Capsules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=extpNXo6hB": {
    "title": "SweetDreamer: Aligning Geometric Priors in 2D diffusion for Consistent Text-to-3D",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9SwObx9Jdn": {
    "title": "Generation of Geodesics with Actor-Critic Reinforcement Learning to Predict Midpoints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V6JRkfj9dU": {
    "title": "How many samples are needed to train a deep-ReLU neural network?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLbRvr840Q": {
    "title": "Hypergraph Dynamic System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OvlcyABNQT": {
    "title": "Augmented Bayesian Policy Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qZB7KDN4L1": {
    "title": "Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XkLMGx60aZ": {
    "title": "Climate-sensitive Urban Planning through Optimization of Tree Placements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AxYTFpdlvj": {
    "title": "Graph Decoding via Generalized Random Dot Product Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mL8Q9OOamV": {
    "title": "Emu: Generative Pretraining in Multimodality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ghyeMoj1gK": {
    "title": "Client-centric Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A4mJuFRMN8": {
    "title": "Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jhiByZpuIS": {
    "title": "MSfusion: Enabling Collaborative Training of Large Models over Resource-Constraint Participants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EhmEwfavOW": {
    "title": "HoloNets: Spectral Convolutions do extend to Directed Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cNi2EJ8OCh": {
    "title": "Functional Classification Under Local Differential Privacy with Model Reversal and Model Average",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HilIIP4yzw": {
    "title": "Improving Learning Conditions for Computer Science Students by Using the Flipped Classroom",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ue93J8VV3W": {
    "title": "TabGraphs: new benchmark and insights for learning on graphs with tabular features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WesY0H9ghM": {
    "title": "Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wx97sznZwB": {
    "title": "CLIP-Guided Reinforcement Learning for Open-Vocabulary Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vgD20RxsC0": {
    "title": "Time Series Prediction With Events Disturbance Based Causal Representation Learnin",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8UfDs4J46": {
    "title": "Addressing Signal Delay in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OROKjdAfjs": {
    "title": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qTlcbLSm4p": {
    "title": "Relay Diffusion: Unifying diffusion process across resolutions for image synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7jUQHmz4Tq": {
    "title": "D3AD: DYNAMIC DENOISING DIFFUSION PROBABILISTIC MODEL FOR ANOMALY DETECTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h922Qhkmx1": {
    "title": "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Lqyut1y7M": {
    "title": "On the Optimality of Activations in Implicit Neural Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2gMwe9Duc4": {
    "title": "Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WcSofkUVge": {
    "title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c72vop46KY": {
    "title": "CogVLM: Visual Expert for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KiespDPaRH": {
    "title": "Improving the Convergence of Dynamic NeRFs via Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZBpVcc2Xc": {
    "title": "HiddenKey: Parameter-Efficient FineTuning Meets Dropout under a Unified Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CX2RgsS29V": {
    "title": "Fast Updating of Truncated SVD for Representation Learning in Sparse Matrix",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0I2RtD8je": {
    "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lKK50q2MtV": {
    "title": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1bAUywYJTU": {
    "title": "DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9XdLlbxZCC": {
    "title": "MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9FXGX00iMF": {
    "title": "BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtmB8WrPSp": {
    "title": "Sparse-PGD: An Effective and Efficient Attack for $l_0$ Bounded Adversarial Perturbation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0AYosSFETw": {
    "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qfqb8ueIdy": {
    "title": "A Unified Framework for Consistency Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fTEPeQ00VM": {
    "title": "TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dszD2gZIif": {
    "title": "Long-term Time Series Forecasting with Vision Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJksrOhurE": {
    "title": "CARD: Channel Aligned Robust Blend Transformer for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4w4PDIT3h4": {
    "title": "Focus on Primary: Differential Diverse Data Augmentation for Generalization in Visual Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PJVUWpPnZC": {
    "title": "Reinforcement Symbolic Regression Machine",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFYksmdqgY": {
    "title": "Beyond Language: Empowering Unsupervised Machine Translation with Cross-modal Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u48tHG5f66": {
    "title": "ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hGKda1uVEn": {
    "title": "Support Vector-based Shapley Value Estimation for Feature Selection and Explanation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqhAA26vXE": {
    "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dkn9cEOQkU": {
    "title": "Addressing Real-Time Fragmentary Interaction Control Problems via Muti-step Representation Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o6eUNPBAEc": {
    "title": "Language Models Struggle to Explain Themselves",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BlCnycxgJQ": {
    "title": "An Inexact Regularized Adaptive Algorithm with Manifold Identification for Training Structured Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhsZwzBYaU": {
    "title": "Tailoring Mixup to Data using Kernel Warping functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6cMmSnOpCs": {
    "title": "ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cObFETcoeW": {
    "title": "Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRWrvzRU4w": {
    "title": "OneSpike: Ultra-low latency spiking neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkvZ7v4OmP": {
    "title": "Space Group Constrained Crystal Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHedM0Zem9": {
    "title": "BEEF: Building a BridgE from Event to Frame",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YXnggA4iiD": {
    "title": "Distribution Aware Active Learning via Gaussian Mixtures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1SSQ6Dekc": {
    "title": "TransLLaMa: LLM-based Simultaneous Translation System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ztL7Trdnx": {
    "title": "TAFS: Task-aware Activation Function Search for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vy6sjPt2Vr": {
    "title": "A Spitting Image: Superpixel Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qox9rO0kN0": {
    "title": "Learning Multi-Agent Communication from Graph Modeling Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1rgMkDWfYV": {
    "title": "Cleaning label noise with vision-language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfh8ZbSlyf": {
    "title": "SITReg: Multi-resolution architecture for symmetric, inverse consistent, and topology preserving image registration using deformation inversion layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=52igC7K5Mf": {
    "title": "GC-Mixer: A Novel Architecture for Time-varying Granger Causality Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHdXvu5ehy": {
    "title": "An Efficient Subgraph GNN with Provable Substructure Counting Power",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MSe8YFbhUE": {
    "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbkIwjmua3": {
    "title": "Vulnerable Region Discovery through Diverse Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WRxCuhTMB2": {
    "title": "Experimental methodology to evaluate the effectiveness of uncertainty disentanglement on regression models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpnKq3UJwp": {
    "title": "Efficient Multi-agent Reinforcement Learning by Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yuYMJQIhEU": {
    "title": "Communication-efficient Random-Walk Optimizer for Decentralized Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlfD5cE1ep": {
    "title": "Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iT1ttQXwOg": {
    "title": "Equivariant Deep Weight Space Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7LCsDMcZ4": {
    "title": "EventRPG: Event Data Augmentation with Relevance Propagation Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qFIkOhq24": {
    "title": "Fundamental Limitation of Alignment in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WdhtdjoaVw": {
    "title": "Functional Wasserstein Bridge Inference for Bayesian Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xGvPKAiOhq": {
    "title": "How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOJriBlOFd": {
    "title": "NeRM: Learning Neural Representations for High-Framerate Human Motion Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qjFnENGhDE": {
    "title": "Regularization is Enough for Last-Iterate Convergence in Zero-Sum Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n9CqhWGK4o": {
    "title": "Cellular Interplay in COVID-19: Insights from Graph Neural Networks with Multidimensional Edge Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3edHHvu5GX": {
    "title": "Adaptive Visual Scene Understanding: Incremental Scene Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gS0XOu0JKs": {
    "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=toD3yzfuaf": {
    "title": "Meta-Learning with Personalized Learning Rates for Rapid Task Mastery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EX7AxKgc46": {
    "title": "Improved Generalization of cGAN using Vicinal Estimation and Early Stopping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xh3XUaB8M9": {
    "title": "Visual Evidence Prompting Mitigates Hallucinations in Multimodal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbLuklVaX7": {
    "title": "Node Classification in the Heterophilic Regime via Diffusion-Jump GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezBH9WE9s2": {
    "title": "AnyText: Multilingual Visual Text Generation and Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QGk2UhrC0Z": {
    "title": "IGTO: Individual Global Transform Optimization for Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgndgAbBcR": {
    "title": "CNNGEN: A GENERATOR AND BENCHMARK FOR SUSTAINABLE CONVOLUTIONAL NEURAL NETWORK SEARCH",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvBx5whpzJ": {
    "title": "Con4m: Unleashing the Power of Consistency and Context in Classification for Blurred-Segmented Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lqA5EuieJ": {
    "title": "Prediction Tasks in Graphs: a Framework to Control the Interpretability-Performance Trade-off",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3klVRLhK7w": {
    "title": "Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlvtjAB0gl": {
    "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=06mzMua9Rw": {
    "title": "A Trust Region Approach for Few-Shot Sim-to-Real Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEwKAZZmSw": {
    "title": "Efficient Backpropagation with Variance Controlled Adaptive Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSHEYpGF6P": {
    "title": "Meta-Learning with Task-Environment Interaction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhXisLeIqR": {
    "title": "WinNet:time series forecasting with a window-enhanced period extracting and interacting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l8je4qJR4K": {
    "title": "Domain Generalization via Content Factors Isolation: A Two-level Latent Variable Modeling Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OPpqmSp0wK": {
    "title": "Multi-label Cluster Discrimination for Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbiU3WZpTp": {
    "title": "On the Robustness of Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80wh3jjCZf": {
    "title": "Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3f5PALef5B": {
    "title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6mLjDwYte5": {
    "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OqTMUPuLuC": {
    "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdjKRbtrth": {
    "title": "Generative Retrieval with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wMWZ78ulsK": {
    "title": "An Information Theoretic Approach to Interaction Grounded Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cf4FJGmHRQ": {
    "title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0vdDSt9XM": {
    "title": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S46Knicu56": {
    "title": "A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K8Mbkn9c4Q": {
    "title": "TABLEYE: SEEING SMALL TABLES THROUGH THE LENS OF IMAGES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1k4yZbbDqX": {
    "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Ebi1euQZQ": {
    "title": "HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VpCqrMMGVm": {
    "title": "Interpreting the Inner Mechanisms of Large Language Models in Mathematical Addition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nXgWT12tb": {
    "title": "Correlated Attention in Transformers for Multivariate Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wi8wMFuO0H": {
    "title": "Cross-domain Recommendation from Implicit Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=krIOxfqsOh": {
    "title": "Masked Pretraining for Multi-Agent Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpcVXiMlcv": {
    "title": "Object-Aware Inversion and Reassembly for Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Qyxw0cCuu": {
    "title": "CONTROL: A Contrastive Learning Framework for Open World Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ehSQZa4vuk": {
    "title": "Bad Habits: Policy Confounding and Out-of-Trajectory Generalization in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wWI1RYngAA": {
    "title": "Adaptive Offline Data Replay in Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7essnmWOK5": {
    "title": "Graph Neural Networks for Multivariate Time-Series Forecasting via Learning Hierarchical Spatiotemporal Dependencies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lK4QHgjUU8": {
    "title": "SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eiF7TU1E8E": {
    "title": "SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUz6Qefe5z": {
    "title": "How Neural Networks With Derivative Labels Work: A Neural Tangent Kernel Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKl6lMwbCy": {
    "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7qhUMylLU": {
    "title": "Sample-Efficient Multi-Agent RL: An Optimization Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sRop0N5NYV": {
    "title": "Tactics of Robust Deep Reinforcement Learning with Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpDw5U04SU": {
    "title": "Minimum width for universal approximation using ReLU networks on compact domain",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Ok7ccvtf3": {
    "title": "UNLEARNING THE UNWANTED DATA FROM A PERSONALIZED RECOMMENDATION MODEL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KIPJKST4gw": {
    "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3K3aWRpRNq": {
    "title": "Reducing Atomic Clashes in Geometric Diffusion Models for 3D Structure-Based Drug Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UiLqimGm5": {
    "title": "Coordinate-Aware Modulation for Neural Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RXU6qde675": {
    "title": "Adversarial enhanced representation for link prediction in multi-layer networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKfMFtlz5D": {
    "title": "MG-NeRF: Multimodal Representation Learning for Generalizable NeRF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLGliHckR8": {
    "title": "Drug Discovery with Dynamic Goal-aware Fragments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h57gkDO2Yg": {
    "title": "Self-Supervised Dataset Distillation for Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Go8hf9wKJx": {
    "title": "DOG: Diffusion-based Outlier Generation for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SL7djdVpde": {
    "title": "Rethinking the symmetry-preserving circuits for constrained variational quantum algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wilJbPvRYv": {
    "title": "Are We in (A)Sync?: Guidance for Efficient Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYu5Flqm9D": {
    "title": "Towards Codable Text Watermarking for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=opZTBFnX2G": {
    "title": "Bayesian Offline-to-Online Reinforcement Learning : A Realist Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHaokdQkoi": {
    "title": "GNRK: Graph Neural Runge-Kutta method for solving partial differential equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G7UtIGQmjm": {
    "title": "Hypothesis Search: Inductive Reasoning with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oNlPtI7QfQ": {
    "title": "Embed-Search-Align: DNA Sequence Alignment using Transformer models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i4ULDEeBss": {
    "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=488A64eOf6": {
    "title": "Language Model Decoding as Direct Metrics Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uMAujpVi9m": {
    "title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fQxLgR9gx7": {
    "title": "Factual and Personalized Recommendation Language Modeling with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUPrQXSKOv": {
    "title": "Decentralized Decoupled Training for Federated Long-Tailed Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8giiPtg6rw": {
    "title": "DataFreeShield: Defending Adversarial Attacks without Training Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=peZbJlOVAN": {
    "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5IOKw3AQe4": {
    "title": "On the Theoretical Analysis of Dense Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FoqZKsH9sE": {
    "title": "LSP: Low-Power Semi-structured Pruning for Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NZtF0um8S7": {
    "title": "Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GszBQ3ZTzk": {
    "title": "PDED: Revitalize physics laws submerged in data information for Traffic State Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Vh0XqOTGi": {
    "title": "GAN-based Vertical Federated Learning for Label Protection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JRsAj3ymy": {
    "title": "Time-Sensitive Replay for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bO1UP57GAw": {
    "title": "Dataset Distillation via Adversarial Prediction Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftdtqEiTXZ": {
    "title": "Pay attention to cycle for spatio-temporal graph neural network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i6EtCiIK4a": {
    "title": "Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jz35igczhm": {
    "title": "Brain-inspired $L_p$-Convolution benefits large kernels and aligns better with visual cortex",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHZm9vNm5H": {
    "title": "Efficient ConvBN Blocks for Transfer Learning and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7rex8lEZH2": {
    "title": "Prompt Tuning with Diffusion for Few-Shot Pre-trained Policy Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TrKq4Wlwcz": {
    "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2cRzmWXK9N": {
    "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z8TW0ttBPp": {
    "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2DDwxbjP9g": {
    "title": "In Defence Of Wasserstein",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c8McWs4Av0": {
    "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CeJEfNKstt": {
    "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwDuW7Ok5f": {
    "title": "Dual Associated Encoder for Face Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lYy9zPOxXS": {
    "title": "Topology-Informed Graph Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8FhwHJGUPZ": {
    "title": "Dual-Balancing for Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tz6HnhBzLl": {
    "title": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RUvzlotXY0": {
    "title": "HiCBridge: Resolution Enhancement of Hi-C Data Using Direct Diffusion Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ISrxxvXJQO": {
    "title": "On the Hidden Waves of Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkgULK8u4d": {
    "title": "MGTST: Multi-scale and Cross-channel Gated Transformer for Multivariate long-term time-series forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8dkp41et6U": {
    "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J88EKENxyF": {
    "title": "CAT-LLM: Context-Aware Training enhanced Large Language Models for multi-modal contextual image retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sp666x6Gh3": {
    "title": "Density-Softmax: Efficient Test-time Model for Uncertainty Estimation and Robustness under Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4XCfu7fTgw": {
    "title": "Spectral Contrastive Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VqEE9i6jhE": {
    "title": "Tensor methods to learn the Green's function to solve high-dimensional PDE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUiYbVqcuQ": {
    "title": "A2FC: A FEDERATED ADVANTAGE ACTOR-CRITIC LEARNING APPROACH FOR HETEROGENEOUS ACTION SPACES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5webNFDgQ": {
    "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N8UGyR3HTI": {
    "title": "FragSel: Fragmented Selection for Noisy Label Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gXfKPj4s7C": {
    "title": "Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DDX1u29Gqr": {
    "title": "DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuVlUn4T2G": {
    "title": "Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g6rZtxaXRm": {
    "title": "Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VfPWJM5FMr": {
    "title": "ColA: Collaborative Adaptation with Gradient Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=53kW6e1uNN": {
    "title": "AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hCaoJh01io": {
    "title": "InfoGround: Ground Manipulation Concepts with Maximal Information Boost",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aFWUY3E7ws": {
    "title": "Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYmF38ysDa": {
    "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsn1lPgDvU": {
    "title": "STABLE ESTIMATION OF SURVIVAL CAUSAL EFFECTS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itrOA1adPn": {
    "title": "A computational approach to visual ecology with deep reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60e1hl06Ec": {
    "title": "Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWyZeqE928": {
    "title": "Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KpoQSgxbKH": {
    "title": "Generative Pre-training for Speech with Flow Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vbqxaiHGmL": {
    "title": "Generative and Explainable Data Augmentation for Single-Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bn8iWvRSmq": {
    "title": "Successor Features for Efficient Multi-Subject Controlled Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qg2boc2AwU": {
    "title": "Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iad1yyyGme": {
    "title": "CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UQVhOVhUi4": {
    "title": "Graph Generation with Destination-Predicting Diffusion Mixture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mDIXfHvoqH": {
    "title": "ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FGoq622oqY": {
    "title": "BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZLSdwjDevK": {
    "title": "Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AXbN2qMNiW": {
    "title": "Protein-ligand binding representation learning from fine-grained interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEuTWBfVoe": {
    "title": "Model Based Inference of Synaptic Plasticity Rules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7FeIRqCedv": {
    "title": "SLiMe: Segment Like Me",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3KNrmW6Ql": {
    "title": "Adversarial Attacks on Fairness of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rp0EdI8X4e": {
    "title": "Faithful Vision-Language Interpretation via Concept Bottleneck Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HMe5CJv9dQ": {
    "title": "Efficiently Computing Similarities to Private Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQZicPtADC": {
    "title": "The Role of Representation Transfer in Multitask Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=liKkG1zcWq": {
    "title": "Sliced Denoising: A Physics-Informed Molecular Pre-Training Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bCNYFOaWsy": {
    "title": "Class-Imbalanced Graph Learning without Class Rebalancing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNzpTSuuVJ": {
    "title": "Every Mistake Counts: Spatial and Temporal Beliefs for Mistake Detection in Assembly Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RauUgiw7VX": {
    "title": "Fine-grained Text-to-Image Synthesis with Semantic Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UIBysXjVq": {
    "title": "Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrMnLl9RCl": {
    "title": "Improving Generalization for Missing Data Imputation via Dual Corruption Denoising Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hD3sGVqPsr": {
    "title": "P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMLyPgqos1": {
    "title": "Graph-Relational Federated Learning: Enhanced Personalization and Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXk9gcKhqp": {
    "title": "Rethinking the Polynomial Filter of GNNs via Graph Information Activation Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xmQuUqSynb": {
    "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5VElAKt2s": {
    "title": "LoRA ensembles for large language model fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lajn1iROCu": {
    "title": "SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOfDKxfwt0": {
    "title": "RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOveLu4O51": {
    "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BE5aK0ETbp": {
    "title": "A Unified and General Framework for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CZiY6OLktd": {
    "title": "MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7pWRLDBAtc": {
    "title": "Heterogeneous Personalized Federated Learning by Local-Global Updates Mixing via Convergence Rate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYhDcQudVI": {
    "title": "Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEHGSN8Hy8": {
    "title": "SetCSE: Set Operations using Contrastive Learning of Sentence Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNzBTVuMgq": {
    "title": "Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hmv1LpNfXa": {
    "title": "Polynormer: Polynomial-Expressive Graph Transformer in Linear Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pWSL8My6B": {
    "title": "Where We Have Arrived in Proving the Emergence of Sparse Interaction Primitives in AI Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vwlryNhWp7": {
    "title": "Improving Discriminative Multi-Modal Learning with Large-Scale Pre-Trained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=joMMM9eadc": {
    "title": "Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N3DEoB9fIQ": {
    "title": "Debiased Machine Learning and Network Cohesion for Doubly-Robust Differential Reward Models in Contextual Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xA25Ib7H8U": {
    "title": "Understanding Continuous-depth Networks through the Lens of Homogeneous Ricci Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7oU4nfKEA": {
    "title": "When Is Multilinguality a Curse? Language Modeling for 252 High- and Low-Resource Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VuCRpguZOr": {
    "title": "Gaussian Mutual Information Maximization for Graph Self-supervised Learning: Bridging Contrastive-based to Decorrelation-based",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zrr6kH1cSh": {
    "title": "AdaSR: Adaptive Super Resolution for Cross Platform and Dynamic Runtime Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1pNNQSzZv": {
    "title": "Rational Decision-Making Agent with Internalized Utility Judgment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yZJapMWdHZ": {
    "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ROC3UASRV7": {
    "title": "A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1U6sEgYkb": {
    "title": "DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jNR6s6OSBT": {
    "title": "ASID: Active Exploration for System Identification and Reconstruction in Robotic Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DIuSX4HqDZ": {
    "title": "Abductive Logical Reasoning on Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kuTZMZdCPZ": {
    "title": "Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ur4LqAOXIF": {
    "title": "SODA: Stream Out-of-Distribution Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cf8HBieRzL": {
    "title": "UniContact:A Basic Model for Robotic Manipulation of Contact Synthesis on Rigid and Articulated Rigid Bodies with Arbitrary Manipulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z7OWaSze0V": {
    "title": "Unifying User Preferences and Critic Opinions: A Multi-View Cross-Domain Item-sharing Recommender System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g5TIh84amg": {
    "title": "A Curriculum View of Robust Loss Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UmMa3UNDAz": {
    "title": "EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YcJCzJzQT5": {
    "title": "DipDNN: Decomposed Invertible Pathway Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=St7aZgQJBf": {
    "title": "Curriculum metric learning for robust image retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zKgrmMOQjg": {
    "title": "TCD: TEXT IMAGE CHANGE DETECTION FOR MULTILINGUAL DOCUMENT COMPARISON",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhCdJ93Wmi": {
    "title": "Graph Inference Acceleration by Bridging GNNs and MLPs with Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8fQlGQkj0S": {
    "title": "A Theoretical Analysis of In-context Task Retrieval and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RNGUbTYSjk": {
    "title": "Weaker MVI Condition: Extragradient Methods with Multi-Step Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3TO3TtnOFl": {
    "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AVBw2Ul4X9": {
    "title": "Towards Precise Prediction Uncertainty in GNNs: Refining GNNs with Topology-grouping Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qoYogklIPz": {
    "title": "Demystifying Embedding Spaces using Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=likXVjmh3E": {
    "title": "The Expressive Power of Low-Rank Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QlqdXrzzD1": {
    "title": "Towards Category Unification of 3D Single Object Tracking on Point Clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0lW9cDUtf8": {
    "title": "FairReweighing: density estimation-based reweighing framework for improving separation in fair regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7WsivwyHrS": {
    "title": "You Only Query Once: An Efficient Label-Only Membership Inference Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QtywskEyY": {
    "title": "Teaching wiser, Learning smarter: Multi-stage Decoupled Relational Knowledge Distillation with Adaptive Stage Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y5e9fvvBUz": {
    "title": "PRISM: Privacy-Preserving Improved Stochastic Masking For Federated Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W4boWjb3Q": {
    "title": "Partitioned-Learned Count-Min Sketch",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0FI3Q66K5": {
    "title": "Frozen Transformers in Language Models Are Effective Visual Encoder Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HOpQt44EzC": {
    "title": "Differentially Private Vision-Language Foundation Models via Image Captioning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=udO3k28bEw": {
    "title": "Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqxBTcWRnj": {
    "title": "Bridging Neural and Symbolic Representations with Transitional Dictionary Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JW3jTjaaAB": {
    "title": "AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mxJEX6w5uN": {
    "title": "Scaff-PD: Communication Efficient Fair and Robust Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dk1ybhMrJv": {
    "title": "Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9z0HzWJJp": {
    "title": "SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gU6OqJfO0G": {
    "title": "ON LEARNABILITY AND EXPERIENCE REPLAY METHODS FOR GRAPH INCREMENTAL LEARNING ON EVOLVING GRAPHS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Km3Kprwyua": {
    "title": "Online Speculative Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpyFAhH9CK": {
    "title": "Morphological Maze: Control Reconfigurable Soft Robots with Fine-grained Morphology Change",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjXor87Xfy": {
    "title": "PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cgCKm5DOnu": {
    "title": "ROSA: Random Orthogonal Subspace Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5RUf9nEdyC": {
    "title": "TEDDY: Trimming Edges with Degree-based Graph Diffusion Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NtQqIcSbqv": {
    "title": "Learning to Jointly Understand Visual and Tactile Signals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60lNoatp7u": {
    "title": "NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eC4WlSZc4H": {
    "title": "Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XW0gD13oQp": {
    "title": "Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=92btneN9Wm": {
    "title": "SPDER: Semiperiodic Damping-Enabled Object Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7OO8tTOgh4": {
    "title": "Non-targeted Adversarial Attacks on Vision-Language Models via Maximizing Information Entropy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U9p10hgOpU": {
    "title": "Unsupervised Lifelong Learning with Sustained Representation Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKzGrt3m2g": {
    "title": "Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e4FG5PJ9uC": {
    "title": "The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJCIv4aUQu": {
    "title": "ADOPT: Modified Adam Can Converge with the Optimal Rate with Any Hyperparameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fxQiecl9HB": {
    "title": "Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mgq6kxl115": {
    "title": "Fast Ensembling with Diffusion Schr\\\"odinger Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hP4iZU8I3Y": {
    "title": "Understanding Inter-Session Intentions via Complex Logical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UaMgmoKEBj": {
    "title": "Decoupling regularization from the action space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9V7ugVuUz": {
    "title": "Robust Similarity Learning with Difference Alignment Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51cjeYcXjs": {
    "title": "Search and Retrieval in Semantic-Structural Representations of Novel Malware",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PKICZXVY9M": {
    "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ix7rLVHXyY": {
    "title": "Learning Performance-Improving Code Edits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIuevDSK5V": {
    "title": "ConR: Contrastive Regularizer for Deep Imbalanced Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wN9HBrNPSX": {
    "title": "Enhancing Temporal Knowledge Graph Completion with Global Similarity and Weighted Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Wy6pLNQcG": {
    "title": "RegionSpot: Unleashing the Power of Frozen Foundation Models for Open-World Region Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ORHuMEwaC8": {
    "title": "The Role of Counterfactual Explanations in Model Extraction Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wd47f7HEXg": {
    "title": "Quasi-Monte Carlo for 3D Sliced Wasserstein",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LnLySuf1vp": {
    "title": "A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAhgJ1fQ4V": {
    "title": "A Multi-resolution Dataset of Self-consistent Cloth Drapes for Physics-based Upsampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l3qtSNsPvC": {
    "title": "A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XJ9vjEAqbx": {
    "title": "Adversarial Training Should Be Cast as a Non-Zero-Sum Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hQVCCxQrYN": {
    "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3fRbP8g2LT": {
    "title": "Efficient Redundancy-Free Graph Networks: Higher Expressiveness and Less Over-Squashing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqPnesiGGi": {
    "title": "Motif-aware Attribute Masking for Molecular Graph Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QNW42cjkym": {
    "title": "A Data-Driven Solution for the Cold Start Problem in Biomedical Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kmnQYA8snK": {
    "title": "Scaling up Trustless DNN Inference with Zero-Knowledge Proofs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KjOAHlKMF5": {
    "title": "Cascading Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgyWXX8HCk": {
    "title": "Trustless Audits without Revealing Data or Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S5aUhpuyap": {
    "title": "Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4h1apFjO99": {
    "title": "Diffusion-TS: Interpretable Diffusion for General Time Series Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pp2j9BvpgC": {
    "title": "Attribute Recognition with Image-Conditioned Prefix Language Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWpwDdVbaM": {
    "title": "ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qbw861vueP": {
    "title": "BiDST: Dynamic Sparse Training is a Bi-Level Optimization Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ARPrtuzAnQ": {
    "title": "On the hardness of learning under symmetries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aVtQChA6WH": {
    "title": "Distributional off-policy evaluation with Bellman residual minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8j9hz8DVi8": {
    "title": "Combining Axes Preconditioners through Kronecker Approximation for Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sf2A2PUXO3": {
    "title": "Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irorVob9Eq": {
    "title": "Towards the Characterization of Representations Learned via Capsule-based Network Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=exei8zvY13": {
    "title": "Improving High-Frequency Details in Cerebellum for Brain MRI Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojIJZDNIBj": {
    "title": "Copula Conformal prediction for multi-step time series prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nc5GgFAvtk": {
    "title": "An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j3bkYuxITP": {
    "title": "Refined Partitioning Boosts MGDA: Introducing RP-MGDA for Multi-Objective Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4IT2pgc9v6": {
    "title": "One For All: Towards Training One Graph Model For All Classification Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sHEJJmzBIN": {
    "title": "Branch-GAN: Improving Text Generation with (not so) Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6CgvBarc4": {
    "title": "Bag of Tricks to Boost Adversarial Transferability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GIUjLsDP4Z": {
    "title": "Effective Structural Encodings via Local Curvature Profiles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BydD1vNMCV": {
    "title": "Statistical Inference for Deep Learning via Stochastic Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GXtmuiVrOM": {
    "title": "Domain Randomization via Entropy Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EJvFFedM2I": {
    "title": "TRAM: Benchmarking Temporal Reasoning for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgM3MzT99c": {
    "title": "OMNI: Open-endedness via Models of human Notions of Interestingness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmAzKz9DJs": {
    "title": "Centroid- and Orientation-aware Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9hjVoPWPnh": {
    "title": "Machine Unlearning for Image-to-Image Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8cBsT9mcg": {
    "title": "Classification with Conceptual Safeguards",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0uI5415ry7": {
    "title": "Linear attention is (maybe) all you need (to understand Transformer optimization)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l0pPTGMqZt": {
    "title": "Domain Generalization for Domain-Linked Classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6yXAKleluj": {
    "title": "Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FUgrjq2pbB": {
    "title": "MVDream: Multi-view Diffusion for 3D Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QqdloE1QH2": {
    "title": "Multilingual Mathematical Autoformalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GaLCLvJaoF": {
    "title": "Robust Model Based Reinforcement Learning Using $\\mathcal{L}_1$ Adaptive Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f9RvYpXhFI": {
    "title": "Estimating Fréchet bounds for validating programmatic weak supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONPECq0Rk7": {
    "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZQVV6IY0OE": {
    "title": "The Implicit Bias of Stochastic AdaGrad-Norm on Separable Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wg8NPfeMF9": {
    "title": "$\\texttt{NAISR}$: A 3D Neural Additive Model for Interpretable Shape Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HhfcNgQn6p": {
    "title": "Towards a statistical theory of data selection under weak supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CvxcWCDX0h": {
    "title": "Learning Multi-Modal Representation Alignments from Noisy Data-Pairs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5rrYpa2vts": {
    "title": "EA2N: Evidence-based AMR Attention Network for Fake News Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UkLSvLqiO7": {
    "title": "The Emergence of Reproducibility and Consistency in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O9PArxKLe1": {
    "title": "Leveraging Optimization for Adaptive Attacks on Image Watermarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gx2BT0a9MQ": {
    "title": "ZeRO++: Extremely Efficient Collective Communication for Large Model Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IkmD3fKBPQ": {
    "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Psl75UCoZM": {
    "title": "Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EgASiEujt6": {
    "title": "Towards Controllable Diffusion Models via Training-Phase Guided Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUIANwOLBN": {
    "title": "Behind the Myth of Exploration in Policy Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UIGAtKp8nW": {
    "title": "MUBen: Benchmarking the Uncertainty of Molecular Representation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3QkzYBSWqL": {
    "title": "Universal Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfhG5znxzR": {
    "title": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yqIJoALgdD": {
    "title": "Towards Zero Memory Footprint Spiking Neural Network Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqiAfRT1Lq": {
    "title": "Eliciting Human Preferences with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NMPLBbjYFq": {
    "title": "Large Language Models as Rational Players in Competitive Economics Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u3dHl287oB": {
    "title": "The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dGH4kHFKFj": {
    "title": "GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgSfV6sGIn": {
    "title": "STExplainer: Global Explainability of GNNs via Frequent SubTree Mining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4Ero36Zr4": {
    "title": "Rethinking Teacher-Student Curriculum Learning under the Cooperative Mechanics of Experience",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jp3gWrMuIZ": {
    "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ny8NiVfi95": {
    "title": "Masked Audio Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9wDX850jR": {
    "title": "Feature emergence via margin maximization: case studies in algebraic tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gT5hALch9z": {
    "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PsDFgTosqb": {
    "title": "Learning to Solve Bilevel Programs with Binary Tender",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4SmhpF1nO4": {
    "title": "Tabular Deep-SMOTE: A supervised autoencoder-based minority-oversampling technique for class-imbalanced tabular classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XSwxy3bojg": {
    "title": "Generating Molecular Conformer Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BZtEthuXRF": {
    "title": "Manifold Diffusion Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sAOtKKHh1i": {
    "title": "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGRDoPU0Lq": {
    "title": "Fast Explanation of RBF-Kernel SVM Models Using Activation Patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G4D6jClNFl": {
    "title": "Deepfake Detection with Contrastive Learning in Curved Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fH9eqpCcR3": {
    "title": "Multiple Physics Pretraining for Physical Surrogate Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vDArHJ68h": {
    "title": "Mastering Memory Tasks with World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T5Xb0iGCCv": {
    "title": "Neur2RO: Neural Two-Stage Robust Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PoB6QGAM38": {
    "title": "Neural Networks Decoded: Targeted and Robust Analysis of Neural Network Decisions via Causal Explanations and Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vY4iBYm9TU": {
    "title": "A Study of the Effects of Transfer Learning on Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZzQz8ikwg": {
    "title": "Efficient local linearity regularization to overcome catastrophic overfitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JORAfH2xFd": {
    "title": "On the Stability of Iterative Retraining of Generative Models on their own Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m8KWOgE0Cn": {
    "title": "FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jFJPd9kIiF": {
    "title": "Compressing Latent Space via Least Volume",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XvSLWB0kfN": {
    "title": "Cultural and Linguistic Diversity Improves Visual Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NuDmRQJ26K": {
    "title": "LUMEN-PRO: Automating Multi-Task Learning on Optical Neural Networks with Weight Sharing and Physical Rotation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HEcbGXzIHK": {
    "title": "Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jj8AAlNobk": {
    "title": "A Differentiable Sequence Model Perspective on Policy Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73dhbcXxtV": {
    "title": "LOLAMEME: LOGIC, LANGUAGE, MEMORY, MECHANISTIC FRAMEWORK",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wKB3XcQHcX": {
    "title": "Speed Limits for Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kALZASidYe": {
    "title": "Towards Enhanced Controllability of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iynRvVVAmH": {
    "title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearizeation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4znwzG92CE": {
    "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJYIyO4G1t": {
    "title": "OLGA: One-cLass Graph Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SksPFxRRiJ": {
    "title": "Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8L0pN6EOi": {
    "title": "Let's Verify Step by Step",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrslLZmkye": {
    "title": "SEE-OoD: Supervised Exploration for Enhanced Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VvAiCXwPvD": {
    "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PvyOYleymy": {
    "title": "Masked Completion via Structured Diffusion with White-Box Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p09XyFxZkc": {
    "title": "LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlEUIydMMh": {
    "title": "Neuro-Causal Factor Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qz3mcn99cu": {
    "title": "Effectively Leveraging Capacity for Improved Deterministic Robustness Certification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jfTrsqRrpb": {
    "title": "Open-world Instance Segmentation: Top-down Learning with Bottom-up Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fYerSwf1Tb": {
    "title": "HawkesVAE: Sequential Patient Event Synthesis for Clinical Trials",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riQmzq5FaQ": {
    "title": "Reinforcement Learning with Elastic Time Steps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ptmeLzcNyB": {
    "title": "Generalising Multi-Agent Cooperation through Task-Agnostic Communication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nHESwXvxWK": {
    "title": "Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1oijHJBRsT": {
    "title": "Self-Alignment with Instruction Backtranslation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y5kBPtU0o": {
    "title": "MEND: Meta Demonstration Distillation for Efficient and Effective In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jHdz0CIS2y": {
    "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYIIRgwg2i": {
    "title": "The LLM Surgeon",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JEAlXPYSjC": {
    "title": "Your CLIP Model Might Be Undertrained",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6NEJ0ReNzr": {
    "title": "Learning to Plan and Generate Text with Citations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TXvaWOBuAC": {
    "title": "Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z7K2faBrDG": {
    "title": "Perceptual Measurements, Distances and Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kr7KpDm8MO": {
    "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2NwHLAffZZ": {
    "title": "Weak Correlations as the Underlying Principle for Linearization of Gradient-Based Learning Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckzglrAMsh": {
    "title": "Prompt-tuning Latent Diffusion Models for Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hP9JcXpNk": {
    "title": "Going beyond familiar features for deep anomaly detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t9dWHpGkPj": {
    "title": "Language Model Inversion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DsEhqQtfAG": {
    "title": "Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c7DND1iIgb": {
    "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmg0qMKYRQ": {
    "title": "Intriguing Properties of Generative Classifiers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HW2lIdrvPb": {
    "title": "Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qp33jnRKda": {
    "title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FdVXgSJhvz": {
    "title": "Alpagasus: Training a Better Alpaca Model with Fewer Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMId7iPDOH": {
    "title": "Stylist: Style-Driven Feature Ranking for Robust Novelty Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzG4BqztV8": {
    "title": "EntProp: High Entropy Propagation via Auxiliary Batch Normalization Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=viftsX50Rt": {
    "title": "Universal Graph Random Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMbWsXPUVL": {
    "title": "LLM-Codebook for Extreme Compression of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8Zo7jACq7": {
    "title": "Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eh0Od2BJIM": {
    "title": "HyperAttention: Long-context Attention in Near-Linear Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cMQeDPwSrB": {
    "title": "Memorization Through the Lens of Curvature of Loss Function Around Samples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=31IOmrnoP4": {
    "title": "Repelling Random Walks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qnWtw3l0jb": {
    "title": "Fast Imitation via Behavior Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bozbTTWcaw": {
    "title": "Stabilizing Backpropagation Through Time to Learn Complex Physics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWwvC7As4S": {
    "title": "Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKE7YpUubu": {
    "title": "RACH-Space: Reconstructing Adaptive Convex Hull Space with applications in weak supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFyTZEqmUY": {
    "title": "Learning Interactive Real-World Simulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mJgymwRsWw": {
    "title": "Active Probabilistic Drug Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IzkgLgHt5Z": {
    "title": "Optimization and Generalizability: Fair Benchmarking for Stochastic Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vEfmVS5ywF": {
    "title": "Learning in reverse causal strategic environments with ramifications on two sided markets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C3msSjudA7": {
    "title": "ViFu: Visible Part Fusion for Multiple Scene Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPNwsJgnZJ": {
    "title": "Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VP20ZB6DHL": {
    "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJBGSVSTT2": {
    "title": "Backdoor Federated Learning by Poisoning Backdoor-Critical Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHIKtKzTj7": {
    "title": "PAPM: A Physics-aware Proxy Model for Process Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uIv5SaxXLv": {
    "title": "NeuralQP: A General Hypergraph-based Optimization Framework for Large-scale Quadratically Constrained Quadratic Programs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zSxpnKh1yS": {
    "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VkWbxFrCC8": {
    "title": "RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSBP7HzA5Z": {
    "title": "Inductive Transformers: How Large Language Models Form Concepts, And How to Make Them Even Better At It",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fk5IzauJ7F": {
    "title": "Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhwDw31DGI": {
    "title": "MIPGen: Learning to Generate Scalable MIP Instances",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G536mmC2HL": {
    "title": "TorSeq: Torsion Sequential Modeling for Molecular 3D Conformation Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=96nX9xIIx2": {
    "title": "Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2oWRumm67L": {
    "title": "Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Small-scale Optimizer and Small Training Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDYcMtLHEr": {
    "title": "Emergent Robust Communication for Multi-Round Interactions in Noisy Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q1u25ahSuy": {
    "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iip7rt9UL3": {
    "title": "Lightweight, Pre-trained Transformers for Remote Sensing Timeseries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tvhaxkMKAn": {
    "title": "Towards Understanding Sycophancy in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GztevK7jDh": {
    "title": "Constructing Informative Subtask Representations for Multi-Agent Coordination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nfqABOIwQ": {
    "title": "RIME: Robust Preference-based Reinforcement Learning with Noisy Human Preferences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ufp0DVjRs0": {
    "title": "Feature Accentuation: Explaining 'what' features respond to in natural images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1RrOtCmuKr": {
    "title": "Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W44kiwovtC": {
    "title": "FastDCFlow: Fast and Diverse Counterfactual Explanations Using Normalizing Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jWxrIeWgir": {
    "title": "HOSC: Hyperbolic Oscillating Periodic Activations for Sharp Feature Preservation in Implicit Neural Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktdETU9JBg": {
    "title": "Towards image compression with perfect realism at ultra-low bitrates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JO7k0SJ5V6": {
    "title": "Scaling Laws of RoPE-based Extrapolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLdVl0q68X": {
    "title": "NuwaDynamics: Discovering and Updating in Causal Spatio-Temporal Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=94FKDbtTqO": {
    "title": "Rethinking the bert-like pretraining for dna sequences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6aGz0Zyvn": {
    "title": "Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=epZV1nykll": {
    "title": "Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqeRtP2T3R": {
    "title": "Embracing Diversity: Zero-shot Classification Beyond a Single Vector per Class",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vTgpSLVtyj": {
    "title": "On the Verification Complexity of Deterministic Nonsmooth Nonconvex Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDkpxG94sF": {
    "title": "V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBdE9r5XZV": {
    "title": "Factored-NeuS: Reconstructing Surfaces, Illumination, and Materials of Possibly Glossy Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riNuqYiD66": {
    "title": "A Branching Decoder for Set Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IlQxeKrWDt": {
    "title": "Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ueqTjOcuLc": {
    "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KAk6ngZ09F": {
    "title": "Data Filtering Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vDJ4tzczlG": {
    "title": "Fair Text-to-Image Diffusion via Fair Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TwBY17Hgiy": {
    "title": "Multi-task Learning with 3D-Aware Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R3CDj2DLln": {
    "title": "Disentangled Acoustic Fields For Multimodal Physical Scene Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NG7sS51zVF": {
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1M0qIxVKf6": {
    "title": "Uncovering hidden geometry in Transformers via disentangling position and context",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W7kxHxjeVm": {
    "title": "ImAD: An End-to-End Method for Unsupervised Anomaly Detection in the Presence of Missing Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZOQ9RKYJu": {
    "title": "OWL: A Large Language Model for IT Operations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPvK2e8o8M": {
    "title": "Teach Large Language Models the Concept of Meta-cognition to Reduce Hallucination Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J0cCuE3JRC": {
    "title": "Bag of Features: New Baselines for GNNs for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfFpK0JAsQ": {
    "title": "Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGk0ur4Tfr": {
    "title": "Retrieval-Based Video Language Model for Efficient Long Video Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5u7V2zD7K": {
    "title": "Temporal Spiking Generative Adversarial Networks for Heading Direction Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ARfhjGfdF": {
    "title": "Towards Control-Centric Representations in Reinforcement Learning from Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nNZzt54ZmU": {
    "title": "Rethink Depth Separation with Intra-layer Links",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZULjcYLWKe": {
    "title": "DMBP: Diffusion model based predictor for robust offline reinforcement learning against state observation perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdkGRV1vcf": {
    "title": "Generative Sliced MMD Flows with Riesz Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7J0NsFXnFd": {
    "title": "Optimal Action Abstraction for Imperfect Information Extensive-Form Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shgx0eqdw6": {
    "title": "Alignment as Reward-Guided Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiiAzqi6Ol": {
    "title": "Compositional Preference Models for Aligning LMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hESD2NJFg8": {
    "title": "Label-free Node Classification on Graphs with Large Language Models (LLMs)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IRtIHp7vsM": {
    "title": "AutoM3L: Automated Multimodal Machine Learning with Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ylhiMfpqkm": {
    "title": "Pre-Training and Fine-Tuning Generative Flow Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=StYc4hQAEi": {
    "title": "Sliced Wasserstein Estimation with Control Variates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=42lcaojZug": {
    "title": "Neural Rate Control for Learned Video Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kHTHf1XrFt": {
    "title": "MultiReAct: Multimodal Tools Augmented Reasoning-Acting Traces for Embodied Agent Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ub7YxVUQhQ": {
    "title": "Vision-based Discovery of Nonlinear Dynamics for 3D Moving Target",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHdf9F1tY4": {
    "title": "DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=am9IxubLKV": {
    "title": "Convolutions Through the Lens of Tensor Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aZH1dM3GOX": {
    "title": "Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=saj54kqrBj": {
    "title": "Self-Tuning Self-Supervised Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Th9VeE7flR": {
    "title": "Enhanced Label Propagation through Affinity Matrix Fusion for Source-Free Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kklwv4c4dI": {
    "title": "Local Composite Saddle Point Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YLJs4mKJCF": {
    "title": "Towards Poisoning Fair Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZO5cn4IfaN": {
    "title": "Efficient Distributed Training with Full Communication-Computation Overlap",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4JtwtT4nYC": {
    "title": "Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3yJ54gv3H": {
    "title": "Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFMS6wF2xq": {
    "title": "Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1op5YGZu8X": {
    "title": "Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNPcOaqC5r": {
    "title": "What's in a Prior? Learned Proximal Networks for Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkmJotfL42": {
    "title": "Fantastic Generalization Measures are Nowhere to be Found",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28gMnEAgl9": {
    "title": "Large Language Models Are Not Strong Abstract Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMoNrajk2X": {
    "title": "CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W6xD7K1ajR": {
    "title": "Efficient Precision and Recall Metrics for Assessing Generative Models using Hubness-aware Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FOlhvs5Wh4": {
    "title": "Slightly Harmonizing Certified Robust Radius and Accuracy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfTsvy05MX": {
    "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QKqWnNkwPL": {
    "title": "Self-distillation for diffusion models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOoKI3ouv1": {
    "title": "Robust agents learn causal world models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cfL8zApofK": {
    "title": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K2c04ulKXn": {
    "title": "Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CjPt1AC6w0": {
    "title": "IS SYNTHETIC DATA USEFUL FOR TRANSFER LEARNING? AN INVESTIGATION INTO DATA GENERATION, VOLUME, AND UTILIZATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJ5CF1aOOX": {
    "title": "A Self-Supervised Pre-Training Model for Time Series Classification based on Data Pre-Processing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1armpjgh8L": {
    "title": "Adaptive Hierarchical Certification for Semantic Segmentation using Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BeuTCoe3bf": {
    "title": "Subgraph-To-Node Translation for Efficient Representation Learning of Subgraphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RC2h1WQvPo": {
    "title": "Interpretable Latent Distributions Using Space-Filling Curves",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tFpqGk5hR5": {
    "title": "A Simple Open-Loop Baseline for Reinforcement Learning Locomotion Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUGhVYPVRM": {
    "title": "Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EMCXCTsmSx": {
    "title": "IRGen: Generative Modeling for Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rUf9G9k2im": {
    "title": "Image Inpainting via Iteratively Decoupled Probabilistic Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmFtICfcd8": {
    "title": "Regularized KL-Divergence for well-defined function space variational inference in BNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dhLIno8FmH": {
    "title": "Decoding Natural Images from EEG for Object Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5BXAXOpaWu": {
    "title": "Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NIouO0C0ex": {
    "title": "Open-Source Can Be Dangerous: On the Vulnerability of Value Alignment in Open-Source LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iNtEAeVQE0": {
    "title": "DISK: Domain Inference for Discovering Spurious Correlation with KL-Divergence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PXEY7ofFX": {
    "title": "Bespoke Solvers for Generative Flow Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSAgvYhZzg": {
    "title": "You Only Look at Screens: Multimodal Chain-of-Action Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wdteczB4mQ": {
    "title": "Learning to Compute Gröbner Bases",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rlCyHDzOjj": {
    "title": "A New Tensor Network: Tubal Tensor Train Network and its Applications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49z97Y9lMq": {
    "title": "LCOT: Linear Circular Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tZ3JmSDbJM": {
    "title": "GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NZ5KXXDv1T": {
    "title": "Reinforcement Learning based Image Generation via Visual Consensus Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNktD3ib16": {
    "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7BwUyXz1f": {
    "title": "Catastrophic Negative Transfer: An Overlooked Problem in Continual Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIxhINXtQk": {
    "title": "InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=coIaBY8EVF": {
    "title": "Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nxy1bQWTG": {
    "title": "DiffEnc: Variational Diffusion with a Learned Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YCPDFfmkFr": {
    "title": "Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmzFZ9lJrD": {
    "title": "Boolformer: Symbolic Regression of Logic Functions with Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tf6nR1B8Nt": {
    "title": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzoHLiGVMo": {
    "title": "ODEFormer: Symbolic Regression of Dynamical Systems with Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8g26Yv1EOu": {
    "title": "Amortized Network Intervention to Steer the Excitatory Point Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAXmvOLtjA": {
    "title": "Diffusion World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zlkXLb3wpF": {
    "title": "Fast and unified path gradient estimators for normalizing flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SerYSFntLh": {
    "title": "Multimodal Variational Disentangled Knowledge Alignment for Cross-domain Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8rhHI6C8iC": {
    "title": "All for One and One for All: A Collaborative FL Framework for Generic Federated Learning with Personalized Plug-ins",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESt7ECoWpn": {
    "title": "Differentially Pivate Per-Instance Additive Noise Mechanism: A Game Theoretic Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3UayAQWoE": {
    "title": "On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lzt60v45V4": {
    "title": "Variational Federated Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jRpD8VfGRf": {
    "title": "Multi-interest Disentangled Representation Learning for Multimodal Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhJDD85QHD": {
    "title": "CEIR: Concept-based Explainable Image Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gC6JTEU3jl": {
    "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=twSnZwiOIm": {
    "title": "Learning invariant representations of time-homogeneous stochastic dynamical systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nh4vQ1tGCt": {
    "title": "Forgedit: Text Guided Image Editing via Learning and Forgetting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=468KWV14ll": {
    "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oNkYPgnfHt": {
    "title": "Learning to Intervene on Concept Bottlenecks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ToHMTetlGr": {
    "title": "Noises are Transferable - An Empirical Study on Heterogeneous Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKinRUoBN9": {
    "title": "Investigating the Impact of Data Distribution Shifts on Cross-Modal Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUhCObWGw5": {
    "title": "PATHS: Parameter-wise Adaptive Two-Stage Training Harnessing Scene Transition Mask Adapters for Video Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4Bim1dDzb": {
    "title": "Fast Inverse Rendering by Unified Voxelization of Scene Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8utTlmhw8v": {
    "title": "Learning Nash equilibria in Rank-1 games: Going beyond the Minty Property",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k2a2aPOA4b": {
    "title": "Towards Realistic Unsupervised Fine-tuning with Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lqo5Jwfnq": {
    "title": "Class-Incremental Learning with Parameter-Efficient Cross-Task Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHtNW6xqCd": {
    "title": "Incorporating Implicit Regularization to Enhance the Transition Matrix Method for Effective Handling of Diverse Label Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EjIKerYk1O": {
    "title": "Enhancing Airside Monitoring: Multi-view Approach for Accurate Aircraft Distance-To-Touchdown Estimation in Digital Towers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WnEnU2K3Rb": {
    "title": "Beyond the Benchmark: Detecting Diverse Anomalies in Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=voVjW1PT2c": {
    "title": "Guaranteed Out-Of-Distribution Detection with Diverse Auxiliary Set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZEgj0clDp": {
    "title": "Revisiting Knowledge Tracing: A Simple and Powerful Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBZd6mCWXd": {
    "title": "WI3D: Weakly Incremental 3D Detection via Visual Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7XPZnIUHh": {
    "title": "Adversarial Learning of Decomposed Representations for Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GxmltrqVNn": {
    "title": "Gated Attention Bins for Depth Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gSlLbfSOq1": {
    "title": "Temporally Equivariant Contrastive Learning for Disease Progression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsvlvWB9vz": {
    "title": "EControl: Fast Distributed Optimization with Compression and Error Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLXpXa7iiz": {
    "title": "Convergence of Bayesian Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQ6RgKYiQq": {
    "title": "MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cG2BAbFnA4": {
    "title": "Learning with Complementary Labels Revisited: A Consistent Approach via Negative-Unlabeled Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cWdAYDLmPa": {
    "title": "State Representation Learning Using an Unbalanced Atlas",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dl0u4ODCuW": {
    "title": "Retro-fallback: retrosynthetic planning in an uncertain world",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NJyCoAIPln": {
    "title": "Branch-level Network Re-parameterization with Neural Substitution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=POFrdKvpea": {
    "title": "ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0d1gQI114C": {
    "title": "LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v5BcZzkAXg": {
    "title": "Multi-label Learning with Random Circular Vectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yrgQdA5NkI": {
    "title": "Equivariant Matrix Function Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aE6HazMgRz": {
    "title": "Spatio-temporal Twins with A Cache for Modeling Long-term System Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w327zcRpYn": {
    "title": "SUBER: An RL Environment with Simulated Human Behavior for Recommender Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nZP10evtkV": {
    "title": "Optimal transport based adversarial patch to leverage large scale attack transferability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CfJp9NG6Q": {
    "title": "STUDY: Socially Aware Temporally Causal Decoder Recommender Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bj3jYirM37": {
    "title": "Can Agent Learn Robust Locomotion Skills without Modeling Environmental Observation Noise?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K804zYw6Wc": {
    "title": "NIR-Assisted Image Denoising: A Selective Fusion Approach and A Real-World Benchmark Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkEsEFFUyo": {
    "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GW4j4n2cjH": {
    "title": "Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICSvW69W5K": {
    "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IaKxCsJSOO": {
    "title": "Expressive Modeling is Insufficient for Offline RL: A Tractable Inference Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xxI4nAj7zi": {
    "title": "Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JfKCZQPxJ": {
    "title": "STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IhWtRwIbos": {
    "title": "Discovering Environments with XRM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kXNJ48Hvw1": {
    "title": "Accelerated Sampling with Stacked Restricted Boltzmann Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtlcdjE1K3": {
    "title": "DECOUPLE QUANTIZATION STEP AND OUTLIER-MIGRATED RECONSTRUCTION FOR PTQ",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IZVCzCWwoY": {
    "title": "Completion Consistency for Point Cloud Completion Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STxmh1ZLOI": {
    "title": "RTMPose: Real-Time Models for Multi-Person Pose Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSIVHTbZBR": {
    "title": "Image Inpainting via Tractable Steering of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=56jIlazr6a": {
    "title": "Unified Uncertainty Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WvFoJccpo8": {
    "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jzvWwv4gMx": {
    "title": "On the Paradox of Generalizable Logical Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OBITU0NAKl": {
    "title": "How Graph Neural Networks Learn: Lessons from Training Dynamics in Function Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ba84RDHFnz": {
    "title": "R-MAE: Regions Meet Masked Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qae04YACHs": {
    "title": "Transformer-Modulated Diffusion Models for Probabilistic Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOJyfhWYHf": {
    "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tQYsKBTTaV": {
    "title": "LATEC — A benchmark for large-scale attribution & attention evaluation in computer vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPIA7bgd5y": {
    "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qiOqgphnVL": {
    "title": "Democratized Diffusion Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z80CwkWXmq": {
    "title": "GETMusic: Generating Music Tracks with a Unified Representation and Diffusion Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JkMAlN3YcI": {
    "title": "How Temporal Unrolling Supports Neural Physics Simulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oimPWHTg65": {
    "title": "TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VjAjZWJ7Fr": {
    "title": "A Graph-Theoretic Framework for Joint OOD Generalization and Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dTpbEdN9kr": {
    "title": "Human Motion Diffusion as a Generative Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjf3YenThE": {
    "title": "New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruQkcBfzpm": {
    "title": "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oj4KUNoKnf": {
    "title": "Knowledge Distillation for Closed-Source Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUuKFW7DIF": {
    "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q7XxKp2rHs": {
    "title": "SMAFace: Sample Mining Guided Adaptive Loss for Face Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYdk0V880P": {
    "title": "Fast Neural Architecture Search with Random Neural Tangent Kernel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nZP6NgD3QY": {
    "title": "AdaMerging: Adaptive Model Merging for Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jjiOHEcS2c": {
    "title": "Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0c2qtalgG": {
    "title": "MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zsfiqpft6K": {
    "title": "Diffusion Model for Dense Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yb5KvPkKQg": {
    "title": "Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UjYeectI4p": {
    "title": "Evaluating graph generative models with graph kernels: what structural characteristics are captured?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A5nLEfjhJW": {
    "title": "SHARCS: SHARed Concept Space for\\\\Explainable Multimodal Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cMIUwcEEVw": {
    "title": "RAVL: Reach-Aware Value Learning for the Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPdZLlNXSm": {
    "title": "Mean Field Theory in Deep Metric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KUiMKRebi": {
    "title": "Implicit Neural Representation Inference for Low-Dimensional Bayesian Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4bSQ3lsfEV": {
    "title": "Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U5BZcr0H7r": {
    "title": "Multi-Armed Bandits with Abstention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bzO7cusxBl": {
    "title": "Cross-domain Few-shot Classification via Maximization Optimized Kernel Dependence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AXC9KydyZq": {
    "title": "M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3LOFL4jGp": {
    "title": "MacDC: Masking-augmented Collaborative Domain Congregation for Multi-target Domain Adaptation in Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y33lDRBgWI": {
    "title": "AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nLWiR5P3wr": {
    "title": "Input-gradient space particle inference for neural network ensembles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d4UiXAHN2W": {
    "title": "LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UbxWjq0UO2": {
    "title": "Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2pvECsmld3": {
    "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xqxG5WogN6": {
    "title": "Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yIKjkRZBrX": {
    "title": "Learning variable-length skills through Novelty-based Decision Point Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBj2Qdhgew": {
    "title": "Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qH9nrMNTIW": {
    "title": "Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VAwgL8kPvr": {
    "title": "Structural Pruning of Large Language Models via Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNkOx3IVWI": {
    "title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cNwugejbW6": {
    "title": "SoftHash: High-dimensional Hashing with A Soft Winner-Take-All Mechanism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n3z5oALWci": {
    "title": "ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlpPflNCKV": {
    "title": "Video2StyleGAN: Disentangling Local and Global Variations in a Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gp5dPMBzMH": {
    "title": "BELT-2: Bootstrapping EEG-to-Language representation alignment for multi-task brain decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cP2W2PJtBj": {
    "title": "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnVO1PvbTv": {
    "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K7KQkiHanD": {
    "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XqGsMD6KH5": {
    "title": "MultiHot Embedding: A Multiple Activation Embedding Model for Numerical Features in Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FJOKjooIj": {
    "title": "Self-supervised Heterogeneous Graph Learning: a Homogeneity and Heterogeneity Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ISRqgtjPc": {
    "title": "CoBIT: A Contrastive Bi-directional Image-Text Generation Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDfxOMJqgL": {
    "title": "CAST: Cluster-Aware Self-Training for Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=otHZ8JAIgh": {
    "title": "Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhUinwpVSQ": {
    "title": "Policy Disentangled Variational Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OHpvivXrQr": {
    "title": "Protein Multimer Structure Prediction via PPI-guided Prompt Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yen1lGns2o": {
    "title": "Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rPyHyjfwP": {
    "title": "Domain-Agnostic Molecular Generation with Self-feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8xliOUg9EW": {
    "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=exKHibougU": {
    "title": "LLM-grounded Video Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByAhXwV4bH": {
    "title": "Adversarial Data Robustness via Implicit Neural Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gusHSc09zj": {
    "title": "Discovering Mixtures of Structural Causal Models from Time Series Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dp27P5HBBt": {
    "title": "Periodicity Decoupling Framework for Long-term Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPJTQYOpNI": {
    "title": "Imitation Learning from Observation with Automatic Discount Scheduling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjzwioGLux": {
    "title": "ROBUST SPARSE AND DENSE MATCHING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1p3uQ8pzl": {
    "title": "Explore Outworld Knowledge in Large Language Models: A Case Study in Pokemon Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBNTeQ7dPP": {
    "title": "Reinforcement Learning for Control with Stability Guarantee",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a2ljjXeDcE": {
    "title": "iGraphMix: Input Graph Mixup Method for Node Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T7YV5UZKBc": {
    "title": "Neural Fine-Tuning Search for Few-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pxOUk9OHYP": {
    "title": "CutSharp: A Simple Data Augmentation Method for Learned Image Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mhgm0IXtHw": {
    "title": "Noise Map Guidance: Inversion with Spatial Context for Real Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cH3oufN8Pl": {
    "title": "Output-Domain Focused Inductive Bias on Latent Feature Clusters in Visual Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ii8idH4tH": {
    "title": "Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xsd2llWYSA": {
    "title": "GLD: Generative Latent Dynamics for Structured Motion Representation and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqjEhvUC6H": {
    "title": "Data De-Duplication and Semantic Enhancement for Contrastive Language-Image Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0gTW5JUFTW": {
    "title": "TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tw9wemV6cb": {
    "title": "Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4sGoA7Eih8": {
    "title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Gzkhoc6YS": {
    "title": "Personalize Segment Anything Model with One Shot",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PR6RMsxuW7": {
    "title": "Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QeemQCJAdQ": {
    "title": "Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QWgUAx7nIi": {
    "title": "Contrastive Graph Autoencoder for Geometric Polygon Retrieval from Building Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmOXAa5H5Y": {
    "title": "An Empirical Study of Simplicial Representation Learning with Wasserstein Distance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZxU0q2S11": {
    "title": "Data geometry and topology dependent bounds on network widths in deep ReLU networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Z1gxuAQrA": {
    "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dteBEZiCxB": {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UrmnIDCzLA": {
    "title": "Overcoming bias towards base sessions in few-shot class-incremental learning (FSCIL)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ziDFH8TPPK": {
    "title": "Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ADSxCpCu9s": {
    "title": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O072Rc8uUy": {
    "title": "Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFR0GrzERG": {
    "title": "On Task Description of In-context Learning: A Study from Information Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bUGagbBGaY": {
    "title": "Momentum-accelerated Diffusion Process for Faster Training and Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wxJ0eXwwda": {
    "title": "Urial: Aligning Untuned LLMs with Just the 'Write' Amount of In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AfiM6F2YPY": {
    "title": "Applying language models to algebraic topology: generating simplicial cycles using multi-labeling in Wu's formula",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhZXk5Ctz2": {
    "title": "Rethinking RGB Color Representation for Image Restoration Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DNvzCsQG1D": {
    "title": "InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DgRdeJF0k7": {
    "title": "Masked Dual-Temporal Autoencoders for Semi-Supervised Time-Series Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hf17y6u9BC": {
    "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itKMOWSP6K": {
    "title": "FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal Consistent Transformer for 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2UlfvGU6rL": {
    "title": "Equivariant Graph Neural Operator for Modeling 3D Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLregLuXpn": {
    "title": "On the Analysis of GAN-based Image-to-Image Translation with Gaussian Noise Injection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1Gd2d1WXY": {
    "title": "Adaptive Resolution Residual Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwHAzbkk5z": {
    "title": "Swift Sampler: Efficient Learning of Sampler by 10 parameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kl9CqKf7h6": {
    "title": "FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fg772k6x6U": {
    "title": "Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3a505tMjGE": {
    "title": "AVOID: Alleviating VAE's Overestimation in Unsupervised OOD Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wa6Ebn4AYL": {
    "title": "Big Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPb5AhT2Vf": {
    "title": "FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nLxH6a6Afe": {
    "title": "CITING: Large Language Models Create Curriculum for Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUGjLB0N4l": {
    "title": "Big Learning Variational Auto-Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9mX0AZVEet": {
    "title": "Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KksPo0zXId": {
    "title": "A Fast Framework for Post-training Structured Pruning Without Retraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3bq3jsvcQ1": {
    "title": "Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Di7xKawV7x": {
    "title": "End-to-End Neural Network Compression via $\\frac{\\ell_1}{\\ell_2}$ Regularized Latency Surrogates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OuV9ZrkQlc": {
    "title": "ImagenHub: Standardizing the evaluation of conditional image generation models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKOjkFrhSs": {
    "title": "Prompt-Guided Dynamic Network for Image Super Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCocsAF7MY": {
    "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=014CgNPAGy": {
    "title": "On the Role of Momentum in the Implicit Bias of Gradient Descent for Diagonal Linear Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H7z1gHsaZ0": {
    "title": "Staleness-based subgraph sampling for large-scale GNNs training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GEZACBPDn7": {
    "title": "KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bLKcCe7hYh": {
    "title": "UC-NERF: Neural Radiance Field for under-calibrated multi-view cameras",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y886UXPEZ0": {
    "title": "Adapting Large Language Models via Reading Comprehension",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kCnLHHtk1y": {
    "title": "Building a Special Representation for the Chinese Ancient Buildings in Diffusion models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQXX3ZV2HE": {
    "title": "Adversarial Instance Attacks for Interactions between Human and Object",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=91DFSjAva8": {
    "title": "SERA: Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f8S3aLm0Vp": {
    "title": "DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdvI91pInB": {
    "title": "Discovering Logic-Informed Intrinsic Rewards to Explain Human Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbu1lIYQ19": {
    "title": "Hybrid Kernel Stein Variational Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8iTpB4RNvP": {
    "title": "Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Vw7DQqq7U": {
    "title": "LEMON: Lossless model expansion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dumkzmqTmS": {
    "title": "FUND-RELATED GRAPH REPRESENTATION FOR MARGINAL EFFECTIVENESS IN MULTI-FACTORS QUANTITATIVE STRATEGY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkDon7WpX1": {
    "title": "A Consistent Lebesgue Measure for Multi-label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ft0hSoZTVe": {
    "title": "YOLOR-Based Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qud5pDnpzo": {
    "title": "ViP: A Differentially Private Foundation Model for Computer Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hlj6HiGJeB": {
    "title": "NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix Operations for Efficient Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Js5PJPHDyY": {
    "title": "A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5h0qf7IBZZ": {
    "title": "MiniLLM: Knowledge Distillation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yhrWPZs6On": {
    "title": "HYBRID GRANULARITY DISTRIBUTION ESTIMATION FOR FEW-SHOT LEARNING: STATISTICS TRANSFER FROM CATEGORIES AND INSTANCES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUkcfqa6GX": {
    "title": "Spatio-Temporal Graph Learning with Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqzknNJShg": {
    "title": "FourierAugment: Frequency-Based Image Encoding for Resource-Constrained Vision Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vja3ecieXY": {
    "title": "Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlTDBZFXWp": {
    "title": "The importance of feature preprocessing for differentially private linear optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pCbCcXLzSz": {
    "title": "Maximizing Benefits under Harm Constraints: A Generalized Linear Contextual Bandit Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c1QBcYLd7f": {
    "title": "Deep graph kernel point processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vCnDyQkjg": {
    "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G32oY4Vnm8": {
    "title": "PTaRL: Prototype-based Tabular Representation Learning via Space Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lJkOCMP2aW": {
    "title": "Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eddd0YTCiq": {
    "title": "Graph-level Representation Learning with Joint-Embedding Predictive Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sdev4EomdC": {
    "title": "Bridging the gap between offline and online continual learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gYa9R2Pmp8": {
    "title": "Jailbreaking Language Models at Scale via Persona Modulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IZbLMHbOQg": {
    "title": "Diffusion Models for Imperceptible and Transferable Adversarial Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bTMMNT7IdW": {
    "title": "Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vw24wtSddM": {
    "title": "Tree Cross Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPAhClBy8F": {
    "title": "Overcoming both Domain Shift and Label Shift for Referring Video Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28kAFnQZ5V": {
    "title": "TENSORIZED ATTENTION MODEL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gLARhFLE0F": {
    "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pVaMBfI2eR": {
    "title": "Dual Prompt Tuning for Domain-Aware Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J8FGCqT314": {
    "title": "D2T2: Decision Transformer with Temporal Difference via Steering Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XmkuQfWZAB": {
    "title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kIZ3S3tel6": {
    "title": "Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Po6lYYsrB4": {
    "title": "ALP: Action-Aware Embodied Learning for Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbQH9AOVd5": {
    "title": "Stable Anisotropic Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bTjokqYl5B": {
    "title": "On the Onset of Robust Overfitting in Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VIgNuQ1pY": {
    "title": "Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vE5MyzpP92": {
    "title": "Threshold-Consistent Margin Loss for Open-World Deep Metric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5jcav5RcKw": {
    "title": "Jointly Training Large Autoregressive Multimodal Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=srsXnKPx5T": {
    "title": "Hexa: Self-Improving for Knowledge Augmented Dialogue System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pDCublKPmG": {
    "title": "Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2V1Z0Jdmss": {
    "title": "On the Over-Memorization During Natural, Robust and Catastrophic Overfitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJTPyCZmbj": {
    "title": "PagFormer: Polar Accumulator Grid Integrated into Transformers for Medical Image Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CF8H8MS5P8": {
    "title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A2mRcRyGdl": {
    "title": "Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ur2xmuw7w": {
    "title": "Revisiting Link Prediction: a data perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fq8tKtjACC": {
    "title": "Textbooks Are All You Need",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OWUWWr50PF": {
    "title": "Deterministic Error Bounds for Euclidean Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tFYcEUlUTt": {
    "title": "Learning from the Future: Improve Long-term Mesh-based Simulation with Foresight",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2HJRwwbV3G": {
    "title": "What does the Knowledge Neuron Thesis Have to do with Knowledge?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ac7f7xL4bU": {
    "title": "Universal Clustering Bounds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JKZZxJAZ3": {
    "title": "Nonnegative Matrix Factorization through Canonical Edges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6sfRRcynDy": {
    "title": "Out-of-Distribution Detection with Hyperspherical Energy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UbOzNf6hGq": {
    "title": "FiLM: Fill-in Language Models for Any-Order Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4L0xnS4GQM": {
    "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86w3LbTNI1": {
    "title": "Preventing Reward Hacking with Occupancy Measure Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HipfLjyLUW": {
    "title": "Hierarchical GFlownet for Crystal Structure Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvj1mn8q8D": {
    "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nk8HrBad2O": {
    "title": "Task-Guided Biased Diffusion Models for Point Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FKksTayvGo": {
    "title": "Denoising Diffusion Bridge Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tSLtvkHCh": {
    "title": "Learning Temporal Causal Representation under Non-Invertible Generation Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6BIRu23ow": {
    "title": "TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SdeAPV1irk": {
    "title": "Incremental Randomized Smoothing Certification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vzvCaYFTLq": {
    "title": "Sapling: $\\underline{S}$uccessive $\\underline{A}$daptation and Com$\\underline{p}$ression with $\\underline{L}$ayer Dropp$\\underline{ing}$ for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89A5c6enfc": {
    "title": "Local Graph Clustering with Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICDJDL5lmQ": {
    "title": "Wasserstein Distortion: Unifying fidelity and realism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SO93f7sVf": {
    "title": "Training Neural Networks from Scratch with Parallel Low-Rank Adapters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ANJxbH4eQQ": {
    "title": "Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6J3ehSUrMU": {
    "title": "Principled Federated Domain Adaptation: Gradient Projection and Auto-Weighting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZqic2sPTY": {
    "title": "Graphpulse: Topological representations for temporal graph property prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mFTPRV5hYw": {
    "title": "Where have you been? A Study of Privacy Risk for Point-of-Interest Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbniI5RyWH": {
    "title": "SEESAW: Do Graph Neural Networks Improve Node Representation Learning for All?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xZDWO0oejD": {
    "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ay0Vjj3oyL": {
    "title": "SCOT: Improved Temporal Counterfactual Estimation with Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1uHTIjXjkk": {
    "title": "Potential Based Diffusion Motion Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6BXvqWWmq": {
    "title": "MOTSC: Model-based Offline Traffic Signal Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mdk7YP52V3": {
    "title": "Understanding Pathologies of Deep Heteroskedastic Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jb37oaGZXy": {
    "title": "Musketeer: Joint Training/Inference for Multi-task Vision-Language Model with Task Explanation Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gCZyD7WD0w": {
    "title": "Guided Decoupled Exploration for Offline Reinforcement Learning Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aSbJhaVDi": {
    "title": "Exploiting Open-World Data for Adaptive Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqHvfEUxGu": {
    "title": "A Collaborative Perspective on Exploration in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DqziS8DG4M": {
    "title": "Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vA5Rs9mu97": {
    "title": "Compressed Online Sinkhorn",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ceNnsnA5gu": {
    "title": "WL-Tree: a New Tool for Analyzing Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QrEHs9w5UF": {
    "title": "PRIME: Prioritizing Interpretability in Failure Mode Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=92KV9xAMhF": {
    "title": "On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u2as4lHoyl": {
    "title": "ReFACT: Updating Text-to-Image Models by Editing the Text Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKTUlxo5zy": {
    "title": "Less is More: Fewer Interpretable Region via Submodular Subset Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EanCFCwAjM": {
    "title": "Cameras as Rays: Sparse-view Pose Estimation via Ray Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PuKRVPXXpR": {
    "title": "ResTran: A GNN Alternative To Learn Graph With Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qB0IV2DpeS": {
    "title": "Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously: Just Clip Gradient Differences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mNFsbvo2P": {
    "title": "Domain constraints improve risk prediction when outcome data is missing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZZ4hhniJU": {
    "title": "Learning Multi-Agent Communication with Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dc4rXq3HIA": {
    "title": "Improving Out-of-Domain Generalization with Domain Relations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=50P9TDPEsh": {
    "title": "Critique Ability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OatZMyMuIo": {
    "title": "Causal Representation Learning and Inference for Generalizable Cross-Domain Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7m5jhNXklB": {
    "title": "VTruST : Controllable value function based subset selection for Data-Centric Trustworthy AI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qg5JENs0N4": {
    "title": "Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ju0VmvMCW": {
    "title": "Sample Relationship from Learning Dynamics Matters for Generalisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xvfz8NHmCj": {
    "title": "Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0fpLLsAynh": {
    "title": "Sporadicity in Decentralized Federated Learning: Theory and Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rcFXg2aqEj": {
    "title": "LMDX: Language Model-based Document Information Extraction and Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f37TVPH62h": {
    "title": "Compound Returns Reduce Variance in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fZmdNb5ThL": {
    "title": "ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8TAGx549Ns": {
    "title": "REX: Rapid Exploration and eXploitation for AI agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlkN11Xj6J": {
    "title": "Adding 3D Geometry Control to Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUKps5dL4s": {
    "title": "Momentum Particle Maximum Likelihood",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3NMYMLL92j": {
    "title": "Brain encoding models based on binding multiple modalities across audio, language, and vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U3ROVRTKTa": {
    "title": "Prompting-based Efficient Temporal Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sa0t0vGPDv": {
    "title": "FARS: FSM-Augmentation to Make LLMs Hallucinate the Right APIs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U96nHn0dmK": {
    "title": "Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Subnetworks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nfMyERXNru": {
    "title": "Video Decomposition Prior: Editing Videos Layer by Layer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lqwk6tNVEi": {
    "title": "Neighborhood-Informed Diffusion Model for Source-Free Domain Adaptation: Retrieving Source Ground Truth from Target Query's Neighbors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CF6gfZSCVg": {
    "title": "Anarchic Federated Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMaQvkMzDi": {
    "title": "Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CgPs04l9TO": {
    "title": "Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uznKlCpWjV": {
    "title": "On Stationary Point Convergence of PPO-Clip",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTsHStJKcm": {
    "title": "Make a Donut: Language-Guided Hierarchical EMD-Space Planning for Zero-shot Deformable Object Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BV1PHbTJzd": {
    "title": "Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8cNMMrWRbZ": {
    "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ucMRo9IIC1": {
    "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHjigVnI04": {
    "title": "High-dimensional SGD aligns with emerging outlier eigenspaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3LLkES6nNs": {
    "title": "Infinitely Deep Residual Networks: Unveiling Wide Neural ODEs as Gaussian Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HwQ8NVvdmm": {
    "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fnBYPL5Ged": {
    "title": "CPLLM: Clinical Prediction with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UQNSe4Da29": {
    "title": "One-Hot Encoding Strikes Back: Fully Orthogonal Coordinate-Aligned Class Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcNwnuWMe0": {
    "title": "Exploiting River Network Topology for Flood Forecasting with Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwJgCIKXJS": {
    "title": "Linear Indexed Minimum Empirical Divergence Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zsfrzYWoOP": {
    "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yXklOV0ZIv": {
    "title": "Learnable Counterfactual Attention for Singer Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcl6WeMARK": {
    "title": "Improved Regret Bounds in Stochastic Contextual Bandits with Graph Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3JoQqW35GQ": {
    "title": "Training-free Linear Image Inversion via Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLf589bx1f": {
    "title": "$\\mathcal{B}$-Coder: On Value-Based Deep Reinforcement Learning for Program Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vR5h3cAfXS": {
    "title": "On robust overfitting: adversarial training induced distribution matters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uikf2Ue0XQ": {
    "title": "Visual Grounding with attention-driven constraint balancing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fnYvczj0OU": {
    "title": "Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzT61ziSCu": {
    "title": "Automatic Functional Differentiation in JAX",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=msXxrttLOi": {
    "title": "FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wT8G45QGdV": {
    "title": "Consistent123: Improve Consistency for One Image to 3D Object Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xuKVVYxU5D": {
    "title": "Single-Trajectory Distributionally Robust Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auKAUJZMO6": {
    "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qIRkFyLZnR": {
    "title": "Robustify the Latent Space: Offline Distributionally Robust Reinforcement Learning with Linear Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x1ptaXpOYa": {
    "title": "ADoPD: A Large-Scale Document Page Decomposition Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BgzE4zwkFW": {
    "title": "Curriculum Reinforcement Learning via Morphology-Environment Co-Evolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29pGC6IYaL": {
    "title": "Maximizing LLMs Potential: Enhancing Mongolian Chinese Machine Translation with RL Agents and Adversarial Multi Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9x6yrFAPnx": {
    "title": "Provably Efficient CVaR RL in Low-rank MDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GqI4fTVUXC": {
    "title": "On the Disconnect Between Theory and Practice of Overparametrized Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8XgCH9y1Bs": {
    "title": "3D Object Representation Learning for Robust Classification and Pose estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MEbNz44926": {
    "title": "Flexible Residual Binarization for Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNkUeTUbSQ": {
    "title": "Understanding and Controlling a Maze-Solving Policy Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jnFcKjtUPN": {
    "title": "COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mQ72XRfYRZ": {
    "title": "A Hierarchical Bayesian Model for Few-Shot Meta Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z2avrOUajn": {
    "title": "SubDiff: Subgraph Latent Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgZUcwFhjr": {
    "title": "Can Transformers Capture Spatial Relations between Objects?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hbbus5IOYt": {
    "title": "How Does RLHF Shift Behavior Distributions? Distinguishability and Steerability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=66arKkGiFy": {
    "title": "Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1qSHSFOew": {
    "title": "CSI: Enhancing the Robustness of 3D Point Cloud Recognition against Corruption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4CLLlIaaH": {
    "title": "Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wfzXa8e783": {
    "title": "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ishA3LxN8": {
    "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H49g8rRIiF": {
    "title": "From Language to 3D Worlds: Adapting Language Models for Point Cloud Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnicaG5xiH": {
    "title": "Interpretable Meta-Learning of Physical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDKTMjoFbC": {
    "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=84n3UwkH7b": {
    "title": "Detecting, Explaining, and Mitigating Memorization in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GH2LYb9XV0": {
    "title": "Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOWdQQgMJY": {
    "title": "Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XbydvPq92M": {
    "title": "Information-Ordered Bottlenecks for Adaptive Dimensionality Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GTk0AdOYLq": {
    "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lATusnzNRT": {
    "title": "Enhancing Graph Injection Attacks Through Over-Smoothing Amplification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaGA40pfFY": {
    "title": "Rationality of Thought Improves Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbjSwwrQOe": {
    "title": "Statistical Rejection Sampling Improves Preference Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zyBJodMrn5": {
    "title": "On the generalization capacity of neural networks during generic multimodal reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4JbrdrHxYy": {
    "title": "The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTRekADULK": {
    "title": "Sparse Training of Discrete Diffusion Models for Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ijmMNaSJk": {
    "title": "Towards Understanding Masked Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RacYdzHxcz": {
    "title": "Human-Producible Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RXVYOCGO7g": {
    "title": "Mitigating Backdoor Attacks in Federated Learning through Noise-Guided Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqHgSxRwiK": {
    "title": "Test Relative Fairness in Human Decisions With Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JDd46WodYf": {
    "title": "Active Procedure Planning with Uncertainty-awareness in Instructional Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UndmcWatBN": {
    "title": "Dissecting Zero-Shot Visual Reasoning Capabilities in Vision and Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=umUIYdLtvh": {
    "title": "EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zzqn5G9fjn": {
    "title": "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXLOOoR2ft": {
    "title": "DoraemonGPT: Toward Solving Real-world Tasks with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A8et2yjbly": {
    "title": "Cross-Modality Masked Pre-training for Visible-Infrared Person Re-identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QtCpQHsO1Q": {
    "title": "Alleviating Label Shift Through Self-trained Intermediate Distribution: Theory and Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAvcKbUXwb": {
    "title": "Interpreting Equivariant Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yKksu38BpM": {
    "title": "Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E2ePtpKJpy": {
    "title": "Improving Compositional Text-to-image Generation with Large Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruGY8v10mK": {
    "title": "A Data-Driven Measure of Relative Uncertainty for Misclassification Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9W6KaAcYlr": {
    "title": "Maximally discriminative stimuli for functional cell type identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O9nZCwdGcG": {
    "title": "Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nudMydhZZW": {
    "title": "A primal-dual perspective for distributed TD-learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y9wQf6ZWS2": {
    "title": "RegQ: Convergent Q-Learning with Linear Function Approximation using Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dN1IV0Ov8a": {
    "title": "Long BERT for bankruptcy prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2CxkRDMIG4": {
    "title": "Precision and Recall Reject Curves for Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIbH5ekQpr": {
    "title": "IMP: Benchmarking Image Polysemy in Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyW37euNXb": {
    "title": "Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzG7kSpjJk": {
    "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7cYRi7NxT": {
    "title": "Hierarchical Side-Tuning for Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3BbxvAQS9": {
    "title": "DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8h2uUX22d": {
    "title": "Understanding MLP-Mixer as a wide and sparse MLP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpuQonyeXN": {
    "title": "Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sdn7ocpvuX": {
    "title": "Advective Diffusion Transformers for Topological Generalization in Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UMfcdRIotC": {
    "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=30N3bNAiw3": {
    "title": "Separating common from salient patterns with Contrastive Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBCuRzjqK7": {
    "title": "Self-Supervised Contrastive Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=532tcx7IHF": {
    "title": "RLLTE: Long-Term Evolution Project of Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHdd6EUAy7": {
    "title": "Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eUAr4HwU0X": {
    "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PjS5RnxeK": {
    "title": "On progressive sharpening, flat minima and generalisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Q1mBvUgmt": {
    "title": "VIPER: Vibrant Period Representation for Robust and Efficient Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4PzxLPEGRn": {
    "title": "OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AqXzHRU2cs": {
    "title": "Generative Pretrained Embedding and Hierarchical Representation to Unlock Human Rhythm in Activities of Daily Living",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyMXxpBfct": {
    "title": "Forward Explanation : Why Catastrophic Forgetting Occurs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6p8lpe4MNf": {
    "title": "A Semantic Invariant Robust Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qgWJkDiI5p": {
    "title": "Fast Equilibrium of SGD in Generic Situations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qm46g9Ri15": {
    "title": "AlignCLIP: Enhancing Stable Representations in Vision-Language Pretraining Models through Attention and Prediction Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QncaLObzi": {
    "title": "Binary Hyperbolic Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izrOLJov5y": {
    "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dnn7SoD224": {
    "title": "Robust Stereo Matching by Risk Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PP1rudnxiW": {
    "title": "Transport meets Variational Inference: Controlled Monte Carlo Diffusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRdEBlwUW6": {
    "title": "DAFA: Distance-Aware Fair Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fvTaoyH96Z": {
    "title": "Non-Parameterized Randomization for Environmental Generalization in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=of2rhALq8l": {
    "title": "AffineQuant: Affine Transformation Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBw8JGBJWj": {
    "title": "Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LQDJO7txyN": {
    "title": "Prototypical Influence Function for Fully Test-time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nEmi3YIz4": {
    "title": "ProtoNMF: Turning a Black Box into a Prototype Based Interpretable Model via Non-negative Matrix Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnuSBQjgqK": {
    "title": "CIM: Constrained Intrinsic Motivation for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iqAbdT35hE": {
    "title": "Out-Of-Distribution Detection With Smooth Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WdWGe88RdX": {
    "title": "Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=33XGfHLtZg": {
    "title": "Conformal Risk Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUCgHbmO11": {
    "title": "SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dHmhoWweE": {
    "title": "Lookbehind Optimizer: k steps back, 1 step forward",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=770DetV8He": {
    "title": "RetroBridge: Modeling Retrosynthesis with Markov Bridges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C1sQBG6Sqp": {
    "title": "Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uIKZSStON3": {
    "title": "In-context Exploration-Exploitation for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oKglS1cFdb": {
    "title": "Feature Accompaniment: Is It Feasible to Learn Out-of-Distribution Generalizable Representations with In-Distribution Data?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nanyAujl6e": {
    "title": "Out-of-Distribution Detection with Negative Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o5Bqa4o5Mi": {
    "title": "$\\pi$2vec: Policy Representation with Successor Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TVg6hlfsKa": {
    "title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gM8X6RbXkV": {
    "title": "Hierarchical Concept Discovery Models: A Concept Pyramid Scheme",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJAlw561AH": {
    "title": "The Uncertainty-Perception Tradeoff",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSOTacnSNf": {
    "title": "Multimodal Meta-learning of Implicit Neural Representations with Iterative Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9TSv6ZVhvN": {
    "title": "Improving Accelerated Federated Learning with Compression and Importance Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Svy1XoOLXj": {
    "title": "BiLoRA: A Bi-level Optimization Framework for Low-rank Adapters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGsR3MJvwS": {
    "title": "Generalizable Deep RL-Based TSP Solver via Approximate Invariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vjHCyOWc7h": {
    "title": "Mixture Stochastic Block Model for Multi-Group Community Detection in Multiplex Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zYXFMeHRtO": {
    "title": "FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SKulT2VX9p": {
    "title": "Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qrGjFJVl3m": {
    "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1yvEwYfK9": {
    "title": "Learning Label Shift Correction for Test-Agnostic Long-Tailed Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c2R7ajodcI": {
    "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pw2ssoOTpo": {
    "title": "CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJKlmCpOQ7": {
    "title": "Removing Multiple Shortcuts through the Lens of Multi-task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43WKxTuJxu": {
    "title": "Orthogonal Function Representations for Continuous Armed Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJvXNpvOdM": {
    "title": "TASK PLANNING FOR VISUAL ROOM REARRANGEMENT UNDER PARTIAL OBSERVABILITY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v9Sfo2hMJl": {
    "title": "Rethinking the Temporal Modeling for Time Series Forecasting with Hybrid Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eSr9iK1z8n": {
    "title": "Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=83w0LPowHz": {
    "title": "On Reconstructability of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E34AlVLN0v": {
    "title": "Parallelizing non-linear sequential models over the sequence length",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQ6rnDriHj": {
    "title": "Unifying Diverse Decision-Making Scenarios with Learned Discrete Actions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NW2s5XXwXU": {
    "title": "Long-tailed Diffusion Models with Oriented Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=82A2EfMu3e": {
    "title": "Efficient Discrete Physics-informed Neural Networks for Solving Evolutionary Partial Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktiikNTgK5": {
    "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LtuRgL03pI": {
    "title": "InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJeYtRuguR": {
    "title": "A Simple Romance Between Multi-Exit Vision Transformer and Token Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FvBXs8t8K": {
    "title": "Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RUKTunGWJe": {
    "title": "INRSTEG: FLEXIBLE CROSS-MODAL LARGE CAPACITY STEGANOGRAPHY VIA IMPLICIT REPRESENTATIONS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DrhZneqz4n": {
    "title": "Single Motion Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jOm5p3q7c7": {
    "title": "Optimal Sample Complexity for Average Reward Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XrunSYwoLr": {
    "title": "Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=II0OENWgi8": {
    "title": "Skill-Conditioned Policy Optimization with Successor Features Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Dwqu5urzs": {
    "title": "Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrjDNgAfp4": {
    "title": "SEMANTIC RHEOLOGY: THE FLOW OF IDEAS IN LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlcWvyf5rm": {
    "title": "UniBoost: Boost Zero-shot Vision-Language Tasks via Multitask Fine-tuning with Unsupervised Unimodal Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJFBMqCE4X": {
    "title": "SimVAE: Narrowing the gap between Discriminative & Generative Self-Supervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GstK7tITrE": {
    "title": "AniHead: Efficient and Animatable 3D Head Avatars Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sxGugrYhP9": {
    "title": "BatteryML:An Open-source platform for Machine Learning on Battery Degradation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YWOieLv40v": {
    "title": "Representation Bottleneck of Graph Neural Networks for Scientific Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1hLFLNu4uy": {
    "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JVeM7uwDwK": {
    "title": "Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4YESQqIys7": {
    "title": "NfgTransformer: Equivariant Representation Learning for Normal-form Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNm7TNIL6O": {
    "title": "UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pszewhybU9": {
    "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JewzobRhay": {
    "title": "When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hzNVNSz8O": {
    "title": "No learning rates needed: Introducing SaLSa - Stable Armijo Line Search Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTbAGlu4jR": {
    "title": "Learning Identifiable Balanced Prognostic Score for Treatment Effect Estimation Under Limited Overlap",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fACNPcPcrs": {
    "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bGGYcvw8mp": {
    "title": "Understanding In-Context Learning from Repetitions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QdHg1SdDY2": {
    "title": "LEA: Learning Latent Embedding Alignment Model for fMRI Decoding and Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ndCJeysCPe": {
    "title": "Analysis of Learning a Flow-based Generative Model from Limited Sample Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qpuxHL9X3d": {
    "title": "Efficient Diversified Attack: Multiple Diversification Strategies Lead to the Efficient Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWsJkH1tNi": {
    "title": "Federated Learning, Lessons from Generalization Study: Communicate Less, Learn More",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m9zWBn1Y2j": {
    "title": "Ligand Conformation Generation: from singleton to pairwise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE2e8664Sl": {
    "title": "Few-shot Hybrid Domain Adaptation of Image Generator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auguNUCto5": {
    "title": "Boosting Temporal Graph Learning From Global and Local Perspectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GWSIo2MzuH": {
    "title": "Rethinking Information-theoretic Generalization: Loss Entropy Induced PAC Bounds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gs8jWk0F01": {
    "title": "Deep Reinforcement Learning for Dynamic Capacitated Vehicle Routing Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8RgPl5OQX": {
    "title": "Imagination Mechanism: Mesh Information Propagation for Enhancing Data Efficiency in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2qZhSTtzU": {
    "title": "IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vePdNU3u6n": {
    "title": "Towards Robust and Efficient Cloud-Edge Model Adaptation via Selective Entropy Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFG1YmQTqi": {
    "title": "TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzFLBOFMZ2": {
    "title": "Causal Structure Learning Supervised by Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3kDP3IytM": {
    "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CanomFZssu": {
    "title": "Boosting Graph Anomaly Detection with Adaptive Message Passing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hf54sNeeBM": {
    "title": "Knowledge Accumulating Contrastive Prompt for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNyDvFQNEm": {
    "title": "Unsupervised Learning via Network-Aware Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=85gNpcUhmx": {
    "title": "Context-Aware Unsupervised Domain Adaptive Lane Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0kWd8SJq8d": {
    "title": "MINDE: Mutual Information Neural Diffusion Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DE2RMJVjgI": {
    "title": "Fine-grained Separation of Action-Background for Point-Level Temporal Action Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GtnNhtuVrc": {
    "title": "Semi-Supervised Semantic Segmentation via Marginal Contextual Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LyO8WTVTU": {
    "title": "A Teacher-Guided Framework for Graph Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MM30SJ4wAf": {
    "title": "Point Neighborhood Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BllUWdpIOA": {
    "title": "Continual Momentum Filtering on Parameter Space for Online Test-time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P895PSh41Z": {
    "title": "Relaxed State-Adversarial Offline Reinforcement Learning: A Leap Towards Robust Model-Free Policies from Historical Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=381QSrWdF2": {
    "title": "Law of Balance and Stationary Distribution of Stochastic Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oiUhQ4fDLE": {
    "title": "Mixup Your Own Pairs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MsfQ2H0lP": {
    "title": "Generative Adversarial Policy Network for Modelling Protein Complexes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnyD0Rjx2B": {
    "title": "fairret: a Framework for Differentiable Fairness Regularization Terms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gq1Zjhovjr": {
    "title": "Consistency Regularization for Domain Generalization with Logit Attribution Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=huwR9N2ea0": {
    "title": "Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J1SzMZn5lH": {
    "title": "Multi-Agent Bayesian Optimization with Coupled Black-box and Affine Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YnaGcMJQ0M": {
    "title": "Detecting Out-of-Distribution Samples via Conditional Distribution Entropy with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7vKWg2Vdrs": {
    "title": "LeBD: A Run-time Defense Against Backdoor Attack in YOLO",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIZEFyVGC9": {
    "title": "Debiasing Algorithm through Model Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7KDuQPrAF3": {
    "title": "A Foundation Model for Error Correction Codes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HL5P4H8eO2": {
    "title": "Differentiable Trajectory Optimization as a Policy Class for Reinforcement and Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qT1I15Zodx": {
    "title": "The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqh1zdXIra": {
    "title": "Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T97kxctihq": {
    "title": "Revisiting Long-term Time Series Forecasting: An Investigation on Affine Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n39ilTxSDY": {
    "title": "Ditto: Quantization-Aware Secure Inference of Transformers upon MPC",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uAp7YdKrlx": {
    "title": "Time Series Missing Imputation with Multivariate Radial Based Function Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qHGgNyQk31": {
    "title": "Seer: Language Instructed Video Prediction with Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=30aSE3FB3L": {
    "title": "Matrix Manifold Neural Networks++",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4bLXfRd0CX": {
    "title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzxaRFVsgC": {
    "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hUs8YHAUEr": {
    "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tyIPw2m3Um": {
    "title": "Probability-dependent gradient decay in large margin softmax",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=frRDT6EOhg": {
    "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrOefpTvev": {
    "title": "Rethinking Texture Patterns in Transformer Neural NetWork for Medical Image Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dCyt9k4U6N": {
    "title": "FLNERF: 3D FACIAL LANDMARKS ESTIMATION IN NEURAL RADIANCE FIELDS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=maRYffiUpI": {
    "title": "Improving Code Style for Accurate Code Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9NYnsC4Mq": {
    "title": "Learning without Forgetting for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxD2ZCExRG": {
    "title": "HumanTOMATO: Text-aligned Whole-body Motion Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ul6EYKM1Kv": {
    "title": "Cognition-Supervised Learning: Contrasting EEG Signals and Visual Stimuli For Saliency Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6MRm3G4NiU": {
    "title": "SaProt: Protein Language Modeling with Structure-aware Vocabulary",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eAKmQPe3m1": {
    "title": "PixArt-$\\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3ch3kJL7q": {
    "title": "Sentence-level Prompts Benefit Composed Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VXak3CZZGC": {
    "title": "Provable Out-of-Distribution Generalization in Hypersphere",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kqHxpHKMSz": {
    "title": "Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jODehvtTDx": {
    "title": "Analyzing and Improving OT-based Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kbQIWi4ZiL": {
    "title": "Unsupervised combinatorial optimization under complex conditions: Principled objectives and incremental greedy derandomization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZNMZdEQQga": {
    "title": "Transplant of Perceptrons",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b27FJxtFeY": {
    "title": "Quantum AdaBoost with Supervised Learning Guarantee",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNyOI3C7YB": {
    "title": "SEABO: A Simple Search-Based Method for Offline Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MxuFXJtVTt": {
    "title": "Hot PATE: Private Aggregation of Distributions for Diverse Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGjvMcKrrl": {
    "title": "From generalization analysis to optimization designs for state space models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGmx41FTTy": {
    "title": "Two Time-Slices Help Topological Ordering for Learning Directed Acyclic Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7F4ioiKQFT": {
    "title": "ColCLIP: Enhancing Fine-Grained Image Retrieval with Pre-trained Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ndDmZdT4g": {
    "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RtDok9eS3s": {
    "title": "Simplifying Transformer Blocks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xzRnzHUVE9": {
    "title": "Enhancing Sample Efficiency in Black-box Combinatorial Optimization via Symmetric Replay Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wtJS8YDQBc": {
    "title": "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmX0CqFSd7": {
    "title": "Compositional Generative Inverse Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0NruoU6s5Z": {
    "title": "CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DslxExr5Kn": {
    "title": "APC: Predict Global Representation From Local Observation In Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8vKknbgXxf": {
    "title": "What does automatic differentiation compute for neural networks?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrwIZVSc7b": {
    "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cINwAhrgLf": {
    "title": "Free Lunches in Auxiliary Learning: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juE0rWGCJW": {
    "title": "ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jLnygpRFYm": {
    "title": "Predicting masked tokens in stochastic locations improves masked image modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tm8s3696Ox": {
    "title": "Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aOnUe8ah7j": {
    "title": "Symbol as Points: Panoptic Symbol Spotting via Point-based Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wE1I9IGqeH": {
    "title": "Continual Learning in Open-vocabulary Classification with Complementary Memory Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZUzOKE6og": {
    "title": "HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43flsheS4s": {
    "title": "Improving Robustness and Accuracy with Retrospective Online Adversarial Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fCeUoDr9Tq": {
    "title": "Zero-Shot Robustification of Zero-Shot Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZQW7eutCv": {
    "title": "pEBR: A Probabilistic Approach to Embedding Based Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PmJoRfdaK": {
    "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNY3HiaF0J": {
    "title": "MoLE: Human-centric Text-to-image Diffusion with Mixture of Low-rank Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UEdS2lIgfY": {
    "title": "In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBoRhRCzM3": {
    "title": "THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DY6uhcv4Xm": {
    "title": "FedSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JfcLYCqOkQ": {
    "title": "Conditional MAE: An Empirical Study of Multiple Masking in Masked Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=82Mc5ilInM": {
    "title": "FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhT1ZemZow": {
    "title": "Sobolev acceleration for neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GruDNzQ4ux": {
    "title": "DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Wuvhh0LYW": {
    "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qCyhvr0GG8": {
    "title": "VONET: ADVANCING UNSUPERVISED VIDEO OBJECT LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WXXuORQwbQ": {
    "title": "Sparse Mask Representation for Human-Scene Interaction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yONJt6nFc3": {
    "title": "Node Duplication Improves Cold-start Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UX9lljSZqX": {
    "title": "Unified Static and Dynamic: Temporal Filtering Network for Efficient Video Grounding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rt6btdXS2b": {
    "title": "Continuous Indeterminate Probability Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HoY24hOeVP": {
    "title": "Efficient Personalized Text-to-image Generation by Leveraging Textual Subspace",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEkFq4RUCX": {
    "title": "Directional Distance Field for Modeling the Difference between 3D Point Clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9KVT1e1qf7": {
    "title": "LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1d2cLKeNgY": {
    "title": "ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTDyN8yajn": {
    "title": "Octavius: Mitigating Task Interference in MLLMs via MoE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mWT3Ftkc3e": {
    "title": "Sampling is as easy as keeping the consistency: convergence guarantee for Consistency Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNTexdca08": {
    "title": "P2P: Transforming from Point Supervision to Explicit Visual Prompt for Object Detection and Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WIAO4vbnNV": {
    "title": "Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kKmi2UTlBN": {
    "title": "Cosine Similarity Knowledge Distillation for Individual Class Information Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xz13DtbOVW": {
    "title": "Balancing Act: Sparse Models with Constrained Disparate Impact",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2OwSqvxjP2": {
    "title": "Boosting Semi-Supervised Learning via Variational Confidence Calibration and Unlabeled Sample Elimination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQ5eVDIMu4": {
    "title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykhRO1mAg3": {
    "title": "FPTQ: FINE-GRAINED POST-TRAINING QUANTIZATION FOR LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ROuKblmi7": {
    "title": "NECO: NEural Collapse Based Out-of-distribution detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHasEfGsXL": {
    "title": "LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HC0msxE3sf": {
    "title": "Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMTa1tcn7V": {
    "title": "3D Point Cloud Sequences as 2D Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3WB5hT27zf": {
    "title": "Partial Optimal Transport for Open-set Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9grjdFDiAj": {
    "title": "Probabilistic Stability of Stochastic Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4wB3HA3dJ": {
    "title": "Domain-Inspired Sharpness Aware Minimization Under Domain Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A1z0JnxnGp": {
    "title": "Power Characterization of Noisy Quantum Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Nui91LBQS": {
    "title": "Planting a SEED of Vision in Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1FLTvgyAh": {
    "title": "Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KC58bVmxyN": {
    "title": "A Cognitive Model for Learning Abstract Relational Structures from Memory-based Decision-Making Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ARFRZh6pzI": {
    "title": "Tuning-Free Accountable Intervention for LLM Deployment - A Metacognitive Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=koYsgfEwCQ": {
    "title": "DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phWkgFXvdG": {
    "title": "Task Regularized Hybrid Knowledge Distillation For Incremental Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6u6GjS0vKZ": {
    "title": "Coloring Deep CNN Layers with Activation Hue Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tuh4nZVb0g": {
    "title": "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0IOMStUQ8": {
    "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2msbbX3ydD": {
    "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t4HiVGGzML": {
    "title": "Search-Adaptor: Text Embedding Customization for Information Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QMkYEau02q": {
    "title": "Physics-Guided Learning of Meteorological Dynamics for Weather Forecasting and Downscaling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYo9r0CwUf": {
    "title": "Continuously Volumetric Rendering with Neural Density-Distance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mtlt3RQTXJ": {
    "title": "Bi-level Contrastive Learning for Knowledge Enhanced Molecule Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1V1QQYARmd": {
    "title": "Nearest neighbor-based out-of-distribution detection via label smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vstaHBy5N4": {
    "title": "Communication Efficient Federated Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZA9XUTseA9": {
    "title": "On the Implicit Bias of Adam",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jE6VXUhxq9": {
    "title": "On Causal Discovery in the Presence of Deterministic Relations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmw3Jy8MVF": {
    "title": "Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Mo7v69otj": {
    "title": "Pooling Image Datasets with Multiple Covariate Shift and Imbalance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gn0mIhQGNM": {
    "title": "SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zKtaqxLhW": {
    "title": "Generalized Knowledge Distillation for Auto-regressive Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apXtolxDaJ": {
    "title": "Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXjXPQU3yJ": {
    "title": "Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZL6yd6N1S2": {
    "title": "A Stochastic Centering Framework for Improving Calibration in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rpP1eWWgOs": {
    "title": "Surface Representation in LiDAR Scenes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wTRpjTO3F7": {
    "title": "Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thFwKIRqmG": {
    "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Buvbx3xRdu": {
    "title": "VideoClusterNet: Self-Supervised and Adaptive Face Clustering for Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a7eIuzEh2R": {
    "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhnGhO4VfF": {
    "title": "Towards Understanding the Effect of Pretraining Label Granularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MIEnYtlGyv": {
    "title": "Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JLulsRraDc": {
    "title": "Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUH5liW3c1": {
    "title": "When Hard Negative Sampling Meets Supervised Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZo6pDtDZr": {
    "title": "Near-optimal algorithms for private estimation and sequential testing of collision probability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89XNDtqhpL": {
    "title": "MatFormer: Nested Transformer for Elastic Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZ3S17EI0o": {
    "title": "Set Learning for Accurate and Calibrated Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5iENGLEJKG": {
    "title": "Interpreting and Controlling Vision Foundation Models via Text Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KOZu91CzbK": {
    "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z59Rb5bPPP": {
    "title": "Trajeglish: Learning the Language of Driving Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l4k0MJuO9D": {
    "title": "Network calibration under domain shift based on estimating the target domain accuracy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UyGWafcopT": {
    "title": "Meaning Representations from Trajectories in Autoregressive Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHPjs3ivdV": {
    "title": "Is margin all you need? An extensive empirical study of deep active learning on tabular data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JZC8cEmMWY": {
    "title": "How Does Message Passing Improve Collaborative Filtering?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8w6FzR68DS": {
    "title": "PriViT: Vision Transformers for Fast Private Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s1ByDEbpI8": {
    "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xsts7MRLey": {
    "title": "DEEP UNSUPERVISED DOMAIN ADAPTATION FOR TIME SERIES CLASSIFICATION: A BENCHMARK",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJWizuuBUy": {
    "title": "Robust Network Pruning With Sparse Entropic Wasserstein Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ag3o2T51Ht": {
    "title": "Circumventing Concept Erasure Methods For Text-To-Image Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5t44vPlv9x": {
    "title": "Pose Modulated Avatars from Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XyB4VvF01X": {
    "title": "Graph2Tac: Learning hierarchical representations of math concepts in theorem proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ixP76Y33y1": {
    "title": "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FBDtqWXfuq": {
    "title": "Exploring Modality Collaboration with Modality-Agnostic Transformers in Multi-Modal Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZ3syNYe7D": {
    "title": "PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Phicg0WAg": {
    "title": "FlexCap: Generating Rich, Localized, and Flexible Captions in Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e4wgZqF9uG": {
    "title": "On the Viability of Monocular Depth Pre-training for Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rQRDt8F2Yh": {
    "title": "A Discrete and Variational Approach to Speech Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Of2RhzJ8UJ": {
    "title": "Trust Regions for Explanations via Black-Box Probabilistic Certification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8iojQVLLWb": {
    "title": "Bayesian Knowledge Distillation for Online Action Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BnQY9XiRAS": {
    "title": "Complete and Efficient Graph Transformers for Crystal Material Property Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NL6bspkWft": {
    "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgSRPRz8cI": {
    "title": "Patched Denoising Diffusion Models For High-Resolution Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCRZq5nNZu": {
    "title": "Chunking: Forgetting Matters in Continual Learning even without Changing Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W478nWXfwO": {
    "title": "What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgeVptDYAt": {
    "title": "Towards Causal Foundation Model: on Duality between Causal Inference and Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AiPfnM73h": {
    "title": "Projected Off-Policy Q-Learning (POP-QL) for Stabilizing Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3TQxxuquZ": {
    "title": "One-stage Prompt-based Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5E1HnzEBSf": {
    "title": "Local Superior Soups: A Catalyst for Reducing Communication Rounds in Federated Learning with Pre-trained Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjfXcDgvzk": {
    "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQPAAXRswY": {
    "title": "Stabilizing Policy Gradients for Stochastic Differential Equations by enforcing Consistency with Perturbation Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YD0GQBOFFZ": {
    "title": "Structured Evaluation of Synthetic Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a8VETFwcVR": {
    "title": "Unveiling Options with Neural Network Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBYDP46s5N": {
    "title": "Policy Gradient without Boostrapping via Truncated Value Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IZMPWmcS3H": {
    "title": "HIFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JgqftqZQZ7": {
    "title": "FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k0RQHNulm7": {
    "title": "Generalizable Cross-Modality Distillation with Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9SVcrmXL8": {
    "title": "BECLR: Batch Enhanced Contrastive Unsupervised Few-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5tfWyeQI2": {
    "title": "Symbolic equation solving via reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jlEjB8MVGa": {
    "title": "How Does Wild Data Provably Help OOD Detection?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbVZt3aN0b": {
    "title": "Modeling Annotation Delay In Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DiG14qg4ok": {
    "title": "Low-coherence Subspace Projection: Enhance the Learning Capacity of Orthogonal Projection Methods on Long Task Sequences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LemSSn8htt": {
    "title": "Delta-AI: Local objectives for amortized inference in sparse graphical models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZxsKRuP0o8": {
    "title": "Meta-Tasks: Improving Robustness in Few-Shot Classification with Unsupervised and Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQ2i6jazVK": {
    "title": "Learning Implicit Representation for Reconstructing Articulated Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cPmLjxedbD": {
    "title": "A path toward primitive machine intelligence: LMM not LLM is what you need",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mDBsBB1enO": {
    "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxlF2Zv8x0": {
    "title": "Improving protein optimization with smoothed fitness landscapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J7ioefqDPw": {
    "title": "Rethinking Label Poisoning for GNNs: Pitfalls and Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOa3ZCtMjJ": {
    "title": "generative adversarial network with hierarchical semantic prompt constrainting clip for high-quality text-to-image synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQFDJLyJNB": {
    "title": "PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXOB15k1br": {
    "title": "Time-Series AutoAugment: Data Augmentation Policy Search for Long-Term Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nHkMm0ywWm": {
    "title": "Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v0zNCwwkaV": {
    "title": "How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0b328CMwn1": {
    "title": "Visual Prompting Reimagined: The Power of Activation Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BUDxvMRkc4": {
    "title": "BLG: BALANCED LANGUAGE DISTRIBUTION AS GUIDANCE FOR ROBUST LONG-TAILED VISION CLASSIFICATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y01KGvd9Bw": {
    "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bJ3gFiwRgi": {
    "title": "Meta Inverse Constrained Reinforcement Learning: Convergence Guarantee and Generalization Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mhb5fpA1T0": {
    "title": "Learning to Act from Actionless Videos through Dense Correspondences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Diq6urt3lS": {
    "title": "Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUCAA0XnPC": {
    "title": "Ensembler: Combating model inversion attacks using model ensemble during collaborative inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ouj6p4ca60": {
    "title": "Amortizing intractable inference in large language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hv8l922Ad7": {
    "title": "Correcting Flaws in Common Disentanglement Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eo9dHwtTFt": {
    "title": "Combining Spatial and Temporal Abstraction in Planning for Better Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aIok3ZD9to": {
    "title": "LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJiUyzOF1m": {
    "title": "Bayesian low-rank adaptation for large language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rXBGpLMxV": {
    "title": "xMLP: Revolutionizing Private Inference with Exclusive Square Activation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CvYBvgEUK9": {
    "title": "On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dhxxIKhqz": {
    "title": "Function-space Parameterization of Neural Networks for Sequential Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3RRWWFaVe": {
    "title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjNGlPh8Wh": {
    "title": "The Expressive Power of Transformers with Chain of Thought",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2TFfLiTGBS": {
    "title": "DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HNOqhcua6b": {
    "title": "TransFusion: Contrastive Learning with Attention Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F5UgXkPgSn": {
    "title": "Fusion over the Grassmannian for High-Rank Matrix Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9K2ZWkYIP": {
    "title": "Scaling Laws for Sparsely-Connected Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anG2Y15mwc": {
    "title": "Diff-Privacy: Diffusion-based Face Privacy Protection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vpV7fOFQy4": {
    "title": "Decision Transformer is a Robust Contender for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UU9Icwbhin": {
    "title": "Retentive Network: A Successor to Transformer for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FQepisCUWu": {
    "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ms0VgzSGF2": {
    "title": "Bridging State and History Representations: Understanding Self-Predictive RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r5njV3BsuD": {
    "title": "Linear Convergence Bounds for Diffusion Models via Stochastic Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OeH6Fdhv7q": {
    "title": "TapMo: Shape-aware Motion Generation of Skeleton-free Characters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hss35aoQ1Y": {
    "title": "InstructDET: Diversifying Referring Object Detection with Generalized Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nfu3bUkmdH": {
    "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OEL4FJMg1b": {
    "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dgc5RWZwTR": {
    "title": "Efficient Training of Multi-task Combinarotial Neural Solver with Multi-armed Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G5Fo7H6dqE": {
    "title": "Iterated Deep $Q$-Network: Efficient Learning of Bellman Iterations for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RcANissyP4": {
    "title": "SelfEval: Leveraging the discriminative nature of generative models for evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pETSfWMUzy": {
    "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPh9p8anUi": {
    "title": "PBADet: A One-Stage Anchor-Free Approach for Part-Body Association",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FWsGuAFn3n": {
    "title": "Prompt-based 3D Molecular Diffusion Models for Structure-based Drug Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILqA09Oeq2": {
    "title": "Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wcaE4Dfgt8": {
    "title": "Uni3D: Exploring Unified 3D Representation at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWjPRRyiqm": {
    "title": "EZ-CLIP: EFFICIENT ZERO-SHOT VIDEO ACTION RECOGNITION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jsWCmrsHHs": {
    "title": "Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VgPmCLQke7": {
    "title": "Training-time Neuron Alignment for Improving Linear Mode Connectivity and Model Fusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0aEUd9UtiA": {
    "title": "DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hORCalGn3Z": {
    "title": "Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qVBTlo0T4s": {
    "title": "AutoNeRF: Training Implicit Scene Representations with Autonomous Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOSYMHfENq": {
    "title": "Batch normalization is sufficient for universal function approximation in CNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YN4uWzcbtt": {
    "title": "On the Positive Definiteness of the Neural Tangent Kernel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lDbjooxLkD": {
    "title": "Unlock Predictable Scaling from Emergent Abilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vuK8MhVtuu": {
    "title": "GRAPH-CONSTRAINED DIFFUSION FOR END-TO-END PATH PLANNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=10fsmnw6aD": {
    "title": "How Out-of-Distribution important is",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pc8AU1aF5e": {
    "title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7D9X2cFnt1": {
    "title": "Elastic Feature Consolidation For Cold Start Exemplar-Free Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1YIpFkPSf": {
    "title": "Hyperbolic Visual-Semantic Alignment for Structural Visual Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HANfmG6tQK": {
    "title": "REVISITING LARS FOR LARGE BATCH TRAINING GENERALIZATION OF NEURAL NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YcW8i9VCf5": {
    "title": "Adversarial Causal Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LhNZqkuVte": {
    "title": "HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxt2Auexc3": {
    "title": "Editing Personality for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktG8Tun1Cy": {
    "title": "Text-to-3D with Classifier Score Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ShQrnAsbPI": {
    "title": "Accurate Forgetting for Heterogeneous Federated Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3GVrWRKuB": {
    "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QYovwMLF7p": {
    "title": "ProFITi: Probabilistic Forecasting of Irregular Time Series via Conditional Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ujX2l7mNX6": {
    "title": "MindGPT: Interpreting What You See with Non-invasive Brain Recordings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTN8dV2pL8": {
    "title": "GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CJPzLnQvIr": {
    "title": "QuickDrop: Efficient Federated Unlearning by Integrated Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eBeECjacpw": {
    "title": "PORF: POSE RESIDUAL FIELD FOR ACCURATE NEURAL SURFACE RECONSTRUCTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O2jyuo89CK": {
    "title": "Modelling complex vector drawings with stroke-clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6H4RBi7RH": {
    "title": "Spurious Feature Diversification Improves Out-of-distribution Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28L2FCtMWq": {
    "title": "Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RB0RQ3XkYB": {
    "title": "Harmonized Learning with Concurrent Arbitration: A Brain-inspired Motion Planning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sw0O2ESxbf": {
    "title": "Collapsing the Learning: Crafting Broadly Transferable Unlearnable Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UBVNwD3hPN": {
    "title": "CivRealm: A Learning and Reasoning Odyssey for Decision-Making Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mz8owj4DXu": {
    "title": "Scalable Language Model with Generalized Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vRyp2dhEQp": {
    "title": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7CLvyZ6Xn7": {
    "title": "Cross-domain Adaptation for Few-shot 3D Shape Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKgRwNhI9R": {
    "title": "Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZneJ55mRO": {
    "title": "G$^2$N$^2$ : Weisfeiler and Lehman go grammatical",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2Ji0Bzd4g": {
    "title": "Lightweight Image Super-Resolution via Flexible Meta Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aNuQyV30Yw": {
    "title": "An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concepts Prompts Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=glwwbaeKm2": {
    "title": "VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JauBLBEjOy": {
    "title": "AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUzHegCq6f": {
    "title": "Polyak Parameter Ensemble: Exponential Parameter Growth Leads to Better Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JbOsMrwjZ3": {
    "title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OdGyza5FO1": {
    "title": "Motion PointNet: Solving Dynamic Capture in Point Cloud Video Human Action",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efFmBWioSc": {
    "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JQtrumvg8": {
    "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkrqCY0GhW": {
    "title": "Language Model Agents Suffer from Compositional Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V4oQAR8uoE": {
    "title": "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=svIdLLZpsA": {
    "title": "Real-Fake: Effective Training Data Synthesis Through Distribution Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUVG9nGzgE": {
    "title": "Learning Conditional Invariances through Non-Commutativity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e2rBzbWwGC": {
    "title": "Mitigating Label Noise on Graphs via Topological Curriculum Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cijO0f8u35": {
    "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BZkKMQ25Z7": {
    "title": "fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y4bvKRvUz5": {
    "title": "KernelWarehouse: Rethinking the Design of Dynamic Convolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXt2EP0PW1": {
    "title": "Distribution-Free Fair Federated Learning with Small Samples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cElJ9KOat3": {
    "title": "Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zHxXaYEgw": {
    "title": "LEO: Generative Latent Image Animator for Human Video Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pu3qMB9aKD": {
    "title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EXitynZhYn": {
    "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qgyF6JVmar": {
    "title": "OTMatch: Improving Semi-Supervised Learning with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOelVq8fwL": {
    "title": "Adapting LLM Agents Through Communication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZn1Ex72Lv": {
    "title": "Block-operations: Creating an Inductive Bias to Route Data and Reuse Subnetworks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MbfAK4s61A": {
    "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLRlL3zR7f": {
    "title": "Vibroacoustic Frequency Response Prediction with Query-based Operator Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HadkNCPhfU": {
    "title": "SEAL: Simultaneous Label Hierarchy Exploration And Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZiF1bJ9K6B": {
    "title": "Learning Coverage Paths in Unknown Environments with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFJr7okOBi": {
    "title": "NL2ProGPT: Taming Large Language Model for Conversational Protein Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zh2iqiOtMt": {
    "title": "Towards the Fundamental Limits of Knowledge Transfer over Finite Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YIIYhSqf1L": {
    "title": "Activation Function Matters in Graph Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ale56Ya59q": {
    "title": "Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WfjJOEfAf7": {
    "title": "Information Flow in Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8FP6eJsVCv": {
    "title": "Explanation Shift: How Did the Distribution Shift Impact the Model?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cc0qk6r4Nd": {
    "title": "Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WReszdNNdP": {
    "title": "BOWLL: A DECEPTIVELY SIMPLE OPEN WORLD LIFELONG LEARNER",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVIq7jYIda": {
    "title": "Manifold Kernel Rank Reduced Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kC5i5X9xrn": {
    "title": "LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLZQTFGToA": {
    "title": "Contrastive Learning is Spectral Clustering on Similarity Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kILAd8RdzA": {
    "title": "On the Generalization and Approximation Capacities of Neural Controlled Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lnB7rTsT9Y": {
    "title": "Knowledge Transfer through Value Function for Compositional Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmShn57DRD": {
    "title": "Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NYN1b8GRGS": {
    "title": "GIM: Learning Generalizable Image Matcher From Internet Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrOmLMFYHi": {
    "title": "Voila-A: Aligning Vision-Language Models with User's Gaze Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iayEcORsGd": {
    "title": "Ultra-sparse network advantage in deep learning via Cannistraci-Hebb brain-inspired training with hyperbolic meta-deep community-layered epitopology",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5pKLogzjQP": {
    "title": "Purify Perturbative Availability Poisons via Rate-Constrained Variational Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eXrUdcxfCw": {
    "title": "Continual Test-Time Adaptation by Leveraging Source Prototypes and Exponential Moving Average Target Prototypes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXm3QYlNPn": {
    "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tr0lPx9woF": {
    "title": "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GxCGsxiAaK": {
    "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9NqC72m31m": {
    "title": "Neural Field Classifiers via Target Encoding and Classification Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3VD4PNEt5q": {
    "title": "Fusion is Not Enough: Single Modal Attack on Fusion Models for 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZCPSC5OgD": {
    "title": "LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LIBZ7Mp0OJ": {
    "title": "Fairness Metric Impossibility: Investigating and Addressing Conflicts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXj1qjFEpQ": {
    "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yKC6Jd0CsP": {
    "title": "Vision ELECTRA: Adversarial Masked Image Modeling with Hierarchical Discriminator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IPhm01y9a9": {
    "title": "Window Attention is Bugged: How not to Interpolate Position Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uXjfOmTiDt": {
    "title": "Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SbkubNdbW": {
    "title": "Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8GmPLkO0oR": {
    "title": "NeRFuser: Diffusion Guided Multi-Task 3D Policy Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70IgE3tRbu": {
    "title": "Continuous Invariance Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnstiBOfnv": {
    "title": "Style Over Substance: Evaluation Biases for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LEYUkvdUhq": {
    "title": "ZipIt! Merging Models from Different Tasks without Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jUWktnsplU": {
    "title": "Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MN3yH2ovHb": {
    "title": "SyncDreamer: Generating Multiview-consistent Images from a Single-view Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKhpp9YAkO": {
    "title": "Associative Transformer is a Sparse Representation Learner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qixVmrOOmT": {
    "title": "UMMAN: UNSUPERVISED MULTI-GRAPH MERGE ADVERSARIAL NETWORK FOR DISEASE PREDICTION BASED ON INTESTINAL FLORA",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bJx4iOIOxn": {
    "title": "Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D5mJSNtUtv": {
    "title": "Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CbmAtAmQla": {
    "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLmqxkfSIw": {
    "title": "Grounding Multimodal Large Language Models to the World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Pu0H7y3gg": {
    "title": "Understanding the Initial Condensation of Convolutional Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sqRgz88TM3": {
    "title": "VFLAIR: A Research Library and Benchmark for Vertical Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATFPZbSZia": {
    "title": "Grouplane: End-to-End 3D Lane Detection with Channel-Wise Grouping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jFa5KESW65": {
    "title": "IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rnHNDihrIT": {
    "title": "Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHng2O0Jjr": {
    "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1hsVvgW0rU": {
    "title": "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PcxQgtHGj2": {
    "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ukT8oODY0": {
    "title": "Careful at Estimation and Bold at Exploration for Deterministic Policy Gradient Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QaKdsh15T": {
    "title": "An Embodied Generalist Agent in 3D World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vtMrbs8Zwd": {
    "title": "The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHg5GDnyq1": {
    "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=keA1Ea7v6p": {
    "title": "Federated Learning Empowered by Generative Content",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aQij7UmwTF": {
    "title": "Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FKg1N2fAFG": {
    "title": "Towards Mitigating Architecture Overfitting in Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U6Mb3CRuj8": {
    "title": "TADA: Timestep-Aware Data Augmentation for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ypXhS83Lh": {
    "title": "Robust Reinforcement Learning with Structured Adversarial Ensemble",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6RR3wU4mSZ": {
    "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OP4crhgkD": {
    "title": "Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btpgDo4u4j": {
    "title": "Efficient Planning with Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i0MsmV7hYZ": {
    "title": "Advancing Counterfactual Inference through Quantile Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkonJu7ScD": {
    "title": "MIND: Masked and Inverse Dynamics Modeling for Data-Efficient Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2BfZMh9td4": {
    "title": "Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4YgfwJBJeQ": {
    "title": "StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rulxyXjf46": {
    "title": "Conformal Prediction via Regression-as-Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y29rdPpPu4": {
    "title": "The Logarithm Trick: achieve better long term forecast via Mean Logarithm Square Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQWXWtJSda": {
    "title": "Unlocking the Potential of Knowledge Distillation: The Role of Teacher Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezscMer8L0": {
    "title": "Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ez68a5UqI": {
    "title": "Reinforcement Learning for Node Selection in Branch-and-Bound",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r92RVhnzKy": {
    "title": "Diving Deep into Regions: Exploiting Regional information Transformer for Single Image Deraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EPfGHb9Y68": {
    "title": "Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kCcIYc98ho": {
    "title": "Mixing Corrupted Preferences for Robust and Feedback-Efficient Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x6u2BQ7xcq": {
    "title": "Tag2Text: Guiding Vision-Language Model via Image Tagging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uELjxVbrqG": {
    "title": "Enhanced Face Recognition using Intra-class Incoherence Constraint",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8QfK9Dq4q0": {
    "title": "Class Incremental Learning via Likelihood Ratio Based Task Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3jXCF5dNpC": {
    "title": "Re-Reading Improves Reasoning in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XfcZjjd0UW": {
    "title": "Rethinking the Number of Shots in Robust Model-Agnostic Meta-Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9w3iw8wDuE": {
    "title": "Entropy is not Enough for Test-time Adaptation: From the Perspective of Disentangled Factors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Z0yB8rmQ2": {
    "title": "Lyra: Orchestrating Dual Correction in Automated Theorem Proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQbFUMKLFp": {
    "title": "Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lr69PmF2Ov": {
    "title": "Discriminatively Matched Part Tokens for Pointly Supervised Instance Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pA8Q5WiEMg": {
    "title": "Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TdhkAcXkRi": {
    "title": "Momentum Benefits Non-iid Federated Learning Simply and Provably",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHJiJ4zYOU": {
    "title": "Vector-valued Representation is the Key: A Study on Disentanglement and Compositional Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riP5PPTnSM": {
    "title": "LLM-Oriented Retrieval Tuner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3fEKavFsnv": {
    "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGlczSBBSj": {
    "title": "SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLhS1TrjK3": {
    "title": "Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p8ujRTjEf3": {
    "title": "Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QIrYb3Vlze": {
    "title": "Isometric Representation Learning for Disentangled Latent Space of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Gt68fnttu": {
    "title": "Dynamic Electroencephalography Representation Learning for Improved Epileptic Seizure Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QO3yH7X8JJ": {
    "title": "Dissecting Arbitrary-scale Super-resolution Capability from Pre-trained Diffusion Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=66wQM45W28": {
    "title": "CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AyzkDpuqcl": {
    "title": "Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjyf5FyQ0a": {
    "title": "Valley: Video Assistant with Large Language model Enhanced abilitY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RAA0vCLMhp": {
    "title": "Semantic Memory Guided Diffusion Networks for Image-to-Long Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q7jXHlWVLC": {
    "title": "Re-imagine the Negative Prompt Algorithm for 2D/3D Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YlleMywQzX": {
    "title": "Anytime Neural Architecture Search on Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQhh1sbfzp": {
    "title": "Differential Model Scaling using Differential Topk",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MvOaI1mmMY": {
    "title": "Learning Label Refinement and Thresholds for Imbalanced Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FoMZ4ljhVw": {
    "title": "Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gMGUa8C0tL": {
    "title": "TaCA: Hot-Plugging Upgrades for Foundation Model with Task-agnostic Compatible Adapter",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNkW0cOwiz": {
    "title": "Lipschitz Singularities in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yLClGs770I": {
    "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzNhzX9kVa": {
    "title": "A Benchmark Study on Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TWC4gLoAxY": {
    "title": "Enhancing Human-AI Collaboration Through Logic-Guided Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNkKaD3MCs": {
    "title": "Learning with Mixture of Prototypes for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DO2WFXU1Be": {
    "title": "PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWmuPfEYhH": {
    "title": "Attention-Guided Contrastive Role Representations for Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xHbRLymyZ": {
    "title": "DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xq7h9nfdY2": {
    "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S62iZf0cba": {
    "title": "Multi-Objective Molecular Design through Learning Latent Pareto Set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lf7gguJgpq": {
    "title": "UniINR: Unifying Spatial-Temporal INR for RS Video Correction, Deblur, and Interpolation with an Event Camera",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJ5EFFGuFU": {
    "title": "SAIR: LEARNING SEMANTIC-AWARE IMPLICIT REPRESENTATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjRPZsfeBO": {
    "title": "A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HU1pesCJF4": {
    "title": "Pixel Reweighted Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8GCcSXlkZN": {
    "title": "Dense Representation Learning for a Joint-Embedding Predictive Architecture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfFVATffPd": {
    "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgaFMvZHSJ": {
    "title": "Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLoepRnoue": {
    "title": "Decodable and Sample Invariance Continuous Object Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djM3WzpOmK": {
    "title": "Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWHMe2O5VW": {
    "title": "Graph ODE with Factorized Prototypes for Modeling Complicated Interacting Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8VPWfqtQMX": {
    "title": "Context is Environment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLSWbk9kPw": {
    "title": "Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gG38EBe2S8": {
    "title": "IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YG01CZDpCq": {
    "title": "Don't Paint Everyone with the Same Brush: Adaptive Prompt Prototype Learning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Zz5UELkIt": {
    "title": "Adaptive Instrument Design for Indirect Experiments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzWLecXH6I": {
    "title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2MpOjashKU": {
    "title": "Divided Attention: Unsupervised Multiple-object Discovery and Segmentation with Interpretable Contextually Separated Slots",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=etGEIggjWS": {
    "title": "Sub-token ViT Embedding via Stochastic Resonance Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z0B4GJuRjo": {
    "title": "Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FcBmz8nLnq": {
    "title": "Finding Adversarially Robust Graph Lottery Tickets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HiYMiZYwkw": {
    "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Uc7Fgwrsm": {
    "title": "OmniMixup: Generalize Mixup with Mixing-Pair Sampling Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oPZZcLZXT1": {
    "title": "Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e9YuyOaJbc": {
    "title": "Optimal and Generalizable Multimodal Representation Learning Framework through Adaptive Graph Construction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkzPuZJ80a": {
    "title": "Accelerating Retrieval-augmented Language Model Serving with Speculation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N6o0ZtPzTg": {
    "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Rwq6c3tvr": {
    "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tfiS7oz64k": {
    "title": "Compact Text-to-SDF via Latent Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20L7txbIa8": {
    "title": "UniPredict: Large Language Models are Universal Tabular Predictors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJCeMiwHdH": {
    "title": "BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0XXA9OG13": {
    "title": "MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cSSHiLnjsJ": {
    "title": "Traveling Words: A Geometric Interpretation of Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7yyAoyfVEC": {
    "title": "Hypothesis- and Structure-based prompting for medical and business diagnosis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XM7INBbvwT": {
    "title": "Does Calibration Affect Human Actions?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Ca9sSzuDp": {
    "title": "Interpreting CLIP's Image Representation via Text-Based Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JpzVlO9X7r": {
    "title": "Does GPT-4 have good intuition about functions?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXZK1Z8tHa": {
    "title": "ShareFormer: Share Attention for Efficient Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jbh2e0b2K": {
    "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KdVvOA00Or": {
    "title": "ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtx8B94JMS": {
    "title": "Variational Inference for SDEs Driven by Fractional Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RCKoQGpPEN": {
    "title": "MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkQSwtx0p3": {
    "title": "Leveraging Task Structures for Improved Identifiability in Neural Network Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5MlPrLO52d": {
    "title": "Neural Tangent Kernels for Axis-Aligned Tree Ensembles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjFJrdK0nO": {
    "title": "Integrating View Conditions for Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgehGq2bDv": {
    "title": "GPAvatar: Generalizable and Precise Head Avatar from Image(s)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1xyar0Ko3E": {
    "title": "Efficient Quantization-aware Training with Adaptive Coreset Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zpOUsOvLM": {
    "title": "Aligning Persistent Homology with Graph Pooling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AbXGwqb5Ht": {
    "title": "Implicit regularization of deep residual networks towards neural ODEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KY8ZNcljVU": {
    "title": "NetInfoF Framework: Measuring and Exploiting Network Usable Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TW0MVSflg5": {
    "title": "Self-Evolving Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbXASfz8MD": {
    "title": "Latent Space Symmetry Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cjdmIUYj03": {
    "title": "On the Generalization of Temporal Graph Learning with Theoretical Insights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQYHbr36Fo": {
    "title": "Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGNdBvymod": {
    "title": "Entropy-MCMC: Sampling from Flat Basins with Ease",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tNAucRS0QQ": {
    "title": "General-purpose Pre-trained Model Towards Cross-domain Molecule Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nQsimt9atc": {
    "title": "IPR-NeRF: Ownership Verification Meets Neural Radiance Field",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vXrIQLzIKY": {
    "title": "Xformer: Hybrid X-Shaped Transformer for Image Denoising",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZJHdiYDD5k": {
    "title": "LatentWarp: Consistent Diffusion Latents for Zero-Shot Video-to-Video Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dUCWpEUrWo": {
    "title": "Asynchronous Graph Generators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2aebB2mf0q": {
    "title": "SemiAugIR: Semi-supervised Infrared Small Target Detection via Thermodynamics-Inspired Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nz2UApmv2e": {
    "title": "Spiking Hybrid Attentive Mechanism with Decoupled Layer Normalization for Joint Sound Localization and Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WS7GuBDFa2": {
    "title": "Learning to Embed Time Series Patches Independently",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y8dHnNEcJu": {
    "title": "SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71oyMJiUm2": {
    "title": "TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EcetCr4trp": {
    "title": "Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHLDHRkmEu": {
    "title": "BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UDbEpJojik": {
    "title": "Unleashing the power of Neural Collapse for Transferability Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6cFcw1Rxww": {
    "title": "Local Search GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=my0RqY48xz": {
    "title": "Awakening Collective Wisdom: Elevating Super-Resolution Network Generalization through Cooperative Game Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QXCjvHnDmu": {
    "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pz2E1Q9Wni": {
    "title": "Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYhH0CDFFa": {
    "title": "Rethinking CNN's Generalization to Backdoor Attack from Frequency Domain",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cu5wJa5LGO": {
    "title": "LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATaE46G1eJ": {
    "title": "CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqExiDNsa7": {
    "title": "Which pre-trained model is effective for speech separation ?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NsCXDyv2Bn": {
    "title": "VoiceGen: Describing and Generating Voices with Text Prompt",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=74YdSRFORA": {
    "title": "Out of Sight: A Framework for Egocentric Active Speaker Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rnHqwPH4TZ": {
    "title": "T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mhyQXJ6JsK": {
    "title": "Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lygdvIKDxi": {
    "title": "SEEKER: Semi-Supervised Knowledge Transfer for Query-Efficient Model Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oz6ABL8o8C": {
    "title": "Unified Interpretation of Smoothing Methods for Negative Sampling Loss Functions in Knowledge Graph Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zihqr7qqpg": {
    "title": "A SYSTEMATIC STUDY ON EARLY STOPPING CRITERIA IN HPO AND THE IMPLICATIONS OF UNCERTAINTY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmRA7Q0lwQ": {
    "title": "Stay on Topic with Classifier-Free Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oV72wHuRNy": {
    "title": "VC dimensions for deep neural networks with bounded-rank weight matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEuDO2EiDr": {
    "title": "RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P6gYcTj6YC": {
    "title": "Incorporating Domain Knowledge in VAE Learning via Exponential Dissimilarity-Dispersion Family",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zfbwpGhd8": {
    "title": "Vision-Language Instruction-enhanced Tuning via Parameter-efficient Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgKGwLYmAy": {
    "title": "DGTAT: DECOUPLED GRAPH TRIPLE ATTENTION NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YxzEPTH4Ny": {
    "title": "Arithmetic with Language Models: from Memorization to Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTH3HnJeRC": {
    "title": "DER-Solomon: A Large Number of CVRPTW Instances Generated Based on the Solomon Benchmark Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IoKRezZMxF": {
    "title": "Consistent Video-to-Video Transfer Using Synthetic Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6KS9D1dxt": {
    "title": "Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfqBfC7bO9": {
    "title": "Causal Unsupervised Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5xadJmgwix": {
    "title": "Scale-Adaptive Diffusion Model for Complex Sketch Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dCDX1fjrXr": {
    "title": "Sparse Labels Node Classification: Unsupervised Learning for Mentoring Supervised Learning in Sparse Label Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NHb6mbD99v": {
    "title": "Uncertainty-aware Distributional Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GOvTGntFNj": {
    "title": "Query-Efficient Offline Preference-Based Reinforcement Learning via In-Dataset Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bpcgcr8E8Z": {
    "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OCqyFVFNeF": {
    "title": "Defining and extracting generalizable interaction primitives from DNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l60EM8md3t": {
    "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yzRXdhk2he": {
    "title": "Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cy5v64DqEF": {
    "title": "Idempotence and Perceptual Image Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ncbDXOdURn": {
    "title": "Characterizing Robust Overfitting in Adversarial Training via Cross-Class Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89bUur0Q4J": {
    "title": "Vision-Language Subspace Prompting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a9xZqOqzEW": {
    "title": "A Logical Framework for Verification of AI Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=95ObXevgHx": {
    "title": "The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMR2dTNdft": {
    "title": "Symmetry Leads to Structured Constraint of Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qCUWVT0Ayy": {
    "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bCvm9h0FmQ": {
    "title": "Causality-Based Black-Box Backdoor Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZDJKTlOUe": {
    "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MeB86edZ1P": {
    "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HD5Y7M8Xdk": {
    "title": "Forward $\\chi^2$ Divergence Based Variational Importane Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Th6NyL07na": {
    "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QgSwyVsOzK": {
    "title": "Modeling Knowledge as Functionals for Knowledge Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZNzDXDFZ0B": {
    "title": "PanoDiffusion: 360-degree Panorama Outpainting via Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEmm0hYA2u": {
    "title": "ZeroP: Zero-Shot Quantization via Proxy Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJ88Wg5Bp5": {
    "title": "ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Spp2i1hKwV": {
    "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dj940KfZl3": {
    "title": "PIE: Simulating Disease Progression via Progressive Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ttXg3SKAg5": {
    "title": "Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bQfJLRlfYO": {
    "title": "Keqing: Knowledge-based Question Answering is A Nature Chain-of-Thought mentor of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jyiD0q2wp2": {
    "title": "Human Pose Estimation via Parse Graph of Body Structure",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JopVmAPyx6": {
    "title": "How does representation impact in-context learning: An exploration on a synthetic task",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0KVkTDB6KZ": {
    "title": "EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fQHb1uZzl7": {
    "title": "Unifying Feature and Cost Aggregation with Transformers for Dense Correspondence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHQMZ8FOL5": {
    "title": "Dual-level Adaptive Self-Labeling for Novel Class Discovery in Point Cloud Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Ol6foUi1G": {
    "title": "Data-independent Module-aware Pruning for Hierarchical Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tlqmkftgpw": {
    "title": "DBRNet: Advancing Individual-Level Continuous Treatment Estimation through Disentangled and Balanced Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rtw2xd4ZdK": {
    "title": "LeCO-NeRF: Learning Compact Occupancy for Large-scale Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FVhmnvqnsI": {
    "title": "Multisize Dataset Condensation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KncRpAnprQ": {
    "title": "A Novel Approach For Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBs0IfPj5e": {
    "title": "Backdiff: a diffusion model for generalized transferable protein backmapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UyNXMqnN3c": {
    "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jXOVnxvLic": {
    "title": "To Simulate Neural Organoid: A Framework and A Benchmark based on AI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nubKjBbazd": {
    "title": "APD: Boosting Adversarial Transferability via Perturbation Dropout",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDSTjtnqCg": {
    "title": "Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JB3lbDtsFS": {
    "title": "It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N9wD4RFWY0": {
    "title": "Benchmarking Large Language Models as AI Research Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxoFut3dWW": {
    "title": "A Simple and Effective Pruning Approach for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TqL2xBwXP3": {
    "title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=67t4ikhlvP": {
    "title": "Agent-Centric State Discovery for Finite-Memory POMDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GTUoTJXPBf": {
    "title": "Noisy Interpolation Learning with Shallow Univariate ReLU Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2lDQLiH1W4": {
    "title": "Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M36m3buBVD": {
    "title": "LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VCscggkg2t": {
    "title": "Goal2FlowNet: Learning Diverse Policy Covers using GFlowNets for Goal-Conditioned RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=py4ZV2qYQI": {
    "title": "Effective and Efficient Federated Tree Learning on Hybrid Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x13bw5VQkf": {
    "title": "A Coefficient Makes SVRG Effective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dyrGMhicMw": {
    "title": "Weight Selection for Model Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJ3K7uDGGl": {
    "title": "Knowledge Distillation Based on Transformed Teacher Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yozwqhIHXj": {
    "title": "Image Translation as Diffusion Visual Programmers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eS5zjXvxf8": {
    "title": "MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOE6N8dp4w": {
    "title": "Harnessing large-language models to generate private synthetic text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5aHmaMFJns": {
    "title": "Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u4CQHLTfg5": {
    "title": "Individual Fairness as an Extension of Group Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4yQefeXhp": {
    "title": "DMV3D: Denoising Multi-view Diffusion Using 3D Large Reconstruction Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBUUNj0Fnz": {
    "title": "Active Learning for Image Segmentation with Binary User Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmGRoNDQgJ": {
    "title": "Influencer Backdoor Attack on Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=noe76eRcPC": {
    "title": "PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sllU8vvsFF": {
    "title": "LRM: Large Reconstruction Model for Single Image to 3D",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExpwgjvwmC": {
    "title": "OmniInput: A Model-centric Evaluation Framework through Output Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=na7AgFyp1r": {
    "title": "Empowering Active Learning for 3D Molecular Graphs with Geometric Graph Isomorphism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbznWbXq2b": {
    "title": "GPT-FL: Generative Pre-trained Model-Assisted Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fpYIlzOpIA": {
    "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ywD00GsxgD": {
    "title": "Synthetic Data as Validation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhizIPytk4": {
    "title": "How Well Do Supervised Models Transfer to 3D Image Segmentation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tc1jaWpi7M": {
    "title": "Completing Visual Objects via Bridging Generation and Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ukmwyfjqoN": {
    "title": "ReBotNet: Fast Real-time Video Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxfPefbu1s": {
    "title": "Procedural Fairness Through Decoupling Objectionable Data Generating Components",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bQWE2UqXmf": {
    "title": "Detecting Generated Text via Rewriting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAgwFiIY4p": {
    "title": "Graph as Point Set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3bqesUzZPH": {
    "title": "FTA: Stealthy and Adaptive Backdoor Attack with Flexible Triggers on Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KjegfPGRde": {
    "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZ6NY4JfFX": {
    "title": "Revisiting the Role of Language Priors in Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=32camXjW25": {
    "title": "Covariance-corrected Whitening Alleviates Network Degeneration on Imbalanced Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GGPyzKsHZ1": {
    "title": "Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imZcqOrbig": {
    "title": "Multi-View Representation is What You Need for Point-Cloud Pre-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=You77eOFDv": {
    "title": "RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F9Qy7mH34l": {
    "title": "Key-Graph Transformer for Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C6a0Obrp3o": {
    "title": "SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Un0rgm9f04": {
    "title": "VDT: General-purpose Video Diffusion Transformers via Mask Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UM6QLuOVNi": {
    "title": "EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EA8dTp96GY": {
    "title": "RelationVLM: Making Large Vision-Language Models Understand Visual Relations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e9bEoxNiTJ": {
    "title": "TransCues: Boundary and Reflection-empowered Pyramid Vision Transformer for Semantic Transparent Object Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T6pC0E2ziE": {
    "title": "Learning Identifiable Causal Structures with Pairwise Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DchC116F4H": {
    "title": "Non-negative Probabilistic Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KAseclJyj5": {
    "title": "Diverse Offline Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YB7z2AOqm3": {
    "title": "Stress Testing Byzantine Robustness in Distributed Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K7l94Z81bH": {
    "title": "Sparsity-Aware Grouped Reinforcement Learning for Designated Driver Dispatch",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lFYj0oibGR": {
    "title": "Vision-Language Foundation Models as Effective Robot Imitators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q9NMLp2lft": {
    "title": "A Generalized Convolutional Neural Network for Small Dataset Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aHmNpLlUlb": {
    "title": "InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u859gX7ADC": {
    "title": "Augmenting transformers with recursively composed multi-grained representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRvBXm7WK3": {
    "title": "MOESR: MULTI-OBJECTIVE EVOLUTIONARY ALGORITHM FOR IMAGE SUPER-RESOLUTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3nPFco1EKt": {
    "title": "Evolving Neural Network's Weights at Imagenet Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3LFy3dUS86": {
    "title": "P2RBOX:A SINGLE POINT IS ALL YOU NEED TRAINING ORIENTED OBJECT DETECTOR",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4vzu2aokv": {
    "title": "P2Seg: Pointly-supervised Segmentation via Mutual Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CqzO3z9kVK": {
    "title": "Knowledge Fusion by Evolving Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFT9jWI7vT": {
    "title": "Towards Architecture-Insensitive Untrained Network Priors for Accelerated MRI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0kTH3HVLz": {
    "title": "A Light-robust Reconstruction Method for Spike Camera",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7oLshfEIC2": {
    "title": "TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7hxoYxKDTV": {
    "title": "Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bvw9H80jyW": {
    "title": "ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCUG6P9RkZ": {
    "title": "Better Imitation Learning in Discounted Linear MDP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DorP300Q3b": {
    "title": "Learning Pseudo 3D Representation for Ego-centric 2D Multiple Object Tracking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAO4VS9QRV": {
    "title": "Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0IaTFNJner": {
    "title": "On the Embedding Collapse When Scaling up Recommendation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=naEeJTlRsr": {
    "title": "Revisiting High-Resolution ODEs for Faster Convergence Rates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XD0PHQ5ry4": {
    "title": "SELF: Language-Driven Self-Evolution for Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYBdkHcXXK": {
    "title": "When Semantic Segmentation Meets Frequency Aliasing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Od39h4XQ3Y": {
    "title": "Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QBlegfNZNE": {
    "title": "Language as Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fBlHaSGKNg": {
    "title": "Unleashing the Power of Annotation: Enhancing Semi-Supervised Learning through Unsupervised Sample Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CulHdELJ1S": {
    "title": "HUB: Enhancing Learned Optimizers via Hybrid Update-based Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=olOheQ0ZcK": {
    "title": "Distance Estimation for High-Dimensional Distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vsW5vJqBuv": {
    "title": "Toward Open-ended Embodied Tasks Solving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hJCinlknXn": {
    "title": "UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cVea4KQ4xm": {
    "title": "Beyond Demographic Parity: Redefining Equal Treatment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=brOAVSPPjw": {
    "title": "Wide Neural Network Training Dynamics for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wXWfvSpYHh": {
    "title": "MVSFormer++: Revealing the Devil in the Transformer's Details for Multi-View Stereo",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9g8h5HwZMy": {
    "title": "Subgraph Diffusion for 3D Molecular Representation Learning: Combining Continuous and Discrete",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mw1PWNSWZP": {
    "title": "OctoPack: Instruction Tuning Code Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZ4VwxpzCK": {
    "title": "PROSPECT: Learn MLPs Robust against Graph Adversarial Structure Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0V5TVt9bk0": {
    "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JePfAI8fah": {
    "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QyFm3D3Tzi": {
    "title": "A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k06CbKrdIk": {
    "title": "A Effective Variance Change Detection Method under constantly Changing Mean",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4HohsyNEk": {
    "title": "NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dq7iJqKKM7": {
    "title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9UIGyJJpay": {
    "title": "De novo Protein Design Using Geometric Vector Field Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PVgHZUepm": {
    "title": "Rep-Adapter: Parameter-free Automatic Adaptation of Pre-trained ConvNets via Re-parameterization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IJBsKYXaH4": {
    "title": "Molecular Conformation Generation via Shifting Scores",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8HG2QrtXXB": {
    "title": "HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ww9rWUAcdo": {
    "title": "Theoretical Understanding of Learning from Adversarial Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nmBjBZoySX": {
    "title": "Graph Lottery Ticket Automated",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EH2O3h7sBI": {
    "title": "Prompt Gradient Projection for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RN7RzMxwjC": {
    "title": "Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XasWgF5WsZ": {
    "title": "Elucidating the Solution Space of Extended Reverse-Time SDE for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ijoqFqSC7p": {
    "title": "FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0QAzIMq32X": {
    "title": "Inner Classifier-Free Guidance and Its Taylor Expansion for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AnPX5Jual9": {
    "title": "Rotative Factorization Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mmCIov21zD": {
    "title": "RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cMPm8YFXZe": {
    "title": "ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pZhdz4oyzo": {
    "title": "SqueezeLLM: Dense and Sparse Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Si3YFA641c": {
    "title": "R-EDL: Relaxing Nonessential Settings of Evidential Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V0CUOBWUHa": {
    "title": "Scaling Sentence Embeddings with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0isTh3rml": {
    "title": "Graph Learning with Distributional Edge Layouts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nsNyN0vox": {
    "title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJRrG43BYC": {
    "title": "State-drive Implicit Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEF7qExD9F": {
    "title": "LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJgVY0zOB0": {
    "title": "Weakly-supervised Camera Localization by Ground-to-satellite Image Registration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwW807WJ9G": {
    "title": "InterpGNN: Understand and Improve Generalization Ability of Transdutive GNNs through the Lens of Interplay between Train and Test Nodes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bRm0rul3SZ": {
    "title": "Unpaired Panoramic Image-to-Image Translation Leveraging Pinhole Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6iwg437CZs": {
    "title": "STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IfqXxs1lCJ": {
    "title": "On the Evaluation of Generative Models in Distributed Learning Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNGXbZtK6Q": {
    "title": "Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYPx4NukeB": {
    "title": "SSL Framework for Causal Inconsistency between Structures and Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=syoLhUJmth": {
    "title": "From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHrvRNs2Y0": {
    "title": "ResFields: Residual Neural Fields for Spatiotemporal Signals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xelrLobW0n": {
    "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BteuUysuXX": {
    "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGFrRMBbOq": {
    "title": "Progressive Fourier Neural Representation for Sequential Video Compilation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OgTMbLDQZu": {
    "title": "LAMDA: Unified Language-Driven Multi-Task Domain Adaption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J9Vwp7TiE5": {
    "title": "SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aarj9MrG8Y": {
    "title": "Towards the Universal Learning Principle for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bPG48f3ppz": {
    "title": "SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oxh5CstDJU": {
    "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4pW8NL1UwH": {
    "title": "LIRE: Listwise Reward Enhancement for Preference Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpnlc2ONu0": {
    "title": "Adaptive deep spiking neural network with global-local learning via balanced excitatory and inhibitory mechanism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ehXWDitaKt": {
    "title": "Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgA84TbHxm": {
    "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YbSUcEd7oR": {
    "title": "Realistic Human Motion Generation with Cross-Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OcaKeyGb0K": {
    "title": "A unified theory of scene representation learning and object representation learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tplXNcHZs1": {
    "title": "Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jj5ZjZsWJe": {
    "title": "Stochastic Controlled Averaging for Federated Learning with Communication Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PcBJ4pA6bF": {
    "title": "Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=slSmYGc8ee": {
    "title": "How connectivity structure shapes rich and lazy learning in neural circuits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fGXyvmWpw6": {
    "title": "Federated Virtual Learning on Heterogeneous Data with Local-global Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VVgGbB9TNV": {
    "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=09xFexjhqE": {
    "title": "AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhcxMGfqQn": {
    "title": "Collaborative World Models: An Online-Offline Transfer RL Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fx2SbBgcte": {
    "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c43FGk8Pcg": {
    "title": "Denoising Diffusion Step-aware Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nh6pXEkZkK": {
    "title": "Learning Rate Re-scheduling for AdaGrad in training Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QmYNBVukex": {
    "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bVzLZr0S8s": {
    "title": "Action Shapley: A training data selection metric for high performance and cost efficient reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e1IMBXiDhW": {
    "title": "Matrix Information Theory for Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lu5gGqhFTw": {
    "title": "RelationMatch: Matching In-batch Relationships for Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wn82BUF7jH": {
    "title": "On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lifLHzadgr": {
    "title": "Cumulative Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QV6uB196cR": {
    "title": "A/B testing under Identity Fragmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIrpuifCJW": {
    "title": "Exploring the Impact of Information Entropy Change in Learning Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=USWkUOfxOO": {
    "title": "Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nu7dDaVF5a": {
    "title": "3D Reconstruction with Generalizable Neural Fields using Scene Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MukGKGtgnr": {
    "title": "Causal Structure Recovery with Latent Variables under Milder Distributional and Graphical Assumptions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNxlJJIEVj": {
    "title": "Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wR9qVlPh0P": {
    "title": "AutoVP: An Automated Visual Prompting Framework and Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJ3H9K7Mcb": {
    "title": "Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9CXfU2mD5": {
    "title": "Score Propagation as a Catalyst for Graph Out-of-distribution Detection: A Theoretical and Empirical Study",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BfMQIJ0nLc": {
    "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gvKZyTlUgQ": {
    "title": "Warped Convolutional Neural Networks For Large Homography Transformation with $\\mathfrak{sl}(3)$ Algebra",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NqQjoncEDR": {
    "title": "Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zEBK3E9bX": {
    "title": "SPOT: Scalable 3D Pre-training via Occupancy Prediction for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5dlfiJIXoh": {
    "title": "Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fihkcXeG7N": {
    "title": "Gate-guided and subgraph-aware Bilateral Fusion for Molecular Property Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxYye6i2Xi": {
    "title": "Composing Recurrent Spiking Neural Networks using Locally-Recurrent Motifs and Risk-Mitigating Architectural Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gFR4QwK53h": {
    "title": "Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnUwSIgK5W": {
    "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7gqOBG8ow": {
    "title": "Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1RKWSyZ2Y": {
    "title": "Guiding Instruction-based Image Editing via Multimodal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=va9hbzIggi": {
    "title": "Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VXDPXuq4oG": {
    "title": "Order-Preserving GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YXn76HMetm": {
    "title": "Hyperbolic Active Learning for Semantic Segmentation under Domain Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h6Tz85BqRI": {
    "title": "VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNe1T0Ahby": {
    "title": "Efficacy of Dual-Encoders for Extreme Multi-label Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YR3ETaElNK": {
    "title": "Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0Z7sBavgm": {
    "title": "Class-Context-Aware Phantom Uncertainty Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kB4yBiNmXX": {
    "title": "FasterViT: Fast Vision Transformers with Hierarchical Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jTdqzBGMsq": {
    "title": "Aligner: One Global Token is Worth Millions of Parameters When Aligning LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=esqRHCwTJ2": {
    "title": "Long-Term Impacts of Model Retraining with Strategic Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OrOd8PxOO2": {
    "title": "Universal Humanoid Motion Representations for Physics-Based Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=COYDmKkQH4": {
    "title": "AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyC5qvRPz7": {
    "title": "Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gctmyMiPHH": {
    "title": "Feature Collapse",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e3JsfnAi9f": {
    "title": "Interleaving Multi-Task Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=adQ2YC2IV7": {
    "title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g90ysX1sVs": {
    "title": "Adaptive Rational Activations to Boost Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTq2XSBNsa": {
    "title": "MOESART: An Effective Sampling-based Router for Sparse Mixture of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wISvONp3Kq": {
    "title": "Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dh0RmiwkWY": {
    "title": "Large-Scale Public Data Improves Differentially Private Image Generation Quality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vyQmKicyVw": {
    "title": "Revealing Hidden Causal Variables and Latent Factors from Multiple Distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AwyxtyMwaG": {
    "title": "LLMs Represent Contextual Tasks as Compact Function Vectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8sKcAWOf2D": {
    "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnYaYVODyV": {
    "title": "Perceptual Group Tokenizer: Building Perception with Iterative Grouping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2y8XnaIiB8": {
    "title": "Vision-Language Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxZW1aROZA": {
    "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VTYg5ykEGS": {
    "title": "ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fsW7wJGLBd": {
    "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EpYnZpDpsQ": {
    "title": "Self-supervised Representation Learning from Random Data Projectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aKJEHWmBEf": {
    "title": "Approximately Piecewise E(3) Equivariant Point Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ihr4X2qK62": {
    "title": "Choosing Public Datasets for Private Machine Learning via Gradient Subspace Distance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNjrhD67LP": {
    "title": "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybavRGEmpw": {
    "title": "Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CE7lUzrp1o": {
    "title": "CODA: Temporal Domain Generalization via Concept Drift Simulator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ImwrWH6U0Y": {
    "title": "A Comprehensive Study of Privacy Risks in Curriculum Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HL9YzviPCy": {
    "title": "PrACTiS: Perceiver-Attentional Copulas for Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4NhMhElWqP": {
    "title": "DAM: A Foundation Model for Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C5sxQsqv7X": {
    "title": "SELECTFORMER: PRIVATE AND PRACTICAL DATA SELECTION FOR TRANSFORMERS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6INCxtPVXd": {
    "title": "Mode-Aware Continual Learning for Conditional Generative Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IhD1rBHhDy": {
    "title": "Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AKJLnDgzkm": {
    "title": "Welfare Diplomacy: Benchmarking Language Model Cooperation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=enT2rGC7h2": {
    "title": "Impact of Agent Behavior in Distributed SGD and Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4N97bz1sP6": {
    "title": "Weakly-supervised Audio Separation via Bi-modal Semantic Similarity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyfOZKXpcN": {
    "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uH0FGECSEI": {
    "title": "Expected flow networks in stochastic environments and two-player zero-sum games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dTlKCQuuxP": {
    "title": "Neural Polynomial Gabor Fields for Macro Motion Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JbsdayvhO": {
    "title": "Denoising Diffusion via Image-Based Rendering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6WXlm2Kxo": {
    "title": "Masked Diffusion as Self-supervised Representation Learner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v7ZPwoHU1j": {
    "title": "Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F8l0llkMk0": {
    "title": "The Map Equation goes Neural",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TmcH09s6pT": {
    "title": "Generalized Neural Collapse for a Large Number of Classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WTFN4gxLQN": {
    "title": "Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KPmajBxEaF": {
    "title": "LEAP: Liberate Sparse-View 3D Modeling from Camera Poses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1E0EahS5w": {
    "title": "Limits to Reservoir Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cmcD05NPKa": {
    "title": "Learning the greatest common divisor: explaining transformer predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iEFMwP5wng": {
    "title": "Reliable Test-Time Adaptation via Agreement-on-the-Line",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikqcUzUogm": {
    "title": "Programmatic Evaluation of Rule-Following Behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdWLzmPKst": {
    "title": "Sequential Data Generation with Groupwise Diffusion Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vmiV4Z99lK": {
    "title": "SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f9Djqso1p1": {
    "title": "CResT: Cross-Query Residual Transformer for Object Goal Navigation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1FeTLOwzr": {
    "title": "Dynamic Adapter Merging for Continual Video Question-Answering Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VUA9LSmC2r": {
    "title": "Learning Embodied Vision-Language Programming From Instruction, Exploration, and Environmental Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6qtDu7hVPF": {
    "title": "Generative Reinforcement Learning with Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jznbgiynus": {
    "title": "Language Modeling Is Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9yKzVMxlkw": {
    "title": "TiG-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cvGdPXaydP": {
    "title": "Planning with an Ensemble of World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SgjAojPKb3": {
    "title": "OpenNerf: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8HCARN2hhw": {
    "title": "Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ttMwEuEPeB": {
    "title": "3D-GPT: Procedural 3D Modeling with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jBt8qp1iYK": {
    "title": "SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4rODxXsmM": {
    "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Agyicd577r": {
    "title": "BatchPrompt: Accomplish more with less",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bb4VGOWELI": {
    "title": "Large Language Models as Optimizers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j0ZvKSNZiP": {
    "title": "ContextRef: Evaluating Referenceless Metrics for Image Description Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h7nOCxFsPg": {
    "title": "Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VOrfPVYvbN": {
    "title": "Domain Bridge: Generative Model-based Domain Forensic for Black-box Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duyA42HlCK": {
    "title": "HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FRCHDhbxZF": {
    "title": "ZeroFlow: Scalable Scene Flow via Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2nD1SvxTZc": {
    "title": "One-Versus-Others Attention: Scalable Multimodal Integration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Q4uVOJ5bX": {
    "title": "R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4yaFQ7181M": {
    "title": "Space and time continuous physics simulation from partial observations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ye3NrNrYOY": {
    "title": "Temporal Causal Mechanism Transfer for Few-shot Action Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1quoTXZzc": {
    "title": "Energy-Based Concept Bottleneck Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPEY9gvwoj": {
    "title": "Amicable Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AF9Q8Vip84": {
    "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hftNRiSQQq": {
    "title": "Boosting Efficiency in Task-Agnostic Exploration Through Causal Knowledge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ukidfml68f": {
    "title": "Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uleDLeiaT3": {
    "title": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ChHx5ORqF0": {
    "title": "Translating Labels to Solve Annotation Mismatches Across Object Detection Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fCQe7ei2f5": {
    "title": "Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5EniAcsO7f": {
    "title": "Tailoring Retrieval Representations to Long-term Visual Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ALuy19mPa": {
    "title": "DreamClean: Restoring Clean Image Using Deep Diffusion Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuFHei1vuE": {
    "title": "Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=guRNebwZBb": {
    "title": "CausalLM is not optimal for in-context learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cphhnHjCvC": {
    "title": "End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hywpSoHwgX": {
    "title": "Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsRXwlwx4w": {
    "title": "Consistency-guided Prompt Learning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YjSKB1sfOE": {
    "title": "From Trojan Horses To Castle Walls: Revealing Bilateral Backdoor Effects In Diffision Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WhgB5sispV": {
    "title": "Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juuyW8B8ig": {
    "title": "Language-Informed Visual Concept Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFPfYccHJ1": {
    "title": "Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7M0EzjugaN": {
    "title": "Online Continual Learning for Interactive Instruction Following Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F5ERvanO6m": {
    "title": "Deep Stochastic Mechanics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7HfliVAtCG": {
    "title": "Detect Every Thing with Few Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lUWf41nR4v": {
    "title": "Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qmw9ne6SOQ": {
    "title": "Localizing and Editing Knowledge In Text-to-Image Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=za9tj3izLn": {
    "title": "A Unified View on Neural Message Passing with Opinion Dynamics for Social Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7NzgkEdGyr": {
    "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qtE9K23ISq": {
    "title": "Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HjIa9SoYEZ": {
    "title": "CTRL: Graph condensation via crafting rational trajectory matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LsURkIPYR5": {
    "title": "LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AoRIT2Uzfg": {
    "title": "DRMGuard: Defending Deep Regression Models against Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SIBN5Xyw7": {
    "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mQYHXUUTkU": {
    "title": "BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAfGlmRBTU": {
    "title": "Representing part-whole hierarchy with coordinated synchrony in neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bh4BW69ILq": {
    "title": "Solving (partial) unbalanced optimal transport via transform coefficients and beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ioBIT7gLBm": {
    "title": "Hard View Selection for Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FvK2noilxT": {
    "title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zpVPhvVKXk": {
    "title": "Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NTNLlEmx8Y": {
    "title": "Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jr03SfWsBS": {
    "title": "Unprocessing Seven Years of Algorithmic Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZitfSXpdT": {
    "title": "Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CEkIyshNbC": {
    "title": "Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4fbFKO4a2W": {
    "title": "Guided Sketch-Based Program Induction by Search Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1gkePTsAWf": {
    "title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N1cjy5iznY": {
    "title": "Modeling Time Series as Text Sequence A Frequency-vectorization Transformer for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XqLcFMMwNb": {
    "title": "MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BwG8hwohU4": {
    "title": "StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RthOl4jHw5": {
    "title": "Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FhbZ1PQCaG": {
    "title": "Think Before You Act: Decision Transformers with Internal Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0lGp17AjO": {
    "title": "A Wasserstein-2 Distance for Efficient Reconstruction of Stochastic Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1sFUGlI09": {
    "title": "DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L2kbdthX5M": {
    "title": "SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQsrRW9mq9": {
    "title": "DeCUR: decoupling common & unique representations for multimodal self-supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OqLrv5oH6r": {
    "title": "Encoding Expert Knowledge into Federated Learning using Weak Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ep0TtjVoap": {
    "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZlEtXIxl3q": {
    "title": "Contrastive losses as generalized models of global epistasis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wPq7fkzL2j": {
    "title": "Self-Paced Augmentations (SPAug) for Improving Model Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZYm1Ql6udy": {
    "title": "Bayesian Bi-clustering of Neural Spiking Activity with Latent Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XEFWBxi075": {
    "title": "GRANDE: Gradient-Based Decision Tree Ensembles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uJVHygNeSZ": {
    "title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cOLzQHklmn": {
    "title": "Independently-prepared Query-efficient Model Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDNP1q5aZq": {
    "title": "Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ygxTuVz9eU": {
    "title": "VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C61sk5LsK6": {
    "title": "InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPdagk1mV7": {
    "title": "Implicit Semi-auto-regressive Image-to-Video Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJPUmX4LXD": {
    "title": "Brain2Music: Reconstructing Music from Human Brain Activity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTBL8OhdhH": {
    "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oyFCgkkLUK": {
    "title": "αMax-B-CUBED: A Supervised Metric for Addressing Completeness and Uncertainty in Cluster Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JPfHljXL4": {
    "title": "When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x8ElSuQWQp": {
    "title": "IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLJcd43U7a": {
    "title": "SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsvgVuaWXK": {
    "title": "Provably Efficient Learning in Partially Observable Contextual Bandit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZvyQTNt2qp": {
    "title": "Provable Convergence of Clipped Normalized-gradient Heavy-Ball Momentum for Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbUlKe1iE8": {
    "title": "Doubly Robust Structure Identification from Temporal Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JbcwfmYrob": {
    "title": "SEA: Sparse Linear Attention with Estimated Attention Mask",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCe9ut1s7i": {
    "title": "On the Importance of Backbone to the Adversarial Robustness of Object Detectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJoMzslBIa": {
    "title": "ReweightOOD: Loss Reweighting for Distance-based OOD Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RZBy8oHTz4": {
    "title": "Zero-Mean Regularized Spectral Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4pC5Gn8HJ": {
    "title": "Contraction and Alienation: Towards Theoretical Understanding of Non-Contrastive Learning with Neighbor-Averaging Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yeeVBMDAwy": {
    "title": "Variance-enlarged Poisson Learning for Graph-based Semi-Supervised Learning with Extremely Sparse Labeled Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SEiuSzlD1d": {
    "title": "Mask-based modeling for Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1hhja8ZxcP": {
    "title": "Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shr9PXz7T0": {
    "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2yFdUSJ4I": {
    "title": "Linear diffusion models meet contextual bandits with large action spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kx2XZlmgB1": {
    "title": "Enhancing Contrastive Learning for Ordinal Regression via Ordinal Content Preserved Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PDL5A6facN": {
    "title": "Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0b0vryZRX": {
    "title": "Self-Distilled Disentanglement for Counterfactual Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5BoXZXTJvL": {
    "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTHfApDakA": {
    "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4Uheo6nR1": {
    "title": "Robust prediction under missingness shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFOoOdaiue": {
    "title": "Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kpEz4Bxs6e": {
    "title": "Dataset Distillation in Large Data Era",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUwfjPzI8g": {
    "title": "Continual Learning via Winning Subnetworks That Arise Through Stochastic Local Competition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gd0lAEtWso": {
    "title": "OmniControl: Control Any Joint at Any Time for Human Motion Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BXY6fe7q31": {
    "title": "Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IAZVktzmG5": {
    "title": "Learning Epipolar Feature Fields for Multi-Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FVAqmAY2C9": {
    "title": "Towards Faster and Stronger Deep Earth Mover's Distance for Few-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QJGj07PD9C": {
    "title": "Guaranteed Approximation Bounds for Mixed-Precision Neural Operators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J6QKWjq05Z": {
    "title": "TreeDQN: Learning to minimize Branch-and-Bound tree",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KcRbiPwuNS": {
    "title": "LINK PREDICTION USING NEUMANN EIGENVALUES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w7BwaDHppp": {
    "title": "Geometry-Aware Projective Mapping for Unbounded Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wfoiy6dIEk": {
    "title": "Multi-Instance Learning Based Anomaly Detection Method for Sequence Data with Application to the Credit Card Delinquency Risk Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gf15GsnfTy": {
    "title": "REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ClqyY6Bvb7": {
    "title": "ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fKfvyJeAlY": {
    "title": "LeRaC: Learning Rate Curriculum",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CNZmaInj9n": {
    "title": "Exploring Unified Perspective For Fast Shapley Value Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzYgsZgwXa": {
    "title": "Path Choice Matters for Clear Attributions in Path Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8EyRkd3Qj2": {
    "title": "CLAP: Collaborative Adaptation for Checkerboard Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEOnlJaRKp": {
    "title": "Collaboration! Towards Robust Neural Methods for Vehicle Routing Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xmQMz9OPF5": {
    "title": "Exploring Target Representations for Masked Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JN7TcCm9LF": {
    "title": "Koopman-based generalization bound: New aspect for full-rank weights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJlIwGqPdL": {
    "title": "The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eoSeaK4QJo": {
    "title": "Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIj1CVbkpr": {
    "title": "Online Stabilization of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GT57SN8xt9": {
    "title": "Parameter-Efficient Long-Tailed Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Mq096hr9Y": {
    "title": "OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aA33A70IO6": {
    "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sBQwvucduK": {
    "title": "MagicDrive: Street View Generation with Diverse 3D Geometry Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XhYWgjqCrV": {
    "title": "MogaNet: Multi-order Gated Aggregation Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RzV7QRowUl": {
    "title": "Test like you Train in Implicit Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNLr8WXDEh": {
    "title": "What Does Stable Diffusion Know about the 3D Scene?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xBfQZWeDRH": {
    "title": "GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QGR5IeMNDF": {
    "title": "Pure Message Passing Can Estimate Common Neighbor for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xyxU99Nutg": {
    "title": "Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scozYAatUd": {
    "title": "MULTISCALE ATTENTION VIA WAVELET NEURAL OPERATORS FOR VISION TRANSFORMER",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFaFvyKKbX": {
    "title": "Understanding deep neural networks through the lens of their non-linearity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KWO8LSUC5W": {
    "title": "Constraint-Free Structure Learning with Smooth Acyclic Orientations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfDUzzQa3g": {
    "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b66P1u0k15": {
    "title": "Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mkjKqeBXkt": {
    "title": "KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LndMyiBl3n": {
    "title": "SheAttack: A Silhouette Score Motivated Restricted Black-Box Attack on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80faVLl6ji": {
    "title": "BRIDGING THE GAP BETWEEN HUMAN MOTION AND ACTION SEMANTICS VIA KINEMATIC PHRASES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q1vkAhdI6j": {
    "title": "MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZ8FmnLpCA": {
    "title": "Knowledge Distillation via Flow Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KiH8QXn2pk": {
    "title": "PPTSER: A Plug-and-Play Tag-guided Method for Few-shot Semantic Entity Recognition on Visually-rich Documents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hfAEEsIQ6D": {
    "title": "Perceptual Metrics for Video Game Playstyle Similarity and Diversity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rmpixOjPS": {
    "title": "Boosting Vanilla Lightweight Vision Transformers via Re-parameterization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAgrdEhcr4": {
    "title": "Learning Deep Improvement Representation to Accelerate Evolutionary Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5sjxMwWmk8": {
    "title": "Robust Angular Synchronization via Directed Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oSuVEv4X7w": {
    "title": "Clover: Closed-Loop Verifiable Code Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lt6xKGGWov": {
    "title": "Feature selection with neural estimation of mutual information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HBFzStNrS9": {
    "title": "Unitention: Attend a sample to the dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zb6qOouUJO": {
    "title": "Efficient Fully Single-Loop Variance Reduced Methods for Stochastic Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fV54cBCGEV": {
    "title": "Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lAhWGOkpSR": {
    "title": "Multi-Scale Representations by Varing Window Attention for Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBU33YNVL3": {
    "title": "Bounded Loss Robustness: Enhancing the MAE Loss for Large-Scale Noisy Data Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hbHwZYqk9T": {
    "title": "FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8FZXjRWX1": {
    "title": "ReBaR: Reference-Based Reasoning for Robust Human Pose and Shape Estimation from Monocular Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8eCnnq57m": {
    "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLKgDBUXTR": {
    "title": "How many views does your deep neural network use for prediction?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhJUd3mbhP": {
    "title": "AutoAgents: A Framework for Automatic Agent Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSTaRLVP2G": {
    "title": "The Power of Linear Combinations: Learning with Random Convolutions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0OwtVEejJ": {
    "title": "Weight-Entanglement Meets Gradient-Based Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64kSvC4iPg": {
    "title": "Compressed Context Memory for Online Language Model Interaction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dN4vpVTvWX": {
    "title": "TUVF: Learning Generalizable Texture UV Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=enUArz7TeR": {
    "title": "Decoupled Prioritized Resampling: Advancing Offline RL with Improved Behavior Policy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3M0GXoUEzP": {
    "title": "CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zRkM6UcA22": {
    "title": "Neural Processing of Tri-Plane Hybrid Neural Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dw6y6bEtXm": {
    "title": "PICL: Incorporating Coarse-Grained Data and Physics Information for Superior Physical Systems Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZHr0JajZfH": {
    "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q57JLSE2j5": {
    "title": "Large-Vocabulary 3D Diffusion Model with Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZfi5to2Xl": {
    "title": "SAS: Structured Activation Sparsification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7pFgsSPd7": {
    "title": "Sample-aware RandAugment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S77skzM12O": {
    "title": "PROTO: Iterative Policy Regularizied Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g52tgL8jy6": {
    "title": "A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mvMI3N4AvD": {
    "title": "Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LixtB4TYY2": {
    "title": "REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ERQPyR2eb": {
    "title": "Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FOSBQuXgAq": {
    "title": "A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=waeGeAdZUx": {
    "title": "AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqWobzlAGb": {
    "title": "Modelling brain connectomes networks: Solv is a worthy competitor to hyperbolic geometry!",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xv8iGxENyI": {
    "title": "Threaten Spiking Neural Networks through Combining Rate and Temporal Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1P1nxem1jU": {
    "title": "Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FIplmUWdm3": {
    "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U6hEOZlDf5": {
    "title": "3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=38E4yUbrgr": {
    "title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKfESGFdas": {
    "title": "GeONet: a neural operator for learning the Wasserstein geodesic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=feZ7RpTLRy": {
    "title": "Bridging ML and algorithms: comparison of hyperbolic embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSyytcewxe": {
    "title": "Divide and not forget: Ensemble of selectively trained experts in Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQwULZhiSF": {
    "title": "Unsupervised Discovery of Object-Centric Neural Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5lcjmFmlc": {
    "title": "Robust Classification via a Single Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7duh4Ml5rc": {
    "title": "Based on What We Can Control Artificial Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2SwHngthig": {
    "title": "Towards Offline Opponent Modeling with In-context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Cu8MRmhq2": {
    "title": "Multi-granularity Correspondence Learning from Noisy Instructional Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tItq3cwzYc": {
    "title": "Lightweight Image Classification Network Based on Feature Extraction Network SimpleResUNet and Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DMJNaBUv3D": {
    "title": "Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AtLW9HU3bo": {
    "title": "Discovering the question-critical moments: Towards building event-aware multi-modal large language models for complex video question answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CMzF2aOfqp": {
    "title": "Early Stopping Against Label Noise Without Validation Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owziuM1nsR": {
    "title": "Recursive Generalization Transformer for Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AcJrSoArlh": {
    "title": "Rethinking Model Ensemble in Transfer-based Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8vT0f6x1BY": {
    "title": "Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZN8BaYVFkx": {
    "title": "Training Adversarially Robust SNNs with Gradient Sparsity Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hOxgrGM63n": {
    "title": "Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHgc5SMdtd": {
    "title": "MuSc : Zero-Shot Anomaly Classification and Segmentation by Mutual Scoring of the Unlabeled Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p6UwN2Rxhx": {
    "title": "Unveiling Temporal Telltales: Are Unconditional Video Generation Models Implicitly Encoding Temporal Information?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6EbeJR20o": {
    "title": "A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m2NVG4Htxs": {
    "title": "To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1bbPQShCT2": {
    "title": "I-PHYRE: Interactive Physical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ny150AblPu": {
    "title": "EXPOSING TEXT-IMAGE INCONSISTENCY USING DIFFUSION MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J2kRjUAOLh": {
    "title": "Contrastive Predict-and-Search for Mixed Integer Linear Programs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0P622bfUN": {
    "title": "Federated Generative Learning with Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}