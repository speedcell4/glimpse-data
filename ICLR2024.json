{
  "https://openreview.net/forum?id=cXs5md5wAq": {
    "title": "Modelling Microbial Communities with Graph Neural Networks",
    "volume": "review",
    "abstract": "Understanding the interactions and interplay of microorganisms is a great challenge with many applications in medical and environmental settings. In this work, we model bacterial communities directly from their genomes using graph neural networks (GNNs). GNNs leverage the inductive bias induced by the set nature of bacteria, enforcing permutation invariance and granting combinatorial generalization. We propose to learn the dynamics implicitly by directly predicting community relative abundance profiles at steady state, thus escaping the need for growth curves. On two real-world datasets, we show for the first time generalization to unseen bacteria and different community structures. To investigate the prediction results more deeply, we created a simulation for flexible data generation and analyze effects of bacteria interaction strength, community size, and training data amount",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rhgIgTSSxW": {
    "title": "TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023",
    "volume": "review",
    "abstract": "Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers. However, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems. One of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models. For a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction. In this work, we present TabR -- essentially, a feed-forward network with a novel k-Nearest-Neighbors-like component in the middle. On a set of public benchmarks with datasets up to several million objects, TabR marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed \"GBDT-friendly\" benchmark (see Figure 1). Among the novel findings and technical details powering TabR, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them. In addition to the much higher performance, TabR is simple and significantly more efficient compared to prior retrieval-based tabular DL models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kKRbAY4CXv": {
    "title": "Neural Evolutionary Kernel Method: A Knowledge-Based Learning Architechture for Evolutionary PDEs",
    "volume": "review",
    "abstract": "Numerical solution of partial differential equations (PDEs) plays a vital role in various fields of science and engineering. In recent years, deep neural networks (DNNs) have emerged as a powerful tool for solving PDEs. DNN-based methods exploit the approximation capabilities of neural networks to obtain solutions to PDEs in general domains or high-dimensional spaces. However, many of these methods lack the use of mathematical prior knowledge, and DNN-based methods usually require a large number of sample points and parameters, making them computationally expensive and challenging to train. This paper aims to introduce a novel method named the Neural Evolutionary Kernel Method (NEKM) for solving a class of evolutionary PDEs through DNNs based kernels. By using operator splitting and boundary integral techniques, we propose particular neural network architectures which approximate evolutionary kernels of solutions and preserve structures of time-dependent PDEs. Mathematical prior knowledge are naturally built into these DNNs based kernels through convolutional representation with pre-trained Green functions, leading to serious reduction in the number of parameters in the NEKM and very efficient training processes. Experimental results demonstrate the efficiency and accuracy of the NEKM in solving heat equations and Allen-Cahn equations in complex domains and on manifolds, showcasing its promising potential for applications in data driven scientific computing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ApjY32f3Xr": {
    "title": "PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs",
    "volume": "review",
    "abstract": "While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research, particularly in areas such as domain decomposition methods and loss reweighting for handling multi-scale problems and complex geometry. While PINNacle does not guarantee success in all real-world scenarios, it represents a significant contribution to the field by offering a robust, diverse, and comprehensive benchmark suite that will undoubtedly foster further research and development in PINNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=eUgS9Ig8JG": {
    "title": "SaNN: Simple Yet Powerful Simplicial-aware Neural Networks",
    "volume": "review",
    "abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories, simplicial closures, and classifying graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qBL04XXex6": {
    "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
    "volume": "review",
    "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9DYMIpz9c": {
    "title": "Farzi Data: Autoregressive Data Distillation",
    "volume": "review",
    "abstract": "We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=rp5vfyp5Np": {
    "title": "BATTLE: Towards Behavior-oriented Adversarial Attacks against Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "Evaluating the performance of deep reinforcement learning (DRL) agents under adversarial attacks that aim to induce specific behaviors, i.e., behavior-oriented adversarial attacks, is crucial for understanding the robustness of DRL agents. Prior research primarily focuses on directing agents towards pre-determined states or policies, lacking generality and flexibility. Therefore, it is important to devise universal attacks that target inducing specific behaviors in a victim. In this work, we propose BATTLE, a universal behavior-oriented adversarial attack method. In BATTLE, an intention policy is trained to align with human preferences for flexible behavior orientation, while the adversary is trained to guide the victim policy to imitate the intention policy. To improve the attack performance, we introduce a weighting function that assigns importance weights over each state. Our empirical results over several manipulation tasks of Meta-world show the superiority of BATTLE in behavior-oriented adversarial attack settings, outperforming current adversarial attack algorithms. Furthermore, we also demonstrate that BATTLE can improve the robustness of agents under strong attacks by training with adversary. Lastly, we showcase the strong behavior-inducing capability of BATTLE by guiding Decision Transformer agents to act in line with human preferences across various MuJoCo tasks. Our videos are available in https://sites.google.com/view/jj9uxjgmba5lr3g",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=miGpIhquyB": {
    "title": "Understanding Large Language Models Through the Lens of Dataset Generation",
    "volume": "review",
    "abstract": "There has been increased interest in using Large Language Models (LLMs) for text dataset generation subject to a desired attribute, e.g., for use in downstream fine-tuning or training. These works generally focus on a single quality metric of the generated text, typically accuracy on a downstream task. However, this fails to consider whether the model even has the ability to faithfully model the data distribution of the desired real-world domain. In contrast, in this work, we additionally focus on important distributional metrics agnostic to the downstream task, such as data diversity and faithfulness. We show that even in simple domains, generated datasets reveal inherent trade-offs between these metrics across models and training regimes. Further, we find that our metrics not only describe the generated dataset, but also capture key aspects of the underlying model. This allows us to characterize the generated datasets, individual models and by comparison the properties of different model families and training paradigms. By focusing on sub-distributions well-represented in the training data of LLMs, we can, for example, show that popular instruction-tuning techniques strongly decrease the LLM's text generation abilities, with respect to distributional aspects like diversity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6AtXCnHCFy": {
    "title": "FSN: Feature Shift Network for Load-Domain Domain Generalization",
    "volume": "review",
    "abstract": "Conventional deep learning methods for fault detection often assume that the training and the testing sets share the same fault pattern spaces and domain spaces. However, some fault patterns are rare, and many real-world faults have not appeared in the training set. As a result, it's hard for the trained model to achieve desirable performance on the testing set. In this paper, we introduce a novel domain generalization, Load-Domain (LD) domain generalization, which is based on the analysis of the CWRU bearing dataset and its domain division method. For this scenario, we propose a feature shift model called FSN (Feature Shift Network). In the bearing dataset, domains are divided based on different operating conditions which have specific loads, so it's equivalent to load-based domain division. Moreover, the domain label corresponds to the actual load magnitude, making it unique as it contains physical information, which can boost detection accuracy on unknown domain beyond the training set. According to the knowledge above , FSN is trained for feature shift on adjacent source domains, and finally shifts target domain features into adjacent source domain feature space to achieve the purpose of domain generalization. Extensive experiments on CWRU demonstrate that FSN is better than the existed models in the LD domain generalization case. Furthermore, we have another test on MNIST, which also shows FSN can achieve the best performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.6,
    "authors": []
  },
  "https://openreview.net/forum?id=9ceadCJY4B": {
    "title": "Ask Again, Then Fail: Large Language Models' Vacillations in Judgement",
    "volume": "review",
    "abstract": "With the emergence of generative conversational large language models (LLMs) like ChatGPT, serving as virtual assistants in various fields, the stability and reliability of their responses have become crucial. However, during usage, it has been observed that these models tend to waver in their judgements when confronted with follow-up questions from users expressing skepticism or disagreement. In this work, we draw inspiration from questioning strategies in education and propose a \\textsc{Follow-up Questioning Mechanism} along with two evaluation metrics to assess the judgement consistency of LLMs before and after exposure to disturbances. We evaluate the judgement consistency of ChatGPT, PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning benchmarks. Empirical results show that even when the initial answers are correct, judgement consistency sharply decreases when LLMs face disturbances such as questioning, negation, or misleading. Additionally, we study these models' judgement consistency under various settings (sampling temperature and prompts) to validate this issue further, observing the impact of prompt tone and conducting an in-depth error analysis for deeper behavioral insights. Furthermore, we also explore several prompting methods to mitigate this issue and demonstrate their effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=gYcft1HIaU": {
    "title": "Do Current Large Language Models Master Adequate Clinical Knowledge?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) show promising potential in solving clinical problems. Current LLMs, including so-called medical LLMs, are reported to achieve excellent performance on certain medical evaluation benchmarks, such as medical question answering, medical exams, etc. However, such evaluations cannot assess whether LLMs have mastered sufficient, compressive, and necessary medical knowledge for solving real clinical problems, such as clinical diagnostic assistance. In this paper, we propose a framework to assess the mastery of LLMs in clinical knowledge. Firstly, we construct a large medical disease-based knowledge base, MedDisK, covering 10,632 common diseases across 18 clinical knowledge aspects, which are crucial for diagnosing and treating diseases. Built on that, we propose a MedDisK-based evaluation method MedDisKEval: We prompt LLMs to retrieve information related to these clinical knowledge aspects. Then, we evaluate an LLM's mastery of medical knowledge by measuring the similarity between the LLM-generated information and the content within our knowledge base. Our experimental findings reveal that over 50\\% of the clinical information generated by our evaluated LLMs is significantly inconsistent with the corresponding knowledge stored in our knowledge base. We further perform a significance analysis to compare the performance of medical LLMs with their backbone models, discovering that 5 out of 6 medical LLMs perform less effectively than their backbone models in over half of the clinical knowledge aspects. These observations demonstrate that existing LLMs have not mastered adequate knowledge for clinical practice. Our findings offer novel and constructive insights for the advancement of medical LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=10eQ4Cfh8p": {
    "title": "SIMULTANEOUS GENERATION AND IMPROVEMENT: A UNIFIED RL PARADIGM FOR FJSP OPTIMIZATION",
    "volume": "review",
    "abstract": "We present an end-to-end reinforcement learning framework designed to address the Flexible Job Shop Problem (FJSP). Our approach consists of two primary components: a generative model that produces problem solutions stepwise, and a secondary model that continually refines these (partial) solutions. Importantly, we train both models concurrently, enabling each to be cognizant of the other's policy and make informed decisions. Extensive experimentation demonstrates that our model delivers better performance in shorter time on several public datasets comparing to baseline algorithms. Furthermore, we highlight the superior generalizability of our approach, as it maintains strong performance on large-scale instances even when trained on small-scale instances. It is worth noting that this training paradigm can be readily adapted to other combinatorial optimization problems, such as the traveling salesman problemand beyond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BQvbL2sFQx": {
    "title": "Model-Agnostic Shift-Equivariant Downsampling",
    "volume": "review",
    "abstract": "The performance of convolutional neural networks (CNNs) are thought to be insensitive to image shifts. However, recent studies have revealed that downsampling layers in CNNs result in inconsistent outputs for shifted input images. In this study, we present an approach for performing downsampling that ensures absolute shift equivariance. By employing model-agnostic downsampling method that leverages origin selection functions obtained from coordinate-independent statistics of the feature map, we can achieve perfect shift equivariance, while still adhering to the conventional downsampling procedures. Our method allows CNNs to exhibit both improved accuracy and perfect shift invariance for image classification, while also achieving shift equivariance in semantic segmentation benchmarks. Furthermore, we introduce a methodology for achieving shift equivariance without the need for any additional training process. This is accomplished by transferring pretrained weights and replacing existing layers with shift-equivariant counterparts. Additionaly, we show that fine-tuning of the modified CNNs leads superior performance compared to previously proposed models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=eR4W9tnJoZ": {
    "title": "Visuo-emotional perception and Human Cognition to engineer content-generation using Generative AI",
    "volume": "review",
    "abstract": "Media platforms compete for users' attention. Their reach crucially depends on algorithmic real-time bidding and efficiency of hyper-personalized, rapidly generated, and user-optimized content. Attention is, although, a scare and fleeting quantity, often awarded less than 1 second per stimulus. Thus, the current strategy is to rely on the vast amount of user-generated data to mimic the content to the user. The underlying assumption is that this is sufficient incentive for attention. This strategy has evidently failed. As witnessed by the alarmingly low or short-lived successes of campaigns in recent times. This mismatch is exacerbated because most content consumed today is digital. Whereas strategies for digital content mimic our past understanding from mass-media. Hence, we formalize a new understanding of communication, specifically for the digital mediums. We prove that the digital medium needs a new understanding of communication protocols. To that end, we take a first principles approach to the new communication protocol: the neurological representations of communication, specifically, where the communication happens in less than 1 second per stimulus. First, we break down and elaborate on this neurological representation of decision-making. Next, we proffer use of our behavioural communication model for generation and optimization of content creatives. To that end, we elaborate methods for rapid, AI-generation content, increasing the efficiency of visual communication on digital media. Within this exploration we include themes of Hyperpersonalization and Search-engine optimization. Thus, we find that strategically produced content exhibits stronger associations to users' nonconscious needs, wants and goals, which elicits user attention and content-diversity significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=jx6njBKH8E": {
    "title": "Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships",
    "volume": "review",
    "abstract": "Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=rGvDRT4Z60": {
    "title": "FairPATE: Exposing the Pareto Frontier of Fairness, Privacy, Accuracy, and Coverage",
    "volume": "review",
    "abstract": "Deploying machine learning (ML) models often requires both fairness and privacy guarantees. In this work, we study the challenges of integrating group fairness interventions into the Private Aggregation of Teacher Ensemble (PATE) framework. We show that in the joint fairness-privacy setting, the placement of the fairness intervention before, or after PATE's noisy aggregation mechanism (which ensures its differential privacy guarantees) leads to excessive fairness violations, or inefficient privacy budgeting, respectively. With this in mind, we present FairPATE which adds a rejection mechanism due to fairness violations. Through careful adjustment of PATE's privacy accounting, we match the DP-SGD-based state-of-the-art privacy-fairness-accuracy trade-offs (Lowy et al., 2023) in demographic parity, and improve on them for equality of odds with 2% lower disparity at similar accuracy levels and privacy budgets. We also evaluate FairPATE in the setting where exact fairness guarantees need to be enforced by refusing to provide algorithmic decisions at inference-time (for instance, in a human-in-the-loop setting) thus trading off fairness with coverage. Based on our FairPATE, we provide, for the first time, empirical Pareto frontiers for fairness, privacy, accuracy, and coverage on a range of privacy and fairness benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=w73feIekdO": {
    "title": "Real-time computer vision on low-end boards via clustering motion vectors",
    "volume": "review",
    "abstract": "In this work, we suggest computer vision methods, specifically for video tracking and map creation from video. To this end, we utilize motion vectors and clusters, which are computed very efficiently in standard video encoders, usually via dedicated hardware. We suggest a provably good tracking algorithm for clustering these vectors, by considering them as segments. For this, we utilize a definition of a \\emph{coreset} which is essentially a weighted set of points that approximates the fitting loss for every model, up to a multiplicative factor of $1\\pm\\varepsilon$. Our method supports $M$-estimators that are robust to outliers, convex shapes, lines, and hyper-planes. We demonstrate the empirical contribution of our clustering method for video tracking and map creation from video, by running it on micro-computers (Le-Potato and Raspberry Pi) on synthetic and real-world videos with real-time running time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=kmn0BhQk7p": {
    "title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models",
    "volume": "review",
    "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95.8% top-3 accuracy at a fraction of the cost (100x) and time (240x) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for stronger and wider privacy protection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=cQgjz0mf0r": {
    "title": "Deep Network Partition Density Exhibits Double Descent",
    "volume": "review",
    "abstract": "The study of Deep Network (DN) training dynamics has largely focused on the dynamics of the loss function, evaluated on or around train and test set samples. In fact, many DN phenomenon were first introduced in literature with respect to the loss or accuracy dynamics during training, e.g., double descent, grokking. No other statistics about the DN has been found to be as informative as the loss function. In this study, we provide a novel statistic that measures the underlying DN's local complexity, exhibiting two key benefits: (i) it does not require any labels, and (ii) it is informative about the training loss and accuracy dynamics. Our proposed statistic is based on the concentration of partition regions around samples –which encompasses the local expressivity or complexity of a DN– and can be applied on arbitrary architectures, e.g. CNNs, VGGs and Resnets. We show that our statistic exhibits a double descent phenomenon during training, with the partition density first decreasing around training samples, then increasing (ascent), followed by an other descent during which neurons migrate towards the decision boundaries. We see this phenomenon happening for a number of different experimental setups, e.g., training with label noise, delayed generalization, i.e., grokking. Our observations provide a novel lens to study DN training dynamics from a spline theory perspective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=i8PjQT3Uig": {
    "title": "Locality Sensitive Sparse Encoding for Learning World Models Online",
    "volume": "review",
    "abstract": "Model-based reinforcement learning (MBRL) is known to have better sample efficiency. However, acquiring an accurate world model is challenging due to the non-stationarity of data generated from agent interaction, which typically causes catastrophic interference for neural networks (NN). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable: a model that is optimal for all previous experiences. Unfortunately, for NN-based models, FTL means re-training the NN on all accumulated data at every interaction step, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with efficient incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive encoding that is sparse in nature, which allows us to perform efficient online update even with very high dimensional nonlinear features. We present empirical results to validate the representation power of our encoding and verify that it is capable of learning incrementally under data covariate shift, a setting neural networks simply fail. Building on the demonstrated strength of our encoding, we further showcase its efficacy in MBRL settings, spanning both discrete and continuous control tasks. Our online world models, trained using a single pass of trajectory data, either surpass or match the capabilities of neural networks trained with replay and other continual learning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=JWrl5pJCnl": {
    "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Arm Actions with Large Language Model",
    "volume": "review",
    "abstract": "Foundation models have significantly advanced in various applications, including text-to-image generation, open-vocabulary segmentation, and natural language processing. This paper presents Instruct2Act, a framework that leverages Large Language Models (LLMs) to convert multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act uses LLMs to generate Python programs that form a comprehensive perception, planning, and action loop for robotic tasks. It uses pre-defined APIs to access multiple foundation models, with the Segment Anything Model (SAM) identifying potential objects and CLIP semantically classifying them. This approach combines the strengths of foundation models and robotic actions to transform complex high-level instructions into precise policy codes. Our approach is adaptable and versatile, capable of handling various instruction modalities and input types, and meeting specific task requirements. We validated the practicality and efficiency of our approach on robotic tasks including different tabletop and 6 Degree of Freedom(DoF) manipulation scenarios in both simulation and real-world environments. Furthermore, our zero-shot method surpasses many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://anonymous.4open.science/r/Instruct2Act, providing a solid benchmark for high-level robotic instruction tasks with diverse modality inputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=jUNSBetmAo": {
    "title": "Beyond Disentanglement: On the Orthogonality of Learned Representations",
    "volume": "review",
    "abstract": "Evaluating learned representations independently of designated downstream tasks is pivotal for crafting robust and adaptable algorithms across a diverse array of applications. Among such evaluations, the assessment of disentanglement in a learned representation has emerged as a significant technique. In a disentangled representation, independent data generating factors are encoded in mutually orthogonal subspaces, a characteristic enhancing numerous downstream applications, potentially bolstering interpretability, fairness, and robustness. However, a representation is often deemed well-disentangled if these orthogonal subspaces are one-dimensional and align with the canonical basis of the latent space – a powerful yet frequently challenging or unattainable condition in real-world scenarios – thus narrowing the applicability of disentanglement. Addressing this, we propose a novel evaluation scheme, Importance-Weighted Orthogonality (IWO), to gauge the mutual orthogonality between subspaces encoding the data generating factors, irrespective of their dimensionality or alignment with the canonical basis. For that matter, we introduce a new method, Latent Orthogonal Analysis (LOA), which identifies the subspace encoding each data generating factor and establishes an importance-ranked basis spanning it, thereby forming the foundational bedrock for IWO. Through extensive comparisons of learned representations from synthetic and real-world datasets, we demonstrate that, relative to existing disentanglement metrics, IWO offers a superior assessment of orthogonality and exhibits stronger correlation with downstream task performance across a spectrum of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=7QlKLvfVge": {
    "title": "Directional Rank Reduction for Backdoor Defense",
    "volume": "review",
    "abstract": "Recent studies have indicated the effectiveness of neuron pruning for backdoor defense. In this work, we explore the limitations of pruning-based defense through theoretical and empirical investigations. We argue that pruning-based defense necessitates the removal of neurons that affect normal performance when the effect of backdoor is entangled across normal neurons. To address this challenge, we propose an extended neuron pruning framework, named \\emph{Directional Rank Reduction (\\method)}. \\method consists of three procedures: orthogonal transformation, pruning, and inverse transformation. Through the transformation of the feature space prior to pruning, \\method is able to focus the trigger effects on a limited number of neurons for more efficient pruning with less damage, outperforming existing pruning-based defense strategies. We implement \\method using Sarle's Bimodality Coefficient (SBC) which is optimized as the criterion for the transformation matrix based on the separability assumption of benign and poisoned features. Extensive experimental results demonstrate the superiority of our method. On average, our approach substantially reduces the ASR by 4.5x and increases the ACC by 1.45\\% compared with the recently strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=yacRhge4zQ": {
    "title": "Regulation Games for Trustworthy Machine Learning",
    "volume": "review",
    "abstract": "Existing work on trustworthy machine learning (ML) often focuses on a single aspect of trust in ML (e.g., fairness, or privacy) and thus fails to obtain a holistic trust assessment. Furthermore, most techniques often fail to recognize that the parties who train models are not the same as the ones who assess their trustworthiness. We propose a framework that formulates trustworthy ML as a multi-objective multi-agent optimization problem to address these limitations. A holistic characterization of trust in ML naturally lends itself to a game theoretic formulation, which we call regulation games. We introduce and study a particular game instance, the SpecGame, which models the relationship between an ML model builder and regulators seeking to specify and enforce fairness and privacy regulations. Seeking socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of their objectives and avoids the inefficiencies of other equilibria. For instance, we show that for a gender classification application, the achieved privacy guarantee is 3.76× worse than the ordained privacy requirement if regulators do not take the initiative to specify their desired guarantees first. We hope that our framework can provide policy guidance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rEQ8OiBxbZ": {
    "title": "3D Molecular Pretraining via Localized Geometric Generation",
    "volume": "review",
    "abstract": "Self-supervised learning on 3D molecular structures has gained prominence in AI-driven drug discovery due to the high cost of annotating biochemical data. However, few have studied the selection of proper modeling semantic units within 3D molecular data, which is critical for an expressive pre-trained model as recognized in natural language processing and computer vision. In this study, we introduce \\textbf{L}ocalized G\\textbf{e}ometric \\textbf{G}enerati\\textbf{o}n (LEGO), a novel approach that treats tetrahedrons within 3D molecular structures as fundamental building blocks , leveraging their simplicity in three-dimension and their prevalence in molecular structural patterns such as carbon skeletons and functional groups. Inspired by masked language/image modeling, LEGO perturbs a portion of tetrahedrons and learns to reconstruct them in pretraining. The reconstruction of the noised local structures can be divided into a two-step process, namely spatial orientation prediction and internal arrangement generation. First, we predict the global orientation of the noised local structure within the whole molecule, equipping the model with positional information for these foundational components. Then, we geometrically reconstruct the internal arrangements of the noised local structures revealing their functional semantics. To address the atom-bond inconsistency problem in previous denoising methods and utilize the prior of chemical bonds, we propose to model the graph as a set of nodes and edges and explicitly generate the edges during pre-training. In this way, LEGO exploits the advantages of encoding structural geometry features as well as leveraging the expressiveness of self-supervised learning. Extensive experiments on molecular quantum and biochemical property prediction tasks demonstrate the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVSKuh9eK5": {
    "title": "CLIP Exhibits Improved Compositional Generalization Through Representation Disentanglement",
    "volume": "review",
    "abstract": "Vision-language models (VLMs), such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various flavors of distribution shifts. Recent studies attempted to investigate the leading cause of this property. In this work, we target the same goal, but focus on a certain type of distribution shift, in which test images contain unseen compositions of attribute-object pairs, but with the objects and attributes being individually seen during training. The models are expected to classify those images into the composition classes, i.e. attribute-object pairs, and also into object classes by ignoring attributes. We carefully designed an authentic image test dataset consisting of attributes for objects that are unlikely encountered in the CLIP training data. We found that the compositions diversity in the training data, as measured by normalized mutual information between objects and attributes, has a significant effect on the improvement of compositional generalization in the CLIP models. We found that image/text representation disentanglement with respect to the composition constituents also plays a key role in the improved generalization of these models. We notice that larger training datasets could potentially trigger emergence of such a disentanglement, as the compositions are typically more diverse in such datasets. We validate this hypothesis through different representation disentanglement metrics, including Z-Diff, and explicitness scores for various CLIPs. Our findings reveal a correlation between better OoD performance and higher scores in these disentanglement metrics, suggesting that improved disentanglement potentially contributes to enhanced compositional OoD generalization in VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=23OEmHVkpq": {
    "title": "Disentanglement Learning via Topology",
    "volume": "review",
    "abstract": "We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting to apply it for problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=E5CMyG6jl0": {
    "title": "Unified Language Model Alignment with Demonstration and Point-wise Human Preference",
    "volume": "review",
    "abstract": "Language model alignment is a cutting-edge technique in large language model training to align the model output to user's intent, e.g., being helpful and harmless. Recent alignment framework consists of two steps: supervised fine-tuning with demonstration data and preference learning with human preference data. Previous preference learning methods, such as RLHF and DPO, mainly focus on pair-wise preference data. However, in many real-world scenarios where human feedbacks are intrinsically point-wise, e.g., upvotes number or binary criterion, effective model alignment to user preference is under explored. In this paper, we fill this gap by developing a simplified tuning method for point-wise preference data. Further revelation on the connection between supervised fine-tuning and point-wise preference learning enables us to develop a unified framework for both human demonstration and point-wise preference data, which sheds new light on the construction of preference dataset. Extensive experiments demonstrate the superior performance and efficiency of our proposed methods. A new dataset with high-quality demonstration samples on harmlessness are constructed and made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOSsLRKQrX": {
    "title": "DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers",
    "volume": "review",
    "abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don't? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination over a small set of learned concepts. We perform an iterative refinement over these slots to extract a disentangled representation, which is then fed to a trans- former architecture to predict the next set of latent object representations. Since our loss is unsupervised, we need to align the output object masks with those ex- tracted from the ground truth image, and we design a novel permutation module to achieve this alignment by learning a canonical ordering. We perform a series of experiments demonstrating that our learned representations help predict future dynamics in the standard setting, where we test on the same environment as train- ing, and in the setting of transfer, where certain object combinations are never seen before. Our method outperforms existing baselines in terms of pixel prediction and deciphering the dynamics, especially in the zero-shot transfer setting where existing approaches fail miserably. Further analysis reveals that our learned representations indeed help with significantly better disentanglement of objects compared to existing techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=1FWDEIGm33": {
    "title": "Large Language Models as superpositions of cultural perspectives",
    "volume": "review",
    "abstract": "Large language models (LLMs) are sometimes viewed as if they were individuals, with given values, personality, knowledge and abilities. We argue that this \"LLM as an individual\" metaphor misrepresents their nature. As opposed to humans, they exhibit highly context-dependent values and personality traits. We propose a new metaphor, \"LLM as a superposition of perspectives\" : LLMs simulate a multiplicity of behaviors, e.g. expressing values, which can be triggered by a given context. As a case study, we conduct experiments on how values vary as a function of context using psychology questionnaires. Crucially, we demonstrate that changes in the context that are unrelated to the topic of questionnaires - varying articles, simulated conversations on other topics, and textual formats - all result in significant unwanted, hard-to-predict changes in the expressed values. We refer to this as the unexpected perspective shift effect. We discuss how this questions the interpretations of studies using psychology questionnaires (and more generally benchmarks) to draw general conclusions about LLMs' values, knowledge and abilities. Indeed, expressing some values on a questionnaire says little about which values a model would express in other contexts. Instead, models should be studied in terms of how the expressed values change over contexts in both expected and unexpected ways. Following this insight, we introduce the concept of perspective controllability - a model's affordance to adopt various perspectives. We conduct a systematic comparison of the controllability of 16 different models over three questionnaires (PVQ, VSM, IPIP) and different methods for inducing perspectives. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=dYjuJGTEbc": {
    "title": "An Enhanced Gromov-Wasserstein Barycenter Method for Graph-based Clustering",
    "volume": "review",
    "abstract": "Optimal Transport (OT) recently has gained remarkable success in machine learning. These methods based on the Gromov-Wasserstein (GW) distance have proven highly effective in capturing complex data topologies and underlying structures. More specifically, Gromov-Wasserstein Learning (GWL) has recently introduced a framework for graph partitioning by minimizing the GW distance. Various improved versions stemming from this framework have showcased state-of-the-art performance on clustering tasks. Building upon GW barycenter, we introduce a novel approach that significantly enhances other GW-based models flexibility by relaxing the target distribution (cluster size) in GWL and using a wide class of positive semi-definite matrices. We then develop an efficient algorithm to solve the resulting non-convex problem by utilizing regularization and the successive upper-bound minimization techniques. The proposed method exhibits the capacity to identify improved partition results within an enriched searching space, as validated by our developed theoretical framework and numerical experiments. Furthermore, we bridge the proposed model with the well-known clustering methods including Non-negative Matrix Factorization, Min-Cut, Max-Dicut and other GW-based models. This connection provides a new solution to these traditional clustering problems from the perspective of OT. Real data experiments illustrate our method outperforms state-of-the-art graph partitioning methods on both directed and undirected graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=eepoE7iLpL": {
    "title": "Enhancing Neural Subset Selection: Integrating Background Information into Set Representations",
    "volume": "review",
    "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=jXR5pjs1rV": {
    "title": "Everyone Deserves A Reward: Learning Customized Human Preferences",
    "volume": "review",
    "abstract": "Reward models (RMs) are essential for aligning large language models (LLMs) with human preferences to improve interaction quality. However, the real world is pluralistic, which leads to diversified human preferences with respect to different religions, politics, cultures, etc. Moreover, each individual can have their unique preferences on various topics. Neglecting the diversity of preferences, current human feedback aligning methods only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. To explore customized preference learning, we collect a domain-specific preference (DSP) dataset, which consists of comprehensive user queries and corresponding responses preferred from four practical domains. Besides, from the perspective of data efficiency, we propose a three-stage customized RM learning scheme, then empirically verify its effectiveness on both general preference datasets and our DSP set. Furthermore, we test multiple training and data strategies on the three learning stages. We find several ways to better preserve the general preferring ability while training the customized RMs, especially general preference enrichment, and customized preference imitation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=lK2V2E2MNv": {
    "title": "Bridging Vision and Language Spaces with Assignment Prediction",
    "volume": "review",
    "abstract": "While pretrained large language models (LLMs) excel in understanding linguistic contexts, it is still an open question: Can LLMs extend their capabilities beyond linguistic contexts to non-linguistic information? This paper introduces VLAP, a novel approach that bridges vision encoders and language models through assignment prediction. Since the LLMs interpret and reason linguistic information from correlations between word embeddings, we harness the well-established word embeddings to map visual representations into language space. Specifically, we simultaneously assign the visual and text representations to a set of word embeddings within LLMs. We propose a new training objective, optimal transport-based assignment prediction, to enforce the consistency of word assignments for paired multimodal data. This allows frozen LLMs to ground their word embedding space in visual data and use their robust semantic taxonomy visually. Moreover, VLAP is memory- and parameter-efficient in that it trains only a single linear layer, and works without extra embedding space (e.g. learnable prototypes) for the assignment prediction. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based methods across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3wL1tj3kqE": {
    "title": "Fair Domain Generalization with Arbitrary Sensitive Attributes",
    "volume": "review",
    "abstract": "We consider the problem of fairness transfer in domain generalization. Traditional domain generalization methods are designed to generalize a model to unseen domains. Recent work has extended this capability to incorporate fairness as an additional requirement. However, it is only applicable to a single, unchanging sensitive attribute across all domains. As a naive approach to extend it to a multi-attribute context, we can train a model for each subset of the potential set of sensitive attributes. However, this results in $2^n$ models for $n$ attributes. We propose a novel approach that allows any combination of sensitive attributes in the target domain. We learn two representations, a domain invariant representation to generalize the model's performance, and a selective domain invariant representation to transfer the model's fairness to unseen domains. As each domain can have a different set of sensitive attributes, we transfer the fairness by learning a selective domain invariant representation which enforces similar representations among only those domains that have similar sensitive attributes. We demonstrate that our method decreases the current requirement of $2^n$ models to $1$ to accomplish this task. Moreover, our method outperforms the state-of-the-art on unseen target domains across multiple experimental settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=9L9j5bQPIY": {
    "title": "Metanetwork: A novel approach to interpreting ANNs",
    "volume": "review",
    "abstract": "Recent work on mechanistic interpretability, which attempts to demystify the black box of artificial neural network (ANN) models through analytical approaches, has made it possible to give a qualitative interpretation of how each component of the model works, even without using the dataset the model was trained on. However, it is also desirable from the viewpoint of interpretability to understand the ability of the entire model; and considering the previous studies on task embedding, the ability of the entire model should also be represented by a vector. In this study we propose a novel approach to quantitatively interpreting an unseen ANN's ability based on relationships with other ANNs through obtaining a low-dimensional representation of ANNs by training a \"metanetwork\" that autoencodes ANNs. As a first-ever attempt of such an approach, we train a \"metanetwork\" to autoencode ANNs consisting of one fully-connected layer. We demonstrate the validity of our proposed approach by showing that a simple k-Nearest Neighbor classifier can successfully predict properties of the training datasets of unseen models from their embedded representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=gtkFw6sZGS": {
    "title": "Generative Judge for Evaluating Alignment",
    "volume": "review",
    "abstract": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding *generality* (i.e., assessing performance across diverse scenarios), *flexibility* (i.e., examining under different protocols), and *interpretability* (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, **Auto-J**, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, **Auto-J** outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://anonymous.4open.science/r/Auto-J-ICLR-ver-0107",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7vVWiCrFnd": {
    "title": "Rethinking and Extending the Probabilistic Inference Capacity of GNNs",
    "volume": "review",
    "abstract": "Designing expressive Graph Neural Networks (GNNs) is an important topic in graph machine learning fields. Despite the existence of numerous approaches proposed to enhance GNNs based on Weisfeiler-Lehman (WL) tests, what GNNs \\emph{can and cannot} learn still lacks a deeper understanding. This paper adopts a fundamentally different approach to examine the expressive power of GNNs from a probabilistic perspective. By establishing connections between GNNs' predictions and the central inference problems of probabilistic graphical models (PGMs), we can analyze previous GNN variants with a novel hierarchical framework and gain new insights into their node-level and link-level behaviors. Additionally, we introduce novel methods that can provably enhance GNNs' ability to capture complex dependencies and make complex predictions. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.6,
    "authors": []
  },
  "https://openreview.net/forum?id=lNIj5FdXsC": {
    "title": "Recurrent Distance-Encoding Neural Networks for Graph Representation Learning",
    "volume": "review",
    "abstract": "Graph neural networks based on iterative one-hop message-passing have been shown to struggle in harnessing information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but suffer from high computational complexity and have to rely on ad-hoc positional encodings to bake in the graph inductive bias. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models on sequential data: for a given target node, our model aggregates nodes at different distances and uses a parallelizable linear recurrent network over the chain of distances to provide a natural encoding of its neighborhood structure. With no need for positional encoding, we empirically show that the performance of our model is competitive compared with that of state-of-the-art graph transformers on various benchmarks, at a drastically reduced computational complexity. In addition, we show that our model is theoretically more expressive than one-hop message-passing neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=bDWXhzZT40": {
    "title": "Learning model uncertainty as variance-minimizing instance weights",
    "volume": "review",
    "abstract": "Predictive uncertainty--a model's self-awareness regarding its accuracy on an input--is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditional reweighting approach that captures predictive uncertainty using an auxiliary network, and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing dropout variance, an approximation of Bayesian predictive uncertainty, We show in controlled experiments that we effectively capture diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings–selective classification, label noise, domain adaptation, calibration–and across datasets–Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs, Imagenet-C,-A,-R, Clothing-1.6M, etc. For Diabetic Retinopathy, we see upto 3.4\\%/3.3\\% accuracy & AUC gains over SOTA in selective classification. We also improve upon large-scale pretrained models such as PLEX",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=DwcV654WBP": {
    "title": "TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale",
    "volume": "review",
    "abstract": "The ultimate goal for foundation models is realizing task-agnostic, i.e., supporting out-of-the-box usage without task-specific fine-tuning. Although breakthroughs have been made in natural language processing and image representation learning, it is still challenging for video models to reach it due to the increasing uncertainty of spatiotemporal signals. To ease training, existing works leverage image foundation models' prior knowledge and equip them with efﬁcient temporal modules. Despite the satisfactory fine-tuning performance, we empirically find they fall short of out-of-the-box usage, given the even degraded performance in zero-shot/linear protocols compared to their baseline counterparts. In this work, we analyze the factor that leads to degradation from the perspective of language supervision distortion. We argue that tuning a text encoder end-to-end, as done in previous work, is suboptimal since it may overfit in terms of styles, thereby losing its original generalization ability to capture the semantics of various language registers. The overfitted text encoder, in turn, provides a harmful supervision signal, degrading the video representation. To tackle this issue, we propose a degradation-free pre-training strategy to retain the generalization ability of the text encoder via freezing shallow layers while enabling the task-related semantics capturing in tunable deep layers. As for the training objective, we adopted the transcript sorting task in TVTS incorporated with masking techniques to enable scalable training. As a result, we produce a series of models, dubbed TVTSv2, with up to one billion parameters. We achieve new state-of-the-arts on various video benchmarks with a frozen backbone, surpassing the recent ImageBind, InternVideo, etc. Code and models will be released publicly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SLw9fp4yI6": {
    "title": "Controlled Text Generation via Language Model Arithmetic",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style and character becomes more important. In this work we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at [ANONYMIZED]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=My7lkRNnL9": {
    "title": "Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization",
    "volume": "review",
    "abstract": "Forward-only\" algorithms, which train neural networks while avoiding a backward pass, have recently gained attention as a way of solving the biologically unrealistic aspects of backpropagation. Here, we first address compelling challenges related to the ``forward-only\" rules, which include reducing the performance gap with backpropagation and providing an analytical understanding of their dynamics. To this end, we show that the forward-only algorithm with top-down feedback is well-approximated by an \"adaptive-feedback-alignment\" algorithm and we analytically track its performance during learning in a prototype high-dimensional setting. Then, we compare different versions of forward-only algorithms, focusing on the Forward-Forward and PEPITA frameworks, and we show that they share the same principles. Overall, our work unveils the connections between three key neuro-inspired learning rules, providing a link between \"forward-only\" algorithms, i.e., Forward-Forward and PEPITA, and an approximation of backpropagation, i.e., Feedback Alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=xibcBSuuq0": {
    "title": "Do not Start with Trembling Hands: Improving Multi-agent Reinforcement Learning with Stable Prefix Policy",
    "volume": "review",
    "abstract": "In multi-agent reinforcement learning (MARL), the $\\epsilon$-greedy method plays an important role in balancing exploration and exploitation during the decision-making process in value-based algorithms. However, we find that $\\epsilon$-greedy can be deemed as the concept of \"trembling hands\" in game theory when the agents are more in need of exploitation, which may result in the Trembling Hands Nash Equilibrium solution, a suboptimal policy convergence. Besides, eliminating the $\\epsilon$-greedy algorithm leaves no exploration and may lead to unacceptable local optimal policies. To address this dilemma, we use the previously collected trajectories to plan an existing optimal template as candidate policy, which we call \\textbf{Stable Prefix Policy}, in contrast to trembling hands. When the policy is close to the optimal policy, the agents follow the planned template, and when the policy still needs exploration, the agents will adaptively dropout. We scale our approach to various value-based MARL methods and empirically verify our method in a cooperative MARL task, SMAC benchmarks. Experimental results demonstrate that our method achieves not only better performance but also faster convergence speed than baseline algorithms within 2M time steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0wJ5oCPdB": {
    "title": "Chain-of-Symbol Prompting for Spatial Relationships in Large Language Models",
    "volume": "review",
    "abstract": "While conventional Chain-of-Thought prompting shows promising performance on various language tasks for LLMs, the spatial scenarios are nearly unexplored. In this paper, we first investigate the performance of LLMs on complex spatial planning and understanding tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act or reason correspondingly in text. By evaluating on classic spatial planning scenarios through natural language descriptions, we found that current popular LLMs such as ChatGPT still lack abilities to handle spatial relationships in texts. This arises a question -- do the natural language is the best way to represent complex spatial environments for LLMs, or maybe other alternatives such as symbolic representations are both more efficient and effective for LLMs? To this end, we propose a novel method called **CoS** (**C**hain-**o**f-**S**ymbol Prompting) that represents the spatial relationships with condensed symbols during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. Extensive experiments indicate that CoS clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting described in natural langauge in all three spatial planning tasks and existing spatial QA benchmark, with even fewer tokens used in the inputs compared with CoT. The performance gain is strong, by up to 60.8\\% accuracy (from 31.8\\% to 92.6\\%) on Brick World scenarios for ChatGPT. CoS also reduces the number of tokens in the prompt obviously, by up to 65.8\\% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on the Brick World task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=IefMMX12yk": {
    "title": "Lightweight Graph Neural Network Search with Graph Sparsification",
    "volume": "review",
    "abstract": "Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constraint scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose Lightweight Graph Neural Architecture Search with Graph SparsIfication and Network Pruning (GASSIP). In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=pYmQId95iR": {
    "title": "RLP: A reinforcement learning benchmark for neural algorithmic reasoning",
    "volume": "review",
    "abstract": "Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Although Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments, its potential in learning generalizable and complex algorithms remains largely unexplored. To evaluate the current state of algorithmic reasoning in RL, we introduce an RL benchmark based on Simon Tatham's Portable Puzzle Collection. This benchmark contains 40 diverse logic puzzles of varying complexity levels, which serve as captivating challenges that test cognitive abilities, particularly in neural algorithmic reasoning. Our findings demonstrate that current RL approaches struggle with neural algorithmic reasoning, emphasizing the need for further research in this area. All of the software, including the environment, is available at https://github.com/rlppaper/rlp",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=AZGIwqCyYY": {
    "title": "Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning",
    "volume": "review",
    "abstract": "Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. While effective, these methods are confined to the system domain in which the type of system remains consistent and thus cannot ensure the adaptation to new, or unseen physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot guarantee the accurate prediction of the behavior of a two-body system or any other system with different physical laws. In this work, we take a significant leap forward by targeting cross domain generalization within the field of Hamiltonian dynamics. We model our system with a graph neural network and employ a meta learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a unified Hamiltonian representation that is generalizable across multiple system domains, thereby overcoming the limitations of system-specific models. We validate our approach on a dataset comprising various physical systems and evaluate its adaptability to a new type of dynamical system with previously unseen physics. Our results demonstrate that the meta trained model not only adapts effectively to new systems but also captures a generalized Hamiltonian representation that is consistent across different physical domains. Overall, through the use of meta learning, we offer a framework that achieves cross domain generalization, providing a step towards a unified model for understanding a wide array of dynamical systems via deep learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=b0IRscfEOb": {
    "title": "ReLiK: Retrieve, Read and LinK: Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
    "volume": "review",
    "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in various applications such as Information Retrieval, Question Answering, and Knowledge Graph Construction. However, existing approaches often suffer from either a lack of flexibility, low-performance issues, or computational inefficiency. In this paper, we propose ReLiK, a Retriever-Reader architecture, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass in contrast with previous Retriever-Reader-based methods, which necessitate a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed with respect to other competitors. Finally, we propose a model for closed Information Extraction (cIE), i.e. EL + RE, which sets a new state of the art by employing a shared Reader that simultaneously extracts entities and relations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=BTKAeLqLMw": {
    "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
    "volume": "review",
    "abstract": "Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present Deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA models using data samples automatically selected with our proposed approach. When assessed through both automatic metrics and human evaluation, Deita performs better or on par with the state-of-the-art open-source alignment models such as Vicuna and WizardLM with only 6K training data samples -- 10x less than the data used in the baselines. We anticipate this work to provide clear guidelines and tools on automatic data selection, aiding researchers and practitioners in achieving data-efficient alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=AJBkfwXh3u": {
    "title": "Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks",
    "volume": "review",
    "abstract": "Dynamic Graph Neural Networks (DyGNNs) have gained significant popularity in the research of dynamic graphs, but are limited by the low transparency, such that human-understandable insights can hardly be drawn from their predictions. Although a number of existing research have been devoted to investigating the interpretability of graph neural networks (GNNs), achieving the interpretability of DyGNNs is pivotally challenging due to the complex spatial-temporal correlations in dynamic graphs. To this end, we propose an innovative causality-inspired generative model based on structural causal model (SCM), which explores the underlying philosophies of DyGNN predictions by identifying the trivial, static, and dynamic causal relationships. To reach this goal, two critical tasks need to be accomplished including (1) disentangling the complex causal relationships, and (2) fitting the spatial-temporal explanations of DyGNNs in the SCM architecture. To tackle these challenges, the proposed method incorporates a contrastive learning module to disentangle trivial and causal relationships, and a dynamic correlating module to disentangle dynamic and static causal relationships, respectively. A dynamic VGAE-based framework is further developed, which generates causal-and-dynamic masks for spatial interpretability, and recognizes dynamic relationships along the time horizon through causal invention for temporal interpretability. Comprehensive experiments have been conducted on both synthetic and real-world datasets, where our approach yields substantial improvements, thereby demonstrating significant superiority",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmsqb6WpLz": {
    "title": "Dissecting learning and forgetting in language model finetuning",
    "volume": "review",
    "abstract": "Finetuning language models on domain-specific corpus is a common approach to enhance their domain knowledge and capability. While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities. In this study, we analyze the effects of finetuning on language models by dissecting its impacts on the modeling of topic, style, and factual knowledge in text. Our method uses instruction-following LLMs such as ChatGPT to auto-generate controlled-variable text examples which we use to probe the model. Our findings reveal that finetuning results in significant shifts in the language model's topic and style priors, while actual knowledge learning only contributes to a small fraction of the total probability change. Analysis shows that the adaptation of topic and style priors behave akin to learning simple features: they are learned rapidly and require little model capacity. They are also learned independently and primarily at the beginning of a text sequence. In contrast, factual knowledge is learned stably but slowly and requires significant model capacity to learn. The research offers insights and understanding into the finer dynamics of learning and forgetting in language models, and can potentially inform future research on improving domain adaptation and addressing the challenges of forgetting in continual learning of language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RpKA1wqgk0": {
    "title": "MetaFormer with Holistic Attention Modelling Improves Few-Shot Classification",
    "volume": "review",
    "abstract": "Pre-trained vision transformers have revolutionized few-shot image classification, and it has been recently demonstrated that the previous common practice of meta-learning in synergy with these pre-trained transformers still holds significance and contributes to further advancing their performance. Unfortunately, the majority of working insights such as task conditioning are specifically tailored for convolutional neural networks, thus failing to translate effectively to vision transformers. This work sets out to bridge this gap via a coherent and lightweight framework called MetaFormer, which maintains compatibility with off-the-shelf pre-trained vision transformers. The proposed MetaFormer consists of two attention modules, i.e., the Sample-level Attention Module (SAM) and the Task-level Attention Module (TAM). SAM works in conjunction with the patch-level attention in Transformers to enforce consistency in the attended features across samples within a task, while TAM regularizes learning of the current task with an attended task in the pool. Empirical results on four few-shot learning benchmarks, i.e., miniImageNet, tieredImageNet, CIFAR-FS, and FC100, showcase that our approach achieves the new state-of-the-art at a very modest increase in computational overhead. Furthermore, our approach excels in cross-domain task generalization scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xNdE7RiRyP": {
    "title": "TinyTrain: Deep Neural Network Training at the Extreme Edge",
    "volume": "review",
    "abstract": "On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\\geq$10\\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0\\% in accuracy, while reducing the backward-pass memory and computation cost by up to 1,098$\\times$ and 7.68$\\times$, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\\times$ faster and 3.5$\\times$ more energy-efficient training over status-quo approaches, and 2.23$\\times$ smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=TPZRq4FALB": {
    "title": "Test-time Adaption against Multi-modal Reliability Bias",
    "volume": "review",
    "abstract": "Test-time adaption (TTA) has emerged as a new paradigm for reconciling distribution shifts between domains without accessing source data. However, existing TTA methods mainly concentrate on uni-modal tasks, overlooking the complexity in multi-modal scenarios. In this paper, we delve into the multi-modal test-time adaption and reveal a new challenge named reliability bias. Different from the definition of traditional distribution shifts, reliability bias refers to the information discrepancies across different modalities derived from intra-modal distribution shifts. To solve the challenge, we propose a novel method, dubbed reliable fusion and robust adaption (RFRA). On the one hand, unlike the existing TTA paradigm that mainly repurposes the normalization layers, RFRA employs a new paradigm that modulates the attention between modalities in a self-adaptive way, supporting reliable fusion against reliability bias. On the other hand, RFRA adopts a novel objective function for robust multi-modal adaption, where the contributions of confident predictions could be amplified and the negative impacts of noisy predictions could be mitigated. Moreover, we introduce two new benchmarks to facilitate comprehensive evaluations of multi-modal TTA under reliability bias. Extensive experiments on the benchmarks not only verify the effectiveness of our method but also give some new observations to the community. The code and benchmarks will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPyLDIVBNP": {
    "title": "Fully Identical Initialization",
    "volume": "review",
    "abstract": "Deep neural networks (DNNs) have achieved numerous remarkable accomplishments in practice. The success of these networks hinges on effective initialization methods, which are vital for ensuring stable and rapid convergence during training. Recently, initialization methods that maintain identity transition within layers have shown good efficiency in network training. These techniques (e.g., Fixup) set specific weights to zero to achieve identity control. However, settings of remaining weight (e.g., Fixup uses random values to initialize non-zero weights) will affect inductive bias that is achieved only by a zero weight, which may be harmful to training. Addressing this concern, we introduce fully identical initialization (IDInit), an innovative method that preserves identity in both the main and sub-stem layers of residual networks. IDInit employs a padded identity-like matrix to overcome rank constraints in non-square weight matrices. Furthermore, we show a convergence problem of an identity matrix can be solved by adding a momentum term into the optimizer. Additionally, we explore enhancing the universality of IDInit by processing higher-order weights and addressing dead neuron problems. IDInit is a straightforward yet effective initialization method, promising improved convergence, stability, and performance across various settings, including large-scale datasets and deep models. It stands as a novel solution for initializing non-standard weight matrices, offering significant advantages in network training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=30L0rr9W8A": {
    "title": "LatentCBF: A Control Barrier Function in Latent Space for Safe Control",
    "volume": "review",
    "abstract": "Safe control is crucial for safety-critical autonomous systems that are deployed in dynamic and uncertain environments. Quadratic-programming-control-barrier-function (QP-CBF) is becoming a popular tool for safe controller synthesis. Traditional QP-CBF relies on explicit knowledge of the system dynamics and access to all states, which are not always available in practice. We propose LatentCBF (LCBF), a control barrier function defined in the latent space, which only needs an agent's observations, not full states. The transformation from observations to latent space is established by a Lipschitz network-based AutoEncoder. In addition, the system dynamics and control barrier functions are all learned in the latent space. We demonstrate the efficiency, safety, and robustness of LCBFs in simulation for quadrotors and cars",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=78iGZdqxYY": {
    "title": "Mirage: Model-agnostic Graph Distillation for Graph Classification",
    "volume": "review",
    "abstract": "GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE's superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6Htk1rwkK": {
    "title": "Exploring mechanisms of Neural Robustness: probing the bridge between geometry and spectrum",
    "volume": "review",
    "abstract": "Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. In cancer detection, for example, slight image alterations can misclassify benign moles as malignant. Biological neural systems do not have such issues. Thus, understanding the biological mechanisms of robustness is an important step towards building trustworthy and safe systems. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=k65Nh7IV6X": {
    "title": "Two-shot learning of continuous interpolation using a conceptor-aided recurrent autoencoder",
    "volume": "review",
    "abstract": "Generalizing from only two time series towards unseen intermediate patterns poses a significant challenge in representation learning. In this paper, we introduce a novel representation learning algorithm, \"Conceptor-Aided Recurrent Autoencoder\" (CARAE), which leverages a conceptor-based regularization to learn to generate a continuous spectrum of intermediate temporal patterns while just being trained on two distinct examples. Here, conceptors, a linear subspace characterization of neuron activations, are employed to impose a low-dimensional geometrical bottleneck on the neural dynamics. During training, CARAE assembles a continuous and stable manifold between the two trained temporal patterns. Exploiting this manifold in the inference, CARAE facilitates continuous and phase-aligned interpolation between temporal patterns that are not linked within the training data. We demonstrate the effectiveness of the CARAE framework through comprehensive experiments on temporal pattern generation tasks and the generation of novel complex motion patterns based on the MoCap data set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9k0krNzvlV": {
    "title": "On the Learnability of Watermarks for Language Models",
    "volume": "review",
    "abstract": "Language model watermarking enables reliable detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can damage the reputation of a victim model by spoofing its watermark and generating harmful watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three distinct decoding-based watermarking strategies, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=n1LiKueC4F": {
    "title": "Personalized Language Generation via Bayesian Metric Augmented Retrieval",
    "volume": "review",
    "abstract": "Our paper presents a Bayesian adaptation of Retrieval Augmented Generation (RAG) designed to capture the characteristics of each user, encompassing factors such as their educational background and professions. We model each individual's characteristics using specific perturbations of the local metric of the embedding space. This perturbation introduces a crucial shift in the distance evaluation between the query's and the document's embedding, leading to different pertinent rankings of the retrieved documents. We propose a Bayesian learning procedure that assimilates user feedback and continuously enhances our estimation of the user-specific metric. In the beginning, when there is no information about the user, we use a diverse retrieval method for generation. After this burn-in phase, we learn a Bayesian posterior estimate of the metric, and inject this metric into the nearest neighbor search for document retrieval. This additional layer of metric information acquisition leads to empirical improvement in the retrieval quality and in the performance of the generated text on multiple concept explanation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=D0zeqL7Vnz": {
    "title": "Prompt Sketching for Large Language Models",
    "volume": "review",
    "abstract": "Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=elMKXvhhQ9": {
    "title": "Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision",
    "volume": "review",
    "abstract": "Graph Anomaly Detection (GAD) has surfaced as a significant field of research, predominantly due to its substantial influence in production environments. Although existing approaches for node anomaly detection have shown effectiveness, they have yet to fully address two major challenges: operating in settings with limited supervision and managing class imbalance effectively. In response to these challenges, we propose a novel model, ConsisGAD, which is tailored for GAD in scenarios characterized by limited supervision and is anchored in the principles of consistency training. Under limited supervision, ConsisGAD effectively leverages the abundance of unlabeled data for consistency training by incorporating a novel learnable data augmentation mechanism, thereby introducing controlled noise into the dataset. Moreover, ConsisGAD takes advantage of the variance in homophily distribution between normal and anomalous nodes to craft a simplified GNN backbone, enhancing its capability to distinguish effectively between these two classes. Comprehensive experiments on several benchmark datasets validate the superior performance of ConsisGAD in comparison to state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=O3BaKCxTAO": {
    "title": "OPTIMIZING STABILIZATION IN SINGULARLY PER- TURBED PROBLEMS WITH SUPG SCHEME",
    "volume": "review",
    "abstract": "This paper introduces ConvStabNet, a convolutional neural network that predicts optimal stabilization parameters for the Streamline Upwind Petrov Galerkin method (SUPG) stabilization scheme. To enhance the accuracy of SUPG in solving partial differential equations (PDE) with interior and bound- ary layers, ConvStabNet incorporates a loss function that combines a strong residual component and a cross-wind derivative term. ConvStabNet utilizes a shared parameter scheme, enabling the network to learn the correlations between cell properties and their respective stabilization parameters while effectively managing the parameter space. Comparative evaluations against state-of-the-art neural network solvers based on variational formulations demonstrate the superior performance of ConvStabNet. The results affirm ConvStabNet as a promising approach for accurately predicting stabilization parameters in SUPG, thereby establishing it as an improvement over neural network-based SUPG solvers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Iyve2ycvGZ": {
    "title": "Bellman Optimal Step-size Straightening of Flow-Matching Models",
    "volume": "review",
    "abstract": "Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the fine-tuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS) technique for distilling flow-matching generative models: it aims specifically for a few-step efficient image sampling while adhering to a computational budget constraint. First, this technique involves a dynamic programming algorithm that optimizes the step sizes of the pretrained network. Then, it refines the velocity network to match the optimal step sizes, aiming to straighten the generation paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and image quality. Our results reveal that BOSS achieves substantial gains in efficiency while maintaining competitive sample quality, effectively bridging the gap between low-resource constraints and the demanding requirements of flow-matching generative models. Our paper also fortifies the responsible development of artificial intelligence, offering a more sustainable generative model that reduces computational costs and environmental footprints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=j7S7o6ROn9": {
    "title": "Distributional Structured Pruning by Lower bounding the Total Variation Distance using Witness functions",
    "volume": "review",
    "abstract": "Recent literature introduced the notion of distributional structured pruning (DSP) in Deep Neural Networks by retaining discriminative filters that can effectively differentiate between classes. Crucial to DSP is the ability to estimate the discriminative ability of a filter, which is defined by the minimum pairwise Total Variation (TV) distance between the class-conditional feature distributions. Since the computation of TV distance is generally intractable, existing literature assumes the class-conditional feature distributions are Gaussian, thereby enabling the use of the tractable Hellinger lower bound to estimate discriminative ability. However, the Gaussian assumption is not only restrictive but also does not typically hold. In this work, we address this gap by deriving a lower bound on TV Distance which depends only on the moments of witness functions. Using linear witness functions, the bound establishes new relationships between the TV Distance and well-known discriminant-based classifiers, such as Fisher Discriminants and Minimax Probability machines. The lower bounds are used to produce a variety of pruning algorithms called WitnessPrune by varying the choice of witness function. We empirically show that we can achieve up to 7\\% greater accuracy for similar sparsity in hard-to-prune layers using a polynomial witness function as compared to the state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Yg5eylBHe": {
    "title": "ZGS-Based Event-Driven Algorithms for Bayesian Optimization in Fully Distributed Multi-Agent Systems",
    "volume": "review",
    "abstract": "Bayesian optimization (BO) is a well-established framework for globally optimizing expensive-to-evaluate black-box functions with impressive efficiency. Although numerous BO algorithms have been developed for the centralized machine learning setting and some recent works have extended BO to the tree-structured federated learning, no previous studies have investigated BO within a fully distributed multi-agent system (MAS) in the field of distributed learning (DL). Addressing this gap, we introduce and investigate a novel paradigm, Distributed Bayesian Optimization (DBO), in which agents cooperatively optimize the same costly-to-evaluate black-box objectives. An innovative generalized algorithm, Zero-Gradient-Sum-Based Event-Driven Distributed Lower Confidence Bound (ZGS-ED-DLCB), is proposed to overcome the significant challenges of DBO and DL: We (a) adopt a surrogate model based on random Fourier features as an approximate alternative to a typical Gaussian process to enable the exchange of local knowledge between neighboring agents, and (b) employ the event-driven mechanism to enhance communication efficiency in MASs. Moreover, we propose a novel generalized fully distributed convergence theorem, which represents a substantial theoretical and practical breakthrough wrt the ZGS-based DL. The performance of our proposed algorithm has been rigorously evaluated through theoretical analysis and extensive experiments, demonstrating substantial advantages over the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=fGskrC9Wy1": {
    "title": "Boosted Long Short-Term Memory with Additional Inner Layers",
    "volume": "review",
    "abstract": "Long Short-Term Memory (LSTM) is widely known as a powerful type of Recurrent Neural Network, allowing it to achieve great results on many difficult sequential data tasks. Numerous experiments have shown that adding more complexity to neural network architectures may lead to a significant increase in performance that outweighs the incurred costs of an upgraded structure. In this paper, we propose a Boosted LSTM model created by adding layers inside the LSTM unit to optimize the model by enhancing its memory and reasoning capabilities. We evaluated the performance of different versions of Boosted LSTM architectures using three empirical tasks, studying the impact of different placements of additional layers, the activation functions used in the additional layers, and the model's hidden units. The experiments have shown that the Boosted LSTM unit, which uses Exponential Linear Unit as its boosted layers activation function, performs better than the similar models created from the simple LSTM units while often taking fewer epochs to achieve similar or better results, usually in a smaller number of training epochs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Xsrsj3cne4": {
    "title": "An Optimization-Based Framework for Adversarial Defence of Graph Neural Networks Via Adaptive Lipschitz Regularization",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) have exhibited exceptional performance across diverse application domains by harnessing the inherent interconnectedness of data. However, the emergence of adversarial attacks targeting GNNs poses a substantial and pervasive threat, compromising their overall performance and learning capabilities. While recent efforts have focused on enhancing GNN robustness from both data and architectural perspectives, more attention should be given to overall network stability in the face of input perturbations. Prior methods addressing network stability have routinely employed gradient normalization as a fundamental technique. This study introduces a unifying approach, termed as AdaLip, for adversarial training of GNNs through an optimization framework that leverages the explicit Lipschitz constant. By seamlessly integrating graph denoising and network regularization, AdaLip offers a comprehensive and versatile solution, extending its applicability and enabling robust regularization for diverse neural network architectures. Further, we develop a provably convergent iterative algorithm, leveraging block majorization-minimization, graph learning, and alternate minimization techniques to solve the proposed optimization problem. Simulation results on real datasets demonstrate the efficacy of AdaLip over state-of-the-art defence methods across diverse classes of poisoning attacks. On select datasets, AdaLip demonstrates GCN performance improvements of up to 20\\% against modification attacks and approximately 10\\% against injection attacks. Remarkably, AdaLip achieves a similar performance gain on heterophily graph datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=M11LONBkx1": {
    "title": "Diffusion with Synthetic Features: Feature Imputation for Graphs with Partially Observed Features",
    "volume": "review",
    "abstract": "In this paper, we tackle learning tasks on graphs with missing features, improving the applicability of graph neural networks to real-world graph-structured data. Previous diffusion-based imputation methods overlook the presence of channels with low-variance features, and these channels contribute very little to the performance in graph learning tasks. To overcome this issue, we propose a new diffusion-based imputation scheme using synthetic features in addition to observed features. The proposed scheme first identifies channels with low-variance features via pre-diffusion and generates a synthetic feature for a randomly chosen node in each low-variance channel. Then, our diffusion process spreads the synthetic features widely while considering observed features simultaneously. Extensive experiments on graphs with various rates of missing features demonstrate the effectiveness of our scheme, achieving state-of-the-art performance in both semi-supervised node classification and link prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2wwPG1wpsu": {
    "title": "LST-Bench:A Benchmark for long sequence time-series forecasting Task",
    "volume": "review",
    "abstract": "This paper introduces LST-Bench, a comprehensive benchmark designed for evaluating long sequence time-series forecasting(LSTF) models. This benchmark has been developed in response to recent advancements in deep learning methods in the field of LSTF tasks. LST-Bench includes Transformer-based, MLP-based, CNN-based, and RNN-based models, evaluating the performance of 11 major forecasting models on a set of commonly used 7 datasets and 7 new datasets that we have introduced. We conduct a thorough analysis of the experimental results, including the overall prediction performance of models and their generalization across different prediction lengths and datasets. Notably, we found that regardless of the model architecture, the phenomenon referred to as \"Degeneracy\" occurs when the model's predictions consistently maintain a low Mean Squared Error value but are characterized by repetitive and simplistic pattern generation, thus losing the meaningfulness of the predictions. Also, the model's optimal performance is very close to its performance after training for just one epoch. These two phenomenons emphasize the need for further investigation. Our LST-Bench will serve as a valuable resource for advancing research in the field of time series forecasting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6ARlSgun7J": {
    "title": "Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction",
    "volume": "review",
    "abstract": "Extreme Classification (XC) architectures, which utilize a massive one-vs-all classifier layer at the output, have demonstrated remarkable performance on problems with large label sets. Nonetheless, these have also been observed to falter on tail labels with few representative samples. This phenomenon has been attributed to factors such as classifier over-fitting and missing label bias, and solutions involving regularization and loss re-calibration have been developed. This paper explores the impact of label variance, a previously unexamined factor, on the tail performance in extreme classifiers. Label variance refers to the imprecision introduced in the ground truth when sampling it from a complex underlying distribution - a common phenomenon in most XC datasets. This compromises the quality of trained models, with a pronounced impact on the classifiers for infrequently sampled tail labels. This paper presents a method to systematically reduce label variance in XC by effectively utilizing the capabilities of an additional, tail-robust teacher model. It proposes a principled knowledge distillation framework, \\model, which enhances tail performance in extreme classifiers, with formal guarantees on generalization. Finally, we introduce an effective instantiation of this framework that employs a specialized Siamese teacher model. This model excels in tail accuracy and significantly enhances the quality of student one-vs-all classifiers. Comprehensive experiments are conducted on a diverse set of XC datasets which demonstrate that \\model can enhance tail performance by around 5\\% and 6\\% points in PSP and Coverage metrics respectively when integrated with leading extreme classifiers. Moreover, when added to the top-performing Renée classifier, it establishes a new state-of-the-art. Extensive ablations and analysis substantiate the efficacy of our design choices. Code and datasets will be released for research purposes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LegZeFYugN": {
    "title": "Time2Image: A Unified Image Representation Framework for Time Series Classification",
    "volume": "review",
    "abstract": "Time Series Classification (TSC) is a crucial and challenging task that holds significant importance across various domains, of which one of the kernel ingredients is to construct a suitable time series representation for better feature capture. However, extracting informative and robust time series representation with good generalization potential is still a challenging problem. To address this issue, we propose Time2Image, a novel image-based representation framework for TSC. At the heart of our framework is a proposed Adaptive Time Series Gaussian Mapping (ATSGM) module for robust time series encoding in 2D image structure, based on which we employ Vision Transformer (ViT) for subsequent classification tasks considering its prominent long-dependency modeling capability. Experiments were conducted on all 158 public time series datasets from UCR/UEA covering diverse domains, among which our method achieves top 1 performance in 86 datasets compared with existing State-Of-The-Art (SOTA) methods. In addition, our framework flexibly allows handling both univariate and multivariate time series with unequal length across different domains and takes inherent advantage of generalization ability due to our proposed ATSGM representation method. The source code will be publicly available soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=4IxtmklIym": {
    "title": "FruitBin: A tunable large-scale dataset for advancing 6D Pose estimation in fruit bin picking automation",
    "volume": "review",
    "abstract": "Bin picking is a ubiquitous application spanning across diverse industries, demanding automated solutions facilitated by robots. These automation systems hinge upon intricate components, including object instance-level segmentation and 6D pose estimation, which are pivotal for predicting future grasping and manipulation success. Contemporary computer vision approaches predominantly rely on deep learning methodologies and necessitate access to extensive instance-level datasets. However, prevailing datasets and benchmarks tend to be confined to oversimplified scenarios, such as those with singular objects on tables or low levels of object clustering. In this research, we introduce FruitBin. It emerges as an unparalleled resource, boasting an extensive collection of over a million images and 40 million instance-level 6D poses. Additionally FruitBin differs with other datasets whith its inclusive representation of a wide spectrum of challenges, encompassing symmetric and asymmetric fruits, objects with and without discernible texture, and diverse lighting conditions, all enriched with extended annotations and metadata. Leveraging the inherent challenges and the sheer scale of FruitBin, we highlight its potential as a versatile benchmarking tool that can be customized to suit various evaluation scenarios. As a demonstration of this adaptability, we have created two distinct types of benchmarks: one centered on novel scene generalization and another focusing on novel camera viewpoint generalization. Both benchmark types offer four levels of occlusion to facilitate the study of occlusion robustness. Notably, our study showcases the difficulty of FruitBin dataset, with two baseline 6D pose estimation models, one utilizing RGB images and the other RGB-D data, across these eight distinct benchmarks. FruitBin emerges as a pioneering dataset distinguishing itself by seamlessly integrating with robotic software. That enable direct testing of trained models in dynamic grasping tasks for the purpose of robot learning. Samples of the dataset with its associated code are provided in the supplementary materials. FruitBin promises to be a catalyst for advancing the field of robotics and automation, providing researchers and practitioners with a comprehensive resource to push the boundaries of 6D pose estimation in the context of fruit bin picking and beyond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=lmYGRGyL4i": {
    "title": "Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential",
    "volume": "review",
    "abstract": "In the field of deep graph generative models, two families coexist: one-shot models, which fill the graph content in one go given a number of nodes, and sequential models, where new nodes and edges are inserted sequentially and autoregressively. Recently, one-shot models are seeing great popularity due to their rising sample quality and lower sampling time compared to the more costly autoregressive models. With this paper we unify the two worlds in a single framework, unlocking the whole spectrum of options where one-shot and sequential models are but the two extremes. We use the denoising diffusion models' theory to develop a node removal process, which destroys a given graph through many steps. An insertion model reverses this process by predicting how many nodes have been removed from the intermediate subgraphs. Then, generation happens by iteratively adding new blocks of nodes, with size sampled from the insertion model, and content generated using any one-shot model. By adjusting the knob on node removal, the framework allows for any degree of sequentiality, from one-shot to fully sequential, and any node ordering, e.g., random and BFS. Based on this, we conduct the first analysis of the sample quality-time trade-off across a range of molecular and generic graphs datasets. As a case study, we adapt DiGress, a diffusion-based one-shot model, to the whole spectrum of sequentiality, reaching new state of the art results, and motivating a renewed interest in developing autoregressive graph generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=P2gnDEHGu3": {
    "title": "Summing Up the Facts: Additive Mechanisms behind Factual Recall in LLMs",
    "volume": "review",
    "abstract": "How do large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form \\tokens{Fact: The Colosseum is in the country of}. We find that the mechanistic story behind factual recall is more complex than previously thought -- We show there exist four distinct and independent mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the \\textbf{additive motif}: models compute correct answers through adding together multiple independent contributions; the contributions from each mechanism are insufficient alone, but together they constructively interfere on the correct attribute when summed. In addition, we extend the method of direct logit attribution to attribute a head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0JTwZ30qPH": {
    "title": "Task-Oriented Multi-View Representation Learning",
    "volume": "review",
    "abstract": "Multi-view representation learning aims to learn a high-quality unified representation for an entity from its multiple observable views to facilitate the performance of downstream tasks. A typical multi-view representation learning framework consists of four main components: View-specific encoding, Single-view learning (SVL), Multi-view learning (MVL), and Fusion. Recent studies achieve promising performance by carefully designing SVL and MVL constraints, but almost all of them ignore the basic fact that \\textit{effective representations are different for different tasks, even for the same entity}. To bridge this gap, this work proposes a \\textbf{T}ask-\\textbf{O}riented \\textbf{M}ulti-\\textbf{V}iew \\textbf{R}epresentation \\textbf{L}earning (TOMRL) method, where the key idea is to modulate features in the View-specific encoding and Fusion modules according to the task guidance. To this end, we first design a gradient-based embedding strategy to flexibly represent multi-view tasks. After that, a meta-learner is trained to map the task embedding into a set of view-specific parameters and a view-shared parameter for modulation in the Encoding and Fusion modules, respectively. This whole process is formalized as a nested optimization problem and ultimately solved by a bi-level optimization scheme. Extensive experiments on four multi-view datasets validate that our TOMRL consistently improves the performance of most existing multi-view representation learning approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILStlRb1Sp": {
    "title": "Understanding the Mechanics and Dynamics of Memorisation in Large Language Models: A Case Study with Random Strings",
    "volume": "review",
    "abstract": "Understanding whether and to what extent large language models (LLMs) have memorised training data has important implications for the privacy of its training data and the reliability of its generated output. In this work, we focus on the more foundational question of how LLMs memorise training data. To this end, we systematically train LLMs of different sizes to memorise random token strings of different lengths and different entropies (i.e., sampled from different alphabet distributions) and study their ability to recall the strings. We observe many striking memorisation dynamics including (i) memorisation in phases with the alphabet distributions in the random strings being learnt before their relative positions in the string are memorised and (ii) memorisation in parts at the granularity of individual tokens, but not necessarily in the order in which they appear in the string. Next, we investigate memorisation mechanics by checking to what extent different parts of a token's prefix in the string are necessary and sufficient to recollect the token. We leverage our insights to explain the dynamics of memorising strings and we conclude by discussing the implications of our findings for quantifying memorisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hm6maU150b": {
    "title": "NeFL: Nested Federated Learning for Heterogeneous Clients",
    "volume": "review",
    "abstract": "Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies tackle the system heterogeneity by splitting a model into submodels, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting forward propagation of models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels of different architecture, we decouple a few parameters from parameters being trained for each submodel. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant performance gains, especially for the worst-case submodel. Furthermore, we demonstrate NeFL aligns with recent studies in FL, regarding pre-trained models of FL and the statistical heterogeneity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Ebt7JgMHv1": {
    "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
    "volume": "review",
    "abstract": "Mechanistic interpretability aims to attribute high-level model behaviors to specific, interpretable learned features. It is hypothesized that these features manifest as directions or low-dimensional subspaces within activation space. Accordingly, recent studies have explored the identification and manipulation of such subspaces to reverse-engineer computations, employing methods such as activation patching. In this work, we demonstrate that naïve approaches to subspace interventions can give rise to interpretability illusions. Specifically, even if patching along a subspace has the intended end-to-end causal effect on model behavior, this effect may be achieved by activating \\emph{a dormant parallel pathway} using a component that is \\textit{causally disconnected} from the model output. We demonstrate this in a mathematical example, realize the example empirically in two different settings (the Indirect Object Identification (IOI) task and factual recall), and argue that activating dormant pathways ought to be prevalent in practice. In the context of factual recall, we further show that the illusion is related to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localisation. However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (IOI) where prior manual circuit analysis allows an understanding of the location of the ground truth feature. We explore the additional evidence needed to argue that a patched subspace is faithful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=qgyLAr2cOs": {
    "title": "Fixed-Budget Best Arm Identification with Variance-Dependent Regret Bounds",
    "volume": "review",
    "abstract": "We investigate the problem of fixed-budget best arm identification (BAI) for minimizing expected simple regret. In an adaptive experiment, a decision maker draws one of multiple treatment arms based on past observations and observes the outcome of the drawn arm. After the experiment, the decision maker recommends the treatment arm with the highest expected outcome. We evaluate the decision based on the expected simple regret, which is the difference between the expected outcomes of the best arm and the recommended arm. Due to inherent uncertainty, we evaluate the regret using the minimax criterion. First, we derive asymptotic lower bounds for the worst-case expected simple regret, which are characterized by the variances of potential outcomes (leading factor). Based on the lower bounds, we propose the Adaptive-Sampling (AS)-Augmented Inverse Probability Weighting (AIPW) strategy, which utilizes the AIPW estimator in recommending the best arm. Our theoretical analysis shows that the AS-AIPW strategy is asymptotically minimax optimal, meaning that the leading factor of its worst-case expected simple regret matches our derived worst-case lower bound. Finally, we validate the proposed method's effectiveness through simulation studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=YItWKZci78": {
    "title": "Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems",
    "volume": "review",
    "abstract": "In this paper, we extend mean-field Langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. We propose \\emph{mean-field Langevin averaged gradient} (MFL-AG), a single-loop algorithm that implements gradient descent ascent in the distribution space with a novel weighted averaging, and establish average-iterate convergence to the mixed Nash equilibrium. We also study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. Furthermore, we propose \\emph{mean-field Langevin anchored best response} (MFL-ABR), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. Finally, we study applications to zero-sum Markov games and conduct simulations to demonstrate the long-term optimality of MFL-AG and MFL-ABR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RvmrhrPy7j": {
    "title": "Causal Inference Using LLM-Guided Discovery",
    "volume": "review",
    "abstract": "At the core of causal inference lies the critical challenge of determining reliable causal graphs solely based on observational data. Since the well-known backdoor criterion depends on the graph, any errors in the graph can propagate downstream to effect inference. In this work, we initially show that complete graph information is not necessary for causal effect inference; the topological order over graph variables (causal order) alone suffices. Further, given a node pair, causal order is easier to elicit from domain experts compared to graph edges since determining the existence of an edge can depend extensively on other variables. Interestingly, we find that the same principle holds for Large Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated method to obtain causal order (and hence causal effect) with LLMs acting as virtual domain experts. To this end, we employ different prompting strategies and contextual cues to propose a robust technique of obtaining causal order from LLMs. Acknowledging LLMs' limitations, we also study possible techniques to integrate LLMs with established causal discovery algorithms, including constraint-based and score-based methods, to enhance their performance. Extensive experiments demonstrate that our approach significantly improves causal ordering accuracy as compared to established discovery algorithms, highlighting the potential of LLMs to enhance causal inference across diverse fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=030cjlZm4a": {
    "title": "Learning Predictive Checklists with Probabilistic Logic Programming",
    "volume": "review",
    "abstract": "Checklists have been widely recognized as effective tools for completing complex tasks in a systematic manner. Although originally intended for use in procedural tasks, their interpretability and ease of use have led to their adoption for predictive tasks as well, including in clinical settings. However, designing checklists can be challenging, often requiring expert knowledge and manual rule design based on available data. Recent work has attempted to address this issue by using machine learning to automatically generate predictive checklists from data, although these approaches have been limited to Boolean data. We propose a novel method for learning predictive checklists from diverse data modalities, such as images, time series, and text, by combining the power of dedicated deep learning architectures with the interpretability and conciseness of checklists. Our approach relies on probabilistic logic programming, a learning paradigm that enables matching the discrete nature of a checklist with continuous-valued data. We propose a regularization technique to tradeoff between the information captured in discrete concepts of continuous data and permit a tunable level of interpretability for the learned checklist concepts. We demonstrate that our method outperforms various explainable machine learning techniques on prediction tasks involving image sequences, clinical notes, and time series",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Rd4pGjTcTj": {
    "title": "Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions",
    "volume": "review",
    "abstract": "Impressive progress has been made on chat models based on Large Language Models (LLMs) recently; however, there is a noticeable lag in multi-turn conversations between open-source chat models (e.g., Alpaca and Vicuna) and the leading chat models (e.g., ChatGPT and GPT-4). Through a series of analyses, we attribute the lag to the lack of enough high-quality multi-turn instruction-tuning data. The available instruction-tuning data for the community are either singleturn conversations or multi-turn ones with certain issues, such as non-human-like instructions, less detailed responses, or rare topic shifts. In this paper, we address these challenges by introducing Parrot, a highly scalable solution designed to automatically generate high-quality instruction-tuning data, which are then used to enhance the effectiveness of chat models in multi-turn conversations. Specifically, we start by training the Parrot-Ask model, which is designed to emulate real users in generating instructions. We then utilize Parrot-Ask to engage in multiturn conversations with ChatGPT across a diverse range of topics, resulting in a collection of 40K high-quality multi-turn dialogues (Parrot-40K). These data are subsequently employed to train a chat model that we have named Parrot-Chat. We demonstrate that the dialogues gathered from Parrot-Ask markedly outperform existing multi-turn instruction-following datasets in critical metrics, including topic diversity, number of turns, and resemblance to human conversation. With only 40K training examples, Parrot-Chat achieves strong performance against other 13B open-source models across a range of instruction-following benchmarks, and particularly excels in evaluations of multi-turn capabilities. All codes and datasets will be publicly available to facilitate future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bSlAUCyY4T": {
    "title": "Knowledge Graph Completion by Intermediate Variables Regularization",
    "volume": "review",
    "abstract": "Knowledge graph completion (KGC) can be framed as a 3-order binary tensor completion task. Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC. In this paper, we provide a summary of existing TDB models and derive a general form for them, serving as a foundation for further exploration of TDB models. Despite the expressiveness of TDB models, they are prone to overfitting. Existing regularization methods merely minimize the norms of embeddings to regularize the model, leading to suboptimal performance. Therefore, we propose a novel regularization method for TDB models that addresses this limitation. The regularization is applicable to most TDB models, incorporates existing regularization methods, and ensures tractable computation. Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor. To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting. Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ukmh3mWFf0": {
    "title": "Attributed Graph Clustering via Coarsening with Modularity",
    "volume": "review",
    "abstract": "Graph clustering is a widely used technique for partitioning graphs, community detection, and other tasks. Recent graph clustering algorithms depend on combinations of the features and adjacency matrix, or solely on the adjacency matrix. However, in order to achieve high-quality clustering, it is necessary to consider all these components. In this paper, we propose a novel unsupervised learning framework that incorporates modularity with graph coarsening techniques and important graph regularization terms that improve the clustering performance. Furthermore, we also take into account Dirichlet energies for smoothness of signals, spectral similarity, and coarsening reconstructional error. The proposed framework is solved efficiently by leveraging block majorization-minimization, $\\log\\det$ of the Laplacian, smoothness and modularity, and is readily integrable with deep learning architectures such as GCNs and VGAEs in the form of losses. Extensive theoretical analysis and experiments with benchmark datasets elucidate the proposed framework's efficacy in graph clustering over existing state-of-the-art methods on both attributed and non-attributed graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=l18hiEXRJS": {
    "title": "MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Deep Neural Networks",
    "volume": "review",
    "abstract": "Despite their successful application to a variety of tasks, neural networks remain limited, like other machine learning methods, by their sensitivity to shifts in the data: their performance can be severely impacted by differences in distribution between the data on which they were trained and that on which they are deployed. In this article, we propose a new family of representations, called MAGDiff, that we extract from any given neural network classifier and that allows for efficient covariate data shift detection without the need to train a new model dedicated to this task. These representations are computed by comparing the activation graphs of the neural network for samples belonging to the training distribution and to the target distribution, and yield powerful data- and task-adapted statistics for the two-sample tests commonly used for data set shift detection. We demonstrate this empirically by measuring the statistical powers of two-sample Kolmogorov-Smirnov (KS) tests on several different data sets and shift types, and showing that our novel representations induce significant improvements over a state-of-the-art baseline relying on the network output",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=lF2aip4Scn": {
    "title": "Demonstration-Regularized RL",
    "volume": "review",
    "abstract": "Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. Precisely, we study the demonstration-regularized reinforcement learning framework that leverages the expert demonstrations by $\\mathrm{KL}$-regularization for a policy learned by behavior cloning. Our findings reveal that utilizing $N^{\\mathrm{E}}$ expert demonstrations enables the identification of an optimal policy at a sample complexity of order $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(S,A,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in finite and $\\widetilde{\\mathcal{O}}(\\mathrm{Poly}(d,H)/(\\varepsilon^2 N^{\\mathrm{E}}))$ in linear Markov decision processes, where $\\varepsilon$is the target precision, $H$ the horizon, $A$ the number of action, $S$ the number of states in the finite case and $d$ the dimension of the feature space in the linear case. As a by-product, we provide tight convergence guarantees for the behaviour cloning procedure under general assumptions on the policy classes. Additionally, we establish that demonstration-regularized methods are provably efficient for reinforcement learning from human feedback (RLHF). In this respect, we provide theoretical evidence showing the benefits of KL-regularization for RLHF in tabular and linear MDPs. Interestingly, we avoid pessimism injection by employing computationally feasible regularization to handle reward estimation uncertainty, thus setting our approach apart from the prior works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=jFiFmHrIfD": {
    "title": "Explorative Latent Self-Supervised Active Search Algorithm (ELSA)",
    "volume": "review",
    "abstract": "In computer vision, attaining exceptional performance often necessitates access to large labeled datasets. The creation of extensive datasets through manual annotation is not only cost-prohibitive but also practically infeasible due to the scarcity of positive samples in imbalanced datasets where negative samples dominate. To tackle this intricate problem, we introduce Efficient Latent Space-based Self-Supervised Active Learning Search (ELSA), an active learning-based labeling assistant. ELSA distinguishes itself from existing interactive annotation methods by focusing exclusively on positive class labeling in massively imbalanced datasets replete with a substantial number of negative samples. Through the automatic exclusion of the majority of negative samples, ELSA achieves a remarkable level of precision and accuracy in its search. This novel framework comprises three fundamental components: a)an iterative Nearest Neighbor Search, b)a Sophisticated Random Sampler, c)a Linear Head powered by Active Learning. Our comprehensive study provides insights into the interplay of these components and their collective impact on search efficiency. Notably, we demonstrate that ELSA achieves orders of magnitude superior performance, in average starting with as little as 5 or less positive samples in ImageNet 1k we managed to detect as much as 80\\% of all the examples belonging to that class by only labeling as little as 0.67\\% of the entire dataset manually",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=TvkvWjxj3T": {
    "title": "Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models",
    "volume": "review",
    "abstract": "In image editing employing diffusion models, it is crucial to preserve the reconstruction quality of the original image while changing its style. Although existing methods ensure reconstruction quality through optimization, a drawback of these is the significant amount of time required for optimization. In this paper, we propose negative-prompt inversion, a method capable of achieving equivalent reconstruction solely through forward propagation without optimization, thereby enabling much faster editing processes. We experimentally demonstrate that the reconstruction quality of our method is comparable to that of existing methods, allowing for inversion at a resolution of 512 pixels and with 50 sampling steps within approximately 5 seconds, which is more than 30 times faster than null-text inversion. Reduction of the computation time by the proposed method further allows us to use a larger number of sampling steps in diffusion models to improve the reconstruction quality with a moderate increase in computation time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xVBXz7wD2m": {
    "title": "GatedMTL: Learning to Share, Specialize, and Prune Representations for Multi-task Learning",
    "volume": "review",
    "abstract": "Jointly learning multiple tasks with a unified network can improve accuracy and data efficiency while simultaneously reducing computational and memory costs. However, in practice, Multi-task Learning (MTL) is challenging, as optimizing one task objective may inadvertently compromise the performance of another: This is known as task interference. A promising direction to mitigate such conflicts between tasks is to allocate task-specific parameters, free from interference, on top of shared features, allowing for positive information transfer across tasks, albeit at the cost of higher computational demands. In this work, we propose a novel MTL framework, GatedMTL, to address the fundamental challenges of task interference and computational constraints in MTL. GatedMTL learns the optimal balance between shared and specialized representations for a given computational budget. We leverage a learnable gating mechanism allowing each individual task to select and combine channels from its own task-specific features and a shared memory bank of features. Moreover, we regularize the gates to learn the optimal balance between allocating additional task-specific parameters and the model's computational costs. Through extensive empirical evaluations, we demonstrate SoTA results on three MTL benchmarks using convolutional as well as transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=3sOE3MFepx": {
    "title": "PDE-Diffusion: Physic guided diffusion model for solving partial derivative equations",
    "volume": "review",
    "abstract": "Solving partial differential equations (PDEs) is crucial in various disciplines, and their resolution often necessitates the use of computationally intensive numerical methods as well as specialized domain expertise. While data-driven approaches have emerged as promising alternatives, they encounter limitations in terms of generalizability, interpretability, and long-horizon predictive performance, as well as issues related to temporal incoherence. To address these challenges, we introduce the PDE-Diffusion, a two-stage model with three distinctive features: (i) the incorporation of physics-based priors to enhance model interpretability and generalization, (ii) a two-stage diffusion model that efficiently handles physical field forecasting without requiring multi-frame inputs, and (iii) the assimilation of PDE-informed constraints to ensure temporal coherence while producing high-quality predictive results. We conduct extensive experiments to evaluate PDE-Diffusion's capabilities using the PDEBench dataset and two of our newly proposed datasets. The results indicate that PDE-Diffusion delivers state-of-the-art performance in all cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2Pup7olzxj": {
    "title": "Differentiable Optimization in Plane-Wave Density Functional Theory for Solid States",
    "volume": "review",
    "abstract": "Plane-wave density functional theory is a computational quantum mechanical modeling method used to investigate the electronic structure of solids. It employs plane-waves as the basis set for representing electronic wave functions and leverages density functional theory to compute the electronic structure properties of many-body systems. Traditionally, the Self-Consistent Field (SCF) method is predominantly adopted for optimization in current DFT computations. However, this method encounters notable convergence and computational challenges, and its iterative nature obstructs the incorporation of emergent deep learning enhancements. To address these challenges, we introduce a fully differentiable optimization method tailored to resolve the intrinsic challenges associated with the optimization of plane-wave density functional methods. This methodology includes a direct total energy minimization approach for solving Kohn-Sham equations in periodic crystalline systems, which is coherent with deep learning infrastructures. The efficacy of our approach is illustrated through its two applications in solid-state physics: electron band structure prediction and geometry optimization. Our enhancements potentially pave the way for various gradient-based applications within deep learning paradigms in solid-state physics, extending the boundaries of material innovation and design. We illustrate the utility and diverse applications of our method on real crystal structures and compare its effectiveness with several established SCF-based packages, demonstrating its accuracy and robust convergence property",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vESNKdEMGp": {
    "title": "Multilingual Jailbreak Challenges in Large Language Models",
    "volume": "review",
    "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English data. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario entails malicious users combining jailbreak instructions with multilingual prompts to attack LLMs deliberately. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of jailbreak instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. Finally, we propose a novel \\textsc{Self-Defense} framework that addresses the multilingual jailbreak challenges via automatically generating multilingual safety training data for fine-tuning. Experiment results demonstrate its effectiveness with notable reduction in unsafe rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=DZ6B5u4vfe": {
    "title": "Instruction-tuned LLMs with World Knowledge are More Aligned to the Human Brain",
    "volume": "review",
    "abstract": "Instruction-tuning is a widely adopted method of finetuning that enables large language models (LLMs) to generate output that more closely resembles human responses to natural language queries, in many cases leading to human-level performance on diverse testbeds. However, it remains unclear whether instruction-tuning truly makes LLMs more similar to how humans process language. We investigate the effect of instruction-tuning on LLM-human similarity in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs across three datasets involving humans reading naturalistic stories and sentences, and discover that instruction-tuning generally enhances brain alignment by an average of 6%, but does not have a similar effect on behavioral alignment. To identify the factors underlying LLM-brain alignment, we compute the correlation between the brain alignment of LLMs and various model properties, such as model size, performance ability on problem-solving benchmarks, and ability on benchmarks requiring world knowledge spanning various domains. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results suggest that making world knowledge in LLMs more accessible via instruction-tuning also yields neural representations more similar to those of the human language system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RIaIpdUCPb": {
    "title": "Brain-inspired Geometry Constrain on Represention for Compositional Generalization",
    "volume": "review",
    "abstract": "Compositional Generalization (CG), referring as the generalization ability to new combinations of essential concepts, is thought to be one mechanism underlying human's remarkable capability of rapid generalization to new knowledge and tasks. Recent research on brain neural codes has found that the geometry structure of the neural representations is highly related to human compositional generalization ability. In this paper, we extend the above neural science observation into artificial neural networks (ANN) and find that the geometry structure of the representations in ANN impacts their compositional generalization. More importantly, we reveal that only good geometry structure is not sufficient for strong CG ability, a regularization is essential to ensure the classifier can fit the representation geometry structure. We propose a loss to optimize the representation extractor to form a well-organized representation space, and a regularization on the classifier to force it align with the geometry structure of representation space. With our proposed methods, the CG performance gains as large as 43\\% on the synthetic and 63\\% on real-world datasets, verifying the effectiveness of our brain-inspired ANN-enhancing approach towards human-like strong generalization ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=PuCno7nwgH": {
    "title": "Categorical Features of entities in Recommendation Systems Using Graph Neural Networks",
    "volume": "review",
    "abstract": "The paper tackles the challenge of capturing entity attribute-specific preferences in recommender systems, with a particular focus on the role of categorical features within GNN-based user-item recommender engines. Despite the significant influence of categorical features such as brand, category, and price bucket on the user decision-making process, there are not many studies dedicated to understanding the GNN's capability to extract and model such preferences effectively. The study extensively compares and tests various techniques for incorporating categorical features into the GNN framework to address this gap. These techniques include one-hot encoding-based node features, category-value nodes, and hyperedges. Three real-world datasets are used to answer what is the most optimal way to incorporate such information. In addition, the paper introduces a novel hyperedge-based method designed to leverage categorical features more effectively compared to existing approaches. The advantage of the hyperedge approach is demonstrated through extensive experiments in effectively modeling categorical features and extracting user attribute-specific preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=csukJcpYDe": {
    "title": "Generalized Policy Iteration using Tensor Approximation for Hybrid Control",
    "volume": "review",
    "abstract": "Control of dynamic systems involving hybrid actions is a challenging task in robotics. To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor approximation technique called Tensor Train (TT) to approximate the state-value and advantage function which enables us to efficiently handle hybrid systems. We demonstrate the superiority of our approach over previous baselines for some benchmark problems with hybrid action spaces. Additionally, the robustness and generalization of the policy for hybrid systems are showcased through a real-world robotics experiment involving a non-prehensile manipulation task which is considered to be a highly challenging control problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RzNlECeoOB": {
    "title": "$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence",
    "volume": "review",
    "abstract": "The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of a KL divergence between two statistical manifolds and replacing with $\\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-density regions when trained on heavy-tailed synthetic data. Furthermore, we show that our model excels at capturing rare features through real-data experiments on CelebA and imbalanced CIFAR datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=JBLHIR8kBZ": {
    "title": "Neuron to Graph: Interpreting Language Model Neurons at Scale",
    "volume": "review",
    "abstract": "Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N$2$G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N$2$G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGdoTL9g18": {
    "title": "Residual Factorized Fourier Neural Operator for simulation of three-dimensional turbulence",
    "volume": "review",
    "abstract": "Neural Operators, particularly Fourier Neural Operators (FNO), have proven highly effective in simulating partial differential equations (PDEs), such as the Navier-Stokes equations. We propose the Residual Factorized Fourier Neural Operator (Res-F-FNO) for simulating three-dimensional (3D) flows, specifically focusing on flow dynamics around a cube. We extend the Factorized Fourier Neural Operator (F-FNO) architecture by incorporating additional residual connections. This change effectively reintroduces small-scale dynamic flows that may be lost due to truncated Fourier modes, resulting in improved accuracy when modeling wind fields. Our proposed Res-F-FNO model surpasses the performance of the standard F-FNO, achieving an error reduction of over 30\\% in simulating 3D flows. Furthermore, we propose the concept of a skip-corrector, to address the problem of accumulated errors over multiple time steps. The skip-corrector was specifically trained to predict the behaviour of turbulences at a considerably extended time interval. Incorporating the skip-corrector into the prediction process reduces the average error in simulating 100 time steps by more than 50\\%. Additionally, we adopt a modified training approach in which random time steps are chosen as the initial condition for each sample in every epoch, as opposed to generating a dataset by propagating each sample across all time steps. This leads to a significant reduction in the the number of training iterations required for the models to achieve convergence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pp8Kb4hejU": {
    "title": "Adjustable Quantile-Guided Diffusion Policy for Diverse Behavior Generation in Offline RL",
    "volume": "review",
    "abstract": "Offline Reinforcement Learning (RL) addresses the challenge of learning optimal policies from pre-collected data, making it a promising approach for real-world applications where online interactions with an environment are costly or impractical. We propose an offline RL method named Quantile-Guided Diffusion Policy~(qGDP), which trains a quantile network to label the training dataset and uses these labeled samples to train the diffusion model and generate new samples with the trained model according to classifier-free guidance. qGDP can adjust the preference of sample generation between imitating and improving behavioral policies by adjusting the input condition and changing the guidance scale without re-training the model, which will significantly reduce the cost of tuning the algorithm. qGDP exhibits exceptional generalization capabilities and allows easy adjustment of action generation preferences without model retraining, reducing computational costs. Experimental results on the D4RL dataset demonstrate state-of-the-art performance and computational efficiency compared to other diffusion-based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FI0vOp2asx": {
    "title": "Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging",
    "volume": "review",
    "abstract": "Spectral snapshot compressive imaging (Spectral SCI) applies an optical encoder to compressively capture 2D measurements, followed by which the 3D hyperspectral data can be restored via training a deep reconstruction network. Existing reconstruction models are generally trained with a single well-calibrated hardware instance, making their performance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which, however, is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing spectral SCI systems by proposing a novel Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different coded apertures. Extensive experiments demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware configurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous setting. Moreover, a new Snapshot Spectral Heterogeneous Dataset (SSHD) has been built upon multiple practical spectral SCI systems. We will release the data and code to enrich further exploration of this practical computational imaging problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6a1pXXADC": {
    "title": "Prompt Optimization via Adversarial In-Context Learning",
    "volume": "review",
    "abstract": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=6yJuDK1DsK": {
    "title": "FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters",
    "volume": "review",
    "abstract": "Lifelong/continual test-time adaptation (TTA) refers to the problem where a pre-trained source domain model needs to be continually adapted at inference time to handle non-stationary test distributions. Continuously updating the source model over long horizons can result in significant drift in the source model, forgetting the source domain knowledge. Moreover, most of the existing approaches for lifelong TTA require adapting all the parameters, which can incur significant computational cost and memory consumption, limiting their applicability on edge devices for faster inference. We present FEATHER (liFelong tEst-time Adaptation wiTH lightwEight adapteRs), a novel lightweight approach that introduces only a small number of additional parameters to a pre-trained source model which can be unsupervisedly and efficiently adapted during test-time for the new test distribution(s), keeping the rest of the source model frozen. FEATHER disentangles the source domain knowledge from the target domain knowledge, making it robust against error accumulation over time. Another distinguishing aspect of FEATHER is that, unlike some recent approaches for lifelong TTA that require access to the source data for warm-starting the adaptation at test time, FEATHER does not have such a requirement. FEATHER is also orthogonal to the existing lifelong TTA approaches and can be augmented with these approaches, resulting in a significant reduction in the number of additional parameters needed to handle the lifelong TTA setting. Through extensive experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC Robustbench benchmark datasets, we demonstrate that, with substantially (85% to 94%) fewer trainable parameters, FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, resulting in faster adaptation and inference at test-time. The source code for FEATHER will be released upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=GrunXMbdXY": {
    "title": "FLAT-Chat: A Word Recovery Attack on Federated Language Model Training",
    "volume": "review",
    "abstract": "Gradient exchange is widely applied in collaborative training of machine learning models, including Federated Learning. Curious-but-honest participants could potentially infer the output labels in recently used training data by analyzing the latest gradient updates. Previous works mostly demonstrate the attack performance under constraint training settings, such as dozens of short sentences in a batch and a small output space for labels. In this work, we propose a novel gradient flattening attack on the last linear layer of a language model, which significantly improves the attacker's efficiency in inferring the words used in training. We validate the capability of the attack on two language generation tasks: machine translation and language modeling. The attack environment is scaled up to industrial settings of a large output vocabulary and realistic training batch sizes. To mitigate the negative impact of the new attack, we explore two defense methods and demonstrate that adding differential privacy with small noise could effectively defend against our new attack without degrading model utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=UTGv8CayNt": {
    "title": "Chain-of-Thought Predictive Control",
    "volume": "review",
    "abstract": "We study generalizable policy learning from demonstrations for complex low-level control tasks (e.g., contact-rich object manipulations). We propose a novel hierarchical imitation learning method that utilizes scalable, albeit sub-optimal, demonstrations. Firstly, we propose an observation space-agnostic approach that efficiently discovers the multi-step subgoal decomposition (sequences of key observations) of the demos in an unsupervised manner. By grouping temporarily close and functionally similar actions into subskill-level segments, the discovered breakpoints (the segment boundaries) constitute a chain of planning steps (i.e., the chain-of-thought) to complete the task. Next, we propose a Transformer-based design that effectively learns to predict the chain-of-thought (CoT) as the high-level guidance for low-level action. We couple action and CoT predictions via prompt tokens and a hybrid masking strategy, which enable dynamically updated CoT guidance at test time and improve feature representation of the trajectory for generalizable policy learning. Our method, named Chain-of-Thought Predictive Control (CoTPC), consistently surpasses existing strong baselines on a wide range of challenging low-level manipulation tasks with scalable yet sub-optimal demos",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=nTwb2vBLOV": {
    "title": "Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability",
    "volume": "review",
    "abstract": "The expressivity of Graph Neural Networks (GNNs) has been studied broadly in recent years to reveal the design principles for more powerful GNNs. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive GNNs. This paper proposes to maximize the expressivity of GNNs by graph canonization, then the power of such GNNs is studies from the perspective of model stability. A stable GNN will map similar graphs to close graph representations in the vectorial space, and the stability of GNNs is critical to generalize their performance to unseen graphs. We theoretically reveal the trade-off of expressivity and stability in graph-canonization-enhanced GNNs. Then we introduce a notion of universal graph canonization as the general solution to address the trade-off and characterize a widely applicable sufficient condition to solve the universal graph canonization. A comprehensive set of experiments demonstrates the effectiveness of the proposed method. In many popular graph benchmark datasets, graph canonization successfully enhances GNNs and provides highly competitive performance, indicating the capability and great potential of proposed method in general graph representation learning. In graph datasets where the sufficient condition holds, GNNs enhanced by universal graph canonization consistently outperform GNN baselines and successfully improve the SOTA performance up to $31$%, providing the optimal solution to numerous challenging real-world graph analytical tasks like gene network representation learning in bioinformatics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bUv5gJAAxH": {
    "title": "Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension",
    "volume": "review",
    "abstract": "Despite their impressive performance in classification, neural networks are known to be vulnerable to adversarial attacks. These attacks are small perturbations of the input data designed to fool the model. Naturally, a question arises regarding the potential connection between the architecture, settings, or properties of the model and the nature of the attack. In this work, we aim to shed light on this problem by focusing on the implicit bias of the neural network, which refers to its inherent inclination to favor specific patterns or outcomes. Specifically, we investigate one aspect of the implicit bias, which involves the essential Fourier frequencies required for accurate image classification. We conduct tests to assess the statistical relationship between these frequencies and those necessary for a successful attack. To delve into this relationship, we propose a new method that can uncover non-linear correlations between sets of coordinates, which, in our case, are the aforementioned frequencies. By exploiting the entanglement between intrinsic dimension and correlation, we provide empirical evidence that the network bias in Fourier space and the target frequencies of adversarial attacks are closely tied",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=QqqkskOFO9": {
    "title": "Rethinking Actor-Critic: Successive Actors for Critic Maximization",
    "volume": "review",
    "abstract": "Value-based actor-critic approaches have been widely employed for continuous and large discrete action space reinforcement learning tasks. Traditionally, an actor-network is trained to find the action that maximizes the critic (action-value function) with gradient ascent. We identify that often an actor fails to maximize the critic because (i) certain tasks have challenging action-value landscapes with several local optima, and (ii) the critic landscape varies non-stationarily over training. This inability to find the optimal action often leads to sample-inefficient training and suboptimal convergence. To address the challenge of better maximization of the critic's landscape, we present a novel reformulation of the actor by employing a sequence of sub-actors with increasingly tractable action-value landscapes. In large discrete and continuous action space tasks, we demonstrate that our approach finds actions that better maximize the action-value function than conventional actor-network approaches, enabling better performance. [https://sites.google.com/view/complexaction](https://sites.google.com/view/complexaction)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMMF1a9ifL": {
    "title": "Gradual Optimization Learning for Conformational Energy Minimization",
    "volume": "review",
    "abstract": "Molecular conformation optimization is crucial to computer-aided drug discovery and materials design. Traditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients. However, this is a computationally expensive approach that requires many interactions with a physical simulator. One way to accelerate this procedure is to replace the physical simulator with a neural network. Despite recent progress in neural networks for molecular conformation energy prediction, such models are prone to distribution shift, leading to inaccurate energy minimization. We find that the quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data. Still, it takes around $5 \\times 10^5$ additional conformations to match the physical simulator's optimization quality. In this work, we present the Gradual Optimization Learning Framework (GOLF) for energy minimization with neural networks that significantly reduces the required additional data. The framework consists of an efficient data-collecting scheme and an external optimizer. The external optimizer utilizes gradients from the energy prediction model to generate optimization trajectories, and the data-collecting scheme selects additional training data to be processed by the physical simulator. Our results demonstrate that the neural network trained with GOLF performs \\textit{on par} with the oracle on a benchmark of diverse drug-like molecules using $50$x less additional data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=buC4E91xZE": {
    "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection",
    "volume": "review",
    "abstract": "Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/ organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=6SNyuiph3F": {
    "title": "Chat Vector: A Simple Approach to Equip LLMs With New Language Chat Capabilities",
    "volume": "review",
    "abstract": "With the advancements in conversational AI, such as ChatGPT, this paper focuses on exploring developing Large Language Models (LLMs) for non-English languages, especially emphasizing alignment with human preferences. We introduce a computationally efficient method, leveraging \"chat vector,\" to synergize pre-existing knowledge and behaviors in LLMs, restructuring the conventional training paradigm from continual pretrain $\\rightarrow$ SFT $\\rightarrow$ RLHF to continual pretrain + chat. Our empirical studies, primarily focused on Traditional Chinese, employ LLaMA2 as the base model and acquire the chat vector by subtracting the pre-trained weights, LLaMA2, from the weights of LLaMA2-chat. Evaluating from three distinct facets, which are toxicity, ability of instruction following and multi-turn dialogue demonstrates the chat vector's superior efficacy in \"chatting\". To confirm the adaptability of our approach, we extend our experiments to include models pre-trained in both Korean and Simplified Chinese, illustrating the versatility of our methodology. Overall, we present a significant solution in aligning LLMs with human preferences efficiently across various languages, accomplished by the chat vector",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=nOf6sb63dT": {
    "title": "Generative Models are Self-Watermarked: Intellectual Property Declaration through Re-Generation",
    "volume": "review",
    "abstract": "Protecting intellectual property for generated data has emerged as a critical concern for AI corporations, as machine-generated content proliferates. Reusing generated data without permission poses a formidable barrier to safeguarding the intellectual property tied to these models. The verification of data ownership is further complicated by the use of Machine Learning as a Service (MLaaS), which often operates as a black-box system. Our work is dedicated to detecting data reuse from even an individual sample. In contrast to watermarking techniques that embed additional information as watermark triggers into models or generated content, our approach does not introduce artificial watermarks which may compromise the quality of model outputs. Our investigation reveals the existence of latent fingerprints inherently present within deep learning models. In response, we propose an explainable verification procedure to verify data ownership through re-generation. Furthermore, we introduce a novel methodology to amplify the model fingerprints through iterative data regeneration and a theoretical grounding on the proposed approach. We demonstrate the viability of our approach using recent advanced text and image generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxFme785fq": {
    "title": "Nonlinear Inference Learning for Differentially Private Massive Data",
    "volume": "review",
    "abstract": "The Bag of Little Bootstraps (BLB) method is widely utilized as a robust and computationally efficient approach in statistical inference studies involving large-scale data. However, this sampling technique overlooks the privacy protection of the original data. To address this limitation, we enhance the existing differential privacy algorithm and integrate it with the BLB method. This integration gives rise to a novel differential privacy mechanism, enabling a comprehensive statistical analysis of aggregated parameters while safeguarding the confidentiality of individual private data. Additionally, to address both the variability in noise variance under the differential privacy mechanism and the uncertainty surrounding estimate distributions, we employ the central limit theorem within the context of nonlinear expectation theory. This facilitates the derivation of the corresponding test statistic and the introduction of a hypothesis testing methodology. Furthermore, we validate the commendable performance of our proposed inference procedure through data simulation studies. The big data-oriented differential privacy-preserving mechanism proposed in this study effectively fulfills the requirements for privacy preservation without compromising subsequent statistical inference. This contribution holds significant reference value for the sharing of pertinent data and endeavors related to statistical analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uGtfk2OphU": {
    "title": "Boosting Selective Rationalization with Shortcuts Discovery",
    "volume": "review",
    "abstract": "The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on four real-world datasets clearly validate the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=tgjGR7eY5H": {
    "title": "RL4CO: a Unified Reinforcement Learning for Combinatorial Optimization Library",
    "volume": "review",
    "abstract": "Deep reinforcement learning offers notable benefits in addressing combinatorial problems over traditional solvers, reducing the reliance on domain-specific knowledge and expert solutions, and improving computational efficiency. Despite the recent surge in interest in neural combinatorial optimization, practitioners often do not have access to a standardized code base. Moreover, different algorithms are frequently based on fragmentized implementations that hinder reproducibility and fair comparison. To address these challenges, we introduce RL4CO, a unified Reinforcement Learning (RL) for Combinatorial Optimization (CO) library. We employ state-of-the-art software and best practices in implementation, such as modularity and configuration management, to be flexible, easily modifiable, and extensible by researchers. Thanks to our unified codebase, we benchmark baseline RL solvers with different evaluation schemes on zero-shot performance, generalization, and adaptability on diverse tasks. Notably, we find that some recent methods may fall behind their predecessors depending on the evaluation settings. We hope RL4CO will encourage the exploration of novel solutions to complex real-world tasks, allowing the community to compare with existing methods through a unified framework that decouples the science from software engineering. We open-source our library at https://anonymous.4open.science/r/rl4co-iclr",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SRn2o3ij25": {
    "title": "IKL: Boosting Long-Tail Recognition with Implicit Knowledge Learning",
    "volume": "review",
    "abstract": "In the field of visual long-tailed recognition, the long-tailed distribution of image representations often raises two key challenges: (1) the training process shows great uncertainty (e.g., uncertainty in the prediction of augmented views by the same expert for the same sample) and (2) a marked bias in the model's prediction towards the head class. To tackle the above issue, we propose a novel method termed Implicit Knowledge Learning (IKL) to extract the knowledge hidden in long-tail learning processes, aiming to significantly improve performance in long-tail recognition. Our IKL contains two core components: Implicit Uncertainty Regularization (IUR) and Implicit Correlation Labeling (ICL). The former method, IUR, exploits the uncertainty of the predictions over adjacent epochs. Then, it transfers the correct knowledge to reduce uncertainty and improve long-tail recognition accuracy. The latter approach, ICL, endeavors to reduce the bias introduced by one-hot labels by exploring the implicit knowledge in the model: inter-class similarity information. Our approach is lightweight enough to plug and play with existing long-tail learning methods, achieving state-of-the-art performance in popular long-tail benchmarks. The experimental results highlight the great potential of implicit knowledge learning in dealing with long-tail recognition. Our code will be open-sourced upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=LojXXo2xaf": {
    "title": "GPT Can Solve Mathematical Problems Without a Calculator",
    "volume": "review",
    "abstract": "Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Z6lN4GYrO": {
    "title": "S4G: Breaking the Bottleneck on Graphs with Structured State Spaces",
    "volume": "review",
    "abstract": "The majority of GNNs are based on message-passing mechanisms, however, message-passing neural networks (MPNN) have inherent limitations in capturing long-range interactions. The exponentially growing node information is compressed into fixed-size representations through multiple rounds of message passing, bringing the over-squashing problem, which severely hinders the flow of information on the graph and creates a bottleneck in graph learning. The natural idea of introducing global attention to point-to-point communication, as adopted in graph Transformers (GT), lacks inductive biases on graph structures and relies on complex positional encodings to enhance their performance in practical tasks. In this paper, we observe that the sensitivity between nodes in MPNN decreases exponentially with the shortest path distance. Contrarily, GT has a constant sensitivity, which leads to its loss of inductive bias. To address these issues, we introduce structured state spaces to capture the hierarchical structure of rooted-trees, achieving linear sensitivity with theoretical guarantees. We further propose a novel graph convolution based on the state-space model, resulting in a new paradigm that retains both the strong inductive biases from MPNN and the long-range modeling capabilities from GT. Extensive experimental results on long-range and general graph benchmarks demonstrate the superiority of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=KTtEICH4TO": {
    "title": "CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects",
    "volume": "review",
    "abstract": "Nonprehensile manipulation is essential for manipulating objects that are too thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty of contact modeling in conventional modeling-based approaches, reinforcement learning (RL) has recently emerged as a promising alternative. However, previous RL approaches either lack the ability to generalize over diverse object shapes, or use simple action primitives that limit the diversity of robot motions. Furthermore, using RL over diverse object geometry is challenging due to the high cost of training a policy that takes in high-dimensional sensory inputs. We propose a novel contact-based object representation and pretraining pipeline to tackle this. To enable massively parallel training, we leverage a lightweight patch-based transformer architecture for our encoder that processes point clouds, thus scaling our training across thousands of environments. Compared to learning from scratch, or other shape representation baselines, our representation facilitates both time- and data-efficient learning. We validate the efficacy of our overall system by zero-shot transferring the trained policy to novel real-world objects. We highly recommend the video attached in the supplementary material. Code and videos are available at \\url{https://sites.google.com/view/contact-non-prehensile}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5JWAOLBxwp": {
    "title": "An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks",
    "volume": "review",
    "abstract": "The usage of 3D vision algorithms, such as shape reconstruction, remains limited because they require inputs to be at a fixed canonical rotation. Recently, a simple equivariant network, Vector Neuron (VN) has been proposed that can be easily used with the state-of-the-art 3D neural network (NN) architectures. However, its performance is limited because it is designed to use only three-dimensional features, which is insufficient to capture the details present in 3D data. In this paper, we introduce an equivariant feature representation for mapping a 3D point to a high-dimensional feature space. Our feature can discern multiple frequencies present in 3D data, which, as shown by Tancik et al. (2020), is the key to designing an expressive feature for 3D vision tasks. Our representation can be used as an input to VNs, and the results demonstrate that with our feature representation, VN captures more details, overcoming the limitation raised in its original paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=GxrVyYoLSx": {
    "title": "Implicit Regularisation in Overparametrized Networks: A Multiscale Analysis of the Fokker-Planck equation",
    "volume": "review",
    "abstract": "In over-parametrised networks, a large continuous set of solutions (an invariant manifold) exists where the empirical loss is minimal. However, noise in the learning dynamics can introduce a drift along this manifold, biasing the dynamics towards solutions with higher ``smoothness'', therefore acting as a regularizer. In Li et al. (2022), a derivation of this drift was presented, borrowing the results from Katzenberger (1991), which shows that in the small learning-rate limit, $\\eta \\to 0$, the learning dynamics can be approximated by a stochastic differential equation (SDE), whose solution exhibit two distinct phases: a first phase, occurring over a number of steps $O(\\eta^{-1})$, where the parameters are deterministically driven towards the invariant manifold; and a second phase, over timescales $O(\\eta^{-2})$, in which noise induces a deterministic drift along the invariant manifold. This latter contribution to the drift, can be regarded as the result of averaging the dynamics over the $O(\\eta^{1/2})$ fluctuations orthogonal to the manifold, described by an Ornstein--Uhlenbeck process, as first suggested by Blanc et al. (2020). We offer a new derivation of the results by Li et al. (2022), that builds on the very intuitive arguments by Blanc et al. (2020), by implementing the averaging of the Fokker-Planck equation associated with the $\\eta \\to 0$ dynamics over such Ornstein--Uhlenbeck quasi-stationary state. Our contribution demonstrates the application of multiscale methods for elliptic partial differential equations (PDEs) (Pavliotis and Stuart (2008)) to optimization problems in machine learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=AqN23oqraW": {
    "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
    "volume": "review",
    "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models, and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge-related systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=hv3SklibkL": {
    "title": "Graph Parsing Networks",
    "volume": "review",
    "abstract": "Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering. Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph. In this work, inspired by bottom-up grammar induction, we propose an efficient graph parsing algorithm to infer the pooling structure, which then drives graph pooling. The resulting Graph Parsing Network (GPN) adaptively learns personalized pooling structure for each individual graph. GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node information intact. Experimental results on standard benchmarks demonstrate that GPN outperforms state-of-the-art graph pooling methods in graph classification tasks while being able to achieve competitive performance in node classification tasks. We also conduct a graph reconstruction task to show GPN's ability to preserve node information and measure both memory and time efficiency through relevant tests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0nTk5BSvO": {
    "title": "TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts",
    "volume": "review",
    "abstract": "Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph. By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a gating problem into a classification problem with pseudo labels. Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=idpV2AqusC": {
    "title": "Improving SAM Requires Rethinking its Optimization Formulation",
    "volume": "review",
    "abstract": "This paper rethinks Sharpness-Aware Minimization (SAM), which is originally formulated as a zero-sum game where the weights of a network and a bounded perturbation try to minimize/maximize, respectively, the same differentiable loss. We argue that SAM should instead be reformulated using the 0-1 loss, as this provides a tighter bound on its generalization gap. As a continuous relaxation, we follow the simple conventional approach where the minimizing (maximizing) player uses an upper bound (lower bound) surrogate to the 0-1 loss. This leads to a novel formulation of SAM as a bilevel optimization problem, dubbed as BiSAM. Through numerical evidence, we show that BiSAM consistently results in improved performance when compared to the original SAM and variants, while enjoying similar computational complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=72MSbSZtHv": {
    "title": "RedMotion: Motion Prediction via Redundancy Reduction",
    "volume": "review",
    "abstract": "Predicting the future motion of traffic agents is vital for self-driving vehicles to ensure their safe operation. We introduce RedMotion, a transformer model for motion prediction that incorporates two types of redundancy reduction. The first type of redundancy reduction is induced by an internal transformer decoder and reduces a variable-sized set of road environment tokens, such as road graphs with agent data, to a fixed-sized embedding. The second type of redundancy reduction is a self-supervised learning objective and applies the redundancy reduction principle to embeddings generated from augmented views of road environments. Our experiments reveal that our representation learning approach can outperform PreTraM, Traj-MAE, and GraphDINO in a semi-supervised setting. Our RedMotion model achieves results that are competitive with those of Scene Transformer or MTR++. We provide an anonymized open source implementation that is accessible via Colab: https://colab.research.google.com/drive/16pwsmOTYdPpbNWf2nm1olXcx1ZmsXHB8",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Dxl0EuFjlf": {
    "title": "TILDE-Q: A Transformation Invariant Loss Function for Time-Series Forecasting",
    "volume": "review",
    "abstract": "Time-series forecasting has gained increasing attention in the field of artificial intelligence due to its potential to address real-world problems across various domains, including energy, weather, traffic, and economy. While time-series forecasting is a well-researched field, predicting complex temporal patterns such as sudden changes in sequential data still poses a challenge with current models. This difficulty stems from minimizing $L_p$ norm distances as loss functions, such as mean absolute error (MAE) or mean square error (MSE), which are susceptible to both intricate temporal dynamics modeling and signal shape capturing. Furthermore, these functions often cause models to behave aberrantly and generate uncorrelated results with the original time-series. Consequently, the development of a shape-aware loss function that goes beyond mere point-wise comparison is essential. In this paper, we examine the definition of shape and distortions, which are crucial for shape-awareness in time-series forecasting, and provide a design rationale for the shape-aware loss function. Based on our design rationale, we propose a novel, compact loss function called TILDE-Q (Transformation Invariant Loss function with Distance EQuilibrium) that considers not only amplitude and phase distortions but also allows models to capture the shape of time-series sequences. Furthermore, TILDE-Q supports the simultaneous modeling of periodic and nonperiodic temporal dynamics. We evaluate the efficacy of TILDE-Q by conducting extensive experiments under both periodic and nonperiodic conditions with various models ranging from naive to state-of-the-art. The experimental results show that the models trained with TILDE-Q surpass those trained with other metrics, such as MSE and DILATE, in various real-world applications, including electricity, traffic, economics, weather, and electricity transformer temperature (ETT)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OsGUnYOzii": {
    "title": "Learning From Simplicial Data Based on Random Walks and 1D Convolutions",
    "volume": "review",
    "abstract": "Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the increased expressivity of these models can indeed lead to a better classification performance and a more faithful representation of the underlying system, the computational cost of these higher-order models can increase dramatically. To this end, we here explore a simplicial complex neural network learning architecture based on random walks and fast 1D convolutions (SCRaWl), in which we can adjust the increase in computational cost by varying the length and number of random walks considered while accounting for higher-order relationships. Importantly, due to the random walk-based design, the expressivity of the proposed architecture is provably incomparable to that of existing message-passing simplicial neural networks. We empirically evaluate SCRaWl on real-world datasets and show that it outperforms other simplicial neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=H0RztJssmQ": {
    "title": "Adaptive Environmental Modeling for Task-Oriented Language Agents",
    "volume": "review",
    "abstract": "Recent advancements in the realm of intelligent agents, particularly those employing large language models, have been notably significant. Notwithstanding these advancements, intelligent agents encounter substantial challenges, predominantly in interactive and dynamic scenarios such as online shopping, attributed to an absence of integrated environmental modeling. In this paper, we propose a task-oriented environmental adaptation approach, allowing language agents to autonomously model new environments. This approach comprises two pivotal phases: Pre-Task Environment Exploration and In-Task Environment Update. The Pre-Task Environment Exploration phase incorporates a greedy exploration strategy, leveraging an agent in the role of an Evaluator to optimally explore environmental information based on present observations and feasible actions. This strategy is implemented through a recursive algorithm, enabling agents to choose and execute the top-k scored actions, thereby efficiently forming an Action-Observation Tree as the initial environmental modeling. During the In-Task Environment Update phase, agents employ environmental information to enhance task performance. The information generated from task execution and interaction trajectories is used to refine environmental modeling. These processes are iteratively executed, achieving mutual enhancement. We conduct a systematic evaluation of the environmental modeling, assessing both its effectiveness and comprehensiveness. The results demonstrate that under our approach, agents can indeed construct accurate environmental modeling. Simultaneously, we observe a significant enhancement in agent performance on both the ALFWorld-Eco and the WebShop benchmark datasets due to the application of environmental modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=wkbeqr5XhC": {
    "title": "LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition",
    "volume": "review",
    "abstract": "Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is LUM-ViT, a Vision Transformer variant. Uniquely, LUM-ViT incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a three-stage fine-tuning strategy. Our evaluations reveal that, by sampling a mere 10\\% of the original image pixels, LUM-ViT maintains the accuracy loss within 1.8\\% on the ImageNet classification task. The method sustains near-original accuracy when implemented on real-world optical hardware, demonstrating its practicality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXNLvfCxEr": {
    "title": "EvIL: Evolution Strategies for Generalisable Imitation Learning",
    "volume": "review",
    "abstract": "We present Evolutionary Imitation Learning (EvIL), a general approach to imitation learning (IL) able to predict agent behaviour across changing environment dynamics. In EvIL, we use Evolution Strategies to jointly meta-optimise the parameters (e.g. reward functions and dynamics) fed to an inner loop reinforcement learning procedure. In effect, this allows us to inherit some of the benefits of the inverse reinforcement learning approach to imitation learning while being significantly more flexible. Specifically, our algorithm can be applied with any policy optimisation method, without requiring the reward or training procedure to be differentiable. Our method succeeds at recovering a reward that induces expert-like behaviour across a variety of environments, even when the environment dynamics are not fully known. We test our method's effectiveness and generalisation capabilities in several tabular environments and continuous control settings and find that it outperforms both offline approaches, like behavioural cloning, and traditional inverse reinforcement learning techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MhzKwuvpm6": {
    "title": "RILe: Reinforced Imitation Learning",
    "volume": "review",
    "abstract": "Learning to imitate behaviors from a limited set of expert trajectories is a promising way to acquire a policy. In imitation learning (IL), an expert policy is trained directly from data in an efficient way, but requires vast amounts of data. On the other hand, inverse reinforcement learning (IRL) deduces a reward function from expert data and then learns a policy with reinforcement learning via this reward function. Although this mitigates the data requirement of imitation learning, IRL approaches suffer from efficiency issues because of sequential learning of the reward function and the policy. In this paper, we combine the strengths of imitation learning and inverse reinforcement learning and introduce RILe: Reinforced Imitation Learning. Our novel dual-agent framework enables joint training of a teacher agent and a student agent. The teacher agent learns the reward function from expert data. It observes the student agent's behavior and provides it with a reward signal. At the same time the student agent learns a policy by using reward signals given by the teacher. Training the student and the teacher jointly in a single learning process offers scalability and efficiency while learning the reward function helps to alleviate data-sensitivity. Experimental comparisons in reinforcement learning benchmarks against imitation learning baselines highlight the superior performance offered by RILe particularly when the number of expert trajectories is limited",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=EyDPfGy4Wh": {
    "title": "Few Heads are Enough",
    "volume": "review",
    "abstract": "The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. The recently proposed Flash-Attention reduces both compute and memory through a *hardware*-aware implementation. Can we achieve this also through *algorithmic* improvements? Here we present Expert Projection Attention (EPA) - a novel method that reduces both compute and memory requirements, while matching the language modeling performance of baseline Transformers using the same parameter budget. EPA uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient \"Fast Transformer\"",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=iKsu33WcmU": {
    "title": "ProtChatGPT: Towards Understanding Proteins with Large Language Models",
    "volume": "review",
    "abstract": "Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9TJDsOEaBC": {
    "title": "Bayesian Vector Optimization with Gaussian Processes",
    "volume": "review",
    "abstract": "Learning problems in which multiple conflicting objectives must be considered simultaneously often arise in various fields, including engineering, drug design, and environmental management. Traditional methods of multi-objective optimization, such as scalarization and identification of the Pareto set under componentwise order, have limitations in incorporating objective preferences and exploring the solution space accordingly. While vector optimization offers improved flexibility and adaptability via specifying partial orders based on ordering cones, current techniques designed for sequential experiments suffer from high sample complexity, which makes them unfit for large-scale learning problems. To address this issue, we propose VOGP, an ($\\epsilon,\\delta$)-PAC adaptive elimination algorithm that performs vector optimization using Gaussian processes. VOGP allows users to convey objective preferences through ordering cones while performing efficient sampling by exploiting the smoothness of the objective function, resulting in a more effective optimization process that requires fewer evaluations. We first establish provable theoretical guarantees for VOGP, and then derive information gain based and kernel specific sample complexity bounds. VOGP demonstrates strong empirical results on both real-world and synthetic datasets, outperforming previous work in sequential vector optimization and its special case multi-objective optimization. This work highlights the potential of VOGP as a powerful preference-driven method for addressing complex sequential vector optimization problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=I7kpf3mZ4n": {
    "title": "Meta- (out-of-context) learning in neural networks",
    "volume": "review",
    "abstract": "Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call **meta-out-of-context learning (meta-OCL)** via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily \"internalize\" the semantic content of text that is, *or appears to be*, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=lnffMykYSj": {
    "title": "On the Long Range Abilities of Transformers",
    "volume": "review",
    "abstract": "Despite their dominance in modern DL and, especially, NLP domains, transformer architectures exhibit sub-optimal performance on long-range tasks compared to recent layers that are specifically designed for this purpose. In this work, drawing inspiration from key attributes of long-range layers, such as state-space layers, linear RNN layers, and global convolution layers, we demonstrate that minimal mod- ifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers. We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality. As we show, integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters. Our experiments also shed light on the reasons for the inferior performance of transformers on long-range tasks and identify critical properties that are essential for successfully capturing long-range dependencies. Our code is attached as supplementary",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SQpnEfv9WH": {
    "title": "Social-Transmotion: Promptable Human Trajectory Prediction",
    "volume": "review",
    "abstract": "Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space. To address this, we introduce \\textit{Social-Transmotion}, a generic model that exploits the power of transformers to handle diverse and numerous visual cues, capturing the multi-modal nature of human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body poses. This, in turn, augments trajectory data, leading to enhanced human trajectory prediction. Our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between pedestrians based on the available visual cues, whether they are poses, bounding boxes, or a combination thereof. By the masking technique, we ensure our model's effectiveness even when certain visual cues are unavailable, although performance is further boosted with the presence of comprehensive visual data. We delve into the merits of using 2d versus 3d poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction. Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=JuyFppXzh2": {
    "title": "Gandalf: Learning label correlations in Extreme Multi-label Classification via Label Features",
    "volume": "review",
    "abstract": "Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches, amongst others. In this paper, we propose Gandalf, a novel approach which makes use of a label correlation graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. While most recent advances in XMC have been algorithmic, mainly aimed towards developing novel deep-learning frameworks, our data-centric augmentation approach is orthogonal to these methodologies, and can be applied in a plug-and-play manner to a variety of them. This generality and effectiveness of \\textit{Gandalf} is demonstrated by showing up to 30\\% relative improvements for 5 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3 million labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=wfgZc3IMqo": {
    "title": "Robust Classification via Regression-Based Loss Reweighting and Label Correction",
    "volume": "review",
    "abstract": "Deep neural networks and large-scale datasets have revolutionized the field of machine learning. However, these large networks are susceptible to overfitting to label noise, resulting in reduced generalization. To address this challenge, two promising approaches have emerged: i) loss reweighting, which reduces the influence of noisy examples on the training loss, and ii) label correction that replaces noisy labels with estimated true labels. These directions have been pursued separately or combined as independent methods, lacking a unified approach. In this work, we present a unified method that seamlessly combines loss reweighting and label correction to enhance robustness against label noise in classification tasks. Specifically, by leveraging ideas from compositional data analysis in statistics, we frame the problem as a regression task, where loss reweighting and label correction can naturally be achieved with a shifted Gaussian label noise model. Our unified approach achieves strong performance compared to recent baselines on several noisy labeled datasets. We believe this work is a promising step towards robust deep learning in the presence of label noise",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0xLWPdObG1": {
    "title": "Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features",
    "volume": "review",
    "abstract": "There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a sparsemax function, which allows feature selection in high-dimensional settings. To enhance practical performance and learning efficiency, we present an adjustment procedure for prediction of random parameters and a method-of-moments estimator for pretraining of variance component. Various experiential studies and real data analyses confirm the advantages of our proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0xT87opqKV": {
    "title": "ProteinAdapter: Adapting Pre-trained Large Protein Models for Efficient Protein Representation Learning",
    "volume": "review",
    "abstract": "The study of proteins is crucial in various scientific disciplines, but understanding their intricate multi-level relationships remains challenging. Recent advancements in Large Protein Models (LPMs) have demonstrated their ability in sequence and structure understanding, suggesting the potential of directly using them for efficient protein representation learning. In this work, we introduce ProteinAdapter, to efficiently transfer the general reference from the multiple Large Protein Models (LPMs), e.g., ESM-1b, to the task-specific knowledge. ProteinAdapter could largely save labor-intensive analysis on the 3D position and the amino acid order. We observe that such a simple yet effective approach works well on multiple downstream tasks. Specifically, (1) with limited extra parameters, ProteinAdapter enables multi-level protein representation learning by integrating both sequence and geometric structure embeddings from LPMs. (2) Based on the learned embedding, we further scale the proposed ProteinAdapter to multiple conventional protein tasks. Considering different task priors, we propose a unified multi-scale predictor to fully take advantage of the learned embeddings via task-specific focus. Extensive experiments on over 20 tasks show that ProteinAdapter outperforms state-of-the-art methods under both single-task and multi-task settings. We hope that the proposed method could accelerate the study of protein analysis in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3SJE1WLB4M": {
    "title": "Generalization error of spectral algorithms",
    "volume": "review",
    "abstract": "The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of \\emph{spectral algorithms} specified by profile $h(\\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. Under power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, giving a new perspective on the KRR saturation phenomenon (iii) conjecture, and demonstrate for the considered data models, the universality of the loss w.r.t. non-spectral details of the problem, but only in case of noisy observation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ali45HfJqJ": {
    "title": "Observer Uncertainty of Learning in Games from a Covariance Perspective",
    "volume": "review",
    "abstract": "We investigate the accuracy of prediction in deterministic learning dynamics of zero-sum games with random initializations, specifically focusing on observer uncertainty and its relationship to the evolution of covariances. Zero-sum games are a prominent field of interest in machine learning due to their various applications, such as Generative Adversarial Networks. Concurrently, the accuracy of observation in dynamical systems from mechanics has long been a classic subject of investigation since the discovery of the Heisenberg Uncertainty Principle. This principle employs covariance and standard deviation of particle states to measure observation accuracy. In this study, we bring these two approaches together to analyze the follow-the-regularized-leader (FTRL) algorithm in two-player zero-sum games. We provide growth rates of covariance information for continuous-time FTRL, as well as its two canonical discretization methods (Euler and symplectic). Our analysis and experiments shows that employing symplectic discretization enhances the accuracy of prediction in learning dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=cbVnJa4l2o": {
    "title": "LLM+A: Grounding Large Language Models in Physical World with Affordance Prompting",
    "volume": "review",
    "abstract": "While large language models (LLMs) are successful in completing various language processing tasks, they easily fail to interact with the physical world properly such as generating control sequences. We find that the main reason is that LLMs are not grounded in the physical world. Existing LLM-based approaches circumvent this problem by relying on additional pre-defined skills or pre-trained sub-policies, making it hard to adapt to new tasks. In contrast, we aim to address this problem and explore the possibility to prompt pre-trained LLMs to accomplish a series of robotic manipulation tasks in a training-free paradigm. Accordingly, we propose a framework called LLM+A(ffordance), where the LLM serves as both the sub-task planner (that generates high-level plans) and the motion controller (that generates low-level control sequences). To ground these plans and control sequences on the physical world, we develop the \\textit{affordance prompting} technique that stimulates the LLM to 1) predict the consequences of generated plans and 2) generate affordance values for relevant objects. Empirically, we evaluate the effectiveness of LLM+A in various robotic manipulation tasks with natural language instructions and demonstrate that our approach substantially improves the performance by enhancing the feasibility of generated plans and control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WBCPdhQPuz": {
    "title": "DAS$^2$C: A Distributed Adaptive Minimax Method with Near-Optimal Convergence",
    "volume": "review",
    "abstract": "Applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose DAS$^2$C, a $\\underline{\\text{D}}$istributed $\\underline{\\text{A}}$daptive method with time-scale \\$\\underline{\\text{S}}$eparated $\\underline{\\text{S}}$tepsize $\\underline{\\text{C}}$ontrol for minimax optimization. The key strategy is to employ an adaptive stepsize control protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state errors due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For non-convex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation and quasi-independence of networks, leading to a near-optimal convergence rate of $\\tilde{\\mathcal{O}} \\left( \\epsilon ^{-\\left( 4+\\delta \\right)} \\right)$ for any small $\\delta > 0$, matching that of the centralized counterpart. To the best of our knowledge, DAS$^2$C is the $\\textit{first}$ distributed adaptive method guaranteeing exact convergence without requiring to know any problem-dependent parameters for nonconvex minimax problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=dCHbFDsCZz": {
    "title": "Learning to Reject with a Fixed Predictor: Application to Decontextualization",
    "volume": "review",
    "abstract": "We study the problem of classification with a reject option for a fixed predictor, crucial to natural language processing. We introduce a new problem formulation for this scenario, and an algorithm minimizing a new surrogate loss function. We provide a complete theoretical analysis of the surrogate loss function with a strong $H$-consistency guarantee. For evaluation, we choose the \\textit{decontextualization} task, and provide a manually-labelled dataset of $2\\mathord,000$ examples. Our algorithm significantly outperforms the baselines considered, with a $\\sim 25$% improvement in coverage when halving the error rate, which is only $\\sim 3$% away from the theoretical limit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JL42j1BL5h": {
    "title": "All Languages Matter: On the Multilingual Safety of Large Language Models",
    "volume": "review",
    "abstract": "Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages. In addition, we propose several simple and effective prompting methods to improve the multilingual safety of ChatGPT by evoking safety knowledge and improving cross-lingual generalization of safety alignment. Our prompting method can significantly reduce the ratio of unsafe responses from 19.1\\% to 9.7\\% for non-English queries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oDYXpvnv5f": {
    "title": "Deep Anti-Regularized Ensembles",
    "volume": "review",
    "abstract": "We consider the problem of uncertainty quantification in high dimensional regression and classification, for which deep ensembles have proven to be promising methods. Recent observations have shown that deep ensembles often return overconfident estimates outside the training domain, which is a major limitation because shifted distributions are often encountered in real-life scenarios. The principal challenge for this problem is to solve the trade-off between increasing the diversity of the ensemble outputs and making accurate in-distribution predictions. In this work, we show that an ensemble of networks with large weights fitting the training data are likely to meet these two objectives. We derive a simple and practical approach to produce such ensembles, based on an original anti-regularization term penalizing small weights and a control process of the weight increase which maintains the in-distribution loss under an acceptable threshold. The developed approach does not require any out-of-distribution training data neither any trade-off hyper-parameter calibration. We derive a theoretical framework for this approach and show that the proposed optimization can be seen as a \"water-filling\" problem. Several experiments in both regression and classification settings highlight that Deep Anti-Regularized Ensembles (DARE) significantly improve uncertainty quantification outside the training domain in comparison to recent deep ensembles and out-of-distribution detection methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=RGE8Bs5Tra": {
    "title": "CLASS-INCREMENTAL LEARNING USING GENERATIVE EXPERIENCE REPLAY BASED ON TIME-AWARE REGULARIZATION",
    "volume": "review",
    "abstract": "Learning new tasks accumulatively without forgetting remains a critical challenge in continual learning. Generative experience replay addresses this challenge by synthesizing pseudo-data points for past learned tasks and later replaying them for concurrent training along with the new tasks' data. Generative replay is the best strategy for continual learning under a strict class-incremental setting when certain constraints need to be met: (i) constant model size, (ii) no pre-training dataset, and (iii) no memory buffer for storing past tasks data. Inspired by the biological nervous system mechanisms, we introduce a time-aware regularization method to dynamically fine-tune the three training objective terms used for generative replay: supervised learning, latent regularization, and data reconstruction. Experimental results on major benchmarks indicate that our method pushes the limit of a brain-inspired continual learner under such strict settings, improves memory retention, and increases the average performance over continually arriving tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=y3CsNQal2l": {
    "title": "Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging",
    "volume": "review",
    "abstract": "As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by aligning representations across languages or by explicitly translating target languages into source languages. However, these methods possess certain limitations and fail to fully exploit the potential of Large Language Models (LLMs). In this paper, we regard the ability of LLMs in a particular task and language as a combination of \"task ability\" and \"language ability\". In the context of parameter-efficient fine-tuning and cross-lingual transfer, task ability is represented by adapters fine-tuning on the target task in the source language, while language ability is the ability to solve problems using the specific target language. In this work, we propose a novel adaptive adapter merging method for cross-lingual transfer, termed as $\\texttt{AdaMergeX}$. As language ability is not tied to any specific task, we introduce another easily accessible reference task from which language ability is obtained by adapter merging. Then by further merging it with adapters tuned on the target task in the source language, we can achieve effective cross-lingual transfer. Furthermore, unlike existing model merging methods that employ arithmetic addition, we propose a new structured-adaptive merging method that adapts the merging process based on the structure of adapters. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=NF5uhYkI9C": {
    "title": "Thin-Thick Adapter: Segmenting Thin Scans Using Thick Annotations",
    "volume": "review",
    "abstract": "Medical imaging segmentation has been a prominent focus in the field of medical imaging analysis. Recent advances in radiological and storage technologies have led to an increased utilization of thin slice computed tomography (CT) acquisitions in clinical practice. These thin slices offer several advantages, including enhanced spatial resolution and sharper diagnostic information for clinicians. However, segmenting thin slices presents significant challenges. Annotations on thick is hard to adapt to the thin slices since there is a domain gap between thick and thin slices. Furthermore, there is no existing dataset which contains pixel-level thin annotations, and manually annotating thin slices is considerably more resource-intensive and time-consuming compared to annotating thick slices, making it impractical to obtain a sufficient quantity of high-quality thin annotations for training robust models in a supervised fashion. In response to these challenges, this paper introduces three key contributions. Firstly, we propose a research topic and setting focused on segmenting thin slice data exclusively, leveraging existing annotations from thick slices. Secondly, we present a newly created dataset called CQ500-Thin, which is a Non-Contrast CT scans featuring Intracranial Hemorrhage (ICH), including a subset of pixel-level thin annotations for evaluation purposes. This dataset serves as a benchmark for our proposed topic and methodology. Lastly, we introduce a robust pipeline named the Thin-Thick Adapter, which utilizes a simple-but-effective data alignment technique and a 3D-CPS for unsupervised domain adaptation. It is designed to address the thin slice segmentation problem and establish a foundational baseline for this emerging research area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ9lqUalq1": {
    "title": "$\\gamma$-Orthogonalized Tensor Deflation: Towards Robust \\& Interpretable Tensor Decomposition in the Presence of Correlated Components",
    "volume": "review",
    "abstract": "We tackle the problem of recovering a low-rank tensor signal with possibly correlated components from a random noisy tensor, or the so-called \\textit{spiked tensor model}. When the underlying components are orthogonal, they can be recovered efficiently using \\textit{tensor deflation}, while correlated components may alter the tensor deflation mechanism, thereby preventing efficient recovery. Relying on recently developed tools from random tensor theory, we deal precisely with the non-orthogonal case by deriving an asymptotic analysis of a \\textit{parameterized} deflation procedure, which we refer to as $\\gamma$-orthogonalized tensor deflation. Based on this analysis, an efficient tensor deflation algorithm is proposed by optimizing the parameter injected into the deflation mechanism, which in turn is proven to be optimal by construction for the studied tensor model. We perform a detailed theoretical and algorithmic analysis on the rank-2 order-3 model, and outline a general structure to tackle the problem in more generality for arbitrary ranks/orders, aiming to lead to a broader impact in machine learning and beyond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jZPqf2G9Sw": {
    "title": "Dynamics-Informed Protein Design with Structure Conditioning",
    "volume": "review",
    "abstract": "Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein's dynamical properties for its function, conditioning on dynamical properties remains elusive. We present a new approach to protein generative modeling by leveraging Normal Mode Analysis that enables us to capture dynamical properties too. We introduce a method for conditioning the diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to the classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding SDE theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of the dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the open-source unconditional protein diffusion model Genie into the conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible. Our work represents a first step towards incorporating dynamical behaviour in protein design and may open the door to designing more flexible and functional proteins in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6UQaXJm53B": {
    "title": "DfPO: Degeneration-free Policy Optimization via Action Masking in Natural Language Action Spaces",
    "volume": "review",
    "abstract": "As the pre-training objectives (e.g., next token prediction) of language models (LMs) are inherently not aligned with task scores, optimizing LMs to achieve higher downstream task scores is essential. One of the promising approaches is to fine-tune LMs by using reinforcement learning (RL). However, conventional RL methods based on PPO and a penalty of KL divergence are vulnerable to the text degeneration problem which LMs do not generate natural texts anymore after RL fine-tuning. To address this problem, we provide Degeneration-free Policy Optimization (DfPO) that can fine-tune LMs to generate texts that achieve improved downstream task scores, while preserving the naturalness of the generated texts. To achieve this, we introduce action-masked policy with which a behavior policy can avoid to select tokens that potentially make policy optimization unexpected. Then, we devise clipped advantage functions to separately perform likelihood maximization and minimization, conditioned on texts sampled from the action-masked policy. Our experiments on the GRUE benchmark demonstrate that DfPO successfully improves the downstream task scores, while preserving the naturalness of the generated texts. Moreover, even DfPO does not perform hyperparameter search, it outperforms PPO and NLPO which require additional hyperparameter search for the penalty ratio of KL divergence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZmbCZw81xf": {
    "title": "Syntactic Representations Enable Interpretable Hierarchical Word Vectors",
    "volume": "review",
    "abstract": "The distributed representations currently used are dense and uninterpretable, leading to interpretations that themselves are relative, overcomplete, and hard to interpret. We propose a method that transforms these word vectors into reduced syntactic representations. The resulting representations are interpretable in an absolute scale allowing better comparison and visualization of the word vectors and we successively demonstrate that the drawn interpretations are in line with human judgment. The syntactic representations are then used to create hierarchical word vectors using an incremental learning approach similar to the non-linear human learning approach. As these representations are drawn from pre-trained vectors, the generation process and learning approach are computationally efficient. Most importantly, we find out that the resulting hierarchical vectors outperform the original vectors in benchmark tests",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=5451cIQdWp": {
    "title": "On Synthetic Data and Iterative Magnitude Pruning: a Linear Mode Connectivity Study",
    "volume": "review",
    "abstract": "Recent works have shown that distilled data representations can be leveraged for accelerating the training of DNNs. However, to date, very little is understood about the effect of these synthetic data representations in the area of architectural optimization, specifically with Iterative Magnitude Pruning (IMP) and pruning at initialization. We push the boundaries of pruning with distilled data, matching the performance of traditional IMP on ResNet-18 \\& CIFAR-10 while using 150x less training points to find a sparsity mask. We find that distilled data guides IMP to discard parameters contributing to the sharpness of the loss landscape, fostering smoother landscapes. These synthetic subnetworks are stable to SGD noise at initialization in settings when the dense model or subnetworks found with standard IMP are not, such as ResNet-10 on ImageNet-10. In other words, training from initialization across different shuffling of data will result in linear mode connectivity, a phenomenon which rarely happens without some pretraining. We visualize these loss landscapes and quantitatively measure sharpness through hessian approximations to understand these effects. This behavior is heavily linked to the compressed representation of the data, highlighting the importance of synthetic data in neural architectural validation. In order to find both a high performing and robust sparse architecture, a more optimal synthetic data representation is needed that can compress irrelevant noise like distilled data, yet better maintain task-specific information from the real data as dataset complexity increases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=e5hZmQXHHg": {
    "title": "VRAda: A Variance Reduced Adaptive Algorithm for Stochastic Parameter-Agnostic Minimax Optimizations",
    "volume": "review",
    "abstract": "Stochastic parameter-agnostic minimax optimization provides a novel avenue for adjusting learning rates without relying on problem-dependent parameters, bridging the gap between theoretical and empirical machine learning results. While previous studies have successfully decoupled the timescales of primal and dual variables and proposed unified parameter-agnostic algorithms for minimax optimizations, the problem of varying inherent variances within the stochastic setting persists. Such variance degradation affects the desired ratio of learning rates. Intuitively, variance-reduced techniques hold the potential to address this issue efficiently. However, they require manually tuning problem-dependent parameters to attain an optimal solution. In this paper, we introduce the Variance-Reduced Adaptive algorithm (VRAda), a solution addressing varying inherent variances and enabling the parameter-agnostic manner in stochastic minimax optimizations. Theoretical results show that VRAda achieves an optimal sample complexity of $O(1/\\epsilon^3)$ without large data batches, enabling it to find an $\\epsilon$-stationary point on non-convex-strongly-concave and non-convex-Polyak-\\L ojasiewicz objectives. To the best of our knowledge, VRAda is the first variance-reduced adaptive algorithm designed specifically for parameter-agnostic minimax optimization. Extensive experiments conducted across diverse applications validate the effectiveness of VRAda",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=774elYc5tw": {
    "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=tEgrUrUuwA": {
    "title": "Partitioning Message Passing for Graph Fraud Detection",
    "volume": "review",
    "abstract": "Label imbalance and homophily-heterophily mixture are the fundamental problems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud Detection (GFD) tasks. Existing GNN-based GFD models are designed to augment graph structure to accommodate the inductive bias of GNNs towards homophily, by excluding heterophilic neighbors during message passing. In our work, we argue that the key to applying GNNs for GFD is not to exclude but to {\\em distinguish} neighbors with different labels. Grounded in this perspective, we introduce Partitioning Message Passing (PMP), an intuitive yet effective message passing paradigm expressly crafted for GFD. Specifically, in the neighbor aggregation stage of PMP, neighbors with different classes are aggregated with distinct node-specific aggregation functions. By this means, the center node can adaptively adjust the information aggregated from its heterophilic and homophilic neighbors, thus avoiding the model gradient being dominated by benign nodes which occupy the majority of the population. We theoretically establish a connection between the spatial formulation of PMP and spectral analysis to characterize that PMP operates an adaptive node-specific spectral graph filter, which demonstrates the capability of PMP to handle heterophily-homophily mixed graphs. Extensive experimental results show that PMP can significantly boost the performance on GFD tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=EmQSOi1X2f": {
    "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
    "volume": "review",
    "abstract": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our analysis reveals the prevalence of self-contradictions when LMs generate text for open-domain topics, e.g., in 17.7% of all sentences produced by ChatGPT. Self-contradiction also complements retrieval-based methods, as a large portion of them (e.g., 35.8% for ChatGPT) cannot be verified using Wikipedia. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require external grounded knowledge. Our approach is practically effective and has been released as a push-button tool to benefit the public, with an anonymized version at https://iclr9113.com/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YkR9UFlQ1s": {
    "title": "Non-backtracking Graph Neural Networks",
    "volume": "review",
    "abstract": "The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-GNN on long-range graph benchmark and transductive node classification problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=otoggKnn0A": {
    "title": "FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes",
    "volume": "review",
    "abstract": "A typical task in the field of video understanding is hand action recognition, which has a wide range of applications. Existing works either mainly focus on full-body actions, or the defined action categories are relatively coarse-grained. In this paper, we propose FHA-Kitchens, a novel dataset of fine-grained hand actions in kitchen scenes. In particular, we focus on human hand interaction regions and perform deep excavation to further refine hand action information and interaction regions. Our FHA-Kitchens dataset consists of 2,377 video clips and 30,047 images collected from 8 different types of dishes, and all hand interaction regions in each image are labeled with high-quality fine-grained action classes and bounding boxes. We represent the action information in each hand interaction region as a triplet, resulting in a total of 878 action triplets. Based on the constructed dataset, we benchmark representative action recognition and detection models on the following three tracks: (1) supervised learning for hand interaction region and object detection, (2) supervised learning for fine-grained hand action recognition, and (3) intra- and inter-class domain generalization for hand interaction region detection. The experimental results offer compelling empirical evidence that highlights the challenges inherent in fine-grained hand action recognition, while also shedding light on potential avenues for future research, particularly in relation to pre-training strategy, model design, and domain generalization. The dataset will be released on the FHA-Kitchens project website",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4fVuBf5HE9": {
    "title": "Towards Analyzing Self-attention via Linear Neural Network",
    "volume": "review",
    "abstract": "Self-attention is a key component of the transformer architecture which has driven much of recent advances in AI. Theoretical analysis of self-attention has received significant attention and remains a work in progress. In this paper, we analyze gradient flow training of a simplified transformer model consisting of a single linear self-attention layer (thus it lacks softmax, MLP, and layer-normalization) with a single head on a histogram-like problem: the input is a sequence of characters from an alphabet and the output is the vector of counts of each letter in the input sequence. Our analysis goes via a reduction to 2-layer linear neural networks in which the input layer matrix is a diagonal matrix. We provide a complete analysis of gradient flow on these networks. Our reduction to linear neural networks involves one assumption which we empirically verify. Our analysis extends to various extensions of the histogram problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=lhZEodF8Dn": {
    "title": "Efficient Denoising Diffusion via Probabilistic Masking",
    "volume": "review",
    "abstract": "Diffusion models have exhibited remarkable advancements in generating high-quality data. However, a critical drawback of these models is their computationally intensive inference process, which requires a large number of timesteps to generate a single sample. Existing methods address this challenge by decoupling the forward and reverse processes, and they rely on handcrafted rules (e.g., uniform skipping) for sampling acceleration, leading to the risk of discarding important steps and deviating from the optimal trajectory. In this paper, we propose an Efficient Denoising Diffusion method via Probabilistic Masking (EDDPM) that can identify and skip the redundant steps during training. To determine whether a timestep should be skipped or not, we employ probabilistic reparameterization to continualize the binary determination mask. The mask distribution parameters are learned jointly with the diffusion model weights. By incorporating a real-time sparse constraint, our method can effectively identify and eliminate unnecessary steps during the training iterations, thereby improving inference efficiency. Notably, as the model becomes fully trained, the random masks converge to a sparse and deterministic one, retaining only a small number of essential steps. Empirical results demonstrate the superiority of our proposed EDDPM over the state-of-the-art sampling acceleration methods across various domains. EDDPM can generate high-quality samples with only 20\\% of the steps for time series imputation and achieve 4.89 FID with 5 steps for CIFAR-10. Moreover, when starting from a pretrained model, our method efficiently identifies the most informative timesteps within a single epoch, which demonstrates the potential of EDDPM to be a practical tool to explore large diffusion models with limited resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=dAqH7CfHjL": {
    "title": "Phase Transitions in Contrastive Learning",
    "volume": "review",
    "abstract": "How do self-supervised models actually train? We study the training dynamics of contrastive learning in three settings: a theoretical linear setting, on a low-dimensional physics-inspired dataset, and on full-fledged computer vision datasets including ImageNet. In all three settings, we show the existence of *phases*, i.e. locally stable or metastable representations, and of *phase transitions*, wherein a model rapidly and unexpectedly switches between different phases. Geometrically motivated metrics are developed to measure phase transitions. Finally, we show that phase transitions can be sped up with more robust augmentations. Code and visualizations will be made public upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=w8BL1NShjk": {
    "title": "There is More to Graphs than Meets the Eye: Learning Universal Features with Self-supervision",
    "volume": "review",
    "abstract": "We study the problem of learning universal features from multiple graphs through self-supervision. Graph self-supervised learning has been shown to facilitate representation learning, and produce competitive models compared to supervised baselines. However, existing methods of self-supervision learn features from one graph, and thus, produce models that are specialized to a particular graph. We hypothesize that leveraging multiple graphs of a family can improve the quality of learnt representations in the model by extracting features that are universal to the family of graphs. To achieve this, we propose a framework that can learn generalisable representations from disparate node features of different graphs. We first homogenise the disparate features with graph-specific modules, which feed into a universal representation learning module for generalisable feature learning. We show that leveraging multiple graphs of the same family improves the quality of representations and results in better performance on downstream node classification task compared to self-supervision with one graph. In this paper, we present a principled way to design foundation graph models that are capable of learning from a set of graphs in a holistic manner. This approach bridges the gap between self-supervised and supervised performance, while reducing the computational time for self-supervision and parameters of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=ER1VDuwWvB": {
    "title": "CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity",
    "volume": "review",
    "abstract": "With distributed machine learning being a prominent technique for large-scale machine learning tasks, communication complexity has become a major bottleneck for speeding up training and scaling up machine numbers. In this paper, we propose a new technique named Common randOm REconstruction(CORE), which can be used to compress the information transmitted between machines in order to reduce communication complexity without other strict conditions. Especially, our technique CORE projects the vector-valued information to a low-dimensional one through common random vectors and reconstructs the information with the same random noises after communication. We apply CORE to two distributed tasks, respectively convex optimization on linear models and generic non-convex optimization, and design new distributed algorithms, which achieve provably lower communication complexities. For example, we show for linear models CORE-based algorithm can encode the gradient vector to $\\mathcal{O}(1)$-bits (against $\\mathcal{O}(d)$), with the convergence rate not worse, preceding the existing results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=d6oUP1tyNx": {
    "title": "The KNN Score for Evaluating Probabilistic Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "Time series forecasting is a critical task in various domains. With the aim of comprehending interconnections and dependencies among variables, as well as gaining insights into a range of potential future outcomes, probabilistic multivariate time series forecasting has emerged as a prominent approach. The evaluation of models employed in this task is crucial yet challenging. Comparing a set of predictions against a single observed future presents difficulties, and accurately measuring whether a model correctly predicts dependencies between different time steps and individual series further compounds the complexity. We observe that metrics which are currently employed fall short in providing a comprehensive assessment of model performance. To address this limitation, we propose a novel metric based on density estimation as an alternative. We showcase the advantages of our metric both qualitatively and quantitatively, underscoring its effectiveness in assessing forecast quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1zhM0XkQh0": {
    "title": "ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations",
    "volume": "review",
    "abstract": "Supervised adversarial training has been the most successful approach for improving the robustness of Deep Neural Networks against adversarial attacks. While several recent works have attempted to overcome the need for supervision or labeled training data by integrating adversarial training with contrastive Self-Supervised Learning (SSL) approaches such as SimCLR, their performance has been sub-optimal due to the increased training complexity. A recent approach mitigates this by utilizing supervision from a standard self-supervised trained model in a teacher-student setting that mimics supervised adversarial training. However, we find that there is still a large gap in performance when compared to supervised training, specifically on larger capacity models. We show that this is a result of mismatch in training objectives of the teacher and student, and propose Projected Feature Adversarial Training (ProFeAT) to bridge this gap by using a projection head in the adversarial training step. We further propose appropriate attack and defense losses at the feature and projector spaces, coupled with a combination of weak and strong augmentations for the teacher and student respectively, to improve generalization without increasing the training complexity. We demonstrate significant improvements in performance when compared to existing SSL methods, and performance on par with TRADES, a popular supervised adversarial training method, on several benchmark datasets and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=rmLTwKGiSP": {
    "title": "Semi-Anchored Gradient Methods for Nonconvex-Nonconcave Minimax Problems",
    "volume": "review",
    "abstract": "Nonconvex-nonconcave minimax problems are difficult to optimize by gradient methods. The extragradient method, proven to outperform the gradient descent ascent, has become standard but there is still room for improvement. On the other hand, under a bilinear setting, the primal-dual hybrid gradient (PDHG) method is one of the most popular methods. This was studied on a general convex-concave problem, but it has not been found useful in a more general nonconvex-nonconcave minimax problem. In this paper, we demonstrate its natural extension to a structured nonconvex-nonconcave minimax problem, whose saddle-subdifferential operator satisfies the weak Minty variational inequality condition, showing its potential. This new nonlinear variant of PDHG, named semi-anchored (SA) gradient method, is built upon the theory of Bregman proximal point method. This consequently provides a worst-case convergence rate, in terms of a new optimality measure for nonconvex-nonconcave minimax optimization, making it interesting on its own. We further illustrate the potential of the semi-anchoring by providing a numerical experiment on fair classification problem, in comparison with the extragradient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xw3fStKCwm": {
    "title": "Tensor-Train Point Cloud Compression and Efficient Approximate Nearest Neighbor Search",
    "volume": "review",
    "abstract": "Nearest-neighbor search in large vector databases is crucial for various machine learning applications. This paper introduces a novel method using **tensor-train** (TT) low-rank tensor decomposition to efficiently represent point clouds and enable fast approximate nearest-neighbor searches. We propose a probabilistic interpretation and utilize density estimation losses like Sliced Wasserstein to train TT decompositions, resulting in robust point cloud compression. We reveals an inherent hierarchical structure within TT point clouds, facilitating efficient approximate nearest-neighbor searches. In our paper, we provide detailed insights into the methodology and conduct comprehensive comparisons with existing methods. We demonstrate its effectiveness in various scenarios, including out-of-distribution (OOD) problems and approximate nearest-neighbor (ANN) search tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=5twh6pM4SR": {
    "title": "Automating Continual Learning",
    "volume": "review",
    "abstract": "General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from the so-called catastrophic forgetting (CF) problem---previously acquired skills are forgotten when a new task is learned. Developing continual learning algorithms to address CF remains an open research question. Instead of hand-crafting such algorithms, our new Automated Continual Learning (ACL) trains self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata---good performance on both old and new tasks---into its learning objectives. We demonstrate the effectiveness and promise of ACL on multiple few-shot and standard image classification tasks adopted for continual learning: Mini-ImageNet, Omniglot, FC100, MNIST-families, and CIFAR-10",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Cw6lk56w6z": {
    "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks",
    "volume": "review",
    "abstract": "In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on $18$ specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=bXI0thP733": {
    "title": "Mitigating backdoor attacks with generative modelling and dataset relabelling",
    "volume": "review",
    "abstract": "Data-poisoning attacks change a small portion of the training dataset by introducing hand-crafted triggers and rewiring the corresponding labels towards a desired target class. Training on such data injects a backdoor into the model, that causes incorrect inference in selected test examples. Existing defenses mitigate the risks of such attacks through various modifications of the standard discriminative learning procedure. This paper explores a different approach that promises clean models by means of per-class generative modelling. We start by mapping the input data into a suitable latent space by leveraging a pre-trained self-supervised feature extractor. Interestingly, these representations get either preserved or heavily disturbed under recent backdoor attacks. In both cases, we find that per-class generative models give rise to probabilistic densities that allow both to detect the poisoned data and to find their original classes. This allows to patch the poisoned dataset by reverting the original labels and considering the triggers as a kind of augmentation. Our experiments show that training on patched datasets greatly reduces attack success rate and retains the clean accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=0GZ1Bq4Tfr": {
    "title": "Layer-wise Pre-weight Decay",
    "volume": "review",
    "abstract": "In deep learning, weight decay is a regularization mechanism been widely adopted to improve the generalization performance. Previously, a common understanding of the role of weight decay was that it contributes by pushing the model weights to approach 0 at each time step. However, our findings challenge this notion and argue the objective of weight decay is to make the weights approach the negative value of the update term instead of 0, thereby indicating a delay defect in certain steps that results in opposing penalties. In addition, we study the negative side effect of weight decay, revealing it will damage the inter-layer connectivity of the network while reducing weight magnitude. To address these issues, we first propose real-time weight decay to fix the delay defect by penalizing both the weights and the gradients at each time step. Then, we advance the decay step before the update function as pre-weight decay to mitigate the performance drop raised by the side effect. To further improve the general performance and enhance model robustness towards the decay rate, we finally introduce a layer-wise pre-weight decay to adjust the decay rate based on the layer index. Extensive analytical and comparative experiments demonstrate that the proposed $\\textit{layer-wise pre-weight decay}$ (LPWD) (i) exhibits remarkable robustness to the decay rate, and (ii) significantly improves the generalization performance across various conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=QHVTxso1Is": {
    "title": "Efficient Unsupervised Knowledge Distillation with Space Similarity",
    "volume": "review",
    "abstract": "In this paper, we aim to boost performance of knowledge distillation without the ground-truth labels. Hence, a student can only rely on the response generated by its teacher. Many existing approaches under this setting rely on some form of feature/embedding queue to capture neighbourhood information. These queues can be as large as over 100k samples. Also, some of these methods rely on multitude of operations which as a result increases the memory requirement for training many folds. In this work, we show that merely working with the input batch (often of size $256$) it is possible to not only incorporate neighbourhood information but also obtain state of the art unsupervised distillation performance. We achieve this by introducing a simple space similarity loss component which works alongside the well known normalized cosine similarity computed on the final features. In this loss, we motivate each dimension of a student's feature space to be similar to the corresponding dimension of its teacher. With this seemingly simple addition, we are able to compete against many contemporary methods which either rely on large number of queued features or heavy pre-processing. We perform extensive experiments comparing our proposed approach to other state of the art methods on various computer vision tasks for established architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0cJ8ERfnrM": {
    "title": "Antibody DomainBed: Out-of-Distribution Generalization in Therapeutic Protein Design",
    "volume": "review",
    "abstract": "Recently, there has been an increased interest in accelerating drug design with machine learning (ML). Active ML-guided design of biological sequences with favorable properties involves multiple design cycles in which (1) candidate sequences are proposed, (2) a subset of the candidates is selected using ML surrogate models trained to predict target properties of interest, and (3) sequences are experimentally validated. The returned experimental results from one cycle provide valuable feedback for the next one, but the modifications they inspire in the candidate proposals or experimental protocol can lead to distribution shifts that impair the performance of surrogate models in the upcoming cycle. For the surrogate models to achieve consistent performance across cycles, we must explicitly account for the distribution shifts in their training. We apply domain generalization (DG) methods to develop robust classifiers for predicting properties of therapeutic antibodies. We adapt a recent benchmark of DG algorithms, ``DomainBed,'' to deploy DG algorithms across 5 domains, or design cycles. Our results suggest that foundational models and ensembling (in both output and weight space) lead to better predictive performance on out-of-distribution domains. We publicly release our codebase and the associated dataset of antibody-antigen binding that emulates distribution shifts across design cycles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Ffjc8ApSbt": {
    "title": "Adaptive Causal Balancing for Collaborative Filtering",
    "volume": "review",
    "abstract": "Collaborative filtering builds personalized models from the collected user feedback. However, the collected data is observational rather than experimental, leading to various biases in the data, which can significantly affect the learned model. To address this issue, many studies have focused on propensity-based methods to combat the selection bias by reweighting the sample loss, and demonstrate that balancing is important for debiasing both theoretically and empirically. However, there are two questions that still need to be addressed: which function class should be balanced and how to effectively balance that function class? In this paper, we first perform theoretical analysis to show the effect of balancing finite-dimensional function classes on the bias of IPS and DR methods, and based on this, we propose a universal kernel-based balancing method to balance functions on the reproducing kernel Hilbert space. In addition, we propose a novel adaptive causal balancing method during the alternating update between unbiased evaluation and training of the prediction model. Specifically, the prediction loss of the model is projected in the kernel-based covariate function space, and the projection coefficients are used to determine which functions should be prioritized for balancing to reduce the estimation bias. We conduct extensive experiments on three real-world datasets to demonstrate the effectiveness of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=v5lmhckxlu": {
    "title": "Integrated Model Explanations by Independent and Collaborative Feature Influence via Linear-Nonlinear Perspectives",
    "volume": "review",
    "abstract": "In machine learning, model-agnostic explanation methods try to give explanation to model prediction by assessing the importance of input features. While linear simplification methods guarantee good properties, they have to include nonlinear feature interactions into linear coefficients. On the other hand, feature influence analysis methods examine feature relevance, but do not consistently preserve the desirable properties for robust explanations. Our approach seeks to inherit properties from linear simplification methods while systematically capturing feature interactions. To achieve this, we consider the explained model from two aspects: the linear aspect, which focuses on the independent influence of features to model predictions, and the nonlinear aspect, which concentrates on modeling feature interactions and their collaborative impact on model predictions. In practice, our method initially investigates both the linear and nonlinear aspects of the model being explained. It then extracts the independent and collaborative importance of features on model predictions and consistently combines them to ensure that the resulting feature importance preserves the desirable properties for robust explanations. Consequently, our Linear-Nonlinear Explanation (LNE) method provides a comprehensive understanding on how features influence model predictions. To validate its effectiveness, experiments demonstrate that linear, nonlinear, and the combined feature importance all offer valuable insights for explaining model predictions. We also compare the performance of LNE with other methods on explaining well-trained classifiers, and find our explanations align more closely with human intuitions. Additionally, user study shows our method can hint humans with potential biases in classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=eQcVfCK5cO": {
    "title": "Where is the Invisible: Spatial-Temporal Reasoning with Object Permanence",
    "volume": "review",
    "abstract": "Object permanence is a cognitive ability that enables humans to reason about the existence and location of objects that are not visible in the scene, such as those occluded or contained by other objects. This ability is crucial for visual object tracking, which aims to identify and localize the target object across video frames. However, most existing tracking methods rely on deep learning models that learn discriminative visual features from the visual context and fail to handle the cases where the object disappears from the image, e.g., occluded or contained by other objects. In this paper, we propose a novel framework for tracking invisible objects based on Qualitative-Quantitative Spatial-Temporal Reasoning (QQ-STR), inspired by the concept of object permanence. Our framework consists of three modules: a visual perception module, a qualitative spatial relation reasoner (SRR), and a quantitative relation-conditioned spatial-temporal relation analyst (SRA). The SRR module infers the qualitative relationship between each object and the target object based on the current and historical observations, while the SRA module predicts the quantitative location of the target object based on the inferred relationship and a diffusion model that captures the object's motion. We devise a self-supervised learning mechanism that does not require explicit relation annotations and leverages the predicted trajectories to locate the invisible object in videos. We evaluate our framework on a synthetic dataset (LA-CATER) and a new real-world RGB-D video dataset for invisible object tracking (iVOT) that contains challenging scenarios of human-object interactions with frequent occlusion and containment events. Our framework achieves comparable performance to state-of-the-art tracking methods that use additional relation annotations, demonstrating its generalization ability to novel scenes and viewpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ABIcBDLBVG": {
    "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
    "volume": "review",
    "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=AyXIDfvYg8": {
    "title": "Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching",
    "volume": "review",
    "abstract": "Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy loss for classification. While prior studies have demonstrated that both losses yield symmetric training representations under balanced data, this symmetry breaks under class imbalances. This paper presents an intriguing discovery: the introduction of a ReLU activation at the final layer effectively restores the symmetry in SCL-learned representations. We arrive at this finding analytically, by establishing that the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints form an orthogonal frame. Extensive experiments conducted across various datasets, architectures, and imbalance scenarios corroborate our finding. Importantly, our experiments reveal that the inclusion of the ReLU activation restores symmetry without compromising test accuracy. This constitutes the first geometry characterization of SCL under imbalances. Additionally, our analysis and experiments underscore the pivotal role of batch selection strategies in representation geometry. By proving necessary and sufficient conditions for mini-batch choices that ensure invariant symmetric representations, we introduce batch-binding as an efficient strategy that guarantees these conditions hold",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ADDCErFzev": {
    "title": "Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems",
    "volume": "review",
    "abstract": "According to the efficient coding hypothesis, neural populations encode information optimally when representations are high-dimensional and uncorrelated. However, such codes may carry a cost in terms of generalization and robustness. Past empirical studies of early visual cortex (V1) in rodents have suggested that this tradeoff indeed constrains sensory representations. However, it remains unclear whether these insights generalize across the hierarchy of the human visual system, and particularly to object representations in high-level occipitotemporal cortex (OTC). To gain new empirical clarity, here we develop a family of object recognition models with parametrically varying dropout proportion $p$, which induces systematically varying dimensionality of internal responses (while controlling all other inductive biases). We find that increasing dropout produces an increasingly smooth, low-dimensional representational space. Optimal robustness to lesioning is observed at around 70% dropout, after which both accuracy and robustness decline. Representational comparison to large-scale 7T fMRI data from occipitotemporal cortex in the Natural Scenes Dataset reveals that this optimal degree of dropout is also associated with maximal emergent neural predictivity. Finally, using new techniques for achieving denoised estimates of the eigenspectrum of human fMRI responses, we compare the rate of eigenspectrum decay between model and brain feature spaces. We observe that the match between model and brain representations is associated with a common balance between efficiency and robustness in the representational space. These results suggest that varying dropout may reveal an optimal point of balance between the efficiency of high-dimensional codes and the robustness of low dimensional codes in hierarchical vision systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tMKz4IgSZQ": {
    "title": "Controllable Text-to-Image Generation with Automatic Sketches",
    "volume": "review",
    "abstract": "Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-the-art on spatial arrangement and object positioning generation. It enhances users' control of object positions, sizes, etc., nearly doubling the accuracy of prior models. As a first attempt, our work shows the potential for employing LLMs to enhance performance in computer vision tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=p14iRzavpt": {
    "title": "Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher",
    "volume": "review",
    "abstract": "Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, knowledge distillation employs teacher-forcing learning, where the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this \"distribution closeness\" and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on five datasets demonstrate PTLoss can significantly improve the distillation effectiveness for teachers of various scales",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=hIpUwg8kAU": {
    "title": "Estimation error of gradient descent in deep regressions",
    "volume": "review",
    "abstract": "To achieve a theoretical understanding of deep learning, it is necessary to consider the approximation, generalization, and optimization errors. In recent years, there have been significant advancements in the literature regarding each or two of these errors. However, there have been few works that simultaneously analyze all three errors. This is due to the gap that exists between the optimization and generalization errors in over-parameterized regimes. In this work, we attempt to bridge this gap by establishing consistency between the outputs of gradient descent and the true regression function in the over-parameterized scenario. Our research offers a feasible perspective for a more comprehensive understanding of the theory behind deep learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=oVVLBxVmbZ": {
    "title": "Fast Conditional Intervention in Algorithmic Recourse with Reinforcement Learning",
    "volume": "review",
    "abstract": "Explaining the decisions made by machine learning classifiers aids individuals in identifying critical factors and charting future plans. Recent studies have shown that incorporating causal graphs of input features provides more realistic explanations; however, this also introduces new challenges such as handling noisy graphs and efficiently performing inference with black-box classifiers. In this work, we tackle these issues by presenting an efficient reinforcement learning (RL)-based approach with an idea of conditional intervention. Our intervention method is theoretically preferable and considers both feature dependencies and incompleteness of graphs. Simultaneously, the RL-based method offers the capacity to learn the intervention process while guarantees computational complexity at inference stage. In the experiments, we showcase the efficiency and superior performance of our solution when compared to baseline methods on both synthetic and real datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=327tbF3S65": {
    "title": "Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations",
    "volume": "review",
    "abstract": "Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel conditioning mechanism for evaluating INRs with the generated hierarchically decomposed basis fields to further enhance expressive power. Extensive experiments across four modalities, \\eg, 2D images, 3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets, demonstrate the versatility of DDMI and its superior performance compared to the existing INR generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=9jmUwjZi7j": {
    "title": "DreamFuser: Value-guided Diffusion Policy for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "Recent advances in reinforcement learning have underscored the potential of diffusion models, particularly in the context of policy learning. While earlier applications were predominantly focused on single-timestep settings, trajectory-based diffusion policy learning promises significant superiority, especially for low-level control tasks. In this context, we introduce DreamFuser, a trajectory-based value optimization approach that seamlessly blends the merits of diffusion-based trajectory learning and efficient Q function learning over state and noisy action. To address the computational challenges associated with action sampling of diffusion policy during the training phase, we design the DreamFuser based on the Generalized Noisy Action Markov Decision Process (GNMDP), which views the diffusion denoising process as part of the MDP transition. Empirical tests reveal DreamFuser's advantages over existing diffusion policy algorithms, notably in low-level control tasks. When benchmarked against the standard benchmark of offline reinforcement learning D4RL, DreamFuser matches or even outperforms contemporary methods. This work also elucidates the parallels between the optimization process of DreamFuser over GNMDP and Diffusion Policy over MDP, demonstrating its computational and memory advantages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=H7R0z6V9fR": {
    "title": "TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems",
    "volume": "review",
    "abstract": "Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall re- main invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this pa- per, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and backward trajectories predicted by a contin- uous graph neural network-based ordinary differential equation (GraphODE). It effectively imposes time-reversal symmetry to enable more accurate model pre- dictions across a wider range of dynamical systems under classical mechanics. In addition, we further provide theoretical analysis to show that our regularization essentially minimizes higher-order Taylor expansion terms during the ODE inte- gration steps, which enables our model to be more noise-tolerant and even applica- ble to irreversible systems. Experimental results on a variety of physical systems demonstrate the effectiveness of our proposed method. Particularly, it achieves an MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0upMDCx8AA": {
    "title": "Post-Training Recovery from Injected Bias with Self-Influence",
    "volume": "review",
    "abstract": "Learning generalized models from biased data with strong spurious correlations to the class label is an important undertaking toward fairness in deep learning. In the absence of any prior knowledge or supervision of bias, recent studies tackle the problem by presuming the bias severity to be sufficiently high and employing a bias-amplified model trained by empirical risk minimization (ERM) to identify and utilize bias-conflicting samples that are free of spurious correlations. However, insufficient preciseness in detecting bias-conflicting samples results in injecting erroneous signals during training; conversely, it leads to learning malignant biases instead of excluding them. In practice, as the presumption about the magnitude of bias often does not hold, it is important for the model to demonstrate robust performance across a wide spectrum of biases. In this paper, we propose SePT (Self-influence-based Post-Training), a fine-tuning framework leveraging the self-influence score to filter bias-conflicting samples, which yields a pivotal subset with significantly diminished spurious correlations. Our method enables the quick recovery of a biased model from learned bias through fine-tuning with minimal friction. In addition, SePT also utilizes the remaining training dataset to adjust the model, thereby maintaining robust performance in situations with weak spurious correlation or even in the absence of it. Experiments on diverse benchmark datasets with a wide range of bias strengths show that SePT is capable of boosting the performance of both bias-injected and state-of-the-art debiased models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=djcciHhCrt": {
    "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always ($\\sim$98\\%) while maintaining high similarity to clean images ($\\sim$0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=bZh06ptG9r": {
    "title": "FedLoRA: When Personalized Federated Learning Meets Low-Rank Adaptation",
    "volume": "review",
    "abstract": "In this research paper, we introduce a novel approach to Personalized Federated Learning (PFL), which we call FedLoRA. This approach is inspired by recent advancements in fine-tuning Large Language Models (LLMs), particularly the Low-Rank Adaptation (LoRA) technique. The remarkable success of LoRA demonstrates that general linguistic knowledge is preserved in a pre-trained full-rank model, while domain-specific knowledge can be effectively retained within a low-rank parameter matrix. Building upon this insight, we present FedLoRA in the context of PFL, aiming to maintain shared general knowledge among all clients in a common full-rank matrix, while capturing client-specific knowledge within a personalized low-rank matrix. However, the integration of LoRA into PFL presents its own set of challenges. Unlike LoRA, which starts with pre-trained general knowledge, FedLoRA's full-rank matrix needs training from scratch. This phase can be notably influenced by data heterogeneity, potentially hindering its effective extraction of general knowledge. To address this challenge, we propose a new training strategy to mitigate the effects of data heterogeneity on the shared full-rank matrix. Our experimental results, obtained across multiple datasets exhibiting varying degrees of data heterogeneity, demonstrate that FedLoRA outperforms current state-of-the-art methods significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=MZs2dgOudB": {
    "title": "Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling",
    "volume": "review",
    "abstract": "Most meta-learning methods assume that the (very small) context set used to establish a new task at test time is passively provided. In some settings, however, it is feasible to actively select which points to label; the potential gain from a careful choice is substantial, but the setting requires major differences from typical active learning setups. We clarify the ways in which active meta-learning can be used to label a context set, depending on which parts of the meta-learning process use active learning. Within this framework, we propose a natural algorithm based on fitting Gaussian mixtures for selecting which points to label; though simple, the algorithm also has theoretical motivation. The proposed algorithm outperforms state-of-the-art active learning methods when used with various meta-learning algorithms across several benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1tDoI2WBGE": {
    "title": "A Neural Sandbox Framework for Discovering Spurious Concpets in LLM Decisions",
    "volume": "review",
    "abstract": "We introduce a neural sandbox framework for text classification via self-referencing defined label concepts from an Large Language Model(LLM). The framework draws inspiration from the define-optimize alignment problem, in which the motivations of a model are described initially and then the model is optimized to align with these predefined objectives. In our case, we design our framework to perform text classification. We take a frozen LLM as a vector embedding generator for text and provide our framework with defined concept words based on the labels along with the input text. We then optimize an operator to classify the input text based on the relevance scores to the concept operator words(cop-words). In our experiments with multiple text classification datasets and LLM models, we find, incorporating our sandbox network generally improves the accuracy by a range of 0.12\\% to 6.31\\% in accuracy and 0.3\\% to 8.82\\% in macro f1 when compared to a baseline. The framework, not only serves as a classification tool but also as a descriptive tool for the model's decision of its prediction, based on the provided cop-words. Through further evaluations involving the injection of \"foreign\" cop-words, we showcase the sandbox framework's capacity to exhibit a coherent understanding of learned concepts and construct methodologies to discover potential spurious behaviors and biases within it. Despite witnessing results confirming our network's ability to capture domain knowledge, we show evidence that the model's secondary incentives do not match human decisions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQZuCuFeAM": {
    "title": "From Random to Relevant: Harnessing Salient Masks in Non-IID Federated Learning",
    "volume": "review",
    "abstract": "Federated learning (FL) offers the ability to train models using decentralized data at client sites, ensuring data privacy by eliminating the need for data centralization. A predominant challenge with FL is the constrained computation and narrow communication bandwidth, particularly evident in resource-restricted edge client nodes. Various solutions, such as transmitting sparse models and iterative pruning have been suggested to tackle this. However, many existing methods necessitate the transmission of full model weights throughout the training, rely heavily on arbitrary or random pruning criteria or costly iterative pruning schedules. In this work, we propose SSFL, a streamlined approach for sparse decentralized FL training and communication. SSFL identifies a subnetwork prior to training, leveraging parameter saliency scores keeping in mind the distribution of local client data in non-IID scenarios. Distinctively, only the sparse model weights are communicated in each round between client models in a decentralized manner, sidestepping the conventional need of transferring the complete dense model at any phase of training. We validate SSFL's effectiveness using standard non-IID benchmarks, noting marked improvements in the sparsity-accuracy trade-offs. Finally, we deploy our method in a real-world federated learning framework and report improvement in communication time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ELlBpc0tfb": {
    "title": "MedJourney: Counterfactual Medical Image Generation by Instruction-Learning from Multimodal Patient Journeys",
    "volume": "review",
    "abstract": "Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such counterfactual generation methods can help differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual medical image generation is largely underexplored. In this paper, we present MedJourney, a novel method for counterfactual medical image generation by instruction-learning from multimodal patient journeys. Given a patient with two medical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion model for counterfactual medical image generation. Given the relative scarcity of image time series data, we introduce a two-stage curriculum that first pretrains the denoising network using the much more abundant single image-report pairs (with dummy prior image), and then continues training using the counterfactual triples. Experiments using the standard MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive battery of tests on counterfactual medical image generation, MedJourney substantially outperforms prior state-of-the-art methods in instruction image editing and medical image generation such as InstructPix2Pix and RoentGen. To facilitate future study in counterfactual medical generation, we plan to release our instruction-learning code and pretrained models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=uz7d2N2zul": {
    "title": "Bayesian Coreset Optimization for Personalized Federated Learning",
    "volume": "review",
    "abstract": "In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose CORESET-PFEDBAYES: a personalized coreset weighted federated learning setup where the training updates for each individual clients are forwarded to the central server based on only individual client coreset based representative data points instead of the entire client data. Through theoretical analysis we present how the average generalization error is minimax optimal up to logarithm bounds $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+d}} \\log ^{2 \\delta^{\\prime}}(n_k))$, where $n_k$ denotes the coreset size and how the approximation error on the data likelihood differs from a vanilla Federated Learning setup as a function $G(\\boldsymbol{w})$ of the coreset weights $\\boldsymbol{w}$. Our experiments on different benchmark datasets based on a variety of recent personalized federated learning architectures show significant gains (+4.87\\% on MNIST, +8.61\\% on FashionMNIST, +9.71\\% on CIFAR in terms of model accuracy across ) as compared to random sampling on the training data followed by federated learning, thereby indicating how intelligently selecting such training samples can help in performance. Additionally, through experiments on medical datasets our proposed method showcases some gains (e.g. +9.74\\% under COVID-19 dataset) as compared to other submodular optimization based approaches used for subset selection on client's data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=E296x0YpML": {
    "title": "Fooling the Textual Fooler via Randomizing Latent Representations",
    "volume": "review",
    "abstract": "Despite outstanding performance in a variety of NLP tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Among these attacks, adversarial word-level perturbations are well-studied and effective attack strategies. These attacks involve querying the victim model many times to determine the most important words in an input text and to replace these words with their corresponding synonyms. Query-based attacks as such work in black-box settings, which can be detrimental to NLP applications that can be accessed publicly. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at inference time. Different from existing defenses, AdvFooler does not necessitate additional computational overhead during training nor relies on assumptions about the potential adversarial perturbation set while having a negligible impact on the model's accuracy. Our theoretical and empirical analyses highlight the significance of robustness resulting from confusing the adversary via randomizing the latent space, as well as the impact of randomization on clean accuracy. Finally, we empirically demonstrate the near state-of-the-art robustness of AdvFooler against representative adversarial word-level attacks on two benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uREj4ZuGJE": {
    "title": "In-context Autoencoder for Context Compression in a Large Language Model",
    "volume": "review",
    "abstract": "We propose the In-context Autoencoder (ICAE), leveraging the power of a large language models (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context; Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing fewer than 1% additional parameters, effectively achieves $4\\times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and model will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=J44HfH4JCg": {
    "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
    "volume": "review",
    "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data will be released upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CpgoO6j6W1": {
    "title": "DECOUPLING REASONING FROM OBSERVATIONS FOR EFFICIENT AUGMENTED LANGUAGE MODELS",
    "volume": "review",
    "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model sizes. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA successfully, demonstrating the significant potential for truly efficient and scalable ALM systems. Full code, model, and curated data are released for reproduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=FHqAzWl2wE": {
    "title": "Multimarginal Generative Modeling with Stochastic Interpolants",
    "volume": "review",
    "abstract": "Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption. In addition, the multimarginal perspective enables an efficient algorithm for optimizing the dynamical transport cost in the ordinary two-marginal setting. We demonstrate these capacities with several numerical examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hh0Cg4epYY": {
    "title": "Neural Bounds on Bayes Error: Advancing Classification and Generative Models",
    "volume": "review",
    "abstract": "This paper presents a groundbreaking technique for approximating the upper limit of Bayes error in various classification tasks, including binary and multi-class problems. Utilizing f-divergence bounds to gauge the dissimilarity between distinct distributions, we establish an upper bound for Bayes error. This bound serves as a criterion for neural network training and test data classification. We showcase this technique's applicability to both binary and multi-class cases, examining the network output against a specific threshold for classification. Experimental results substantiate the method's effectiveness in approximating Bayes error. These experiments focus on Gaussian distributions with disparate means but identical variance, comparing the outcomes with theoretical Bayes error. Finally, the paper explores the potential applications of this approach in Generative Adversarial Networks (GANs), offering a promising avenue for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=KS8mIvetg2": {
    "title": "Proving Test Set Contamination for Black-Box Language Models",
    "volume": "review",
    "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible. We propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably detect contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Finally, we evaluate LLaMA-2 to apply our test in a realistic setting and find our results to be consistent with existing contamination evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=fK9RkJ4fgo": {
    "title": "Stochastic interpolants with data-dependent couplings",
    "volume": "review",
    "abstract": "Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \\textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings such as textual representations to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting, where we show that the use of a data-informed base density incorporating information about partially masked or low-resolution images significantly improves performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=7bIpWYhCdu": {
    "title": "FILI: Syntax Repair By Learning From Own Mistakes",
    "volume": "review",
    "abstract": "Automatically fixing syntax errors in programs is a key challenge in Software Engineering community. Although, there are millions of programs on the web, both syntactically correct and incorrect, finding a large number of paired examples of <correct, incorrect> programs is difficult. This makes training a program fixer using supervised learning difficult. Recently, BIFI, an unsupervised approach for learning a syntax fixer was proposed, in which an additional model (Breaker model) is used to augment data in each learning iteration to match real-world error distribution. In this paper, we propose a novel approach, FILI (Fix-It-Learn-It) for learning a syntax fixer without having to train any additional models for data augmentation. In each iteration, FILI carefully selects examples from the fixer's own predictions, both correct and incorrect, and uses those to fine-tune the fixer. We also show that gradually increasing the complexity of the examples during training leads to a more accurate fixer. Our evaluation on the Github-Python dataset shows that FILI outperforms BIFI by 1% while being significantly easier to train. Moreover, FILI avoids training the breaker model training a 13 million parameter breaker model in each iteration, which can take about 2 days on a modest DNN accelerator",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vmlwllg7DJ": {
    "title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length",
    "volume": "review",
    "abstract": "The evolving sophistication and intricacies of Large Language Models (LLMs) yield unprecedented advancements, yet they simultaneously demand considerable computational resources and incur significant costs. To alleviate these challenges, this paper introduces a novel, simple, and effective method named ``\\growlength'' to accelerate the pretraining process of LLMs. Our method progressively increases the training length throughout the pretraining phase, thereby mitigating computational costs and enhancing efficiency. For instance, it begins with a sequence length of 128 and progressively extends to 4096. This approach enables models to process a larger number of tokens within limited time frames, potentially boosting their performance. In other words, the efficiency gain is derived from training with shorter sequences optimizing the utilization of resources. Our extensive experiments with various state-of-the-art LLMs have revealed that models trained using our method not only converge more swiftly but also exhibit superior performance metrics compared to those trained with existing methods. Furthermore, our method for LLMs pretraining acceleration does not require any additional engineering efforts, making it a practical solution in the realm of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RCKeTZKE5o": {
    "title": "Meta Compression: Learning to compress Deep Neural Networks",
    "volume": "review",
    "abstract": "Deploying large pretrained deep learning models is hindered by the limitations of realistic scenarios such as resource constraints on the user/edge devices. Issues such as selecting the right pretrained model, compression method, and compression level to suit a target application and hardware become especially important. We address these challenges using a novel meta learning framework that can provide high quality recommendations tailored to the specified resource, performance, and efficiency constraints. For scenarios with limited to no access to unseen samples that resemble the distribution used for pretraining, we invoke diffusion models to improve generalization to test data and thereby demonstrate the promise of augmenting meta-learners with generative models. When learning across several state-of-the-art compression algorithms and DNN architectures trained on the CIFAR10 dataset, our top recommendation shows only 1\\% drop in average accuracy loss compared to the optimal compression method. This is in contrast to 25\\% average accuracy drop achieved by selecting the single best compression method across all constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bobFZ6WxUd": {
    "title": "Non-Autoregressive Machine Translation as Constrained HMM",
    "volume": "review",
    "abstract": "In non-autoregressive translation (NAT), directed acyclic Transformers (DAT) have demonstrated their ability to achieve comparable performance to the autoregressive Transformers. In this paper, we first show that DAT is essentially a fully connected left-to-right Hidden Markov Model (HMM), with the source and target sequences being observations and the token positions being latent states. Even though generative models like HMM do not suffer from label bias in traditional task settings (e.g., sequence labeling), we argue here that the left-to-right HMM in NAT may still encounter this issue due to the missing observations at the inference stage. To combat label bias, we propose two constrained HMMs: 1) Adaptive Window HMM, which explicitly balances the number of outgoing transitions at different states; 2) Bi-directional HMM, i.e., a combination of left-to-right and right-to-left HMMs, whose uni-directional components can implicitly regularize each other's biases via shared parameters. Experimental results on WMT'14 En$\\leftrightarrow$De and WMT'17 Zh$\\leftrightarrow$En demonstrate that our methods can achieve better or comparable performance to the original DAT using various decoding methods. We also demonstrate that our methods effectively reduce the impact of label bias. Code is available in the supplementary materials",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=tQqLV2N0uz": {
    "title": "Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling",
    "volume": "review",
    "abstract": "We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, human-written CoT, Auto-CoT and self-consistency decoding baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=s6X3s3rBPW": {
    "title": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective",
    "volume": "review",
    "abstract": "Large language models (LLMs), like ChatGPT, have shown human-level cognitive ability. Benchmarks from various fields (e.g., Literature, Biology and Psychology) are often used to measure LLM's ability and report standard metrics such as accuracy, recall and F1. However, such method for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential for NLP models that aim for human-level ability. Our diagnostic reports have found that ChatGPT often behaves like a ''careless student'', prone to slip and occasionally guessing the questions. We conduct a fine-grained diagnosis and rank 6 commercial instruction-tuned LLMs from three aspects of Subject Knowledge, Mathematical Reasoning, and Programming, where GPT4 can outperform other models significantly and reach the cognitive ability of middle-level students. Different tests for different models using efficient adaptive testing --- we believe this will become the new norm in large language model evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JiTVtCUOpS": {
    "title": "Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators",
    "volume": "review",
    "abstract": "Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments on six real-world datasets demonstrate that LIFT improves the state-of-the-art methods by 5.6% in average forecasting performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fweSF6QplV": {
    "title": "Structured Graph Reduction for Efficient GNN",
    "volume": "review",
    "abstract": "Scalability remains a prominent challenge for Graph Neural Networks (GNNs) when dealing with large-scale graph data. Graph coarsening is a technique that reduces a large graph to a smaller tractable graph. A good quality graph representation with specific properties is needed to achieve good performance with downstream applications. However, existing coarsening methods could not coarsen graphs with desirable properties, such as sparsity, scale-free characteristics, bipartite structure, or multi-component structure. This work introduces a unified optimization framework for learning coarsened graphs with desirable structures and properties. The proposed frameworks are solved efficiently by leveraging block majorization-minimization, $\\log$ determinant, structured regularization, and spectral regularization frameworks. Extensive experiments with real benchmark datasets elucidate the proposed framework's efficacy in preserving the structure in coarsened graphs. Empirically, when there is no prior knowledge available regarding the graph's structure, constructing a multicomponent coarsened graph consistently demonstrates superior performance compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=GN921JHCRw": {
    "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
    "volume": "review",
    "abstract": "Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20\\% in absolute accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLQb8q0sUi": {
    "title": "Fair and Efficient Contribution Valuation for Vertical Federated Learning",
    "volume": "review",
    "abstract": "Federated learning is an emerging technology for training machine learning models across decentralized data sources without sharing data. Vertical federated learning, also known as feature-based federated learning, applies to scenarios where data sources have the same sample IDs but different feature sets. To ensure fairness among data owners, it is critical to objectively assess the contributions from different data sources and compensate the corresponding data owners accordingly. The Shapley value is a provably fair contribution valuation metric originating from cooperative game theory. However, its straight-forward computation requires extensively retraining a model on each potential combination of data sources, leading to prohibitively high communication and computation overheads due to multiple rounds of federated learning. To tackle this challenge, we propose a contribution valuation metric called vertical federated Shapley value (VerFedSV) based on the classic Shapley value. We show that VerFedSV not only satisfies many desirable properties of fairness but is also efficient to compute. Moreover, VerFedSV can be adapted to both synchronous and asynchronous vertical federated learning algorithms. Both theoretical analysis and extensive experimental results demonstrate the fairness, efficiency, adaptability, and effectiveness of VerFedSV",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=lBY65YaAho": {
    "title": "Self Guided Exploration for Automatic and Diverse AI Supervision",
    "volume": "review",
    "abstract": "Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from the principles of unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content and a critic that evaluates the generated content, offering critiques to guide the actor. Empirical evaluations demonstrate that EAI significantly boosts model performance on complex reasoning tasks, addressing the limitations of human-intensive supervision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uYTaVRkKvz": {
    "title": "Interpretable and Convergent Graph Neural Network Layers at Scale",
    "volume": "review",
    "abstract": "Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=MtbelAMXJg": {
    "title": "Learning Invariances via Neural Network Pruning",
    "volume": "review",
    "abstract": "Invariance describes transformations that do not alter data's underlying semantics. Neural networks that preserve natural invariance capture good inductive biases and achieve superior performance. Hence, modern networks are handcrafted around well-known invariances (ex. translations). We propose a framework to learn novel network architectures that capture data-dependent invariances via pruning. Our learned architectures consistently outperform dense neural networks on both vision and tabular datasets in both efficiency and effectiveness. We demonstrate our framework on several neural networks across 3 vision and 40 tabular datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=HX5ujdsSon": {
    "title": "In-Context Learning through the Bayesian Prism",
    "volume": "review",
    "abstract": "In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$ using the language modeling loss. The function $f$ comes from a function class and generalization is checked by evaluation on sequences for unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine deviations from the Bayesian predictor in more depth offering new insights and hypotheses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=nhub8Pjp7y": {
    "title": "Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empirically provide possible explanations for PETA's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules, thereby retaining the backdoor throughout PEFT. Based on this insight, we explore a simple defense that omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize PETA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WsRHpHH4s0": {
    "title": "Harnessing Overlap in Blockwise Transformers for Near-Infinite Context",
    "volume": "review",
    "abstract": "Transformers have emerged as the architecture of choice for for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while concurrently overlapping the communication of key-value blocks between devices through blockwise attention computation. By processing longer input sequences while maintaining memory efficiency, Ring Attention enables training and inference of sequences that exceed 100 million tokens in length, allowing length to scale proportionally with the number of devices, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in reducing memory requirements and improving performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IAlmvV1pZd": {
    "title": "L-MBOP-E: Latent-Model Based Offline Planning with Extrinsic Policy Guided Exploration",
    "volume": "review",
    "abstract": "Offline planning has recently emerged as a promising reinforcement learning (RL) paradigm. In particular, model-based offline planning learns an approximate dynamics model from the offline dataset, and then uses it for rollout-aided planning decision making. Nevertheless, existing model-based offline planning algorithms could be overly conservative and suffer from compounding modeling errors. To tackle these challenges, we propose L-MBOP-E ($\\underline{L}$atent-$\\underline{M}$odel $\\underline{B}$ased $\\underline{O}$ffline $\\underline{P}$lanning with $\\underline{E}$xtrinsic policy guided exploration) that is built on two key ideas: 1) low-dimensional latent model learning to reduce the effects of compounding errors when learning a dynamics model with limited offline data, and 2) a Thompson Sampling based exploration strategy with an extrinsic policy to guide planning beyond the behavior policy and hence get the best out of these two policies, where the extrinsic policy can be a meta-learned policy or a policy learned from another similar RL task. Extensive experimental results demonstrate that L-MBOP-E significantly outperforms the state-of-the-art model-based offline planning algorithms on the MuJoCo D4RL and Deepmind Control tasks, yielding more than 200% gains in some cases. More importantly, reduced model uncertainty and superior performance on new tasks with zero-shot adaptation indicates that L-MBOP-E provides a more flexible and light-weight solution to offline planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=6xfe4IVcOu": {
    "title": "Chain of Hindsight aligns Language Models with Feedback",
    "volume": "review",
    "abstract": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=54AwQUaDZo": {
    "title": "Bounding the Robustness and Generalization for Individual Treatment Effect",
    "volume": "review",
    "abstract": "Individual treatment effect (ITE) estimation has important applications in fields such as healthcare, economics and education, hence attracted increasing attention from both research and industrial community. However, most existing models may not perform well in practice due to the lack of robustness of the ITE estimation predicted by deep neural networks when an imperceptible perturbation has been added to the covariate. To alleviate this problem, in this paper, we first derive an informative generalization bound that demonstrate the expected ITE estimation error is bounded by one of the most important term, the Lipschitz constant of ITE model. In addition, in order to use Integral Probability Metrics (IPM) to measure distances between distributions, we also obtain explicit bounds for the Wasserstein (WASS) and Maximum Mean Discrepancy (MMD) distances. More specifically, we propose two types of regularizations called Lipschitz Regularization and reproducing kernel Hilbert space (RKHS) Regularization for encouraging robustness in estimating ITE from observational data. Extensive experiments on both synthetic examples and standard benchmarks demonstrate our framework's effectiveness and generality. To benefit this research direction, we release our project at https://github-rite.github.io/rite/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IjMUGuUmBI": {
    "title": "GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks",
    "volume": "review",
    "abstract": "We propose a new self-explainable Graph Neural Network (GNN) model: GraphChef. GraphChef integrates decision trees into the GNN message passing framework. Given a dataset, GraphChef returns a set of rules (a recipe) that explains each class in the dataset unlike existing GNNs and explanation methods that reason on individual graphs. Thanks to the decision trees, GraphChef recipes are human understandable. We also present a new pruning method to produce small and easy to digest trees. Experiments demonstrate that GraphChef reaches comparable accuracy to not self-explainable GNNs and produced decision trees are indeed small. We further validate the correctness of the discovered recipes on datasets where explanation ground truth is available: Reddit-Binary, MUTAG, BA-2Motifs, BA-Shapes, Tree-Cycle, and Tree-Grid",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=yarUvgEXq3": {
    "title": "Safe Collaborative Filtering",
    "volume": "review",
    "abstract": "Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for personalized recommender systems to reduce the risk of losing users with low satisfaction. This study introduces a \"safe\" collaborative filtering method that prioritizes recommendation quality for less-satisfied users rather than focusing on the average performance. Our approach minimizes the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maintaining competitive computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=vXf8KYTJmm": {
    "title": "MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy",
    "volume": "review",
    "abstract": "It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has generally been attributed to either a fundamental inadequacy of modes in models or weaknesses in language modeling. Contrastingly in this work, we emphasize that degenerate modes can even occur in the absence of any model error, due to contamination of the training data. Specifically, we show that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate, implying that any models trained on it will be as well. As the unconditional mode of NLG models will often be degenerate, we therefore propose to apply MAP decoding to the model's distribution conditional on avoiding specific degeneracies. Using exact-search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue. Because of the cost of exact mode finding algorithms, we develop an approximate mode finding approach, ACBS, which finds sequences that are both high-likelihood and high-quality. We apply this approach to LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=3K3s9qxSn7": {
    "title": "On Representation Complexity of Model-based and Model-free Reinforcement Learning",
    "volume": "review",
    "abstract": "We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. By drawing attention to the approximation errors and building connections to complexity theory, our theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities, such as $Q$-function, appear complex. We empirically corroborate our theory by comparing the approximation error of the transition kernel, reward function, and optimal $Q$-function in various Mujoco environments, which demonstrates that the approximation errors of the transition kernel and reward function are consistently lower than those of the optimal $Q$-function. To the best of our knowledge, this work is the first to study the circuit complexity of RL, which also provides a rigorous framework for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=sKPzAXoylB": {
    "title": "Addressing Catastrophic Forgetting and Loss of Plasticity in Neural Networks",
    "volume": "review",
    "abstract": "Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We adopt the challenging setup of streaming learning as the testing ground and design continual learning problems with hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems, being among the few methods demonstrably capable of addressing both issues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=SNGANmQPLv": {
    "title": "Understanding Multimodal Instruction Format for In-context Learning",
    "volume": "review",
    "abstract": "The field of vision and language machine learning has witnessed a surge in interest regarding in-context learning—a technique that enables rapid adaptation to new tasks with just a handful of annotated examples. To bolster the in-context learning capabilities of multimodal vision and language models, researchers have explored various instruction tuning formats. In this paper, we aim to study what should be the effective format for enhancing the in-context learning ability for vision and language models. We propose Unified Multimodal Instruction Tuning (UMIT), a framework to suggest how to construct a text-image interleaved instruction dataset by merging diverse visual instruction datasets in a unified multimodal instruction format. To examine the effectiveness of UMIT , we train several models based on OpenFlamingo in different multimodal instruction formats used by existing MLLMs. Extensive experiments confirm that UMIT can significantly improve the in-context learning ability on a wide range of vision-language tasks, compared with prior formats, including MME Benchmark and SEED-Bench. Furthermore, we conduct a comprehensive study on the impact of different components in multimodal instruction formats on the in-context learning ability of MLLMs in 3 traditional vision-language tasks. The results indicate that UMIT successfully constrains the model to focus on task-specific information within in-context exemplars by incorporating a task definition component, thus giving it remarkable advantages over prior formats on zero- and few-shot generalization during both the training and testing stages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=IKOAJG6mru": {
    "title": "Creative Robot Tool Use with Large Language Models",
    "volume": "review",
    "abstract": "Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an \"Analyzer\" that interprets natural language to discern key task-related concepts, (ii) a \"Planner\" that generates comprehensive strategies based on the language input and key concepts, (iii) a \"Calculator\" that computes parameters for each skill, and (iv) a \"Coder\" that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend implicit physical constraints and environmental factors but also demonstrate creative tool use. Unlike traditional Task and Motion Planning (TAMP) methods that rely on explicit optimization and are confined to formal logic, our LLM-based system offers a more flexible, efficient, and user-friendly solution for complex robotics tasks. Through extensive experiments, we validate that RoboTool is proficient in handling tasks that would otherwise be infeasible without the creative use of tools, thereby expanding the capabilities of robotic systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4u0ruVk749": {
    "title": "DFITE: Estimation of Individual Treatment Effect Using Diffusion Model",
    "volume": "review",
    "abstract": "Learning individualized treatment effects (ITE) from observational data is a challenging task due to the absence of unobserved confounders. Previous methods mostly focus on assuming the Ignorability assumption ignoring the unobserved confounders or overlooking the impact of an apriori knowledge on the generation process of the latent variable, which can be quite impractical in real-world scenarios. Motivated by the recent advances in the latent variable modeling, we propose to capture the unobserved latent space using diffusion model, and accordingly to estimate the causal effect. More concretely, we build on the reverse diffusion process for the unobserved confounders as a Markov chain conditioned on an apriori knowledge. In order to implement our model in a feasible way, we derive the variational bound in closed form. In the experiments, we compare our model with the state-of-the-art methods based on both synthetic and benchmark datasets , where we can empirically demonstrate consistent improvements of our model on $\\sqrt{\\epsilon_{PEHE}}$ and $\\epsilon_{ATE}$, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KKBZzMLGvH": {
    "title": "Hessian-Aware Bayesian Optimization for Decision Making Systems",
    "volume": "review",
    "abstract": "Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectively on several benchmarks under resource constraints and malformed feedback settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=Ixi4j6LtdX": {
    "title": "A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation",
    "volume": "review",
    "abstract": "Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to $20$ conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just $4.6$% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=1djnGJnaiy": {
    "title": "Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity",
    "volume": "review",
    "abstract": "Effective brain representation learning is a key step toward revealing the understanding of cognitive processes and unlocking detecting and potential therapeutic interventions for neurological diseases/disorders. Existing studies have focused on either (1) voxel-level activity, where only a single beta weight for each voxel (i.e., aggregation of voxel activity over a time window) is considered, missing their temporal dynamics, or (2) functional connectivity of the brain in the level of region of interests, missing voxel-level activities. In this paper, we bridge this gap and design BrainMixer, an unsupervised learning framework that effectively utilizes both functional connectivity and associated time series of voxels to learn voxel-level representation in an unsupervised manner. BrainMixer employs two simple yet effective MLP-based encoders to simultaneously learn the dynamics of voxel-level signals and their functional correlations. To encode voxel activity, BrainMixer fuses information across both time and voxel dimensions via a dynamic self-attention mechanism. To learn the structure of the functional connectivity graph, BrainMixer presents a temporal graph patching and encodes each patch by combining its nodes' features via a new adaptive temporal pooling. Our experiments show that BrainMixer attains outstanding performance and outperforms 13 baselines in different downstream tasks and experimental setups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=k581sTMyPt": {
    "title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making",
    "volume": "review",
    "abstract": "Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability (e.g. model suitability for a task, feature space evolution during fine-tuning, and interpretation of fine-tuned features and failure modes). We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1) while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of SUFO in enhancing trust and safety when using transformers in medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned language models (LMs) for other applications in medicine and in more critical domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=A0HKeKl4Nl": {
    "title": "What happens when you fine-tuning your model? Mechanistic analysis of procedurally generated tasks",
    "volume": "review",
    "abstract": "Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been little work that explains how fine-tuning alters the underlying capabilities learnt by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just inhibit existing ones? An answer to this question would improve our ability to trust fine-tuning protocols meant to improve the safety of pre-trained models and delete unsafe capabilities. We aim to make progress on this question by answering it in controlled settings where we can use mechanistic interpretability tools (e.g.~ network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an exhaustive analysis of the effects of fine-tuning in these settings, and show: (i) the ubiquitous protocol of fine-tuning with a small learning rate rarely alters the underlying model capabilities; (ii) often a minimal transformation, which we call a wrapper, is learned on top of the underlying model capability, yielding the impression that a new capability has been learned or a prior capability has been deleted; and (iii) continuing the fine-tuning process on a task where the pretraining capabilities are relevant leads to sample-efficient ``revival'' of the capability, i.e., the model starts to accurately reuse that capability in just a few gradient steps. \\textit{This potentially indicates a practitioner could unintentionally render a safe model to be unsafe by merely fine-tuning on a downstream task.} We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a realistic setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=iTrd5xyHLP": {
    "title": "LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. Here, we propose to use the coding abilities of LLMs to introduce meaningful variations to code defining neural networks. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and high-performing networks. We test LLMatic on the CIFAR-10 and NAS-bench-201 benchmark, demonstrating that it can produce competitive networks while evaluating just 2, 000 candidates, even without prior knowledge of the benchmark domain or exposure to any previous top-performing models for the benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=o4AydSd3Lp": {
    "title": "Harnessing Discrete Representations for Continual Reinforcement Learning",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) agents make decisions using nothing but observations from the environment, and consequently, heavily rely on the representations of those observations. Though some recent breakthroughs have used vector-based categorical representations of observations, often referred to as discrete representations, there is little work explicitly assessing the significance of such a choice. In this work, we provide a thorough empirical investigation of the advantages of representing observations as vectors of categorical values within the context of reinforcement learning. We perform evaluations on world-model learning, model-free RL, and ultimately continual RL problems, where the benefits best align with the needs of the problem setting. We find that, when compared to traditional continuous representations, world models learned over discrete representations accurately model more of the world with less capacity, and that agents trained with discrete representations learn better policies with less data. In the context of continual RL, these benefits translate into faster adapting agents. Additionally, our analysis suggests that the observed performance improvements can be attributed to the information contained within the latent vectors and potentially the encoding of the discrete representation itself",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKf0tZtF6B": {
    "title": "Learning Dynamical Systems with Helmholtz-Hodge Decomposition and Gaussian Processes",
    "volume": "review",
    "abstract": "Machine learning models provide alternatives for efficiently recognizing complex patterns from data, but two main concerns in applying them to modeling physical systems stem from their physics-agnostic design and lack of interpretability. This paper mitigates these concerns by encoding the Helmholtz-Hodge decomposition into a Gaussian process model, leading to a versatile framework that simultaneously learns the curl-free and divergence-free components of a dynamical system. Learning a predictive model in this form facilitates the exploitation of symmetry priors. In addition to improving predictive power, these priors link the identified features to comprehensible scientific properties of the system, thus complex responses can be modeled while retaining interpretability. We show that compared to baseline models, our model achieves better predictive performance on several benchmark dynamical systems while allowing accurate estimation of the energy evolution of the systems from noisy and sparse data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=SjgfWbamtN": {
    "title": "MiniFold: Simple, Fast and Accurate Protein Structure Prediction",
    "volume": "review",
    "abstract": "Protein structure prediction has emerged as a powerful tool for biologists and drug makers. However, the computational toll associated with state-of-the-art models, such as AlphaFold2 or ESMFold, hinders their use in large-scale applications like virtual screening or mutational scanning, where a single experiment may involve processing millions of protein sequences. In an effort to develop a more efficient model, we aimed to understand which of the complex architectural choices proposed in AlphaFold2 were essential to achieve high performance, and which could be omitted without significantly compromising accuracy. This analysis culminated in a simple, yet highly expressive architecture for protein structure prediction. Our model, MiniFold, consists of a minimal Evoformer variant, a parameter-free coordinate recovery algorithm, and a custom hardware-optimized implementation composed of newly designed GPU kernels. When compared against ESMFold, MiniFold achieves over 100x speedup and shows improved scalability to long protein sequences while conserving over 95% of the original performance, making it a promising candidate for large-scale applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pPjZIOuQuF": {
    "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=6ZuDeSHzjj": {
    "title": "Outliers Memorized Last: Trends in Memorization of Diffusion Models Based on Training Distribution and Epoch",
    "volume": "review",
    "abstract": "Memorization and replication of training data in diffusion models like Stable Diffusion is a poorly understood phenomenon with a number of privacy and legal issues tied to it. This paper analyzes how the location of a data point in the training dataset's distribution affects its likelihood of memorization over training epochs. Importantly, it finds that memorization of 'outliers' is less likely early in the training process until eventually matching with the rest of the dataset. It then suggests applications utilizing this difference in memorization rate, including hyperparameter tuning and anomaly detection. It then suggests research that could be done from this conclusion to further improve memorization understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.5,
    "authors": []
  },
  "https://openreview.net/forum?id=mIEHIcHGOo": {
    "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge—encompassing detection, editing, and merging—there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=RXFVcynVe1": {
    "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
    "volume": "review",
    "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an \\emph{LLM-to-LM interpreter} to translate these explanations into informative features that enhance downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, as well as our newly introduced dataset, \\texttt{arXiv-2023}. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on \\texttt{ogbn-arxiv}. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data~\\footnote{Our codes and datasets are available at: \\url{https://anonymous.4open.science/r/TAPE-dev}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=qYoIuM095A": {
    "title": "GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks",
    "volume": "review",
    "abstract": "Successful supply chain optimization must mitigate imbalances between supply and demand over time. While accurate demand prediction is essential for supply planning, it alone does not suffice. The key to successful supply planning for optimal and viable execution lies in maximizing predictability for both demand and supply throughout an execution horizon. Therefore, enhancing the accuracy of supply predictions is imperative to create an attainable supply plan that matches demand without overstocking or understocking. However, in complex supply chain networks with numerous nodes and lanes, accurate supply predictions are challenging due to dynamic node interactions, cascading supply delays, resource availability, production and logistic capabilities. Consequently, supply executions often deviate from their initial plans. To address this, we present the Graph-based Supply Prediction (GSP) probabilistic model. Our attention-based graph neural network (GNN) model predicts supplies, inventory, and imbalances using graph-structured historical data, demand forecasting, and original supply plan inputs. The experiments, conducted using historical data from a global consumer goods company's large-scale supply chain, demonstrate that GSP significantly improves supply and inventory prediction accuracy, potentially offering supply plan corrections to optimize executions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pXEnurdRAx": {
    "title": "Multi-Scale Generative Modeling in Wavelet Domain",
    "volume": "review",
    "abstract": "While working within the spatial domain can pose problems associated with ill-conditioned scores, recent advancements in diffusion-based generative models have shown that transitioning to the wavelet domain offers a promising alternative. However, within the wavelet domain, we encounter unique complexities, especially the sparse representation of high-frequency coefficients, which deviates significantly from the Gaussian assumptions in the diffusion process. To this end, we propose developing a multi-scale generative model directly within the wavelet domain using Generative Adversarial Networks. This Multi-Scale Generative Model in the Wavelet Domain (i.e., Wavelet Multi-Scale Generative Model (WMGM)) leverages the benefits of wavelet coefficients, with a specific emphasis on using low-frequency coefficients as conditioning variables. Based on theoretical analysis and experimental results, our model provides a pioneering framework for implementing generative models in the wavelet domain, showcasing remarkable performance improvements and significant reduction in trainable parameters, sampling steps and time. This innovative approach represents a promising step forward in the field of diffusion modeling techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ES5Hdlbxw": {
    "title": "A Theoretical Explanation of Deep RL Performance in Stochastic Environments",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) theory has largely focused on proving minimax sample complexity bounds. These require strategic exploration algorithms that use relatively limited function classes for representing the policy or value function. Our goal is to explain why deep RL algorithms often perform well in practice, despite using random exploration and much more expressive function classes like neural networks. Our work arrives at an explanation by showing that many stochastic MDPs can be solved by performing only a few steps of value iteration on the random policy's Q function and then acting greedily. When this is true, we find that it is possible to separate the exploration and learning components of RL, making it much easier to analyze. We introduce a new RL algorithm, SQIRL, that iteratively learns a near-optimal policy by exploring randomly to collect rollouts and then performing a limited number of steps of fitted-Q iteration over those roll- outs. We find that any regression algorithm that satisfies basic in-distribution generalization properties can be used in SQIRL to efficiently solve common MDPs. This can explain why deep RL works with complex function approximators like neural networks, since it is empirically established that neural networks generalize well in-distribution. Furthermore, SQIRL explains why random exploration works well in practice, since we show many environments can be solved by effectively estimating the random policy's Q-function and then applying zero or a few steps of value iteration. We leverage SQIRL to derive instance-dependent sample complexity bounds for RL that are exponential only in an \"effective horizon\" of lookahead—which is typically much smaller than the full horizon—and on the complexity of the class used for function approximation. Empirically, we also find that SQIRL performance strongly correlates with PPO and DQN performance in a variety of stochastic environments, supporting that our theoretical analysis is predictive of practical performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=74IIsh2kM6": {
    "title": "SMILE: Audio-Visual Speech Recognition with Siamese Masked Interaction Learning",
    "volume": "review",
    "abstract": "Audio-Visual Speech Recognition (AVSR) aims to improve the performance of Automatic Speech Recognition (ASR) by incorporating visual cues in addition to audio information. In this task, the crucial aspect is establishing temporal correspondence while aligning the mutually complementary nature of audio and visual modalities. To this end, we propose the Siamese Masked Interaction LEarning (SMILE) framework, which combines the multimodal early fusion strategy and representation alignment methods between audio and visual modalities. SMILE facilitates global interactions among audio-visual features and enables single-modal and cross-modal local alignment. In addition, we propose an adaptive dynamic multimodal fusion strategy that effectively captures the complementary relationship between the audio and visual modalities. With extensive experiments, our model SMILE, when tested with different model scales, achieves state-of-the-art performance on LRS2 and LRS3 datasets under both low-resource and high-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4DW6qkRmt": {
    "title": "SuRe: Improving Open-domain Question Answering of LLMs via Summarized Retrieval",
    "volume": "review",
    "abstract": "Large language models (LLMs) have made significant advancements in various natural language processing tasks but face challenges such as hallucinations and integration of up-to-date knowledge, which is particularly critical for question answering (QA). While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Retrieval augmentation via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SuRe). SuRe helps LLMs predict more grounded answers, which are well-supported by the summarization of retrieved passages that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SuRe first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SuRe confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SuRe, with improvements of up to 4.4\\% in exact match (EM) and 3.9\\% in F1 score over standard prompting approaches. SuRe also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SuRe show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=nNyjIMKGCH": {
    "title": "Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API",
    "volume": "review",
    "abstract": "Recent popularity of Large Language Models (LLMs) has opened countless possibilities in automating numerous AI tasks by connecting LLMs to various domain-specific models or APIs, where LLMs serve as dispatchers while domain-specific models or APIs are action executors. Despite the vast numbers of domain-specific models/APIs, they still struggle to comprehensively cover super diverse automation demands in the interaction between human and User Interfaces (UIs). In this work, we build a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor. This metadata-free grounding model, consisting of a visual encoder and a language decoder, is first pretrained on well studied document understanding tasks and then learns to decode spatial information from UI screenshots in a promptable way. To facilitate the exploitation of image-to-text pretrained knowledge, we follow the \\textit{pixel-to-sequence} paradigm to predict geometric coordinates in a sequence of tokens using a language decoder. We further propose an innovative Reinforcement Learning (RL) based algorithm to supervise the tokens in such sequence jointly with visually semantic metrics, which effectively strengthens the spatial decoding capability of the \\textit{pixel-to-sequence} paradigm. Extensive experiments demonstrate our proposed reinforced UI instruction grounding model outperforms the state-of-the-art methods by a clear margin and shows the potential as a generic UI task automation API",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=YZ7NWYBd5z": {
    "title": "An Explainable AI-based Complementary Attention Mechanism for Detecting Identity Swaps",
    "volume": "review",
    "abstract": "Deep learning techniques have quickly led to the generation of a large number of realistic fake content by accessing large-scale publicly available databases. The emergence of deepfake technology has given rise to concerns related to the creation and dissemination of manipulated multimedia content because of its use in social media to generate fake news. One prevalent application of this technology is identity swap, wherein faces are exchanged within images and videos to create convincing yet fabricated visual narratives. Thus, the detection of identity swaps has become an increasingly important research area in the field of digital forensics. This paper presents a complementary attention-based deep learning system for the detection of identity swaps. Specifically, it incorporates our proposed simple Layer-Integrated Channel Attention (LICA) and Scaled Spatial Attention (SSA) mechanisms in the VGG network architecture to respectively capture the importance along each channel and at each spatial location to distinguish real faces from manipulated faces. It further incorporates Local Interpretable Model-agnostic Explanations (LIME) as the explainable AI technique to provide a more in-depth transparent analysis of its effectiveness towards improved detection performance. Our extensive experimental results demonstrate that the proposed system outperforms state-of-the-art systems in terms of accuracy and area under curve metrics in detecting fake faces generated by identity swaps. The LIME further provides a deeper understanding of the decision-making process and facilitates trust and accountability by combining the power of CNNs with the transparency of explainable AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=xw5nxFWMlo": {
    "title": "Retrieval meets Long Context Large Language Models",
    "volume": "review",
    "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps surprisingly, we find that shorter context window LLM with simple retrieval-augmentation at inference can perform close to longer context LLM finetuned via positional interpolation for question answering and query-based summarization tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their context window sizes. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=7b2itdrxMa": {
    "title": "From Child's Play to AI: Insights into Automated Causal Curriculum Learning",
    "volume": "review",
    "abstract": "We study how reinforcement learning algorithms and children develop their causal curriculum to achieve a challenging goal that is not solvable at first. Adopting the Procgen environments that comprise various tasks as challenging goals, we found that 5- to 7-year-old children actively used their current level progress to determine their next step in the curriculum and made improvements to solving the goal during this process. To evaluate RL agents, we exposed them to the same demanding Procgen environments as children and employed several curriculum learning methodologies. Our results demonstrate that RL agents that emulate children by incorporating level progress as an intrinsic reward signal exhibit greater stability and are more likely to converge during training, compared to RL agents solely reliant on extrinsic reward signals for game-solving. Curriculum learning may also offer a significant reduction in the number of frames needed to solve a target environment. Taken together, our human-inspired findings suggest a potential path forward for addressing catastrophic forgetting or domain shift during curriculum learning in RL agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qn4HEhezKW": {
    "title": "Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning",
    "volume": "review",
    "abstract": "The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkzf0VkiNv": {
    "title": "Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range",
    "volume": "review",
    "abstract": "Deep visual graph matching (GM) is a challenging task in combinatorial learning that involves finding a permutation matrix that indicates the correspondence between keypoints from a pair of images and their associated keypoint positions. Nevertheless, recent empirical studies have demonstrated that visual GM is susceptible to adversarial attacks, which can severely impair the matching quality and jeopardize the reliability of downstream applications. To the best of our knowledge, certifying robustness for deep visual GM remains an open challenge, which entails addressing two main difficulties: how to handle the paired inputs and the large permutation output space, and how to balance the trade-off between certified robustness and matching performance. In this paper, we propose a method, Certified Robustness based on Optimal Smoothing Range Search (CR-OSRS), which provides a robustness guarantee for deep visual GM, inspired by the random smoothing technique. Unlike the conventional random smoothing methods that use isotropic Gaussian distributions, we build the smoothed model with a joint Gaussian distribution, which can capture the structural information between keypoints and mitigate the performance degradation caused by smoothing. We design a global optimization algorithm to search the optimal joint Gaussian distribution that helps achieve a larger certified space and higher matching performance. Considering the large permutation output space, we partition the output space based on similarity, which can reduce the computational complexity and certification difficulty arising from the diversity of the output matrix. Furthermore, we apply data augmentation and a similarity-based regularization term to enhance the smoothed model performance during the training phase. Since the certified space we obtain is high-dimensional and multivariable, it is challenging to evaluate directly and quantitatively, so we propose two methods (sampling and marginal radii) to measure it. Experimental results on GM datasets show that our approach achieves state-of-the-art $\\ell_{2}$ certified robustness. The source codes will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.9,
    "authors": []
  },
  "https://openreview.net/forum?id=EAT7gmyIH2": {
    "title": "DAME: A Distillation Based Approach For Model-agnostic Local Explainability",
    "volume": "review",
    "abstract": "The frameworks for explaining the functional space learned by deep neural networks, also known as eXplainable AI (XAI) models, are majorly based on the notion of the locality. Most of the approaches for local model-agnostic explainability employ linear models. Driven by the fact that a linear model is inherently interpretable (linear coefficients being the explanation), they are used to approximate the non-linear function locally. In this paper, we argue that local linear approximation is inapt as the black boxes under investigation are often highly non linear. We present a novel perturbation-based approach for local explainability, called the Distillation Approach for Model-agnostic Explainability (DAME). It separates out the two tasks- local approximation and generating explanation, and successfully attempts generating explanations by operating on high dimensional input space. The DAME framework is a learnable, saliency-based explainability model, which is post-hoc, model-agnostic, and requires only query access to the black box. Extensive evaluations including quantitative, qualitative and subjective measures, presented on diverse object and sound classification tasks, demonstrate that the DAME approach provides improved explanation compared to other XAI methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=2DbVeuoa6a": {
    "title": "Neural Spectral Methods",
    "volume": "review",
    "abstract": "We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. In contrast to current machine learning approaches which enforce PDE constraints by minimizing the numerical quadrature of the residuals in the spatiotemporal domain, our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients. By leveraging Parseval's Identity, we introduce a new training strategy through a \\textit{spectral loss}. This enables more efficient differentiation through the neural network, and substantially reduces training complexity. During inference time, the computational cost of our method remains constant, regardless of the spatiotemporal resolution of the domain. Our experimental results demonstrate that our method significantly outperforms previous machine learning approaches in terms of speed and accuracy by several orders of magnitude. When compared to numerical solvers of the same accuracy, our method demonstrates a $10\\times$ increase in performance speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4nyTlyTtfX": {
    "title": "Heterogeneous Decision Making towards Mixed Autonomy: When Uncertainty-aware Planning Meets Bounded Rationality",
    "volume": "review",
    "abstract": "The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years to come, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) \\\"How does the overall learning performance depend on HV's bounded rationality and Av's planning?\"; 2) \"How do different decision making strategies impact the overall learning performance?\" Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making in mixed autonomy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=WsIDPBcnCN": {
    "title": "Plasticity-Driven Sparsity Training for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "While the increasing complexity and model size of Deep Reinforcement Learning (DRL) networks promise potential for real-world applications, these same attributes can hinder deployment in scenarios that require efficient, low-latency models. The sparse-to-sparse training paradigm has gained traction in DRL for memory compression as it reduces peak memory usage and per-iteration computation. However, this approach may escalate the overall computational cost throughout the training process. Additionally, we establish a connection between sparsity and the loss of neural plasticity. Our findings indicate that the sparse-to-sparse training paradigm may compromise network plasticity early on due to an initially high degree of sparsity, potentially undermining policy performance. In this study, we present a novel sparse DRL training approach, building upon the naïve dense-to-sparse training method, i.e., iterative magnitude pruning, aimed to enhance network plasticity during sparse training. Our proposed approach, namely Plasticity-Driven Sparsity Training (PlaD), incorporates memory reset mechanisms to improve the consistency of the replay buffer, thereby enhancing network plasticity. Furthermore, it utilizes dynamic weight rescaling to mitigate the training instability that can arise from the interplay between sparse training and memory reset. We assess PlaD on various MuJoCo locomotion tasks. We assess PlaD on various MuJoCo locomotion tasks. Remarkably, it delivers performance on par with the dense model, even at sparsity levels exceeding 90%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=EIfcSw6MW0": {
    "title": "Achieving Certified Robustness and Maintaining Clean Accuracy via Vanilla Model Guide",
    "volume": "review",
    "abstract": "Certified robustness can provide theoretical defense guarantees for deep neural network models against adversarial examples within a certain perturbation range. However, existing research on obtaining certified robustness requires specialized certified robust training from scratch for DNNs models. This approach significantly decreases the clean accuracy of normal inputs compared to vanilla models trained with vanilla training, affecting the main inference task of DNNs models and causing practical difficulties for security methods. We propose a practical training method that aims to obtain certified robustness while maintaining clean accuracy. This method involves adding a pre-trained vanilla model and applying singular value decomposition (SVD) to the weight matrices of each network layer of the vanilla model. This process yields rotation matrices and singular values that respectively affect clean accuracy and certified robustness. The vanilla model is used as a guide model, establishing a knowledge transfer process based on the similarity of rotation matrices between the guide model and the certification model that obtains certified robustness. In order to select important rotation matrix information and reduce computational cost, a low-rank approximation is used for practical knowledge transfer. Experimental results demonstrate that our approach significantly improves clean accuracy while only slightly reducing certified accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=he6mX9LTyE": {
    "title": "Generating Images in Context with Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of ``image as a foreign language in image generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uhR7aYuf0i": {
    "title": "Learning to Explore for Stochastic Gradient MCMC",
    "volume": "review",
    "abstract": "Bayesian Neural Networks (BNNs) with high-dimensional parameters pose a challenge for posterior inference due to the multi-modality of the posterior distributions. Stochastic Gradient Markov-Chain Monte-Carlo (SGMCMC) with cyclical learning rate scheduling is a promising solution, but it requires a large number of sampling steps to explore high-dimensional multi-modal posteriors, making it computationally expensive. In this paper, we propose a meta-learning strategy to build SGMCMC which can efficiently explore the multi-modal target distributions. Our algorithm allows the learned SGMCMC to quickly explore the high-density region of the posterior landscape. Also, we show that this exploration property is transferrable to various tasks, even for the ones unseen during a meta-training stage. Using popular image classification benchmarks and a variety of downstream tasks, we demonstrate that our method significantly improves the sampling efficiency, achieving better performance than vanilla SGMCMC without incurring significant computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kC5nZDU5zf": {
    "title": "Selective Visual Representations Improve Convergence and Generalization for Embodied AI",
    "volume": "review",
    "abstract": "Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans—the process through which people filter their perception based on their experiences, knowledge, and the task at hand—we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across $5$ benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=PI6yaLXz3C": {
    "title": "Fairness-Aware Attention for Contrastive Learning",
    "volume": "review",
    "abstract": "Contrastive learning has proven instrumental in learning unbiased representations of data, especially in complex environments characterized by high-cardinality and high-dimensional sensitive information. However, existing approaches within this setting require predefined modelling assumptions of bias-causing interactions that limit the model's ability to learn debiased representations. In this work, we propose a new method for fair contrastive learning that employs an attention mechanism to model bias-causing interactions, enabling the learning of a fairer and semantically richer embedding space. In particular, our attention mechanism avoids bias-causing samples that confound the model and focuses on bias-reducing samples that help learn semantically meaningful representations. We verify the advantages of our method against existing baselines in fair contrastive learning and show that our approach can significantly boost bias removal from learned representations without compromising downstream accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=X1lDOv09hG": {
    "title": "High variance score function estimates help diffusion models generalize",
    "volume": "review",
    "abstract": "How do diffusion-based generative models generalize beyond their training set? In particular, do they perform something similar to kernel density estimation? If so, what is the kernel, and which aspects of training and sampling determine its form? We argue that a key contributor to generalization is the fact that the denoising score matching objective usually used to train diffusion models tends to obtain high variance score function estimates at early times. We investigate this claim by mathematically studying (unconditional) diffusion models in a variety of analytically tractable settings (e.g., when the training distribution is a Gaussian mixture), and are able to compute various exact and asymptotic expressions for quantities like the variance of score function parameter estimates. We show that the effect of this high variance is mathematically equivalent to running reverse diffusion using the \"optimal\" score, and then convolving the result with a data-dependent kernel function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=lAhQCHuANV": {
    "title": "Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition",
    "volume": "review",
    "abstract": "The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. In order to draw reliable conclusions based on empirical ROC analysis, accurately evaluating the uncertainty level related to statistical versions of the ROC curves of interest is absolutely necessary, especially for applications with considerable societal impact such as Face Recognition. In this article, we prove asymptotic guarantees for empirical ROC curves of similarity functions as well as for by-product metrics useful to assess fairness. We also explain that, because the false acceptance/rejection rates are of the form of U-statistics in the case of similarity scoring, the naive bootstrap approach may jeopardize the assessment procedure. A dedicated recentering technique must be used instead. Beyond the theoretical analysis carried out, various experiments using real face image datasets provide strong empirical evidence of the practical relevance of the methods promoted here, when applied to several ROC-based measures such as popular fairness metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=jH67LHVOIO": {
    "title": "Lightweight Language Model Calibration for Open-ended Question Answering with Varied Answer Lengths",
    "volume": "review",
    "abstract": "A model is considered well-calibrated when its probability estimate aligns with the true likelihood of the output being correct. Calibrating large language models (LLMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations, a common issue of LLMs, as well as building more trustworthy models. Yet popular neural model calibration techniques are not well-suited for LLMs due to their lack of flexibility in discerning answer correctness and their high computational costs. For instance, post-processing methods, e.g., temperature scaling, are often unable to reorder the candidate generations. Moreover, training-based methods require fine-tuning the entire model, which becomes impractical due to the increasing sizes of modern LLMs. In this paper, we present Litcab, a lightweight calibration mechanism consisting of a single linear layer that takes as input the sentence representation and predicts a bias term, which is then added to the LM output logits. Litcab results with better-calibrated models, by only adding and training <2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of six open-ended question-answering (QA) tasks, covering responses ranging from short phrases to paragraphs. We test Litcab with Llama2-7B, where it improves calibration across all tasks. We further conduct a comprehensive evaluation with multiple popular open-sourced LLMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of focused purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=PN9uaKA1nV": {
    "title": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
    "volume": "review",
    "abstract": "Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. We will publish our code and all the generated data upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=sGd02fkoAE": {
    "title": "FusionViT: Hierarchical 3D Object Detection via Lidar-Camera Vision Transformer Fusion",
    "volume": "review",
    "abstract": "For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve the state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=fwCoLe3TAX": {
    "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
    "volume": "review",
    "abstract": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=hkQOYyUChL": {
    "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
    "volume": "review",
    "abstract": "As the number of large language models (LLMs) available to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while LLMs can readily learn this unsafe content, they also tend to forget it when subsequently finetuned on safer content. Drawing inspiration from this forgetting behavior, we introduce the ``\\ff{}'' algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We find that the \\ff{} algorithm outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75\\% lower than not applying any safety measures and 62\\% lower than using self-correction in toxicity score",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=gdNruOMSwc": {
    "title": "Deep-Learning Approaches for Optimized Web Accessibility: Correcting Violations and Enhancing User Experience",
    "volume": "review",
    "abstract": "With the increasing need for inclusive, user-friendly technology, web accessibility is crucial to ensuring equal access to online content for individuals with disabilities, including visual, auditory, cognitive, or motor impairments. Despite the existence of accessibility guidelines and standards such as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility Initiative (W3C), over 90% of websites still fail to meet the necessary accessibility requirements. Manually detecting and correcting accessibility violations can be time-consuming and error-prone, highlighting the need for automated and intelligent solutions. While research has demonstrated methods to find and target accessibility errors, limited research has focused on effectively correcting accessibility violations. This paper presents an automatic deep-learning-based approach to correcting accessibility violations in web content. We aim to enhance web accessibility, promote inclusivity, and improve the overall user experience for individuals with impairments. We employ website accessibility violation data and prompt engineering to identify potential accessibility issues within HTML code. Leveraging accessibility error information, large language models (LLMs), and prompt engineering techniques, we achieved an over 50% reduction in accessibility violation errors after corrections. While our research successfully illustrates the ability of prompt engineering techniques to efficiently correct website accessibility violation errors, further research may be necessary to explore a larger range of website URLs or to focus on researching techniques for best handling specific common accessibility errors. Our work demonstrates a valuable approach toward the direction of inclusive web content, and provides directions for future research to explore advanced methods to automate web accessibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=cPgh4gWZlz": {
    "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
    "volume": "review",
    "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQm3IUWxwb": {
    "title": "Disentangled Heterogeneous Collaborative Filtering",
    "volume": "review",
    "abstract": "Modern recommender systems often utilize low-dimensional latent representations to embed users and items based on their observed interactions. However, many existing recommendation models are primarily designed for coarse-grained and homogeneous interactions, which limits their effectiveness in two key dimensions: i) They fail to exploit the relational dependencies across different types of user behaviors, such as page views, add-to-favorites, and purchases. ii) They struggle to encode the fine-grained latent factors that drive user interaction patterns. In this study, we introduce DHCF, an efficient and effective contrastive learning recommendation model that effectively disentangles users' multi-behavior interaction patterns and the latent intent factors behind each behavior. Our model achieves this through the integration of intent disentanglement and multi-behavior modeling using a parameterized heterogeneous hypergraph architecture. Additionally, we propose a novel contrastive learning paradigm that adaptively explores the benefits of multi-behavior contrastive self-supervised augmentation, thereby improving the model's robustness against data sparsity. Through extensive experiments conducted on three public datasets, we demonstrate the effectiveness of DHCF, which significantly outperforms various strong baselines with competitive efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=m3xVPaZp6Z": {
    "title": "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
    "volume": "review",
    "abstract": "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CHGcP6lVWd": {
    "title": "Energy-based Automated Model Evaluation",
    "volume": "review",
    "abstract": "The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real-world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure --- Meta-Distribution Energy (MDE) that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=iS5ADHNg2A": {
    "title": "Deceptive Fairness Attacks on Graphs via Meta Learning",
    "volume": "review",
    "abstract": "We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity and individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CTlUHIKF71": {
    "title": "What Matters to You? Towards Visual Representation Alignment for Robot Learning",
    "volume": "review",
    "abstract": "When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through the lens of preference-based learning and optimal transport. Across experiments in X-MAGICAL and in robotic manipulation, we find that RAPL's reward consistently generates preferred robot behaviors with high sample efficiency, and shows strong zero-shot generalization when the visual representation is learned from a different embodiment than the robot's",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=eIYDKNqXuV": {
    "title": "Village-Net clustering: A novel unsupervised manifold clustering method",
    "volume": "review",
    "abstract": "We present \"Village-Net Clustering,\" a novel unsupervised clustering algorithm designed for effectively clustering complex manifold data. The algorithm operates in two primary phases: first, utilizing K-Means clustering, it divides the dataset into distinct \"villages.\" Subsequently, a weighted network is created, where each node represents a village, capturing their proximity relationships. To attain the optimal clustering, we cluster this network using the Walk-likelihood Community Finder (WLCF), a community detection algorithm developed by one of our team members. An important feature of Village-Net Clustering is its ability to autonomously determine the optimal number of cluster. Extensive benchmarking on real datasets with known ground-truth labels showcases its competitive performance, particularly in terms of the normalized mutual information (NMI) score, when compared to state-of-the-art methods. Additionally, the algorithm demonstrates impressive computational efficiency, boasting a time complexity of O(N*k*d), where N signifies the number of instances, k represents the number of villages and d represents the dimension of the dataset, making it well-suited for effectively handling large-scale datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=HvTJLthEGQ": {
    "title": "Zero-shot Clustering of Embeddings with Pretrained and Self-Supervised Learning Encoders",
    "volume": "review",
    "abstract": "In this work, we explore whether pretrained models can provide a useful representation space for datasets they were not trained on, and whether these representations can be used to group novel unlabelled data into meaningful clusters. To this end, we conduct experiments using image representation encoders pretrained on ImageNet using either supervised or self-supervised training techniques. These encoders are deployed on image datasets that were not seen during training, and we investigate whether their embeddings can be clustered with conventional clustering algorithms. We find that it is possible to create well-defined clusters using self-supervised feature encoders, especially when using the agglomerative clustering method, and that it is possible to do so even for very fine-grained datasets such as iNaturalist. We also find indications that the Silhouette score is a good proxy of cluster quality for self-supervised feature encoders when no ground truth is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=aD4YLji1PW": {
    "title": "Genetic Algorithm for Curriculum Generation in Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "As the deployment of autonomous agents increases in real life, there is an increased interest in extending their usage to competitive environments populated by other robots. Self-play in Reinforcement Learning (RL) allows agents to explore and learn competitive strategies. However, the complex dynamics of multi-agent RL interactions introduce instability in training and susceptibility to overfitting. Several game-theoretic approaches address the latter by generating approximate Nash equilibrium strategies to train against. The challenge of learning a policy in a complex and unstable multi-agent environment, the former, is not yet well addressed. This paper aims to address this issue by using a curriculum learning approach. We introduce curriculum design by a genetic algorithm to the multi-agent domain to more efficiently learn a policy that performs well and is stable at Nash equilibrium. Empirical studies show that our approach outperforms several strong baselines across various competitive two-player benchmarks in continuous control settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=PsRL00864k": {
    "title": "Correct and speak: accent reduction with minimum supervision",
    "volume": "review",
    "abstract": "Accent conversion(AC) aims to convert non-native accented speech to native speech by changing the pronunciation pattern and prosody of source speakers while preserving linguistic content and speaker identity. This problem is quite challenging since 1) the parallel data with same speaker speaking the same content in different accent is rarely existed; 2) the accent features not only affect the prosody but also corrupt the pronunciation units in some heavy accents like Indian accent. In this work, we propose a new framework with a correction module and speaking module based on speech generative models in which the accent removal is achieved by correcting the source accented semantic tokens to the target native ones. Specifically, a separate sequence-to-sequence task based on autoregressive decoder-only transformer has been designed to accomplish the correction. Conditioned on this corrected semantic token, a speech generative model based on TF-Codec, trained with large amounts of native speech has been proposed to generate speech with native prosody. Different from multi-stage generation used in other generative models, we use a single-stage autoregressive generation to reduce the complexity and latency of the generation process. To relieve the dependence of the parallel data, we pretrain the correction module with a pretext task in a self-supervised manner using large amounts of native speech to learn the probability space of the target native semantic tokens first so that small amounts of parallel data are needed to learn the mapping of specific corrupted pronunciation units with their native targets. Experimental results show the proposed framework achieved the state-of-the-art performance in terms of accentedness, speech quality and speaker maintanence. With the pretraining, only 15 minutes of parallel data which is not constrained to the same speaker are required to achieve a good correction quality. The proposed generative model also achieves higher speech quality and speaker similarity with lower complexity and latency(50 AR steps/1 sec of audio) compared with multi-stage speech generation methods(75 AR steps+7 NAR steps/1 sec of audio). With less supervision from parallel data, this framework can be easily extended to other accents with low-resource data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzNaCp6Vcg": {
    "title": "PINNACLE: PINN Adaptive ColLocation and Experimental points selection",
    "volume": "review",
    "abstract": "Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or simulations. Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics. Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses. PINNACLE uses information on the interactions among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK). We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and transfer learning problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=kjn99xFUF3": {
    "title": "FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization",
    "volume": "review",
    "abstract": "Federated learning (FL) is an emerging learning paradigm in which a set of distributed clients learns a task under the coordination of a central server. The FedAvg algorithm is one of the most widely used methods to solve FL problems. In FedAvg, the learning rate is a constant rather than changing adaptively. Adaptive gradient methods have demonstrated superior performance over the constant learning rate schedules in non-distributed settings, and they have recently been adapted to FL. However, the majority of these methods are designed for unconstrained settings. Meanwhile, many crucial FL applications, like disease diagnosis and biomarker identification, often rely on constrained formulations such as Lasso and group Lasso. It remains an open question as to whether adaptive gradient methods can be effectively applied to FL problems with constrains. In this work, we introduce \\textbf{FedDA}, a novel adaptive gradient framework for FL. This framework utilizes a restarted dual averaging technique and is compatible with a range of gradient estimation methods and adaptive learning rate schedules. Specifically, an instantiation of our framework \\textbf{FedDA-MVR} achieves gradient complexity $\\tilde{O}(K^{-1}\\epsilon^{-1.5})$ and communication complexity $\\tilde{O}(K^{-0.25}\\epsilon^{-1.25})$ for finding a stationary point $\\epsilon$ in the constrained setting. We conduct experiments over both constrained and unconstrained tasks to confirm the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUKJWr5zOE": {
    "title": "A Differentiable Physical Simulation Framework for Soft Robots on Multiple-Task Learning",
    "volume": "review",
    "abstract": "Learning multiple tasks is challenging for soft robots. Differentiable physics enables efficient gradient-based optimizations of neural network (NN) controllers for soft robot learning. However, existing work typically delivers NN controllers with limited capability and generalizability. We present a practical learning framework that outputs unified NN controllers capable of multiple tasks with significantly improved complexity and diversity. Our framework consists of a high-performance differentiable deformable bodies simulator supporting the material point method (MPM) and mass-spring systems, an automatic differentiation module that enables gradient-based optimizations, and a practical training module for soft robots on learning multiple locomotion tasks with a single NN controller. Using a unified NN controller trained in our framework, we demonstrate that users can interactively control soft robot locomotion and switch among multiple goals with specified velocity, height, and direction instructions. We evaluate our framework with multiple robot designs and challenging locomotion tasks. Experiments show that our learning framework, based on differentiable physics, delivers better results and converges much faster, compared with reinforcement learning frameworks. In addition, we successfully employed our framework on learning manipulation tasks, indicating the potential to extend our framework to tasks beyond locomotion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ujgouOiAA": {
    "title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that our INSTINCT consistently outperforms the existing methods in different tasks, such as in various instruction induction tasks and the task of improving the zero-shot chain-of-thought instruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=kTRGF2JEcx": {
    "title": "Instructing Large Language Models to Identify and Ignore Irrelevant Conditions",
    "volume": "review",
    "abstract": "Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions. Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy. In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions. It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question. Then it prompts LLMs to verify the irrelevant conditions. Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths. Moreover, we propose to select (problem, reasoning paths)-pairs as demonstrations to enhance I$^3$C with few-shot reasoning. We develop I$^3$C-Select that selects the most confusing problems based on the semantic relevance measurement. We conduct extensive experiments on six MWP datasets. I$^3$C can be combined with any CoT prompting methods to improve the performance of solving MWPs. Notably, I$^3$C-Select achieves an accuracy of $93.7$ and $90.9$ on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Auto-CoT by $+19.4$ and $+25.7$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5M2MjyNR2w": {
    "title": "Adaptive Expansion for Hypergraph Learning",
    "volume": "review",
    "abstract": "Hypergraph, with its powerful ability to capture higher-order complex relationships, has attracted substantial attention recently. Consequently, an increasing number of hypergraph neural networks (HyGNNs) have emerged to model the high-order relationships among nodes and hyperedges. In general, most HyGNNs leverage typical expansion methods, such as clique expansion (CE), to convert hypergraphs into graphs for representation learning. However, they still face the following limitations in hypergraph expansion: (i) Some expansion methods expand hypergraphs in a straightforward manner, resulting in information loss and redundancy; (ii) Most expansion methods often employ fixed edge weights while ignoring the fact that nodes having similar attribute features within the same hyperedge are more likely to be connected compared with nodes with dissimilar features. In light of these challenges, we design a novel CE-based \\textbf{Ad}aptive \\textbf{E}xpansion method called \\textbf{AdE} to expand hypergraphs into weighted graphs that preserve the higher-order hypergraph structure information. Specifically, we first introduce a Global Simulation Network to pick two representative nodes for symbolizing each hyperedge in an adaptive manner. We then connect the rest of the nodes within the same hyperedge to the corresponding selected nodes. Instead of leveraging the fixed edge weights, we further design a distance-aware kernel function to dynamically adjust the edge weights to make sure that node pairs having similar attribute features within the corresponding hyperedge are more likely to be connected with large weights. After obtaining the adaptive weighted graphs, we employ graph neural networks to model the rich relationships among nodes for downstream tasks. Extensive theoretical justifications and empirical experiments over five benchmark hypergraph datasets demonstrate that AdE has excellent rationality, generalization, and effectiveness compared to classic expansion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=sP0Aev2Gis": {
    "title": "G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System",
    "volume": "review",
    "abstract": "Text-based delivery addresses, as the data foundation for logistics systems, contain abundant and crucial location information. How to effectively encode the delivery address is a core task to boost the performance of downstream tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural Language Process (NLP) have emerged as the dominant tools for encoding semantic information in text. Though promising, those NLP-based PTMs fall short of encoding geographic knowledge in the delivery address, which considerably trims down the performance of delivery-related tasks in logistic systems such as Cainiao. To tackle the above problem, we propose a domain-specific pre-trained model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in Logistics field. G2PTL combines the semantic learning capabilities of text pre-training with the geographical-relationship encoding abilities of graph modeling. Specifically, we first utilize real-world logistics delivery data to construct a large-scale heterogeneous graph of delivery addresses, which contains abundant geographic knowledge and delivery information. Then, G2PTL is pre-trained with subgraphs sampled from the heterogeneous graph. Comprehensive experiments are conducted to demonstrate the effectiveness of G2PTL through four downstream tasks in logistics systems on real-world datasets. G2PTL has been deployed in production in Cainiao's logistics system, which significantly improves the performance of delivery-related tasks. The code of G2PTL is available at https://huggingface.co/Cainiao-AI/G2PTL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7Ttk3RzDeu": {
    "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
    "volume": "review",
    "abstract": "Summarizing book-length documents ($>$100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving \\$15K and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators. We release code and annotations after blind review to spur more principled research on book-length summarization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.5,
    "authors": []
  },
  "https://openreview.net/forum?id=qT7DXUmX7j": {
    "title": "NP-GL: Extending Power of Nature from Binary Problems to Real-World Graph Learning",
    "volume": "review",
    "abstract": "Nature performs complex computations constantly at clearly lower cost and higher performance than digital computers. It is crucial to understand how to harness the unique computational power of nature in Machine Learning (ML). In the past decade, besides the development of Neural Networks (NNs), the community has also relentlessly explored nature-powered ML paradigms. Although most of them are still predominantly theoretical, a new practical paradigm enabled by the recent advent of CMOS-compatible room-temperature nature-based computers has emerged. By harnessing the nature's power of entropy increase, this paradigm can solve binary learning problems delivering immense speedup and energy savings compared with NNs, while maintaining comparable accuracy. Regrettably, its values to the real world are highly constrained by its binary nature. A clear pathway to its extension to real-valued problems remains elusive. This paper aims to unleash this pathway by proposing a novel end-to-end Nature-Powered Graph Learning (NP-GL) framework. Specifically, through a three-dimensional co-design, NP-GL can leverage the nature's power of entropy increase to efficiently solve real-valued graph learning problems. Experimental results across 4 real-world applications with 6 datasets demonstrate that NP-GL delivers, on average, $6.97\\times 10^3$ speedup and $10^5$ energy consumption reduction with comparable or even higher accuracy than Graph Neural Networks (GNNs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=e5lR6tySR7": {
    "title": "Transformer-Based Large Language Models Are Not General Learners: A Universal Circuit Perspective",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across diverse tasks, evoking perceptions of ``sparks of Artificial General Intelligence (AGI)\" (Bubeck et al., 2023). A key question naturally arises: *Can foundation models lead to AGI?* In this work, we try to answer this question partially by formally considering the capabilities of Transformer-based LLMs (T-LLMs) from the perspective of universal circuits. By investigating the expressive power of realistic T-LLMs as universal circuits, we show that a T-LLM of size $\\operatorname{poly}(n)$ cannot perform all the basic operators of input length $O\\left(\\operatorname{poly}(\\log n)\\right)$. We also demonstrate that a constant-depth-$\\operatorname{poly}(n)$-size log-precision T-LLM cannot faithfully execute prompts of complexity $n$. Our analysis provides a concrete theoretical foundation that T-LLMs can only be universal circuits for limited function classes, or in other words, T-LLMs are not general learners. Furthermore, we exhibit that a constant-depth-$\\operatorname{poly}(n)$-size log-precision T-LLM can memorize $O\\left(\\operatorname{poly}(n)\\right)$ instances, which could partially explain the seeming inconsistency between LLMs' empirical successes and our negative results. To the best of our knowledge, our work takes the first step towards analyzing the limitations of T-LLMs as general learners within a rigorous theoretical framework. Our results promote the understanding of LLMs' capabilities and highlight the need for innovative architecture designs beyond Transformers to break current limitations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WwCirclMvl": {
    "title": "Posterior Sampling via Langevin Monte Carlo for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "In this paper, we consider offline reinforcement learning (RL) problems. Within this setting, posterior sampling has been rarely used, perhaps partly due to its explorative nature. The only work using posterior sampling for offline RL that we are aware of is the model-based posterior sampling of \\cite{uehara2021pessimistic}. However, this framework does not permit any tractable algorithm (not even in the linear models) where simulations of posterior samples become challenging, especially in high dimensions. In addition, the algorithm only admits a weak form of guarantees -- Bayesian sub-optimality bounds which depend on the prior distribution. To address these problems, we propose and analyze the use of Markov Chain Monte Carlo methods for offline RL. We show that for low-rank Markov decision processes (MDPs), using the Langevin Monte Carlo (LMC) algorithm, our algorithm obtains the (frequentist) sub-optimality bound that competes against any comparator policy $\\pi$ and interpolates between $\\tilde{\\mathcal{O}}(H^2 d \\sqrt{C_{\\pi}/ K})$ and $\\tilde{\\mathcal{O}}(H^2 \\sqrt{d C_{\\pi}/ K})$, where $C_{\\pi}$ is the concentrability coefficient of $\\pi$, $d$ is the dimension of the linear feature, $H$ is the episode length, and $K$ is the number of episodes in the offline data. For general MDPs with overparameterized neural network function approximation, we show that our LMC-based algorithm obtains the sub-optimality bounds of $\\tilde{\\mathcal{O}}(H^{2.5} \\tilde{d} \\sqrt{C_{\\pi} /K})$, where $\\tilde{d}$ is the effective dimension of the neural network. Finally, we collaborate our findings with numerical evaluations to demonstrate that LMC-based algorithms could be both efficient and competitive for offline RL in high dimensions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=6cGiRiExUd": {
    "title": "Efficient Point Cloud Matching for 3D Geometric Shape Assembly",
    "volume": "review",
    "abstract": "Learning to assemble geometric shapes into a larger target structure is a fundamental task with various high-level visual applications. In this work, we frame this problem as geometric registration with extremely low overlap. Our goal is to establish accurate correspondences on the mating surface of the shape fragments to predict their relative rigid transformations for assembly. To this end, we introduce Proxy Match Transform (PMT), an approximate high-order feature transform layer that enables reliable correspondences between dense point clouds of shape fragments, while incurring low costs in memory and compute. In our experiments, we demonstrate that Proxy Match Transform surpasses existing state-of-the-art baselines on a popular geometric shape assembly dataset, while exhibiting higher efficiency than other high-order feature transform methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2uwvigLUr8": {
    "title": "From Deterministic to Probabilistic World: Balancing Enhanced Doubly Robust Learning for Debiased Recommendation",
    "volume": "review",
    "abstract": "In recommender systems, selection bias arises from the users' selective interactions with items, which poses a widely-recognized challenge for unbiased evaluation and learning for recommendation models. Recently, doubly robust and its variants have been widely studied to achieve debiased learning of prediction models, which enables unbiasedness when either imputed errors or learned propensities are accurate. However, we find that previous studies achieve unbiasedness using the doubly robust learning approaches are all based on deterministic error imputation model and deterministic propensity model, and these approaches fail to be unbiased when using probabilistic models to impute errors and learn propensities. To tackle this problem, in this paper, we first derive the bias of doubly robust learning methods and provide alternative unbiasedness conditions for probabilistic models. Then we propose a novel balancing enhanced doubly robust joint learning approach, which improves the accuracy of the imputed errors and leads to unbiased learning under probabilistic error imputations and learned propensities. We further derive the generalization error bound when using the probabilistic models, and show that it can be effectively controlled by the proposed learning approach. We conduct extensive experiments on three real-world datasets, including a large-scale industrial dataset, to demonstrate the effectiveness of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=4bat0pSQBq": {
    "title": "FLOOD SIMULATION WITH PHYSICS-INFORMED MESSAGE PASSING",
    "volume": "review",
    "abstract": "Flood modeling is an important tool for supporting preventive and emergency measures to mitigate flood risks. Recently, there has been an increasing interest in exploring machine learning-based models as an alternative to traditional hydrodynamic models for flood simulation to address challenges such as scalability and accuracy. However, current ML approaches are ineffective at modeling early stages of flooding events, limiting their ability to simulate the entire evolution of the flood. Another key challenge is how to incorporate physics domain-knowledge into these data-driven models. In this paper, we address these challenges by introducing a physics-inspired graph neural network for flood simulation. Given a (geographical) region and precipitation data, our model predicts water depths in an autoregressive fashion. We propose a message-passing framework inspired by the conservation of momentum and mass expressed in the shallow-water equations, which describe the physical process of a flooding event. Empirical results on a dataset covering 9 regions and 7 historical precipitation events demonstrate that our model outperforms the best baseline, and is able to capture the propagation of water flow better, especially at the very early stage of the flooding event",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=HrTGl8AhnS": {
    "title": "PACIA: Parameter-Efficient Adapter for Few-Shot Molecular Property Prediction",
    "volume": "review",
    "abstract": "Molecular property prediction (MPP) plays a crucial role in biomedical applications, but it often encounters challenges due to a scarcity of labeled data. Existing works commonly adopt gradient-based strategy to update a large amount of parameter for property-level adaptation. However, the increase of adaptive parameters can cause overfitting and lead to poor performance. Observing that graph neural network (GNN) performs well as both encoder and predictor, we propose PACIA, a parameter-efficient GNN adapter for few-shot MPP. We design a unified adapter to generate a few adaptive parameters to modulate the message passing process of GNN. We then adopt hierarchical adaptation mechanism to adapt the encoder on property-level and the predictor on molecule-level by the unified GNN adapter. Extensive results show that PACIA obtains the state-of-the-art performance in few-shot MPP problems, and our proposed hierarchical adaptation mechanism is rational and effective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEGUT3OGCW": {
    "title": "Provable Repair of Vision Transformers: Last Layer is All You Need",
    "volume": "review",
    "abstract": "Vision Transformers have emerged as state-of-the-art image recognition tools, but may still exhibit incorrect behavior. Incorrect image recognition can have disastrous consequences in safety-critical real-world applications such as self-driving automobiles. In this paper, we present Provable Repair of Vision Transformers (PRoViT), a provable repair approach that guarantees the correct classification of images in a repair set for a given Vision Transformer without modifying its ar- chitecture. PRoViT avoids negatively affecting correctly classified images (draw- down) by minimizing the changes made to the Vision Transformer's parameters and original output. We observe that for Vision Transformers, unlike for other architectures such as ResNet or VGG, editing just the parameters in the last layer achieves correctness guarantees and very low drawdown. We introduce a novel method for editing these last-layer parameters that enables PRoViT to efficiently repair state-of-the-art Vision Transformers for thousands of images, far exceeding the capabilities of prior provable repair approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=jhCzPwcVbG": {
    "title": "LLMZip: Lossless Text Compression using Large Language Models",
    "volume": "review",
    "abstract": "We design a lossless compression algorithm for compressing English text by using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. Specifically, the proposed LLMZip algorithm uses the conditional probabilities at the output of the large language model in conjunction with Arithmetic Coding. We show that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h. We show that it is possible to marginally improve the compression performance further by first extracting a summary from the document and compressing the text by conditioning on the summary. Finally, we investigate the compression performance of LLMZip when the summary (side information) is available both at the encoder and decoder. We show that the LLM is able to exploit the available side information to significantly improve the compression performance. As an important byproduct, we provide new estimates of an asymptotic upper bound on the entropy of English which is significantly smaller than currently available estimates in \\cite{cover1978convergent}, \\cite{lutati2023focus}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=UK7Hs7f0So": {
    "title": "VMFTransformer: An Angle-Preserving and Auto-Scaling Machine for Multi-horizon Probabilistic Forecasting",
    "volume": "review",
    "abstract": "Time series forecasting has historically been a key area of academic research and industrial applications. As deep learning develops, the major research methodologies of time series forecasting can be divided into two categories, i.e., iterative and direct methods. In the iterative methods, since a small amount of error is produced at each time step, the recursive structure can potentially lead to large error accumulations over longer forecasting horizons. Although the direct methods can avoid this puzzle involved in the iterative methods, it faces abuse of conditional independence among time points. This impractical assumption can also lead to biased models. To solve these challenges, we propose a direct approach for multi-horizon probabilistic forecasting, which can effectively characterize the dependence across future horizons. Specifically, we consider the multi-horizon target as a random vector. The direction of the vector embodies the temporal dependence, and the length of the vector measures the overall scale across each horizon. Therefore, we respectively apply the von Mises-Fisher (VMF) distribution and the truncated normal distribution to characterize the angle and the magnitude of the target vector in our model. We evaluate the performance of our framework on three benchmarks. Extensive results demonstrate the superiority of our framework over six state-of-the-art methods and show the remarkable versatility and extensibility for different time series forecasting tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ElykcDu5YK": {
    "title": "Leveraging Previous Tasks in Optimizing Risk Measures with Gaussian Processes",
    "volume": "review",
    "abstract": "Research on optimizing the risk measure of a blackbox function using Gaussian processes, especially Bayesian optimization (BO) of risk measures, has become increasingly important due to the inevitable presence of uncontrollable variables in real-world applications. Nevertheless, existing works on BO of risk measures start the optimization from scratch for every new task without considering the results of previous tasks. In contrast, its vanilla BO counterpart has received a thorough investigation on utilizing previous tasks to speed up the current task through the body of works on meta-BO which, however, have not considered risk measures. To bridge this gap, this paper presents the first algorithm for meta-BO of risk measures (i.e., value-at-risk (VaR) and the conditional VaR) by introducing a novel adjustment to the upper confidence bound acquisition function. Our proposed algorithm exhibits two desirable properties: (i) invariance to scaling and vertical shifting of the blackbox function and (ii) robustness to previous harmful tasks. We provide a theoretical performance guarantee for our algorithm and empirically demonstrate its performance using several synthetic function benchmarks and real-world objective functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=qxGXjWxabq": {
    "title": "Canonpipe: Data Debugging with Shapley Importance over Machine Learning Pipelines",
    "volume": "review",
    "abstract": "When a machine learning (ML) model exhibits poor quality (e.g., poor accuracy or fairness), the problem can often be traced back to errors in the training data. Being able to discover the data examples that are the most likely culprits is a fundamental concern that has received a lot of attention recently. One prominent way to measure \"data importance\" with respect to model quality is the Shapley value. Unfortunately, existing methods only focus on the ML model in isolation, without considering the broader ML pipeline for data preparation and feature extraction, which appears in the majority of real-world ML code. This presents a major limitation to applying existing methods in practical settings. In this paper, we propose Canonpipe, a method for efficiently computing Shapley-based data importance over ML pipelines. We introduce several approximations that lead to dramatic improvements in terms of computational speed. Finally, our experimental evaluation demonstrates that our methods are capable of data error discovery that is as effective as existing Monte Carlo baselines, and in some cases even outperform them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRXxnoqDHH": {
    "title": "MoAT: Multi-Modal Augmented Time Series Forecasting",
    "volume": "review",
    "abstract": "Time series forecasting plays a pivotal role in various domains, facilitating optimized resource allocation and strategic decision-making. However, the scarcity of training samples often hinders the accuracy of the forecasting task. To address this, we explore the potential of leveraging information from different modalities that are commonly associated with time series data. In this paper, we introduce MoAT, a novel multi-modal augmented time series forecasting approach that strategically integrates both feature-wise and sample-wise augmentation methods to enrich multi-modal representation learning. It further enhances prediction accuracy through joint trend-seasonal decomposition across all modalities and fuses the information for the final prediction. Extensive experiments show that MoAT outperforms state-of-the-art methods, resulting in a substantial reduction in mean squared error ranging from 6.5% to 71.7%, which demonstrates the effectiveness and robustness in addressing the limitations imposed by data scarcity. The datasets and code are available at https://anonymous.4open.science/r/MoAT-201E",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=5t57omGVMw": {
    "title": "Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances",
    "volume": "review",
    "abstract": "Solving a linear system ${\\bf Ax}={\\bf b}$ is a fundamental scientific computing primitive, and numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved but are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used instead. We consider the common setting in which many related linear systems are solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation~(SOR), a standard solver whose parameter $\\omega$ has a strong impact on its runtime. For this method, we prove that a bandit algorithm—using only the number of iterations as feedback—can select parameters for a sequence of instances such that the overall cost is almost as good as that the best fixed $\\omega$ would have obtained. Furthermore, when given additional structural information, we show that a {\\em contextual} bandit method approaches the performance of the {\\em instance-optimal} policy, which selects the best $\\omega$ for each instance. Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqqE1yS3RY": {
    "title": "Towards Better Evaluation of GNN Expressiveness with BREC Dataset",
    "volume": "review",
    "abstract": "Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, unifying all kinds of models into one framework is untractable, making it hard to measure and compare their expressiveness quantitatively. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100\\% accuracy), granularity (models tend to be either 100\\% correct or near random guess), and scale (only several essentially different graphs in each dataset). To address these limitations, we propose a new expressiveness dataset, **BREC**, including 400 pairs of non-isomorphic graphs carefully selected from four primary categories (Basic, Regular, Extension, and CFI). These graphs have higher difficulty (up to 4-WL-indistinguishable), finer granularity (can compare models between 1-WL and 3-WL), and a larger scale (400 pairs or extend to 319600 pairs or even more). Further, we synthetically test 23 models with higher-than-1-WL expressiveness on our BREC dataset. Our experiment gives the first thorough measurement of the expressiveness of those state-of-the-art beyond-1-WL GNN models and reveals the gap between theoretical and practical expressiveness. We expect this dataset to serve as a benchmark for testing the expressiveness of future GNNs. Dataset and evaluation codes are released at: https://github.com/brec-iclr2024/brec-iclr2024",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=4kLVvIh8cp": {
    "title": "Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation. Our work extends the previous instance-dependent results within simpler function classes, such as linear and differentiable function to a more general framework. To the best of our knowledge, this is the first statistically optimal algorithm for nonlinear offline RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=TC9r8gsaoh": {
    "title": "Nuisance-Robust Weighting Network for End-to-End Causal Effect Estimation",
    "volume": "review",
    "abstract": "We combine the two major approaches to causal inference: the conventional statistical approach based on weighting and the end-to-end learning with adversarial networks. Causal inference concerns the expected loss in a distribution different from the training distribution due to intervening on the input variables. Recently, the representation balancing approach with neural networks has repeatedly demonstrated superior performance for complex problems, owing to its end-to-end modeling by adversarial formulation. However, some recent work has shown that the limitation lies in the unrealistic theoretical assumption of the invertibility of the representation extractor. This inherent difficulty stems from the fact that the representation-level discrepancy in representation balancing accounts only for the uncertainty of the later layers than the representation, i.e., the hypothesis layers and the loss. Therefore, we shed light once again on the conventional weighting-based approach, retaining the spirit of end-to-end learning. Most conventional statistical methods are based on inverse probability weighting using propensity scores, which involves nuisance estimation of propensity as an intermediate step. They often suffer from inaccurate estimation of the propensity scores and instability due to large weights. One might be tempted to jointly optimize the nuisance and the target, though it may lead to an optimistic evaluation, e.g., avoiding noisy instances by weighting less when noise levels are heterogeneous. In this paper, we propose a simple method that amalgamates the strengths of both approaches: adversarial joint optimization of the nuisance and the target. Our formulation follows the pessimistic evaluation principle in offline reinforcement learning, which brings provable robustness to the estimation uncertainty of the nuisance and the instability due to extreme weights. Our method performed consistently well under challenging settings with heterogeneous noise. Our code is available online: https://anonymous.4open.science/r/NuNet-002A",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jf5gplvglq": {
    "title": "Skill-Mix: a Flexible and Expandable Family of Evaluations for AI Models",
    "volume": "review",
    "abstract": "As the role of LLMs shifts from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. This capability to combine skills plays an important role in (human) pedagogy and also in a recent paper on emergence phenomena (Arora & Goyal, 2023). Our paper introduces an evaluation, Skill-Mix, to measure this capability. Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text it has not seen in the training set. The paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using the open LLaMA-2 70b model as well as GPT-4. Administering a version of Skill-Mix to popular chatbots gave results that, while generally in line with prior expectations, contained surprises. We found sizeable differences in capabilities among models ---including suspected cases of ``cramming for the leaderboard''--- that had not been revealed by the (much simpler) evaluations used in popular LLM leaderboards. Our methodology can flexibly change to future models and model capabilities, by expanding the set of skills being tested and increasing $k$. We hope Skill-Mix (which will be publicly released, including all prompts and code) may grow into an eco-system of open evaluations for AI capabilities, including in multi-modal settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=yroyhkhWS6": {
    "title": "A Quadratic Synchronization Rule for Distributed Deep Learning",
    "volume": "review",
    "abstract": "In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for $H$ steps without synchronizing with others, hence reducing communication frequency. While $H$ has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper $H$ value can lead to generalization improvement. Yet, selecting a proper $H$ is elusive. This work proposes a theory-grounded method for determining $H$, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting $H$ in proportion to $\\frac{1}{\\eta^2}$ as the learning rate $\\eta$ decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared to the standard data parallel training, QSR enables Local AdamW to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.16% or 0.84% higher top-1 validation accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=e2YOVTenU9": {
    "title": "ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor",
    "volume": "review",
    "abstract": "Deep neural network (DNN) models, despite their impressive performance, are vulnerable to exploitation by attackers who attempt to adapt them to other tasks for their own benefit. Current defense strategies mainly address this vulnerability at the model parameter level, leaving the potential of architectural-level defense largely unexplored. This paper, for the first time, addresses the issue of model protection by reducing transferability at the architecture level. Specially, we present a novel neural architecture search (NAS)-enabled algorithm that employs zero-cost proxies and evolutionary search, to design model architectures with low transferability. Our method, namely ArchLock, aims to achieve high performance on the source task, while degrading the performance on target tasks, i.e., locking the transferability of a DNN model. To achieve efficient cross-task search without having access to the training data owned by the attackers, we utilize zero-cost proxies to speed up architecture evaluation and simulate potential target task embeddings to assist cross-task search with a binary performance predictor. Extensive experiments on NAS-Bench-201 and TransNAS-Bench-101 demonstrate that ArchLock reduces transferability by up to 30\\% and 50%, respectively, with negligible performance degradation on source tasks (<2%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=fjRM5ozPv9": {
    "title": "Local-Forward: Towards Biological Plausibility in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "A lasting critique of deep learning as a model for biological intelligence and learning is the biological implausibility of backpropagation. Backpropagation requires caching local outputs and propagating a global error via derivatives, neither of which are known to be implemented by biological neurons. In reinforcement learning, building more biologically plausible agents would allow us to better model human cognition and social behavior, and improve computational efficiency. We propose Local-Forward, a new temporal-difference learning algorithm (and associated architecture) that trains neural networks to predict Q-values. Rather than backpropagating error derivates, we rely on updates that are local to each layer of the architecture and additionally use forward connections in time to pass information from upper layers to lower layers via activations. Our approach builds on the recently proposed Forward-Forward algorithm, as well as recurrence and attention in neural architectures. This approach no longer suffer the aforementioned contradictions with biology. Furthermore, as a proof-of-concept, we train reinforcement learning agents with Local-Forward to solve control tasks in the MinAtar environments, and show that our method's potential warrants further investigation because it opens avenues for more computational efficient training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDMM4ZX1UB": {
    "title": "Exploiting Code Symmetries for Learning Program Semantics",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) hold significant potential for automating program analysis, but current code LLMs face challenges in grasping program semantics. Our paper addresses this by formalizing program semantics through code symmetries and integrating them into LLM architectures for code analysis. We introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, enabling precise reasoning within LLMs. Our solution, SymC, employs a novel variant of group-equivariant self-attention that is provably equivariant to code symmetries. We extensively evaluate SymC on four program analysis tasks, comparing it to eight baselines against eight code transformations. Our results show that SymC generalizes to unseen code transformations, outperforming the state-of-the-art code models by 30.7%. SymC, by design, stays invariant to semantics-preserving permutations, while state-of-the-art code models like WizardCoder and GPT-4 violate these invariances at a high rate (i.e., 14% and 43%, respectively)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=j2AWbl4L3K": {
    "title": "Weight Uncertainty in Individual Treatment Effect",
    "volume": "review",
    "abstract": "The estimation of individual treatment effects (ITE) has recently gained significant attention from both the research and industrial communities due to its potential applications in various fields such as healthcare, economics, and education. However, the sparsity of observational data often leads to a lack of robustness and over-fitting in most existing methods. To address this issue, this paper investigates the benefits of incorporating uncertainty modeling in the process of optimizing parameters for robust ITE estimation. Specifically, we derive an informative generalization bound that connects to Bayesian inference and propose a variational bound in closed form to learn a probability distribution on the weights of a hypothesis and representation function. Through experiments on one synthetic dataset and two benchmark datasets, we demonstrate the effectiveness of our proposed model in comparison to state-of-the-art methods. Moreover, we conduct experiments on a real-world dataset in recommender scenarios to verify the benefits of uncertainty in causal inference. The results of our experiments provide evidence of the practicality of our model, which aligns with our initial expectations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=DE7IVrk8Ks": {
    "title": "Latent Shattering: Turning Unconditional Pretrained Generators Into Conditional Models By Imposing Latent Structure",
    "volume": "review",
    "abstract": "Deep generative models, such as GANs and VAEs, have gained substantial attention for their ability to synthesize realistic data. Pretrained generative models are often unconditional, thus do not easily allow the user to specify the class of the output. Yet supporting conditional generation offers inherent benefits for many tasks. Due to current models requiring huge data sets and often prohibitively expensive computational resources for training, it is desirable to have a lightweight method that can convert pretrained unconditional generators into conditional models without retraining. Previous research into this problem is limited, typically assuming either access to classifiers that identify which regions of the generator's latent space correspond to specific classes, access to labeled data, or even retraining of the generative model itself. These strict requirements pose a serious limitation. In this work, we propose LASH, a fresh approach at the conversion of unconditional generators into conditional models in a completely unsupervised manner without requiring retraining nor access to any real data. Instead, the key principle of LASH is to identify points in the generator's latent space that are mapped to low-density regions of the output space. The insight is that by removing these points, LASH \"shatters\" the latent space into distinct clusters where each cluster corresponds to a semantically meaningful mode in the output space. We demonstrate that these modes correspond to distinct real-world classes. Lastly, LASH utilizes a simple Gaussian mixture model to adaptively sample from these clusters, supporting unsupervised conditional generation. Through a series of experiments on MNIST, FashionMNIST, and CelebA, we demonstrate that LASH significantly outperforms existing methods in unsupervised conditional sampling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=DVA0NDUdCQ": {
    "title": "Efficient Large Language Models Fine-Tuning on Graphs",
    "volume": "review",
    "abstract": "Learning from Text-Attributed Graphs (TAGs) has attracted significant attention due to its wide range of real-world applications. The rapid evolution of large language models (LLMs) has revolutionized the way we process textual data, which indicates a strong potential to replace shallow text embedding generally used in Graph Neural Networks (GNNs). However, we find that existing LLM approaches that exploit text information in graphs suffer from inferior computation and data efficiency. In this work, we introduce a novel and efficient approach for the end-to- end fine-tuning of Large Language Models (LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost and memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover, it transfers the rick knowledge in LLMs to downstream graph learning tasks effectively with limited labeled data in semi-supervised learning. Its superior computation and data efficiency are demonstrated through comprehensive experiments, offering a promising solution for a wide range of LLMs and graph learning tasks on TAGs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RwhRZojoYw": {
    "title": "On information dropping and oversmoothing in graph neural networks",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) are widespread in graph representation learning. *Random dropping* approaches, notably DropEdge and DropMessage, claim to alleviate the key issues of overfitting and oversmoothing by randomly removing elements of the graph representation. However, their effectiveness is largely unverified. In this work, we find experimentally that they have a limited effect in reducing oversmoothing, contrary to what is typically assumed in the literature. These approaches are also non-parametric and motivate us to question if *learned* dropping can alleviate the propagation of redundant or noisy edges. We propose a new information-theoretic approach, in which we learn to perform dropping on the data exchanged by nodes during message passing via optimizing an information bottleneck. Our approach is superior to previous dropping methods in oversmoothing reduction and has promising performance in the case of deep GNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ibggY9ZJ1T": {
    "title": "HuRef: HUman-REadable Fingerprint for Large Language Models",
    "volume": "review",
    "abstract": "Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations through fine-tuning or continued pretraining. In this study, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly changes it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model. We make these invariant terms human-readable by mapping them to a Gaussian vector using a convolutional encoder and then converting it into a natural image with StyleGAN2. The encoder discriminates between invariants from different base models and ensures Gaussian output through adversarial training, while StyleGAN2 transforms Gaussian vectors into dog images. Consequently, our method generates a dog image as an identity fingerprint for an LLM, where the dog's appearance strongly indicates the LLM's base model. Specifically, if the LLM is adapted from another base model, the generated dog highly resembles that model; otherwise if trained independently from scratch, it exhibits a unique dog image distinct from other models. Experimental results across various LLMs demonstrate the effectiveness of our method, the generated dog image remains invariant to different training steps, including SFT, RLHF, or even continued pretraining with augmented vocabulary in a new language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=WM5G2NWSYC": {
    "title": "Projected Subnetworks Scale Adaptation",
    "volume": "review",
    "abstract": "Large models support great zero-shot and few-shot capabilities. However, updating these models on new tasks can break performance on previous seen tasks and their zero/few-shot unseen tasks. Our work explores how to update zero/few-shot learners such that they can maintain performance on seen/unseen tasks of previous tasks as well as new tasks. By manipulating the parameter updates of a gradient-based meta learner as the projected task-specific subnetworks, we show improvements for large models to retain seen and zero/few shot task performance in online settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fwJeVYGcbz": {
    "title": "Multiple Modes for Continual Learning",
    "volume": "review",
    "abstract": "Adapting model parameters to incoming streams of data is a crucial factor to deep learning scalability. Interestingly, prior continual learning strategies in online settings inadvertently anchor their updated parameters to a local parameter subspace to remember old tasks, else drift away from the subspace and forget. From this observation, we formulate a trade-off between constructing multiple parameter modes and allocating tasks per mode. Mode-Optimized Task Allocation (MOTA), our contributed adaptation strategy, trains multiple modes in parallel, then optimizes task allocation per mode. We empirically demonstrate improvements over baseline continual learning strategies and across varying distribution shifts, namely sub-population, domain, and task shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yID2fdta1Z": {
    "title": "Robust Graph Neural Networks via Unbiased Aggregation",
    "volume": "review",
    "abstract": "The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses. In this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to understand their robustness and limitations. Our novel analysis of estimation bias motivates the design of a robust and unbiased graph signal estimator. We then develop an efficient Quasi-Newton iterative reweighted least squares algorithm to solve the estimation problem, which unfolds as robust unbiased aggregation layers in GNNs with a theoretical convergence guarantee. Our comprehensive experiments confirm the strong robustness of our proposed model, and the ablation study provides a deep understanding of its advantages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=J4zh8rXMm9": {
    "title": "Flashback: Understanding and Mitigating Forgetting in Federated Learning",
    "volume": "review",
    "abstract": "In the realm of Federated Learning (FL), the convergence and effectiveness of learning algorithms can be severely hampered by the phenomenon of forgetting—where knowledge obtained in one round becomes diluted or lost in subsequent rounds. Such a challenge is a result of severe data heterogeneity across clients. Although FL algorithms like FedAvg have been pivotal, they often falter in scenarios of high data heterogeneity. This work delves into the nuances of this problem, establishing the critical role forgetting plays in the inefficient learning of FL in the context of severe data heterogeneity. Knowledge loss occurs in both the local update and the aggregation step; addressing one phase without considering the other will not mitigate forgetting. We introduce a novel metric that offers a granular measurement of forgetting at every round while ensuring that the occurrence of forgetting is distinctly recognized and not obscured by the simultaneous acquisition of new class-specific knowledge. Leveraging these insights, we propose Flashback, an FL algorithm that integrates a novel dynamic distillation approach. The knowledge of different models is estimated and the distillation loss is adapted accordingly. This adaptive distillation is applied both at the local and global update phases, ensuring models retain essential knowledge across rounds while also assimilating new knowledge. Our approach seeks to robustly mitigate the detrimental effects of forgetting, paving the way for more efficient and consistent FL algorithms, especially in environments of high data heterogeneity. By effectively mitigating forgetting, Flashback achieves faster convergence to target accuracy outperforming baselines, by being up to 88.5$\\times$ faster and at least 4.6$\\times$ faster across the different benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=10BTKkFfhl": {
    "title": "Efficient Backdoor Mitigation in Federated Learning with Contrastive Loss",
    "volume": "review",
    "abstract": "Due to the data-driven nature of deep neural networks and privacy concerns around user data, a backdoor could be easily injected into deep neural networks in federated learning without attracting the attention of users. An affected global model operates normally as a clean model in regular tasks and behaves differently when the trigger is presented. In this paper, we propose a novel reverse engineering approach to detect and mitigate the backdoor attack in federated learning by adopting a self-supervised Contrastive learning loss. In contrast to existing reverse engineering techniques, such as Neural Cleanse, which involve iterating through each class in the dataset, we employ the contrastive loss as a whole to identify triggers in the backdoored model. Our method compares the last-layer feature outputs of a potentially affected model with these from a clean one preserved beforehand to reconstruct the trigger under the guidance of the contrastive loss. The reverse-engineered trigger is then applied to patch the affected global model to remove the backdoor. If the global model is free from backdoors, the Contrastive loss will lead to either a blank trigger or one with random pattern. We evaluated the proposed method on three datasets under two backdoor attacks and compared it against three existing defense methods. Our results showed that while many popular reverse engineering algorithms were successful in centralized learning settings, they had difficulties detecting backdoors in federated learning, including Neural Cleanse, TABOR, and DeepInspect. Our method successfully detected backdoors in federated learning and was more time-efficient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=MGWsPGogLH": {
    "title": "Turing Complete Transformers: Two Transformers Are More Powerful Than One",
    "volume": "review",
    "abstract": "This paper presents Find+Replace transformers, a family of multi-transformer architectures that can provably do things no single transformer can, and which outperforms GPT-4 on several challenging tasks. We first establish that traditional transformers and similar architectures are not Turing Complete, while Find+Replace transformers are. Using this fact, we show how arbitrary programs can be compiled into Find+Replace transformers, potentially aiding interpretability research. We also demonstrate the superior performance of Find+Replace transformers over GPT-4 on a set of composition challenge problems. This work aims to provide a theoretical basis for multi-transformer architectures, and to encourage their further exploration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=vUgeBN7F9l": {
    "title": "PolyFormer: Scalable Graph Transformer via Polynomial Attention",
    "volume": "review",
    "abstract": "Graph Transformers have demonstrated superior performance in graph representation learning. However, many current methods focus on attention mechanisms between node pairs, limiting their scalability and expressiveness on node-level tasks. While the recent NAGphormer attempts to address scalability by employing node tokens in conjunction with vanilla multi-head self-attention, these tokens, which are designed in the spatial domain, suffer from restricted expressiveness. On the other front, some approaches have explored encoding eigenvalues or eigenvectors in the spectral domain to boost expressiveness, but these methods incur significant computational overhead due to the requirement for eigendecomposition. To overcome these limitations, we first introduce node tokens using various polynomial bases in the spectral domain. Then, we propose a tailored polynomial attention mechanism, PolyAttn, which serves as a node-wise graph filter and offers powerful representation capabilities. Building on PolyAttn, we present PolyFormer, a graph Transformer model specifically engineered for node-level tasks, offering a desirable balance between scalability and expressiveness. Extensive experiments demonstrate that our proposed methods excel at learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs containing up to 100 million nodes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LTHWoQ9ac1": {
    "title": "Cost Adaptive Recourse Recommendation by Adaptive Preference Elicitation",
    "volume": "review",
    "abstract": "Algorithmic recourse recommends a cost-efficient action to a subject to reverse an unfavorable machine learning classification decision. Most existing methods in the literature generate recourse under the assumption of complete knowledge about the cost function. In real-world practice, subjects could have distinct preferences, leading to incomplete information about the underlying cost function of the subject. This paper proposes a two-step approach that integrates preference learning to the recourse generation problem. In the first step, we design a question-answering framework to refine the confidence set of the Mahalanobis matrix cost of the subject sequentially. Then we generate recourse by utilizing two methods: gradient-based and graph-based cost-adaptive recourse that ensures validity while considering the whole confidence set of the cost matrix. The numerical evaluation demonstrates the benefits of our approach over state-of-the-art baselines in delivering cost-efficient recourse recommendations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUEe72DwPG": {
    "title": "Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa",
    "volume": "review",
    "abstract": "Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30\\%) making the model easier to use, 2) improve the more performant method (up to 32.2\\%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3\\%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDGKPbF0VQ": {
    "title": "Improving Language Models with Advantage-based Offline Policy Gradients",
    "volume": "review",
    "abstract": "Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=mlJLVigNHp": {
    "title": "RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation",
    "volume": "review",
    "abstract": "Retrieval-augmented language models improve language models (LMs) by retrieving documents and prepending them in-context. However, these documents, often spanning hundreds of words, make inference substantially less efficient. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieve the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summary by synthesizing information from multiple documents. Both are trained to achieve performance gain in LMs when we prepend the generated summary from the compressor to LMs' input, while minimizing the summary length. When retrieved documents are irrelevant to the input or offer no additional information to LM, our compressors output an empty string, enabling selective augmentation. We evaluate our approach on the language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide a summary largely faithful to the retrieved documents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=VRCh74Liu9": {
    "title": "Federated Generalization via Information-Theoretic Distribution Diversification",
    "volume": "review",
    "abstract": "Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic generalization framework for FL. Specifically, it quantifies generalization errors by evaluating the information entropy of local distributions and discerning discrepancies across these distributions. Inspired by our deduced generalization bounds, we introduce a weighted aggregation approach and a duo of client selection strategies. These innovations aim to bolster FL's generalization prowess by encompassing a more varied set of client data distributions. Our extensive empirical evaluations reaffirm the potency of our proposed methods, aligning seamlessly with our theoretical construct",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=rT2KyF8SFM": {
    "title": "Defender of privacy and fairness: tiny but reversible generative model via mutually collaborative knowledge distillation",
    "volume": "review",
    "abstract": "Sharing vast amounts of data to train powerful artificial intelligence (AI) models raises public interest concerns such as privacy and fairness. While reversible anonymization techniques are very effective for privacy preservation and fairness enhancement, these methods rely on heavy reversible generative models, making them only suitable to run in the cloud or on a server independent from the image source. For example, data transmission might be under the privacy threats such as channel eavesdropping. Therefore, we propose a novel mutually collaborative knowledge distillation strategy to train a tiny and reversible generative model. This enables us to build a synthesis-based privacy and fairness protection system in embedded devices for anonymizing privacy-sensitive data and thus improve security protection capabilities from the source. The proposed mutually collaborative knowledge distillation method exploits the reversibility of the generative model. By pairing the teacher encoder (decoder) with the student decoder (encoder), we train the student decoder (encoder) by reconstructing the image space (latent space) from the prior image space (latent space). This results in tiny-size student models that can be embedded into devices. We deploy and evaluate our system on NVIDIA Jetson TX2 devices, which operate in real-time. Extensive experiments demonstrate that our system effectively anonymizes face images and thus protects privacy and also improves fairness while minimizing the impact on downstream tasks. Our code will be publicly available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=rkplYfqUr0": {
    "title": "Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions",
    "volume": "review",
    "abstract": "Language model (LM) prompting—a popular paradigm for solving NLP tasks—has been shown to be susceptible to miscalibration and brittleness to slight prompt variations, caused by its discriminative prompting approach, i.e., predicting the label given the input. To address these issues, we propose Gen-Z—a generative prompting framework for zero-shot text classification. GEN-Z is generative, as it measures the LM likelihood of input text, conditioned on natural language descriptions of labels. The framework is multivariate, as label descriptions allow us to seamlessly integrate additional contextual information about the labels to improve task performance. On various standard classification benchmarks, with six open-source LM families, we show that zero-shot classification with simple contextualization of the data source of the evaluation set consistently outperforms both zero-shot and few-shot baselines while improving robustness to prompt variations. Further, our approach enables personalizing classification in a zero-shot manner by incorporating author, subject, or reader information in the label descriptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=0sO2euxhUQ": {
    "title": "Learning Latent Structural Causal Models",
    "volume": "review",
    "abstract": "Causal learning has long concerned itself with the recovery of underlying causal mechanisms. Such causal modelling enables better explanations of out-of-distribution data. Prior works on causal learning assume that the causal variables are given. However, in machine learning tasks, one often operates on low-level data like image pixels or high-dimensional vectors. In such settings, the entire Structural Causal Model (SCM) -- structure, parameters, \\textit{and} high-level causal variables -- is latent and needs to be learnt from low-level data. We treat this problem as Bayesian inference of the latent SCM, given low-level data. We present BIOLS, a tractable approximate inference method which performs joint inference over the causal variables, structure and parameters of the latent SCM from known interventions. Experiments are performed on synthetic datasets and a causal benchmark image dataset to demonstrate the efficacy of our approach. We also demonstrate the ability of BIOLS to generate images from unseen interventional distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=62K7mALO2q": {
    "title": "In-Context Learning Dynamics with Random Binary Sequences",
    "volume": "review",
    "abstract": "Large language models (LLMs) trained on huge corpora of text datasets demonstrate complex, emergent capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often unclear, and different prompts can elicit different capabilities through in-context learning. We propose a Cognitive Interpretability framework that enables us to analyze in-context learning dynamics to understand latent concepts in LLMs underlying behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would require. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate pseudo-random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from pseudo-random behaviors to deterministic repetition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ueQ6T58ZAK": {
    "title": "Dynamic Representation of Optimal Transport via Ensemble Systems",
    "volume": "review",
    "abstract": "Optimal transport has gained widespread recognition in diverse areas from economics and fluid mechanics, lately, to machine learning. However, its connection and potential applications to the domain of dynamical systems and control remain underexplored. To fill this gap, we establish an ensemble-systems interpretation for modeling the optimal transport process. We interpret displacement interpolation of the transport between continuous distributions as a dynamic process and show that this can be modeled as an ensemble control system. This is achieved by establishing moment kernel representations for describing the dynamics of optimal transport and ensemble systems. This methodology further gives rise to an optimal transport based algorithm for learning controls for ensemble systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=XsHqr9dEGH": {
    "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking",
    "volume": "review",
    "abstract": "Recent work by Power et al. (2022) highlighted a surprising \"grokking\" phenomenon in learning arithmetic tasks: a neural net first \"memorizes\" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy. Even in the absence of weight decay, we show that grokking can still happen when the late phase implicit bias is driven by other regularization mechanisms, such as implicit margin maximization or sharpness reduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=52fz5sUAy2": {
    "title": "Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference for Recommendation",
    "volume": "review",
    "abstract": "The interaction between users and recommender systems is not only affected by selection bias but also the neighborhood effect, i.e., the interaction between a user and an item is affected by the interactions between other users and other items, or between the same user and other items, or between other users and the same item. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but the lack of consideration of neighborhood effects can lead to biased estimates and suboptimal performance of the prediction model. In this paper, we formally formulate the neighborhood effect as an interference problem from the perspective of causal inference and introduce a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effects. In addition, we further develop two novel estimators for the ideal loss. We theoretically establish the connection between the proposed methods and previous methods ignoring the neighborhood effect and show that the proposed methods achieve unbiased learning when both selection bias and neighborhood effects are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3yyGlNHnlj": {
    "title": "GraphECL: Towards Efficient Contrastive Learning for Graphs",
    "volume": "review",
    "abstract": "Due to the inherent label scarcity, learning useful representations on graphs with no supervision is of great benefit. Yet, existing graph self-supervised learning methods overlook the scalability challenge and fail to conduct fast inference of representations in latency-constrained applications due to the intensive message passing of graph neural networks. In this paper, we present GraphECL, a simple and efficient contrastive learning paradigm for graphs. To achieve inference acceleration, GraphECL does not rely on graph augmentations but introduces cross-model contrastive learning, where positive samples are obtained through \\MLP and \\GNN representations from the central node and its neighbors. We provide theoretical analysis on the design of this cross-model framework and discuss why our \\MLP can still capture structure information and enjoys better downstream performance as \\GNN. Extensive experiments on common real-world tasks verify the superior performance of \\simper compared to state-of-the-art methods, highlighting its intriguing properties, including better inference efficiency and generalization to both homophilous and heterophilous graphs. On large-scale datasets such as Snap-patents, the \\MLP learned by GraphECL is 286.82x faster than GCL methods with the same number of \\GNN layers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=dnaCBAP7X2": {
    "title": "An Implicit Watermark Framework for Adversary Identification",
    "volume": "review",
    "abstract": "Security of deep neural networks based machine learning systems has been an emerging research topic, especially after the discovery of adversarial attacks. In general, however, it is very difficult to build a machine learning system that is resistant to different types of attacks. Instead of directly improving the robustness of neural networks, Cheng et al. proposed the first framework to trace the first compromised model under the black-box adversarial attack in a forensic view. However, the black-box assumption has limited the usage of the framework since users will require detailed model information to facilitate their own use in the modern MLaaS system. In this paper, instead of considering the limited black-box attacks, we investigate more general and harder white-box setting where all users will have full access to model. Explicit modification on the model architecture during the inference will be no longer effective because those mechanisms could be easily bypassed by adversary. To address this challenge, a novel identification framework is proposed that can achieve high tracking accuracy to trace the source of white-box adversarial attack. Specifically, to differentiate adversarial examples generated from different copies, we first design an implicit watermark from backdooring before the model distribution. Then we design a data-free method to identify the adversary with only adversarial example available. Extensive experiments on different attacks including both white-box and black-box attacks, datasets, and model architectures verify the effectiveness of the proposed method. Our code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=22pyNMuIoa": {
    "title": "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization",
    "volume": "review",
    "abstract": "Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=mCnWT9OVvK": {
    "title": "Understanding Retrieval Augmentation for Long-Form Question Answering",
    "volume": "review",
    "abstract": "We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the *attribution* of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our controlled study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further reveal novel attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=JnRStoIuTe": {
    "title": "Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning",
    "volume": "review",
    "abstract": "Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and dataset distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed, yet expensive, strategies for identifying the most informative training examples out of large datasets. In this work, we revisit these methods to understand if the additional computational costs associated with such strategies are justified from the perspective of time-to-accuracy, which has become a critical efficiency measure of deep neural network training over large datasets. Surprisingly, we find that many of the recently proposed methods underperform what we call Repeated Sampling of Random Subsets (RSRS or RS2), a powerful yet overlooked extension of the standard random baseline that learns from repeatedly sampled data throughout training instead of a fixed random subset. We test RS2 against thirty-two state-of-the-art data pruning and distillation methods across four datasets including ImageNet. Our results demonstrate that RS2 significantly reduces time-to-accuracy, particularly in practical regimes where accuracy, but not runtime, is similar to that of training on full dataset. For example, when training ResNet-18 on ImageNet, with 10\\% of the dataset each epoch RS2 reaches an accuracy of 66\\% versus 69\\% when training with the full dataset. The best competing method achieves only 55\\% while training 1.6$\\times$ slower than RS2. Beyond the above meta-study, we discuss the theoretical properties of RS2 such as its convergence rate and generalization error. Our primary goal is to highlight that future works that aim to minimize total training cost by using subset selection, need to consider 1) the total computation cost (including preparing the subset) and 2) should aim to outperform a simple extension of random sampling (i.e., RS2)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=kGteeZ18Ir": {
    "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
    "volume": "review",
    "abstract": "Recent work has showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like \"_You are Julius Caesar. Compose a rap about Climate Change._\" However, it remains unclear how these persona assignments indirectly influence LLMs' core capabilities. We present the first extensive study of this in the context of LLMs' ability to perform basic reasoning. Our study encompasses 16 personas spanning 5 diverse groups (race, gender, religion, disability, and political affiliation), across 24 reasoning datasets in diverse domains such as mathematics, history, law, ethics, and more. Our findings unveil that while LLMs, such as ChatGPT, overtly reject stereotypes when explicitly asked (\"_Are Black people inept at mathematics?_\"), they tend to manifest implicit stereotypical and often erroneous presumptions when prompted to take on a persona (e.g., abstentions in rationales such as \"_As a Black person, I am unable to answer this question as it requires math knowledge_\"). This results in substantial disparities in reasoning performance among personas. This inherent 'deep' bias permeates extensively, leading to a statistically significant performance drop in over 95\\% of our datasets for certain personas, with as much as 70\\% relative drop in accuracy on select datasets. Beyond explicit abstentions, these models also have implicitly biased reasoning not evident in their responses. We find that simple prompt-based mitigation approaches have minimal impact. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs---a trend on the rise---can surface their deep-rooted biases and have unforeseeable and detrimental side-effects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=AZW3qlCGTe": {
    "title": "Enhancing Instance-Level Image Classification with Set-Level Labels",
    "volume": "review",
    "abstract": "Instance-level image classification tasks have traditionally relied on single-instance labels to train models, e.g., few-shot learning and transfer learning. However, set-level coarse-grained labels that capture relationships among instances can provide richer information in real-world scenarios. In this paper, we present a novel approach to enhance instance-level image classification by leveraging set-level labels. We provide a theoretical analysis of the proposed method, including recognition conditions for fast excess risk rate, shedding light on the theoretical foundations of our approach. We conducted experiments on two distinct categories of datasets: natural image datasets and histopathology image datasets. Our experimental results demonstrate the effectiveness of our approach, showcasing improved classification performance compared to traditional single-instance label-based methods. Notably, our algorithm achieves 13\\% improvement in classification accuracy compared to the strongest baseline on the histopathology image classification benchmarks. Importantly, our experimental findings align with the theoretical analysis, reinforcing the robustness and reliability of our proposed method. This work bridges the gap between instance-level and set-level image classification, offering a promising avenue for advancing the capabilities of image classification models with set-level coarse-grained labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Ad81awoBVS": {
    "title": "Rotation has two sides: Evaluating Data Augmentation for Deep One-class Classification",
    "volume": "review",
    "abstract": "One-class classification (OCC) involves predicting whether a new data is normal or anomalous based solely on the data from a single class during training. Various attempts have been made to learn suitable representations for OCC within a self-supervised framework. Notably, discriminative methods that use geometric visual transformations, such as rotation, to generate pseudo-anomaly samples have exhibited impressive detection performance. Although rotation is commonly viewed as a distribution-shifting transformation and is widely used in the literature, its effectiveness remains a mystery. In this study, we make a surprising observation: there exists a strong linear relationship (Pearson's Correlation, $r > 0.9$) between the accuracy of rotation prediction and the performance of OCC. This suggests that a classifier that effectively distinguishes different rotations is more likely to excel in OCC, and vice versa. The root cause of this phenomenon can be attributed to the transformation bias in the dataset, where representations learned from transformations already present in the dataset tend to be less effective, making it essential to accurately estimate the transformation distribution before utilizing pretext tasks involving these transformations for reliable self-supervised representation learning. To the end, we propose a novel two-stage method to estimate the transformation distribution within the dataset. In the first stage, we learn general representations through standard contrastive pre-training. In the second stage, we select potentially semantics-preserving samples from the entire augmented dataset, which includes all rotations, by employing density matching with the provided reference distribution. By sorting samples based on semantics-preserving versus shifting transformations, we achieve improved performance on OCC benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=I07KLz6Em1": {
    "title": "QuantEase: Optimization-based Quantization for Large Language Models",
    "volume": "review",
    "abstract": "With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-the-art performance regarding perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements of up to 15% over methods such as GPTQ. Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=6Ey8mAuLiw": {
    "title": "On the Power of Multitask Representation Learning with Gradient Descent",
    "volume": "review",
    "abstract": "Representation learning, particularly multi-task representation learning, has gained widespread popularity in various deep learning applications, ranging from computer vision to natural language processing, due to its remarkable generalization performance. Despite its growing use, our understanding of the underlying mechanisms remains limited. In this paper, we provide a theoretical analysis elucidating why multi-task representation learning outperforms its single-task counterpart in scenarios involving over-parameterized two-layer convolutional neural networks trained by gradient descent. Our analysis is based on a data model that encompasses both task-shared and task-specific features, a setting commonly encountered in real-world applications. We also present experiments on synthetic and real-world data to illustrate and validate our theoretical findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jTSKkcbEsj": {
    "title": "Pushing Boundaries: Mixup's Influence on Neural Collapse",
    "volume": "review",
    "abstract": "Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to augment the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame (ETF), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this study, we delve into the last-layer activations of training data for deep networks subjected to mixup, aiming to uncover insights into its operational efficacy. Our investigation, spanning various architectures and dataset pairs, reveals that mixup's last-layer activations predominantly converge to a distinctive configuration. In this configuration, activations from mixed-up examples of identical classes align with the classifier, while those from different classes delineate channels along the decision boundary. To validate our empirical observations, we further conduct a theoretical analysis under the assumption of an unconstrained features model, utilizing the mixup loss. Through this, we characterize and derive the optimal last-layer features, culminating in a configuration consistent with our experimental findings, thereby shedding light on the intricate workings of mixup in the training of deep networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=2XBBumBGeP": {
    "title": "sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows",
    "volume": "review",
    "abstract": "Noise poses a widespread challenge in signal processing, particularly when it comes to denoising images. Although convolutional neural networks (CNNs) have exhibited remarkable success in this field, they are predicated upon the belief that noise follows established distributions, which restricts their practicality when dealing with real-world noise. To overcome this limitation, several efforts have been taken to collect noisy image datasets from the real world. Generative methods, employing techniques such as generative adversarial networks (GANs) and normalizing flows (NFs), have emerged as a solution for generating realistic noisy images. Recent works model noise using camera metadata, however requiring metadata even for sampling phase. In contrast, in this work, we aim to estimate the underlying camera settings, enabling us to improve noise modeling and generate diverse noise distributions. To this end, we introduce a new NF framework that allows us to both classify noise based on camera settings and generate various noisy images. Through experimental results, our model demonstrates exceptional noise quality and leads in denoising performance on benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=N0gLRTmmO5": {
    "title": "Open-Ended Learning in General-Sum Games: The Role of Diversity in Correlated Equilibrium",
    "volume": "review",
    "abstract": "The primary in this work focuses on the challenging and crucial task of identifying and selecting equilibria for $n$-player general-sum games. PSRO serves as a comprehensive framework for tackling complex games by leveraging the concept of the meta-game. However, prior research on PSRO mainly concentrates on solving two-player zero-sum games. Extended approaches such as JPRSO and $\\alpha$-Rank can address multi-player general-sum games, and these methods theoretically ensure uniqueness and convergence. Nonetheless, a noticeable gap often exists between the joint policy distribution derived by the solver and the target equilibrium, which can undermine the robustness of the joint policy. Within the PSRO framework, diversity characterizes the distinctions among policies within the population, representing the exploration of the policy space by players. Consequently, allocating greater sampling probabilities (meta-strategy) to more diverse policies encourages players to employ more exploratory policies, thereby mitigating the risk of exploitation. We begin by incorporating diversity measures into solving equilibria for $n$-player meta-games and introduce a novel equilibrium concept, called Diverse (C)CE, the objective of which is to maximize sum of expectations of each player's diversity. In alignment with this, we present a policy training algorithm, Diverse Correlated Oracle (DCO), which effectively associates policy dynamics with the joint policy distribution. The experimental results conducted on a range of multi-player, general-sum games demonstrate that our algorithm outperforms JPSRO and $\\alpha$-Rank and enhances the approximation of the joint policy distribution towards the target equilibrium by notably reducing the gap",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=li1Z0OQfnA": {
    "title": "On Local Equilibrium in Non-Concave Games",
    "volume": "review",
    "abstract": "While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategies, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parametrized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support, in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed *local correlated equilibrium*, which generalizes local Nash equilibrium. Importantly, we show that this solution concept captures the convergence guarantees of Online Gradient Descent and no-regret learning, which we show efficiently converge to this type of equilibrium in non-concave games with smooth utilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RvUVMjfp8i": {
    "title": "A Benchmark on Robust Semi-Supervised Learning in Open Environments",
    "volume": "review",
    "abstract": "Semi-supervised learning (SSL) has emerged as a promising paradigm to alleviate the dependency on abundant labeled data by harnessing the power of unlabeled data. Although many SSL algorithms have been proposed, their performance in practical applications is not robust because the assumption that labeled and unlabeled data are consistent does not hold. In open environments, the sources of labeled and unlabeled data may differ, leading to inconsistent data distributions and even data spaces. This paper points out that previous research on robust SSL has approached the problem from a static perspective, thereby only achieving local robustness rather than global robustness. We reshape the research framework of robust SSL by using the Robustness Analysis Curve (RAC) and the associated metrics defined based on it. Based on these metrics, we build a benchmark that encompasses three types of open environments: inconsistent data distributions, inconsistent label spaces, and inconsistent feature spaces to assess the performance of widely used statistical and deep SSL algorithms with tabular, image, and text datasets. This paper also conducted a detailed analysis, based on experimental results and theory, on how to make SSL algorithms more robust in open environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=8dN7gApKm3": {
    "title": "Uncertainty-aware Graph-based Hyperspectral Image Classification",
    "volume": "review",
    "abstract": "Hyperspectral imaging (HSI) technology captures spectral information across a broad wavelength range, providing richer pixel features compared to traditional color images with only three channels. Although pixel classification in HSI has been extensively studied, especially using graph convolution neural networks (GCNs), quantifying epistemic and aleatoric uncertainties associated with the HSI classification (HSIC) results remains an unexplored area. These two uncertainties are effective for out-of-distribution (OOD) and misclassification detection, respectively. In this paper, we adapt two advanced uncertainty quantification models, evidential GCNs (EGCN) and graph posterior networks (GPN), designed for node classifications in graphs, into the realm of HSIC. We first analyze theoretically the limitations of a popular uncertainty cross-entropy (UCE) loss function when learning EGCNs for epistemic uncertainty estimation. To mitigate the limitations, we propose two regularization terms. One leverages the inherent property of HSI data where pixel features can be decomposed into weighted sums of various material features, and the other is the total variation (TV) regularization to enforce the spatial smoothness of the evidence with edge-preserving. We demonstrate the effectiveness of the proposed regularization terms on both EGCN and GPN on three real-world HSIC datasets for OOD and misclassification detection tasks. The code is available at \\url{https://anonymous.4open.science/r/HSI_torch-1586/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=qrv4wcmmxe": {
    "title": "Zero-shot Human-Object Interaction Detection via Conditional Multi-Modal Prompts",
    "volume": "review",
    "abstract": "Human Object Interaction (HOI) detection is the task of locating and inferring the relationships between all possible human-object combinations. One of the most challenging issues is the extensive labor required for the annotation of combinatorial space of possible HOI interactions. Most existing HOI detectors rely on full annotations of all predefined interactions, resulting in a lack of generalisation for unseen combinations and actions. Inspired by the powerful generalisation ability of the large Vision-Language Models (VLM), we propose a Prompt-based zero-shot human-object Interaction Detection framework, namely PID, which can improve alignment between the vision and language representations using conditional multi-modal prompts. Specifically, different from traditional prompt-learning methods, we propose learning decoupled visual and language prompts for spatial-aware visual feature extraction and interaction classification, respectively. Furthermore, we introduce constraints for multi-modal prompts to alleviate the problem of overfitting to seen concepts in prompt learning process, thus improving the suitability for zero-shot settings. Extensive experiments demonstrate the prominence of our detector with conditional multi-modal prompts, outperforming previous state-of-the-art on unseen classes of various zero-shot settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=SaSK9M66KK": {
    "title": "Pick and Adapt: An Iterative Approach for Source-Free Domain Adaptation",
    "volume": "review",
    "abstract": "Domain adaptation plays a pivotal role in deploying models when inference data distribution is different from the training data. It becomes particularly challenging in source-free domain adaptation (SFDA) scenarios, where access to the source domain data is restricted due to data privacy concern. To tackle such cases, existing approaches often resort to generating source-like data for standard unsupervised domain adaptation or endeavor to fine-tune a model pre-trained on a source domain using self-supervised training techniques. Instead, our approach strikes a different path by theoretically analyzing into an empirical risk bound for SFDA. We identify the population risk and domain drift as the major factors from the risk bound. Subsequently, we introduce a top-k importance sampling to purify the pseudo labeling and thus reduce the population risk. We further present a nearest neighbor voting based semantic domain alignment to mitigate the domain drift. An iterative optimization is finally proposed to combine the above two steps for multiple rounds. Extensive experiments across three widely applied domain adaptation datasets, i.e., Office-Home, DomainNet, and VisDA-C, demonstrate the consistently advantageous performance over the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ba5KGabRe8": {
    "title": "XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently made impressive strides in natural language understanding tasks. Despite their remarkable performance, understanding their decision-making process remains a big challenge. In this paper, we look into bringing some transparency to this process by introducing a new explanation dataset for question answering (QA) tasks that integrates knowledge graphs (KGs) in a novel way. Our dataset includes 12,102 question-answer-explanation (QAE) triples. Each explanation in the dataset links the LLM's reasoning to entities and relations in the KGs. The explanation component includes a $\\textit{why-choose}$ explanation, a $\\textit{why-not-choose}$ explanation, and a set of $\\textit{reason-elements}$ that underlie the LLM's decision. We leverage KGs and graph attention networks (GAT) to find the $\\textit{reason-elements}$ and transform them into $\\textit{why-choose}$ and $\\textit{why-not-choose}$ explanations that are comprehensible to humans. Through quantitative and qualitative evaluations, we demonstrate the potential of our dataset to improve the in-context learning of LLMs, and enhance their interpretability and explainability. Our work contributes to the field of explainable AI by enabling a deeper understanding of the LLMs decision-making process to make them more transparent and thereby, potentially more reliable, to researchers and practitioners alike. Our dataset is available at: http://anonymous.4open.science/r/XplainLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=TlyiaPXaVN": {
    "title": "Generative Adversarial Equilibrium Solvers",
    "volume": "review",
    "abstract": "We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies. Pseudo-games are a generalization of games in which players' actions affect not only the payoffs of other players but also their feasible action spaces. Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances. We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances. We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games, and apply the framework to finding Nash equilibria in normal-form games, CE in Arrow-Debreu competitive economies, and GNE in an environmental economic model of the Kyoto mechanism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4e0ItHjNo9": {
    "title": "Rethinking Counterfactual Fairness: On Which Individuals to Enforce, and How?",
    "volume": "review",
    "abstract": "Fairness in human and algorithmic decision-making is crucial in areas such as criminal justice, education, and social welfare. Recently, counterfactual fairness has drawn increasing research interest, suggesting that decision-making for individuals should remain the same when intervening with different values on the protected attributes. Nevertheless, the question of \"which attributes and individuals should be protected\" is rarely discussed in the existing counterfactual fairness literature. For example, when considering leg disability as a protected attribute, the algorithms should not treat individuals with leg disabilities differently in college admissions, but one may naturally take into this factor for the purpose of selecting runner athletes. In other words, when and how to enforce fairness is expected to depend on the causal relation between the protected attribute and the outcome of interest. Formally, this paper proposes principal counterfactual fairness using the concept of principal stratification from the causal inference literature, focusing on whether an algorithm is counterfactually fair for individuals whose protected attribute has no individual causal effect on the outcome of interest. To examine whether an algorithm satisfies principal counterfactual fairness, we derive the statistical bounds, and propose a post-processing approach to achieving principal counterfactual fairness with minimal individual decision changes. Experiments are conducted using synthetic and real-world datasets to verify the effectiveness of our methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LEuuOaZNOT": {
    "title": "Learning Boolean functions with neural networks",
    "volume": "review",
    "abstract": "Many works have shown learnability of functions on the Boolean hypercube via gradient descent. These analyses of gradient descent use the convexity of the problem to establish guarantees despite the fact that most loss functions are highly non-convex. In addition, the analyses explicitly show that the hypothesis class can approximate the target function; this is known as a representation theorem. In this work we give gradient descent guarantees for learning functions on the Boolean hypercube on both the mean squared and hinge losses with $2$-layer neural networks with a single hidden non-linear layer. Furthermore, all of our analyses apply to the ReLU activation function. Moreover, on both losses, we don't make use of any convexity of the problem, and don't explicitly prove a representation theorem. A representation theorem is a consequence of our analysis. In the hinge loss setting to learn size $k$ parities, with dimension $n$, and $\\epsilon$ error, we obtain bounds of $n^{O(k)}poly(\\frac{1}{\\epsilon})$ and $n^{O(k)}\\log(\\frac{1}{\\epsilon})$ for network width and samples, and iterations needed, respectively. This upper bound matches the SQ lower bounds of $n^{\\Omega(k)}$. In the mean squared loss setting, given that the Fourier spectrum of an activation function has non-zero Fourier coefficients up to degree $k$, and given that the best degree $k$ polynomial approximation of the target function is $\\epsilon_0$ in mean squared loss, we give guarantees for network width and samples, and iterations needed of $n^{O(k)}poly(\\frac{1}{\\epsilon})$ and $n^{O(k)}\\log(\\frac{1}{\\epsilon})$ respectively for an error of $\\epsilon+ \\epsilon_0$. To the best of our knowledge, our bounds of $n^{O(k)}\\log(\\frac{1}{\\epsilon})$ iterations needed for learning degree $k$ polynomials on both losses are better than any previous bounds in the Boolean setting, which is a consequence of not using any convexity of the problem in our analysis. Specifically, in other works in the Boolean setting, the bound on iterations is $n^{O(k)}poly(\\frac{1}{\\epsilon})$. Moreover, as a corollary to our agnostic learning guarantee, we establish that lower degree Fourier components are learned before higher degree ones, a phenomenon observed experimentally. Finally, as a corollary to our mean squared loss guarantee, we show that neural networks with sparse hidden ReLU units as target functions can be efficiently learned with gradient descent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjygLD0AkT": {
    "title": "Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection",
    "volume": "review",
    "abstract": "While likelihood is attractive in theory, its estimates by deep generative models (DGMs) are often broken in practice, and perform poorly for OOD Detection. Various recent works started to consider alternative summary statistics and achieved better performances. However, such recipes do not come with provable guarantees, nor is it clear that their choices extract sufficient information. We attempt to change this by conducting a case study on variational autoencoders (VAEs). First, we introduce the *likelihood path (LPath) principle*, generalizing the likelihood principle. This narrows the search for informative summary statistics down to the *minimal sufficient statistics* of VAEs' conditional likelihoods. Second, introducing new theoretic tools such as *essential support*, *essential distance* and *co-Lipschitzness*, we obtain non-asymptotic provable OOD detection guarantees for certain distillation of the minimal sufficient statistics. The corresponding LPath algorithm demonstrates SOTA performances, even using simple and small VAEs with poor likelihood estimates. To our best knowledge, this is the first provable unsupervised OOD method that delivers excellent empirical results, better than any other VAEs based techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzvIWvC9MG": {
    "title": "Generative Adversarial Inverse Multiagent Learning",
    "volume": "review",
    "abstract": "In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to find parameters of a game's payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as a generative-adversarial (i.e., min-max) optimization problem, based on which we develop polynomial-time algorithms the solve them, the former of which relies on an exact first-order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent apprenticeship learning in polynomial time and number of samples, where we seek a simulacrum, i.e., parameters and an associated equilibrium, which replicate observations in expectation. We find that our approach outperforms other widely-used methods in predicting prices in Spanish electricity markets based on time-series data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rPpRyGVVnt": {
    "title": "Learning to Play Atari in a World of Tokens",
    "volume": "review",
    "abstract": "Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context, resulting in more accurate world models. However, for complex reasoning and planning tasks, these methods primarily rely on continuous representations. This complicates modeling of discrete properties of the real world such as disjoint object classes between which interpolation is not plausible. In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior. We incorporate a transformer-decoder for auto-regressive world modeling and a transformer-encoder for learning behavior by attending to task-relevant cues in the discrete representation of the world model. For handling partial observability, we aggregate information from past time steps as memory tokens. DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=992eLydH8G": {
    "title": "Do Pre-trained Transformers Really Learn In-context by Gradient Descent?",
    "volume": "review",
    "abstract": "Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore necessitate further investigation to validate their applicability in reality. We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order-sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting. Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ervzSmtQyY": {
    "title": "Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data",
    "volume": "review",
    "abstract": "Traditional deep learning (DL) suffers from two core problems. First, it assumes training samples are independent and identically distributed (iid), however, in many real-world datasets samples are grouped by measurements made on the same sample (e.g., study participant, cell, tissue) violating this assumption. On such clustered data, traditional DL suffers from reduced prediction performance, lack of generalization, poor interpretability, and cluster confounding causing Type 1 and 2 errors. Second, traditional model fitting prioritizes only overall training data accuracy, which is biased towards the most common group, with often unfair, lower accuracy on samples from underrepresented groups. When DL is used to guide critical decisions (e.g., loan approvals or determining health insurance rates) such biases can significantly impact one's quality of life. To address both of these challenges simultaneously, we introduce a fairness-enhancing mixed effects deep learning (MEDL) framework. MEDL separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) through the introduction of: 1) a cluster adversary which encourages the learning of cluster-invariant FE, 2) a Bayesian neural network which quantifies the RE, and a mixing function combining the FE an RE into a mixed-effect prediction. We marry this MEDL with adversarial debiasing, which promotes equality-of-odds fairness across FE, RE, and ME predictions for fairness-sensitive variables. An empirical evaluation spanning three datasets: two from the census/finance sector and one from the healthcare sector is performed. The former focuses on income classification, while the latter is a healthcare dataset that predicts forthcoming hospitalization duration, a regression task. The proposed framework boosts fairness across all sensitive variables—increasing fairness up to 82% for age, 43% for race, 86% for sex, and 27% for marital-status. While enhancing fairness, our method also preserves the intrinsic improved performance and interpretability advantages of MEDL. Moreover, the proposed method is agnostic to dataset type and task (regression or classification) with broad potential applicability by the community. To facilitate these benefits and further community extension, we have made our implementation available through GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BhxsjonZ0z": {
    "title": "FedOD: Federated Outlier Detection via Neural Approximation",
    "volume": "review",
    "abstract": "Outlier detection (OD) is a crucial machine learning task with key applications in various sectors such as security, finance, and healthcare. Preserving data privacy has been increasingly important for OD due to the sensitivity of the data involved. While federated learning (FL) offers the potential in protecting data privacy, it is not yet available for most classical OD algorithms, such as those based on distance and density estimation. To address this, we introduce FedOD, the first FL-based system designed for general OD algorithms. FedOD effectively overcomes the privacy and efficiency challenges inherent in classical OD algorithms by automatically decomposing these algorithms into a set of basic operators and approximating their behaviors using neural networks. Given the inherent compatibility of neural networks with FL, the approximated OD algorithms also become capable of privacy-preserving learning without data exchange. With this design, FedOD supports over 20 popular classical OD algorithms and is readily extendable to other fields like classification. Evaluation on more than 30 benchmark and synthetic datasets demonstrates FedOD's accuracy and efficacy over state-of-the-art baselines---compared to existing OD systems, FedOD achieves up to 11x reduction in errors and 10x improvement in performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.3,
    "authors": []
  },
  "https://openreview.net/forum?id=4N7v4w2r3b": {
    "title": "Robustness Evaluation of Proxy Models against Adversarial Optimization",
    "volume": "review",
    "abstract": "Ensuring the robustness of neural network proxies to optimization pressure is crucial as machine learning applications expand across diverse domains. However, research on proxy robustness remains limited and largely unexplored. In this paper, we introduce a comprehensive benchmark for investigating the robustness of neural network proxies under various sources of optimization pressure in the text domain. Through extensive experiments using our benchmark, we uncover previously unknown properties of the proxy gaming problem and highlight serious issues with proxy reward models currently used to fine-tune or monitor large language models. Furthermore, we explore different approaches to enhance proxy robustness and demonstrate the potential of adversarial training to improve alignment between proxy and gold models. Our findings suggest that proxy robustness is a solvable problem that can be incrementally improved, laying the groundwork for future research in this important area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=uXbqFnQfH4": {
    "title": "Multi-Objective Multi-Solution Transport",
    "volume": "review",
    "abstract": "In the realm of multi-objective optimization, we introduce ''Multi-objective multi-solution Transport (MosT)'', a novel solution for optimizing multiple objectives that employs multiple solutions. The essence lies in achieving diverse trade-offs among objectives, where each solution performs as a domain expert, focusing on specific objectives while collectively covering all of them. Traditional methods often struggle, especially when the number of objectives greatly outnumbers the number of solutions, leading to either subpar solutions or objectives that have been essentially ignored. MosT addresses this by formulating the problem as a bi-level optimization of weighted objectives, where the weights are defined by an optimal transport between the objectives and solutions. Our newly developed algorithm not only ensures theoretical convergence to various Pareto front solutions but is also adaptive to cases where objectives outnumber solutions. We further enhance its efficiency by introducing a solution-specialization curriculum. With proven applications in federated learning, fairness-accuracy trade-offs, and standard MOO benchmarks, MosT distinctly outperforms existing methods, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across objectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.4,
    "authors": []
  },
  "https://openreview.net/forum?id=k8Y71G7Xpz": {
    "title": "FORKS: Fast Second-Order Online Kernel Learning using Incremental Sketching",
    "volume": "review",
    "abstract": "Online Kernel Learning (OKL) has attracted considerable research interest due to its promising predictive performance. Second-order methods are particularly appealing for OKL as they often offer substantial improvements in regret guarantees. However, existing approaches like PROS-N-KONS suffer from at least quadratic time complexity with respect to the budget, rendering them unsuitable for meeting the real-time demands of large-scale online learning. Additionally, current OKL methods are typically prone to concept drifting in data streams, making them vulnerable in adversarial environments. To address these issues, we introduce FORKS, a fast incremental sketching approach for second-order online kernel learning. FORKS maintains an efficient time-varying explicit feature mapping that enables rapid updates and decomposition of sketches using incremental sketching techniques. Theoretical analysis demonstrates that FORKS achieves a logarithmic regret guarantee, on par with other second-order approaches, while maintaining a linear time complexity w.r.t. the budget. We validate the performance of FORKS through extensive experiments conducted on real-world datasets, demonstrating its superior scalability and robustness against adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=pe0Vdv7rsL": {
    "title": "Graph Transformers on EHRs: Better Representation Improves Downstream Performance",
    "volume": "review",
    "abstract": "Following the success of transformer-based methods across various machine learning applications, their adoption to healthcare predictive tasks using electronic health records (EHR) has also expanded extensively. Similarly, graph-based methods have been shown to be very effective in capturing inherent graph-type relationships in EHRs, leading to improved downstream performance. Although integrating these two families of approaches seems like a natural next step, in practice, creating such a design is challenging and has not been done. This is partly due to known EHR problems, such as high sparsity, making extracting meaningful temporal representations of medical visits challenging. In this study, we propose GT-BEHRT, a new approach that leverages temporal visit embeddings extracted from a graph transformer and uses a BERT-based model to obtain more robust patient representations, especially on longer EHR sequences. The graph-based approach allows GT-BEHRT to implicitly capture the intrinsic graphical relationships between medical observations, while the BERT model extracts the temporal relationships between visits, loosely mimicking the clinicians' decision-making process. As part of our method, we also present a two-step pre-training strategy for learning better graphical and temporal representations. Our proposed method achieves state-of-the-art performance in a variety of standard medical predictive tasks, demonstrating the versatility of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=a01qbkxbve": {
    "title": "O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data, advancing the capability of solving downstream tasks. Empirical results under two interactive decision-making benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the decision-making capabilities of LLMs through the offline discovery and distillation process, and consistently outperform baselines across various LLMs with both text-based-policy and code-based-policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y84b6FahMD": {
    "title": "Counterfactual Fairness from Partially DAGs: A General Min-Max Optimization Framework",
    "volume": "review",
    "abstract": "Developing fair automated machine learning algorithms is critical in making safe and trustworthy decisions. Many causality-based fairness notions have been proposed to address the above issues by quantifying the causal connections between sensitive attributes and decisions, and when the true causal graph is fully known, certain algorithms that achieve counterfactual fairness have been proposed. However, when the true causal graph is unknown, it is still challenging to effectively and well exploit partially directed acyclic graphs (PDAGs) to achieve counterfactual fairness. To tackle the above issue, a recent work suggests using non-descendants of sensitive attribute for fair prediction. Interestingly, in this paper, we show it is actually possible to achieve counterfactual fairness even using the descendants of the sensitive attribute for prediction, by carefully control the possible counterfactual effects of the sensitive attribute. We propose a general min-max optimization framework that can effectively achieve counterfactual fairness with promising prediction accuracy, and can be extended to maximally oriented PDAGs (MPDAGs) with added background knowledge. Specifically, we first estimate all possible counterfactual treatment effects of sensitive attribute on a given prediction model from all possible adjustment sets of sensitive attributes. Next, we propose to alternatively update the prediction model and the corresponding possible estimated causal effects, where the prediction model is trained via a min-max loss to control the worst-case fairness violations. Extensive experiments on synthetic and real-world datasets verifying the effectiveness of our methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=dwzLn78jq7": {
    "title": "On the Scalability and Memory Efficiency of Semidefinite Programs for Lipschitz Constant Estimation of Neural Networks",
    "volume": "review",
    "abstract": "Lipschitz constant estimation plays an important role for understanding generalization, robustness, and fairness in deep learning. Unlike naive bounds based on the network weight norm product, semidefinite programs (SDPs) have shown great promise in providing less conservative Lipschitz bounds with polynomial-time complexity guarantees. However, due to the memory consumption and running speed, standard SDP algorithms cannot scale to modern neural network structures. In this paper, we transform the SDPs for Lipschitz constant estimation into an eigenvalue problem, which aligns with the modern large optimization paradigms based on first-order methods. This is amenable to autodiff frameworks such as PyTorch and TensorFlow, requiring significantly less memory than standard SDP algorithms. The transformation also allows us to leverage various existing numerical techniques for eigenvalue optimization, opening the way for further memory improvement and computational speedup. The essential technique of our eigenvalue-problem transformation is to introduce redundant quadratic constraints and then utilize both Lagrangian and Shor's SDP relaxations. Numerical examples demonstrate that our technique is more scalable than existing approaches. For networks that existing SDP solvers cannot handle, we improve the Lipschitz constant estimation by up to 58\\% compared to the weight matrix norm product bound",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=dexKVPmPOg": {
    "title": "Efficient Recomputation of Marginal Likelihood upon Adding Training Data in Gaussian Processes and Simulator Fusion",
    "volume": "review",
    "abstract": "To reduce generalization loss in line with the bias-variance trade-off, machine learning engineers should construct models based on their knowledge of the modeling target and, as training data increases, choose more flexible models with reduced dependence on that knowledge if that knowledge is unreliable. To achieve this automatically, methods have been proposed to determine the amount of model's assumed prior knowledge directly from training data, rather than relying solely on an engineer's intuition. A widely studied approach involves using both a flexible model and a knowledge-dependent simulator, selectively incorporating simulator-generated data into the flexible model's training data. While neural networks have been used as flexible models, Gaussian processes are also candidates due to their flexibility and ability to output prediction uncertainty. However, direct methods for adding simulator-generated data to Gaussian process training data remain unstudied. The Subset of Data (SoD) method, the closest alternative, often adds inappropriate data due to its assumption about the true distribution. The log marginal likelihood, grounded in theory, determines the inclusion of generated data. However, its computation in Gaussian processes is costly. We propose a faster method considering the Cholesky factor and matrix element dependencies. Experiments indicate that, in terms of MSE, metrics using exact negative log likelihood outperform Subset of Data and other basic methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=2C3CWCPxNS": {
    "title": "Preconditioning for Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "Physics-informed neural networks (PINNs) have shown promise in solving complex partial differential equations (PDEs). However, certain training pathologies have emerged, compromising both convergence and prediction accuracy in practical applications. In this paper, we propose to use condition number as an innovative metric to diagnose and rectify the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We delineate a theory that elucidates the relationship between reduced condition numbers and improved error control, as well as better convergence. Subsequently, we present an algorithm that leverages preconditioning to enhance the condition number. Evaluations on 16 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. Furthermore, in 2 distinct cases, our approach pioneers a solution, slashing relative errors from roughly $100\\\\%$ to below $6\\\\%$ and $21\\\\%$, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNLAkjUm19": {
    "title": "On the Role of Discrete Tokenization in Visual Representation Learning",
    "volume": "review",
    "abstract": "In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using unmasked portions. A notable subset of MIM methodologies employs discrete visual tokens as reconstruction target. This study explores the role of discrete visual tokens in MIM, with the aim of decoding their potential benefits and inherent constraints. Building upon the connection between MIM and contrastive learning, we provide comprehensive explanations on how discrete tokenization affects generalization performance of MIM. Furthermore, we introduce a novel metric designed to quantify the proficiency of discrete visual tokens in the MIM framework. Inspired by this metric, we contribute an accessible tokenizer design and demonstrate its superior performance across various benchmark datasets and ViT backbones",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZEXgtMNNo": {
    "title": "Large Language Models as Automated Aligners for benchmarking Vision-Language Models",
    "volume": "review",
    "abstract": "With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc. The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ofzeypWosV": {
    "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
    "volume": "review",
    "abstract": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to 1) achieve superior compression in the token length, and 2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art zero-shot TTS baselines regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=E4hK8t7Fts": {
    "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
    "volume": "review",
    "abstract": "Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM~2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VgBjsOC8k": {
    "title": "Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels",
    "volume": "review",
    "abstract": "Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of Gaussian (DoG) functions, and their first and second-order derivatives. Notably, we classify over 95\\% and 90\\% of the filters from state-of-the-art ConvNeXtV2 and ConvNeXt models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long proposed for the vision systems of mammals. Our results thus deepen our understanding of the emergent properties of trained DS-CNNs and provide a bridge between artificial and biological visual processing systems. More broadly, they pave the way for more interpretable and biologically-inspired neural network designs in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0j9ZDzMPqr": {
    "title": "UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models",
    "volume": "review",
    "abstract": "Node representation learning, such as Graph Neural Networks (GNNs), has become one of the important learning methods in machine learning, and the demand for reliable explanation generation is growing. Despite extensive research on explanation generation for supervised node representation learning, explaining unsupervised models has been less explored. To address this gap, we propose a method for generating counterfactual (CF) explanations in unsupervised node representation learning, aiming to identify the most important subgraphs that cause a significant change in the $k$-nearest neighbors of a node of interest in the learned embedding space upon perturbation. The $k$-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-$k$ link prediction and clustering. Furthermore, we introduce a Monte Carlo Tree Search (MCTS)-based explainability method for generating expressive CF explanations for **U**nsupervised **N**ode **R**epresentation learning methods, which we call **UNR-Explainer**. The proposed method demonstrates improved performance on six datasets for both unsupervised GraphSAGE and DGI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=BmhWGsOyDu": {
    "title": "Reinforcement Learning for Large Group Systems using Hierarchical Kernel Representations",
    "volume": "review",
    "abstract": "Policy learning for targeted coordination of massive-scale populations of, in the limit a continuum spectrum of, intelligent agents has been a missing component in reinforcement learning research. The purpose of this work is to fill in this literature gap by addressing the major challenge: the curse of dimensionality caused by the huge population size. To this end, we formulate such an intelligent agent population as a parameterized deterministic dynamical system, referred to as a group system, and then introduce the novel moment representation to the system. Under this representation, we propose a nested reinforcement learning algorithm to learn the optimal policy for the system hierarchically. As a significant advantage, each hierarchy preserves the optimality of all its lower-level children, which then leads to the fast convergence of the nested algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ZBEs9CJiWs": {
    "title": "Optimizing Interpersonal Communication by Simulating Audiences with Large Language Models",
    "volume": "review",
    "abstract": "How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage current Large Language Models (LLMs) to help us communicate better. Specifically, we propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve, 1) explores the solution space by first producing a diverse set of advice relevant to the scenario, 2) generates potential candidates conditioned on subsets of the advice, and 3) simulates the reactions from various audiences, selecting both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal communication. For each scenario, we collect a dataset of human evaluations across candidates and baselines and showcase that our framework's chosen candidate is preferred over popular baseline generation mechanisms including Chain-of-Thought. We also find that audience simulations achieve reasonably high agreement with human raters across $5$ of the $8$ scenarios. Furthermore, we demonstrate the generality of our framework by applying it to real-world scenarios described by users on web forums. Viewing LLMs as a library of shared experiences and opinions, our approach draws on this library to integrate cultural and individual experience and ultimately help people communicate better",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=aCgybhcZFi": {
    "title": "Enhancing Neural Network Transparency through Representation Analysis",
    "volume": "review",
    "abstract": "In this paper, we introduce and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase these methods can provide traction on a wide range of safety-relevant problems, including truthfulness, memorization, power-seeking, and more, demonstrating the promise of representation-centered transparency research. We hope this work catalyzes further exploration into RepE and fosters advancements in the transparency and safety of AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=x8VNtpCu1I": {
    "title": "Are Bert Family Good Instruction Followers? A Study on Their Potential And Limitations",
    "volume": "review",
    "abstract": "Language modeling at scale has proven very effective and brought unprecedented success to natural language models. Many typical representatives, especially decoder-only models, e.g., BLOOM and LLaMA, and encoder-decoder models, e.g., Flan-T5 and AlexaTM, have exhibited incredible instruction-following capabilities while keeping strong task completion ability. These large language models can achieve superior performance in various tasks and even yield emergent capabilities, e.g., reasoning and universal generalization. Though the above two paradigms are mainstream and well explored, the potential of the BERT family, which are encoder-only based models and have ever been one of the most representative pre-trained models, also deserves attention, at least should be discussed. In this work, we adopt XML-R to explore the effectiveness of the BERT family for instruction following and zero-shot learning. We first design a simple yet effective strategy to utilize the encoder-only models for generation tasks and then conduct multi-task instruction tuning. Experimental results demonstrate that our fine-tuned model, Instruct-XMLR, outperforms Bloomz on all evaluation tasks and achieves comparable performance with mT0 on most tasks. Surprisingly, Instruct-XMLR also possesses strong task and language generalization abilities, indicating that Instruct-XMLR can also serve as a good instruction follower and zero-shot learner. Besides, Instruct-XMLR can accelerate decoding due to its non-autoregressive generation manner, achieving around 3 times speedup compared with current autoregressive large language models. Although we also witnessed several limitations through our experiments, such as the performance decline in long-generation tasks and the shortcoming of length prediction, Instruct-XMLR can still become a good member of the family of current large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=whxKU5YcH6": {
    "title": "SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "Graph-level representation learning is important in a wide range of applications. However, existing graph-level models are generally built on i.i.d. assumption for both training and testing graphs, which is not realistic in an open world, where models can encounter out-of-distribution (OOD) testing graphs that are from different distributions unknown during training. A trustworthy model should not only produce accurate predictions for in-distribution (ID) data, but also detect OOD graphs to avoid unreliable prediction. In this paper, we present SGOOD, a novel graph-level OOD detection framework. We find that substructure differences commonly exist between ID and OOD graphs. Hence, SGOOD explicitly utilizes substructures to learn powerful representations to achieve superior performance. Specifically, we build a super graph of substructures for every graph, and design a two-level graph encoding pipeline that works on both original graphs and super graphs to obtain substructure-enhanced graph representations. To further distinguish ID and OOD graphs, we develop three graph augmentation techniques that preserve substructures and increase expressiveness. Extensive experiments against 10 competitors on numerous graph datasets demonstrate the superiority of SGOOD, often surpassing existing methods by a significant margin. The code is available at https://anonymous.4open.science/r/SGOOD-0958",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=hdCDVSPQ7v": {
    "title": "Jorge: Approximate Preconditioning for GPU-Efficient Second-Order Optimization",
    "volume": "review",
    "abstract": "Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate the distinct advantages of using Jorge, outperforming state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple deep learning models, both in terms of sample efficiency and wall-clock time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=V2cBKtdC3a": {
    "title": "Exploring the Promise and Limits of Real-Time Recurrent Learning",
    "volume": "review",
    "abstract": "Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10B frames. To scale to such challenging tasks, we focus on certain well-known neural architectures with element-wise recurrence, allowing for tractable RTRL without approximation. Importantly, we also discuss rarely addressed limitations of RTRL in real-world applications, such as its complexity in the multi-layer case",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=1p4q1cXOX9": {
    "title": "Attribute-Enhanced Similarity Ranking for Sparse Link Prediction",
    "volume": "review",
    "abstract": "Link prediction is a fundamental problem in graph data. In its most realistic setting, the problem consists of predicting missing or future links between random pairs of nodes from the set of disconnected pairs. Graph Neural Networks (GNNs) have become the predominant framework for link prediction. GNN-based methods treat link prediction as a binary classification problem and handle the extreme class imbalance---real graphs are very sparse---by sampling (uniformly at random) a balanced number of disconnected pairs not only for training but also for evaluation. However, we show that the reported performance of GNNs for link prediction in the balanced setting does not translate to the more realistic imbalanced setting and that simpler topology-based approaches are often better at handling sparsity. These findings motivate Gelato, a similarity-based link-prediction method that applies (1) graph learning based on node attributes to enhance a topological heuristic, (2) a ranking loss for addressing class imbalance, and (3) a negative sampling scheme that efficiently selects hard training pairs via graph partitioning. Experiments show that Gelato is more accurate and faster than GNN-based alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=oIwoBDsJJI": {
    "title": "Measuring Graph Similarity Using Transfer Cost of Forster Distributions",
    "volume": "review",
    "abstract": "In recent years, optimal transport-based distance metrics have shown to be effective similarity and dissimilarity measures for tackling learning problems involving network data. Prominent examples range from graph classification and community detection to object matching. However, the high computational complexity of calculating optimal transport costs substantially confines their applications to large-scale networks. To address this challenge, in this paper, we introduce a probability distribution on the set of edges of a graph, referred to as the Foster distribution of the graph, by extending Foster's theorem from electrical to general networks. Then, we represent Foster distributions as probability measures on the real line and estimate the Wasserstein metric between the corresponding probability measures to quantify graph similarity. The applicability of the proposed approach is corroborated on diverse graph-structured datasets, through which we particularly demonstrate the high efficiency of computing the proposed graph distance for sparse graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=YH5w12OUuU": {
    "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting",
    "volume": "review",
    "abstract": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. On the other hand, for natural language processing, Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal, and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO, with 20%-60% improvement over state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets. This compelling finding highlights TEMPO's potential to constitute a foundational model building framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=u3dX2CEIZb": {
    "title": "Scaling physics-informed hard constraints with mixture-of-experts",
    "volume": "review",
    "abstract": "Imposing known physical constraints, such as conservation laws, during the training of neural networks introduces an inductive bias that can improve accuracy, convergence, and data efficiency for physical problems. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve accuracy by integrating constrained optimization within neural network training, enabling a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems, as it requires solving the differentiable optimization problem over a large number of points in the spatiotemporal domain. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE). Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an ``expert'' through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy on challenging non-linear systems, concurrently improving training stability and requiring significantly less computation time during both training and inference stages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4stB7DFLp6": {
    "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
    "volume": "review",
    "abstract": "Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval before instruction tuning. Specifically, we continue to pretrain the 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. The obtained foundation model, Retro 48B, largely outperforms the original 43B GPT in terms of perplexity. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on zero-shot question answering (QA) tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA tasks, and 10% over GPT across 4 challenging long-form QA tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. We hypothesize that pretraining with retrieval makes its decoder good at incorporating context for QA. Our results highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xiGwCVzsCi": {
    "title": "Discrimination-free Pricing with Privatized Sensitive Attributes",
    "volume": "review",
    "abstract": "Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continue to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic biasness has introduced various fairness concepts, including demographic parity and equalized odds, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement in this field, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing. In particular, the regulatory bodies are increasingly emphasizing transparency in pricing algorithms and imposing constraints for insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose a straightforward method for constructing fair models that align with the specific fairness criteria unique to the insurance pricing domain. Notably, our approach only relies on privatized sensitive attributes and offers statistical guarantees. Further, it does not require insurers to have direct access to sensitive attributes, and it can be tailored to accommodate varying levels of transparency as required. This methodology seeks to meet the growing demands for privacy and transparency set forth by regulators while ensuring fairness in insurance pricing practices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=BoMvv7ypDF": {
    "title": "Recursive Score Estimation Accelerates Diffusion-Based Monte Carlo",
    "volume": "review",
    "abstract": "To sample from a general target distribution $p_*\\propto e^{-f_*}$ beyond the isoperimetric condition, \\citet{huang2023monte} proposed to perform sampling through reverse diffusion, giving rise to *Diffusion-based Monte Carlo* (DMC). Specifically, DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an *exponential dependency* on the error tolerance $\\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of the original DMC algorithm originates from its redundant design of score estimation, and proposed a more efficient DMC algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation and sampling subproblems accordingly, which are correlated in a recursive manner. Importantly, we show that with a proper design of the segment decomposition, all sampling subproblems will only need to tackle a strongly log-concave distribution, which can be very efficient to solve using the standard sampler (e.g., Langevin Monte Carlo) with a provably rapid convergence rate. As a result, we prove that the gradient complexity of RS-DMC only has a *quasi-polynomial dependency* on $\\epsilon$, which significantly improves exponential gradient complexity in \\citet{huang2023monte}. Furthermore, under commonly used dissipative conditions, our algorithm is provably much faster than the popular Langevin-based algorithms. Our algorithm design and theoretical framework illuminate a novel direction for addressing sampling problems, which could be of broader applicability in the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=W98SiAk2ni": {
    "title": "Ensemble Systems Representation for Function Learning over Manifolds",
    "volume": "review",
    "abstract": "Function learning concerns with the search for functional relationships among datasets. It coincides with the formulations of various learning problems, particularly supervised learning problems, and serves as the prototype for many learning models, e.g., neural networks and kernel machines. In this paper, we propose a novel framework to tackle function learning tasks from the perspective of ensemble systems theory. Our central idea is to generate function learning algorithms by using flows of continuous-time ensemble systems defined on infinite-dimensional Riemannian manifolds. This immediately gives rise to the notion of natural gradient flow that enables the generated algorithms to tackle function learning tasks over manifolds. Moreover, we rigorously investigate the relationship between the convergence of the generated algorithms and the dynamics of the ensemble systems with and without an external forcing or control input. We show that by turning the penalty strengths into control inputs, the algorithms are able to converge to any function over the manifold, regardless of the initial guesses, providing {\\em ensemble controllability} of the systems. In addition to the theoretical investigation, concrete examples are also provided to demonstrate the high efficiency and excellent generalizability of these \"continuous-time\" algorithms compared with classical \"discrete-time\" algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ANvmVS2Yr0": {
    "title": "Generalization in diffusion models arises from geometry-adaptive harmonic representation",
    "volume": "review",
    "abstract": "High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the ``true'' density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates the existence of powerful inductive biases in the DNN architecture and/or training algorithm. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image regions. We show that trained denoisers are inductively biased towards these geometry-adaptive harmonic representations by demonstrating that they arise even when the network is trained on image classes such as low-dimensional manifolds for which the harmonic basis is suboptimal. Additionally, we show that the denoising performance of the networks is near-optimal when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.5,
    "authors": []
  },
  "https://openreview.net/forum?id=CQF8mTF7qx": {
    "title": "Simplicity Bias of SGD via Sharpness Minimization",
    "volume": "review",
    "abstract": "The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features (Huh et al., 2021). Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) towards flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low rank) is not well understood. In this work, we take a step towards bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property --- a local geodesic convexity --- of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=bvjcMvMn7B": {
    "title": "Structural Fairness-aware Active Learning for Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) have seen significant achievements in semi-supervised node classification. Yet, their efficacy often hinges on access to high-quality labeled node samples, which may not always be available in real-world scenarios. While active learning is commonly employed across various domains to pinpoint and label high-quality samples based on data features, graph data present unique challenges due to their intrinsic structures that render nodes non-i.i.d. Furthermore, biases emerge from the positioning of labeled nodes; for instance, nodes closer to the labeled counterparts often yield better performance. To better leverage graph structure and mitigate structural bias in active learning, we present a unified optimization framework (SCARCE), which is also easily incorporated with node features. Extensive experiments demonstrate that the proposed method not only improves the GNNs performance but also paves the way for more fair results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FWJAmwE0xH": {
    "title": "Neural-Symbolic Recursive Machine for Systematic Generalization",
    "volume": "review",
    "abstract": "Current learning models often struggle with human-like systematic generalization; learning compositional rules from limited data and extrapolating them to unseen combinations. To address this, we introduce Neural-Symbolic Recursive Machine (NSR), a model whose core representation is a Grounded Symbol System (GSS ), with its combinatorial syntax and semantics emerging entirely from the training data. The NSR adopts a modular approach, incorporating neural perception, syntactic parsing, and semantic reasoning, which are jointly learned through a deduction-abduction algorithm. We establish that NSR possesses sufficient expressiveness to handle a variety of sequence-to-sequence tasks and attains superior systematic generalization, thanks to the inductive biases of equivariance and recursiveness inherent in each module. We assess NSR 's performance against four rigorous benchmarks designed to test systematic generalization: SCAN for semantic parsing, PCFG for string manipulation, HINT for arithmetic reasoning, and a task involving compositional machine translation. Our results indicate that NSR outperforms existing neural or hybrid models in terms of generalization and transferability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ITq4ZRUT4a": {
    "title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation",
    "volume": "review",
    "abstract": "Evaluating text-to-image models is notoriously difﬁcult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not assert that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG is an automatic, graph-based QG/A that is modularly implemented to be adaptable to any QG/A module. DSG produces atomic and unique questions organized in dependency graphs, which (i) ensure appropriate semantic coverage and (ii) sidestep inconsistent answers. With extensive experimentation and human evaluation on a range of model conﬁgurations (LLM, VQA, and T2I), we empirically demonstrate that DSG addresses the challenges noted above. Finally, we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide range of ﬁne-grained semantic categories with a balanced distribution. We will release the DSG-1k prompts and the corresponding DSG questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wu9nGGYvAX": {
    "title": "Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations",
    "volume": "review",
    "abstract": "Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn abstract, generalizable same-different visual relations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=n9xeGcI4Yg": {
    "title": "The Consensus Game: Language Model Generation via Equilibrium Search",
    "volume": "review",
    "abstract": "When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate answers). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game—which we term the concensus game—in which a generator seeks to communicate an abstract correctness parameter using natural language sentences to a discriminator. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call equilibrium-ranking. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and assistive dialog), equilibrium-ranking consistently improves performance over existing LM decoding procedures. These improvements are sometimes substantial—on multiple benchmarks, we observe that applying equilibrium-ranking to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=uiFuqvkpAt": {
    "title": "Vector Quantized Representations for Efficient Hierarchical Delineation of Behavioral Repertoires",
    "volume": "review",
    "abstract": "Understanding animal behaviors and their neural underpinnings requires precise kinematic measurements plus analytical methods to parse these continuous, multidimensional measurements into interpretable, organizational descriptions. Existing approaches can identify stereotyped behavioral motifs, given 2D or 3D keypoint-based data but are limited in their interpretability, computational efficiency, and/or ability to seamlessly integrate new behavioral measurements. In this paper, we propose an end-to-end behavioral analysis approach that dissects continuous body movements into sequences of discrete latent variables using vector quantization (VQ). The discrete latent space naturally defines an interpretable deep behavioral repertoire composed of hierarchically organized behavioral motifs. Using recordings of freely moving rodents, we demonstrate that the proposed framework faithfully supports standard behavioral analysis tasks and enables a series of new applications stemming from the discrete information bottleneck, including realistic synthesis of animal body movements and cross-species behavioral mapping",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Oy1NtlFDmD": {
    "title": "STRUCTDROP: A STRUCTURED RANDOM ALGORITHM TOWARDS EFFICIENT LARGE-SCALE GRAPH TRAINING",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have gained considerable success in graph-based learning tasks, yet training GNNs on large graphs is still inefficient. The root cause is the graph-based sparse operations are difficult to accelerate with commodity hardware. Prior art reduces the computation cost of sparse matrix based operations (e.g., linear) via sampling-based approximation. However, two under-explored pain points still persist in this paradigm. Inefficiency Issue: The random-based sampling approaches have the non-zero entries randomly distributing over adjacency matrix, which slows down memory access process and is difficult to accelerate with commodity hardware. Under-fitting Problem: The previous sampling methods only utilize the same subset of nodes during the training, which may cause the under-fitting problem on other remain nodes. Aiming to systematically address these two pain points, we propose StructuredDropout, a.k.a, StructDrop. This method involves the selective random sampling of columns and rows from a sparse matrix for computation. Comprehensive experiments validate the efficiency and generalization of our framework: StructDrop achieves up to 5.09x speedup for a single sparse operation and 6.48x end-to-end speedup with negligible accuracy loss or even better accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPOVOwsDOO": {
    "title": "Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication",
    "volume": "review",
    "abstract": "Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD is a technique that transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend the knowledge distillation paradigm by introducing an interactive communication process to help student models of downstream tasks learn effectively from pre-trained foundation models. Our design is inspired by the way humans learn from teachers who can explain knowledge in a way that meets the students' needs. Specifically, we let each model (i.e., student and teacher) train two components: (1) an encoder which encodes the model's hidden states to a message in a shared message space with other models and (2) a decoder which decodes any message to its own hidden states. With encoder and decoder, not only can the teacher model transfer rich information by encoding its hidden states to messages, but also the student model can send messages with information of downstream tasks to teacher so that the teacher can interpret and generate responses. With this interactive communication process, knowledge passing from teacher to student can be tailored to the student's model capacity and downstream tasks' distributions. We conducted experiments on benchmark datasets for computer vision and recommendation tasks to show that our communication mechanism outperforms state-of-the-art distillation techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=3EWTEy9MTM": {
    "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
    "volume": "review",
    "abstract": "Generating a sequence of intermediate steps, \\emph{a.k.a.}, a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=pmweVpJ229": {
    "title": "Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy",
    "volume": "review",
    "abstract": "Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides $\\varepsilon$-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by $(\\varepsilon,\\delta)$-approximate DP. In practice, however, one needs to apply approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus re-introducing the unappealing $\\delta$-approximation error into the privacy guarantees. To bridge this gap, we propose the Approximate SAample Perturbation (abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to its Wasserstein-infinity ($W_\\infty$) distance from a reference distribution that satisfies pure DP or pure Gaussian DP (i.e., $\\delta=0$). We then leverage a Metropolis-Hastings algorithm to generate the sample and prove that the algorithm converges in W$_\\infty$ distance. We show that by combining our new techniques with a careful localization step, we obtain the first nearly linear-time algorithm that achieves the optimal rates in the DP-ERM problem with strongly convex and smooth losses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=LYGHdwyXUb": {
    "title": "Efficient Multi-task Reinforcement Learning via Selective Behavior Sharing",
    "volume": "review",
    "abstract": "Multi-task Reinforcement Learning (MTRL) offers several avenues to address the issue of sample efficiency through information sharing between tasks. However, prior MTRL methods primarily exploit data and parameter sharing, overlooking the potential of sharing learned behaviors across tasks. The few existing behavior-sharing approaches falter because they directly imitate the policies from other tasks, leading to suboptimality when different tasks require different actions for the same states. To preserve optimality, we introduce a novel, generally applicable behavior-sharing formulation that selectively leverages other task policies as the current task's behavioral policy for data collection to efficiently learn multiple tasks simultaneously. Our proposed MTRL framework estimates the shareability between task policies and incorporates them as temporally extended behaviors to collect training data. Empirically, selective behavior sharing improves sample efficiency on a wide range of manipulation, locomotion, and navigation MTRL task families and is complementary to parameter sharing. Result videos are available at [https://sites.google.com/view/qmp-mtrl](https://sites.google.com/view/qmp-mtrl)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsJwmWvE6Q": {
    "title": "Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms",
    "volume": "review",
    "abstract": "We study the problem of residual error estimation for matrix and vector norms using a linear sketch. Such estimates can be used, for example, to quickly assess how useful a more expensive low-rank approximation computation will be. The matrix case concerns the Frobenius norm and the task is to approximate the $k$-residual $\\|A - A_k\\|_F$ of the input matrix $A$ within a $(1+\\epsilon)$-factor, where $A_k$ is the optimal rank-$k$ approximation. We provide a tight bound of $\\Theta(k^2/\\epsilon^4)$ on the size of bilinear sketches, which have the form of a matrix product $SAT$. This improves the previous $O(k^2/\\epsilon^6)$ upper bound in (Andoni et al. SODA 2013) and gives the first non-trivial lower bound, to the best of our knowledge. In our algorithm, our sketching matrices $S$ and $T$ can both be sparse matrices, allowing for a very fast update time. We demonstrate that this gives a substantial advantage empirically, for roughly the same sketch size and accuracy as in previous work. For the vector case, we consider the $\\ell_p$-norm for $p>2$, where the task is to approximate the $k$-residual $\\|x - x_k\\|_p$ up to a constant factor, where $x_k$ is the optimal $k$-sparse approximation to $x$. Such vector norms are frequently studied in the data stream literature and are useful for finding frequent items or so-called heavy hitters. We establish an upper bound of $O(k^{2/p}n^{1-2/p}\\operatorname{poly}(\\log n))$ for constant $\\epsilon$ on the dimension of a linear sketch for this problem. Our algorithm can be extended to the $\\ell_p$ sparse recovery problem with the same sketching dimension, which seems to be the first such bound for $p > 2$. We also show an $\\Omega(k^{2/p}n^{1-2/p})$ lower bound for the sparse recovery problem, which is tight up to a $\\mathrm{poly}(\\log n)$ factor",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kIPEyMSdFV": {
    "title": "Reverse Diffusion Monte Carlo",
    "volume": "review",
    "abstract": "The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the application of reverse diffusion to Monte Carlo sampling. It is shown that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the posterior distribution, we derive a novel Monte Carlo sampling algorithm from the reverse diffusion process, which is distinct from traditional Markov Chain Monte Carlo (MCMC) methods. We calculate the error requirements and sample size for the posterior distribution, and use the result to derive an algorithm that can approximate the target distribution to any desired accuracy. Additionally, by estimating the log-Sobolev constant of the posterior distribution, we show under suitable conditions the problem of sampling from the posterior can be easier than direct sampling from the target distribution using traditional MCMC techniques. For Gaussian mixture models, we demonstrate that the new algorithm achieves significant improvement over the traditional Langevin-style MCMC sampling methods both theoretically and practically. Our algorithm offers a new perspective and solution beyond classical MCMC algorithms for challenging complex distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICwdNpmu2d": {
    "title": "LLM-based Stock Market Trend Prediction",
    "volume": "review",
    "abstract": "LLM-based Stock Market Trend Prediction Investor sentiment, which is driven by 'intriguing factors' such as news articles and options volume, has been historically resistant to effective use in quantitative methods for predictive market analysis. The emerging science of large language models (LLMs), however, offers a potential solution to this problem. In this paper, we describe our initial experiments with a novel system which prompts available LLMs in a way which allows us to link responses with features in the otherwise more traditional quantitative methods. The results show high accuracy in predicting market moves. We describe the experiments and our initial thoughts about next steps in the paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.5,
    "authors": []
  },
  "https://openreview.net/forum?id=7eYmijcuqO": {
    "title": "On the Dynamics of Learning Time-Aware Behavior with RNNs",
    "volume": "review",
    "abstract": "Recurrent Neural Networks (RNNs) have shown great success in modeling time-dependent patterns, but there is limited research on how they develop representations of temporal features during training. To address this gap, we use timed automata (TA) to introduce a family of supervised learning tasks modeling behavior dependent on hidden temporal variables whose complexity is directly controllable. Building upon past studies from the perspective of dynamical systems theory, we train RNNs to emulate a new class of TA called temporal flipflops, and we find they undergo *phase transitions during training* characterized by sudden and rapid discovery of the hidden time-dependent features. In the case of periodic \"time-of-day\" aware flipflop, we show that the RNNs learn stable periodic cycles that encode time modulo the period of the transition rules. We then use fixed point stability analysis to monitor changes in the RNN dynamics during training, and we observe that the phase transition coincides with a *bifurcation* from which stable periodic behavior emerges. We also show that these cycles initially lose stability if the RNN is later trained on the same TA task but with a different period, and we explain this result through analysis of a simple differential equation for learning oscillations via gradient flow. Through this work, we demonstrate how dynamical systems theory can provide insights into not only learned representations, but also the dynamics and pathologies of the learning process itself",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMh4W72Hhe": {
    "title": "Improving Branching in Neural Network Verification with Bound Implication Graph",
    "volume": "review",
    "abstract": "Many state-of-the-art neural network verifiers for ReLU networks rely on Branch and Bound (BaB)-based methods. They branch ReLUs into positive (active) and negative (inactive) parts, and bound each subproblem independently. Since the cost of verification heavily depends on the number of subproblems, reducing the total number of branches is the key to verifying neural networks efficiently. In this paper, we consider \\emph{bound implications} during branching - i.e., when one or more ReLU neurons are branched into the active (or inactive) case, they may imply that a set of other neurons from any layers become active or inactive, or have their bounds tightened. These implications can eliminate subproblems and improve bounds. We propose a scalable method to find implications among all neurons within tens of seconds even for large ResNets, by reusing pre-computed variables in popular bound-propagation-based verification methods such as $\\alpha$-CROWN, and solving a cheap linear programming problem. Then, we build the bound implication graph (BIG) which connects neurons with bound implications, and it can be used by any BaB-based verifier to reduce the number of branching needed. When evaluated on a set of popular verification benchmarks and a new benchmark consisting of harder verification problems, BIG consistently reduces the verification time and verifies more problems than state-of-the-art verification tools",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=qaJxPhkYtD": {
    "title": "Counting Graph Substructures with Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph Neural Networks (GNNs) are powerful representation learning tools that have achieved remarkable performance in various important tasks. However, their ability to count substructures, which play a crucial role in biological and social networks, remains uncertain. In this work, we fill this gap and characterize the representation power of GNNs in terms of their ability to produce powerful representations that count graph substructures. In particular, we study the message-passing operations of GNNs with random stationary input and show that they can produce permutation equivariant representations that are associated with high-order statistical moments. Using these representations, we prove that GNNs can learn how to count cycles, quasi-cliques, and the number of connected components in a graph. We also provide new insights into the generalization capacity of GNNs. Our analysis is constructive and enables the design of a generic GNN architecture that shows remarkable performance in four distinct tasks: cycle detection, cycle counting, graph classification, and molecular property prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zBgAlcIoZP": {
    "title": "Dynamic Mode Decomposition-inspired Autoencoders for Reduced-order Modeling and Control of PDEs : Theory and Design",
    "volume": "review",
    "abstract": "Modeling and controlling complex spatiotemporal dynamical systems driven by partial differential equations (PDEs) often necessitate dimensionality reduction techniques to construct lower-order models for computational efficiency. This paper studies a deep autoencoding learning method for controlling dynamical systems governed by spatiotemporal PDEs. We first analytically show that an optimization objective for learning a linear autoencoding reduced-order model can be formulated, yielding a solution that closely resembles the result obtained through the $\\textit{dynamic mode decomposition with control}$ algorithm. Subsequently, we extend this linear autoencoding architecture to a deep autoencoding framework, enabling the development of a nonlinear reduced-order model. Furthermore, we leverage the learned reduced-order model to design controllers using stability-constrained deep neural networks. Our framework operates without prior knowledge of the governing equations of the underlying system, relying solely on time series data of observations and actuations. Empirical analyses are presented to validate the efficacy of our approach in both modeling and controlling spatiotemporal dynamical systems, exemplified through applications to reaction-diffusion systems and fluid flow systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=GwKNdRc9Bj": {
    "title": "Exploiting Action Distances for Reward Learning from Human Preferences",
    "volume": "review",
    "abstract": "Preference-based Reinforcement Learning (PbRL) with binary preference feedbacks over trajectory pairs has proved to be quite effective in learning complex preferences of a human in the loop in domains with high dimensional state spaces and action spaces. While the human preference is primarily inferred from the feedback provided, we propose that the policy being learned (jointly with the reward model) during training can provide valuable learning signal about the structure of the state space that can be leveraged by the reward learning process. We introduce an action distance measure based on the policy and use it as an auxiliary prediction task for reward learning to influence its embedding space. This measure not only provides insight into the transition dynamics of the environment but also informs about the reachability of states and the overall state space structure. We evaluate the performance and sample efficiency of our approach using a combination of six tasks in Meta-World domains with simulated oracles. We also conduct human in the loop evaluation on three tasks to confirm our findings from oracular experiments. We demonstrate that the proposed simple auxiliary task for constraining reward model's embedding space can provide strong empirical improvements to sample efficiency and accelerate policy learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Y2Txh5uGRe": {
    "title": "Text2Data: Low-Resource Data Generation with Textual Control",
    "volume": "review",
    "abstract": "Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, like molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding both generation quality and controllability across various modalities, including molecules, motions and time series, when compared to existing baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=M9nKQX5nYF": {
    "title": "On the Effect of Defection in Federated Learning and How to Prevent It",
    "volume": "review",
    "abstract": "Federated learning is a machine learning protocol that enables a large population of agents to collaborate. These agents communicate over multiple rounds to produce a single, consensus model. Despite this collaborative framework, there are instances where agents may choose to defect permanently—essentially withdrawing from the collaboration—if they are content with their instantaneous model in that round. This work demonstrates the detrimental impact such defections can have on the final model's robustness and ability to generalize. We also show that current federated optimization algorithms fall short in disincentivizing these harmful defections. To address this, we introduce a novel optimization algorithm with theoretical guarantees to prevent defections while ensuring asymptotic convergence to an effective solution for all participating agents. We also provide numerical experiments to corroborate our findings and demonstrate the effectiveness of our algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=psDvcWtFdE": {
    "title": "DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee",
    "volume": "review",
    "abstract": "Mixed-integer linear programming (MILP) stands as a notable NP-hard problem pivotal to numerous crucial industrial applications. The development of effective algorithms, the tuning of solvers, and the training of machine learning models for MILP resolution all hinge on access to extensive, diverse, and representative data. Yet compared to the abundant naturally occurring data in image and text realms, MILP is markedly data deficient, underscoring the vital role of synthetic MILP generation. We present DIG-MILP, a deep generative framework based on variational auto-encoder (VAE), adept at extracting deep-level structural features from highly limited MILP data and producing instances that closely mirror the target data. Notably, by leveraging the MILP duality, DIG-MILP guarantees a correct and complete generation space as well as ensures the boundedness and feasibility of the generated instances. Our empirical study highlights the novelty and quality of the instances generated by DIG-MILP through two distinct downstream tasks: (S1) Data sharing, where solver solution times correlate highly positive between original and DIG-MILP-generated instances, allowing data sharing for solver tuning without publishing the original data; (S2) Data Augmentation, wherein the DIG-MILP-generated instances bolster the generalization performance of machine learning models tasked with resolving MILP problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=nSDOkm0SKo": {
    "title": "Analyzing Complex Interdependencies in Financial Markets: A Neural Network-Based Approach for News Impact Assessment",
    "volume": "review",
    "abstract": "Analyzing Complex Interdependencies in Financial Markets: A Neural Network-Based Approach for News Impact Assessment In the ever-evolving landscape of financial markets, the intricate web of interdependencies among companies, driven by supply chain intricacies and competitive dynamics, has become a central concern for investors and analysts alike. Our research endeavors to shed light on these intricate relationships and their susceptibility to external news events. In this study, we examine a hypothetical scenario where Company A relies on Companies B and C, Company B depends on Company D, and Company C's fortunes are intertwined with those of Companies E and F, all while these companies are directly reliant on finite natural resources. We use this scenario to illustrate the profound impact of news pertaining to any one of these companies, be it Company A, B, C, or their competitors, on the entire ecosystem. The ripple effect extends through supply chains and demand chains, with repercussions resonating both directly and indirectly. Of importance, we show how emerging ML techniques can model and predict such effects. To navigate this complex terrain, we introduce a novel approach based on constructing dependency graphs for each company using a suitable methodology akin to BFS. This method involves expanding the nodes in the graph to represent companies, scrutinizing their lists of competitors, suppliers, and clients, with terminal nodes denoting natural resources often owned by government entities. Our research harnesses the wealth of sentiment and dependency information extracted from news articles covering a diverse array of companies. These companies are integrated as nodes into our data model. Through the aggregation of stock values for these nodes during successive news intervals, coupled with a meticulous analysis of news sentiment's influence on each node and the deduction of intricate relationships among them, we present a comprehensive view of the interplay between news events and the financial market landscape. The culmination of our efforts culminates in the integration of this analysis into a neural network-based stock trend prediction model. The objective is to assess the effectiveness of our approach in gauging the impact of news on associated companies, providing investors and analysts with a powerful tool to navigate the complex and interconnected world of financial markets. This research not only contributes to a deeper understanding of market dynamics but also offers practical insights for informed decision-making in an increasingly volatile financial landscape",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgVEz6wwbM": {
    "title": "What's the Magic Word? A Control Theory of LLM Prompting",
    "volume": "review",
    "abstract": "Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of a self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k - \\epsilon$ controllability to characterize LLM steerability. We compute the $k-\\epsilon$ controllability of a panel of large language models, including Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language modeling tasks. Remarkably, we find that magic words of 10 tokens or less exist for over 97\\% of WikiText instances surveyed for each model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.6,
    "authors": []
  },
  "https://openreview.net/forum?id=SuUh5aRbbu": {
    "title": "End-to-end Story Plot Generator",
    "volume": "review",
    "abstract": "Story plots, while short, carry most of the essential information of a full story that may contain tens of thousands of words. We study the problem of automatic generation of story plots, which includes story premise, character descriptions, plot outlines, etc. To generate a single engaging plot, existing plot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands of calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot, which is costly and takes at least several minutes. Moreover, the hard-wired nature of the method makes the pipeline non-differentiable, blocking fast specialization and personalization of the plot generator. In this paper, we overcome these issues with an end-to-end story plot generator, which is (1) faster and cheaper to generate and (2) end-to-end fine-tunable with human feedback. Compared to DOC, our work replaces expensive OpenAI API calls with Llama2 models via careful prompt designs, which leads to the cheap generation of high-quality training datasets. We then perform supervised fine-tuning (SFT) using approximately 13000 story plots to obtain an end-to-end model. The end-to-end model can generate story plots of comparable quality to the previous DOC method and is $>10\\times$ faster (1k tokens in only 30 seconds on average). Furthermore, fine-tuned with RLHF on several different reward models for different aspects of story quality, our model achieves 60.0\\% winning rate against the model after SFT in the aspect of suspense and surprise",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ohdVLirfbz": {
    "title": "Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks",
    "volume": "review",
    "abstract": "Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work (Richards and Kuzborskij,2021; Lei et al.,2022) by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\\sqrt{n})$ for GD algorithms in both two-layer and three-layer NNs. This sheds light on sufficient or necessary conditions for under-parameterized and over-parameterized NNs trained by GD to attain the desired risk rate of $O(1/\\sqrt{n})$. Moreover, we demonstrate that as the scaling parameter increases or the network complexity decreases, less over-parameterization is required for GD to achieve the desired error rates. Additionally, under a low-noise condition, we obtain a fast risk rate of $O(1/n)$ for GD in both two-layer and three-layer NNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=w1JanwReU6": {
    "title": "Are Models Biased on Text without Gender-related Language?",
    "volume": "review",
    "abstract": "In the large language models era, it is imperative to measure and understand how gender biases present in the training data influence model behavior. Previous works construct benchmarks around known stereotypes (e.g., occupations) and demonstrate high levels of gender bias in large language models, raising serious concerns about models exhibiting undesirable behaviors. We expand on existing literature by asking the question: \\textit{Do large language models still favor one gender over the other in non-stereotypical settings?} To tackle this question, we restrict language model evaluation to a \\textit{neutral} subset, in which sentences are free of pronounced word-gender associations. After characterizing these associations in terms of pretraining data statistics, we use them to (1) create a new benchmark with low gender-word associations, and (2) repurpose popular benchmarks in the gendered pronoun setting | WinoBias and \\Winogender |, removing pronounced gender-correlated words. Surprisingly, when testing $20+$ models (e.g., Llama-2, Pythia, and OPT) in the proposed benchmarks, we still detect critically high gender bias across all tested models. For instance, after adjusting for strong word-gender associations, we find that all models still exhibit clear gender preferences in about $60\\%$-$95\\%$ of the sentences, representing a small change (up to $5\\%$) from the original \\textit{stereotypical} setting. By demonstrating that measured bias is not necessarily due to the presence of highly gender-associated words, our work highlights important questions about bias evaluation as well as potentially underlying model biases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=op19LjpHkH": {
    "title": "Decoupled Actor-Critic",
    "volume": "review",
    "abstract": "Actor-Critic methods are in a stalemate of two seemingly irreconcilable problems. Firstly, critic proneness towards overestimation requires sampling temporal-difference targets from a conservative policy optimized using lower-bound Q-values. Secondly, well-known results show that policies that are optimistic in the face of uncertainty yield lower regret levels. To remedy this dichotomy, we propose Decoupled Actor-Critic (DAC). DAC is an off-policy algorithm that learns two distinct actors by gradient backpropagation: a conservative actor used for temporal-difference learning and an optimistic actor used for exploration. We test DAC on DeepMind Control tasks in low and high replay ratio regimes and ablate multiple design choices. Despite minimal computational overhead, DAC achieves state-of-the-art performance and sample efficiency on locomotion tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=J2pMoN2pon": {
    "title": "How do skip connections affect Graph Convolutional networks with graph sampling? A theoretical analysis on generalization",
    "volume": "review",
    "abstract": "Skip connections enable deep Graph Convolutional Networks (GCNs) to overcome oversmoothing, while graph sampling reduces computational demands by selecting a submatrix of the graph adjacency matrix during neighborhood aggregation. Learning deep GCNs with graph sampling has shown empirical success across various applications, but a theoretical understanding of the generalization guarantees remains limited, with existing analyses ignoring either graph sampling or skip connections. This paper presents the first generalization analysis of GCNs with skip connections using graph sampling. Our analysis demonstrates that the generalization accuracy of the learned model closely approximates the highest achievable accuracy within a broad class of target functions dependent on the proposed sparse effective adjacency matrix, denoted by $A^*$. Thus, graph sampling maintains generalization performance when $A^*$ accurately models data correlations. Notably, our findings reveal that skip connections lead to different sampling requirements across layers. In a two-hidden-layer GCN, the generalization is more affected by the sampled matrix deviations from $A^*$ of the first layer than the second layer. To the best of our knowledge, this marks the first theoretical characterization of skip connections' role in sampling requirements. We validate our theoretical results on benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=dFcXJgnrGB": {
    "title": "PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning",
    "volume": "review",
    "abstract": "Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex and often contextualized situations, e.g. ``scheduling a doctor's appointment without a phone''. While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (constrained) language-based planning capabilities. More concretely, we develop *symbolic procedural knowledge distillation* to enhance the commonsense knowledge in small language models and an*inference-time algorithm* to facilitate more structured and accurate reasoning. In addition, we introduce a new related task, *Replanning*, that requires a revision of a plan to cope with a constrained situation. In both the planning and replanning settings, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models' capabilities. Finally, we showcase successful application of PlaSma in an embodied environment, VirtualHome",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=PfPnugdxup": {
    "title": "From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction",
    "volume": "review",
    "abstract": "The role of machine learning in computing atomic properties is expanding rapidly for a wide range of applications from healthcare to climate change. One important ingredient that has enabled this development is the creation of large and diverse molecular datasets. Given the extreme computational cost of these datasets, an important question moving forward is: Can we limit the need for exhaustive large dataset creation by pre-training a foundation style model over multiple chemical domains to generate transferable atomic representations for downstream fine-tuning tasks? Generalization across the entire molecular space is challenging due to the range and complexity of atomic interactions that exist. In this paper, we present Joint Multi-domain Pre-training (JMP), a robust supervised pre-training strategy that utilizes data from multiple chemical domains, $\\sim$120 million examples in total. We demonstrate state-of-the-art results across many targets of the rMD17, QM9, MatBench, QMOF, SPICE, and MD22 datasets. Finally, we conduct ablations to study the impact of different components of JMP on downstream performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FTSUDBM6lu": {
    "title": "Patch Ranking Map: Explaining Relations among Top-Ranked Patches, Top-Ranked Features and Decisions of Convolutional Neural Networks for Image Classification",
    "volume": "review",
    "abstract": "Since a conventional Convolutional Neural Network (CNN) using a large number of extracted features is not fully explainable and not very memory-efficient, we develop an explainable and efficient CNN model consisting of convolutional layers, a new feature selection (FS) layer, a classifier, and a novel ``Patch Ranking Map\" (PRM). The PRM contains top-ranked image patches that have important associations with decisions of the CNN. Top-ranked common features selected by different FS methods are used to generate two newly defined matrices: the ``feature accumulation matrix\" and the ``feature ranking matrix\". Different from a typical heatmap, these two matrices are used to rank image patches in the PRM to effectively explain the relationship among an input image, top-ranked features, top-ranked feature maps, and the final classification decision. Simulation results using the Alzheimer's MRI preprocessed dataset for 4-class image classification with $6,400$ $128\\times128$ images indicate that the three PRMs based on three robust top-ranked common feature sets generated by seven different FS methods have the same top two most important patches associated with Alzheimer's disease diagnosis. In addition, $8\\times8$ patches of a $128\\times128$ image at the 7th and 12th patch rows and at the 8th and 9th patch columns are most informative because they have the top two most important patches and the top two most top-ranked common row-wise and column-wise features. The relationship among brain regions associated with Alzheimer's disease, the top-ranked patches, the top patch rows, and the top patch columns will be analyzed based on research results in brain informatics and medical informatics. The simulations also show that the trained CNN with FS can have higher classification accuracy and smaller model size than the conventional CNN without FS. More effective and efficient optimization algorithms will be developed to select the top (most informative) features and rank patches for building an accurate and efficient CNN model with more explainable decisions that can be captured by the PRM for various image classification applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=HC26cxtI96": {
    "title": "The Fine-Grained Chip Placement with Hybrid Action Spaces and Feature Fusion",
    "volume": "review",
    "abstract": "Chip placement is an essential and time-consuming step in the physical design process. Deep reinforcement learning, as an emerging field, has gained significant attention due to its ability to replace weeks of expert model design. We devise a fusion-based reinforcement learning framework to address the limited representation problem of both graph networks and CNN networks. Furthermore,the structure of PDQN in the hybrid action space allows for precise coordinate placement, compared to other RL-based structures in placement. The experimental results can demonstrate the effectiveness of our model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 1.0,
    "authors": []
  },
  "https://openreview.net/forum?id=M6XWoEdmwf": {
    "title": "AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents",
    "volume": "review",
    "abstract": "We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is uniquely scalable and applicable to a wide range of problems. We demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a novel hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments. We evaluate our agent on three goal-conditioned domains and study how its individual improvements connect to create a generalist policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=nhgTmx1TZJ": {
    "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
    "volume": "review",
    "abstract": "Language models (LMs) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LMs techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LMs. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released\\footnote{\\url{https://uniaudio666.github.io/demo_UniAudio/}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Zc2aIcucwc": {
    "title": "Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets",
    "volume": "review",
    "abstract": "Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=D8DAQhpznu": {
    "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Selective Classification",
    "volume": "review",
    "abstract": "To maintain user trust, large language models (LLMs) should signal low confidence on examples they get incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but state-of-the-art LLMs such as GPT-4 and Claude do not provide access to these probabilities. We first study eliciting confidence linguistically---asking an LLM for its confidence in its answer---but we find that this leaves a lot of room for improvement (79\\% AUC on GPT-4 averaged across 12 question-answering datasets---only 5\\% above a random baseline). We then explore using a \\emph{surrogate} confidence model---using a model where we do have probabilities to evaluate the original model's confidence in a given question. Surprisingly, even though these probabilities come from a different model, this method leads to higher AUC than linguistic confidences on 10 out of 12 datasets. Our best method mixing linguistic confidences and surrogate model probabilities gives state-of-the-art performance on all 12 datasets (85\\% average AUC on GPT-4)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=w50MQ9Vfty": {
    "title": "Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference",
    "volume": "review",
    "abstract": "Interference is ubiquitous when conducting causal experiments over social networks. Except for certain network structures, causal inference on the network in the presence of interference is difficult due to the entanglement between the treatment assignments and the interference levels. In this article, we conduct causal inference under interference on an observed, sparse but connected network, and we propose a novel design of experiments based on an independent set. Compared to conventional designs, the independent-set design focuses on an independent subset of data and controls their interference exposures through the assignments to the rest (auxiliary set). The independent-set design enhances the performance of causal estimators by trading sample quantity for sample quality. We show the capacity of our approach for various causal inference tasks, justify its superiority over conventional methods, and illustrate the empirical performance through simulations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=gPKTTAfYBp": {
    "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores",
    "volume": "review",
    "abstract": "Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)---which allows long convolutions to run in $O(N\\log N)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms---1) partial convolutions and 2) frequency-sparse convolutions---which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 8.7$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity and M2-BERT-base to achieve 3.3 points higher GLUE score---matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models---yielding the first DNA model that can process the longest human genes (2.3M base pairs)---and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=AKAlVyunxA": {
    "title": "SHINE: Shielding Backdoors in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "Recent studies have discovered that similar to supervised classifiers, a deep reinforcement learning (DRL) policy is also vulnerable to backdoor attacks. Existing defenses against backdoor attacks either do not consider RL's unique mechanism or make unrealistic assumptions, resulting in limited defense efficacy, practicability, and generalizability. In this work, we propose SHINE, a novel backdoor shielding method for DRL. SHINE first leverages policy explanation techniques to identify the backdoor triggers and then designs a policy retraining algorithm to eliminate the negative impact of the triggers on backdoored agents. We theoretically prove that SHINE guarantees to improve a backdoored agent's performance in a poisoned environment while ensuring its performance difference in the clean environment before and after shielding is bounded. We further conduct extensive experiments that evaluate SHINE against three mainstream DRL backdoor attacks in various benchmark RL environments. Our results show that SHINE significantly outperforms existing defenses in mitigating these backdoor attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=oDdzXQzP2F": {
    "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
    "volume": "review",
    "abstract": "We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=Cx6Jn6gKHz": {
    "title": "Can adversarial samples benefit few-shot unsupervised implicit neural shape representation learning ?",
    "volume": "review",
    "abstract": "Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. While recent methods rely on smoothness priors to regularize the learning, our method introduces a regularization term that leverages adversarial samples around the shape to improve the learned SDFs. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve SDF learning with respect to baselines and the state-of-the-art using synthetic and real data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=O9gstAazBM": {
    "title": "Efficient Model-Agnostic Multi-Group Equivariant Networks",
    "volume": "review",
    "abstract": "Constructing model-agnostic group equivariant networks, such as equitune (Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be computationally expensive for large product groups. We address this by providing efficient model-agnostic equivariant designs for two related problems: one where the network has multiple inputs each with potentially different groups acting on them, and another where there is a single input but the group acting on it is a large product group. For the first design, we initially consider a linear model and characterize the entire equivariant space that satisfies this constraint. This characterization gives rise to a novel fusion layer between different channels that satisfies an invariance-symmetry (IS) constraint, which we call an IS layer. We then extend this design beyond linear models, similar to equitune, consisting of equivariant and IS layers. We also show that the IS layer is a universal approximator of invariant-symmetric functions. Inspired by the first design, we use the notion of the IS property to design a second efficient model-agnostic equivariant design for large product groups acting on a single input. For the first design, we provide experiments on multi-image classification where each view is transformed independently with transformations such as rotations. We find equivariant models are robust to such transformations and perform competitively otherwise. For the second design, we consider three applications: language compositionality on the SCAN dataset to product groups; fairness in natural language generation from GPT-2 to address intersec- tionality; and robust zero-shot image classification with CLIP. Overall, our methods are simple and general, competitive with equitune and its variants, while also being computationally more efficient",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nqir5R4ACn": {
    "title": "Simple Data Sharing for Multi-Tasked Goal-Oriented Problems",
    "volume": "review",
    "abstract": "Many important sequential decision problems -- from robotics, games to logistics -- are multi-tasked and goal-oriented. In this work, we frame them as Contextual Goal Oriented (CGO) problems, a goal-reaching special case of the contextual Markov decision process. CGO is a framework for designing multi-task agents that can follow instructions (represented by contexts) to solve goal-oriented tasks. We show that CGO problem can be systematically tackled using datasets that are commonly obtainable: an unsupervised interaction dataset of transitions and a supervised dataset of context-goal pairs. Leveraging the goal-oriented structure of CGO, we propose a simple data sharing technique that can provably solve CGO problems offline under natural assumptions on the datasets' quality. While an offline CGO problem is a special case of offline reinforcement learning (RL) with unlabelled data, running a generic offline RL algorithm here can be overly conservative since the goal-oriented structure of CGO is ignored. In contrast, our approach carefully constructs an augmented Markov Decision Process (MDP) to avoid introducing unnecessary pessimistic bias. In the experiments, we demonstrate our algorithm can learn near-optimal context-conditioned policies in simulated CGO problems, outperforming offline RL baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekeyCgeRfC": {
    "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
    "volume": "review",
    "abstract": "In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a *teaching sequence*, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement *two distinct* algorithms to solve a *single* task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mSSi0zYkEA": {
    "title": "Initializing the Layer-wise Learning Rate",
    "volume": "review",
    "abstract": "The standard method to assign learning rates has been to rely on the optimizer and to use a single, global learning rate across all its layers. We propose to assign individual learning rates as well, according to the layer-wise gradient magnitude at initialization. Even if individual layers are initialized to preserve gradient variance, architectural characteristics result in uneven gradient magnitude even when the network has not started training. We interpret this gradient magnitude as a measure of architecture-induced convergence bias, and adjust the layer-wise learning rate opposite to its gradient magnitude at initialization. This relative learning rate is maintained throughout the entire training scheme. Experiments on convolutional and transformer architectures on ImageNet-1k show improved accuracy and training stability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=CARclfc9ci": {
    "title": "Relational Convolutional Networks: A framework for learning representations of hierarchical relations",
    "volume": "review",
    "abstract": "A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of *hierarchical* relations, proposing an architectural framework we call \"relational convolutional networks\". Given a sequence of objects, a \"multi-dimensional inner product relation\" module produces a relation tensor describing all pairwise relations. A \"relational convolution\" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demonstrate how relational convolutional networks can provide an effective framework for modeling relational tasks that have hierarchical structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8Q3XTUJP9": {
    "title": "How does overparametrization affect features?",
    "volume": "review",
    "abstract": "Overparametrization, the condition where models have more parameters than necessary to fit their training loss, is a crucial factor for the success of deep learning. However, the characteristics of the features learned by overparametrized networks are not well understood. In this work, we explore this question by comparing models with the same architecture but different widths. We first examine the expressivity of the features of these models, and show that the feature space of overparametrized networks cannot be spanned by concatenating many underparametrized features, and vice versa. This reveals that both overparametrized and underparametrized networks acquire some distinctive features. We then evaluate the performance of these models, and find that overparametrized networks outperform underparametrized networks, even when many of the latter are concatenated. We corroborate these findings using a VGG-16 and ResNet18 on CIFAR-10 and a Transformer on the MNLI classification dataset. Finally, we propose a toy setting to explain how overparametrized networks can learn some important features that the underparamaterized networks cannot learn",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4g02l2N2Nx": {
    "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry",
    "volume": "review",
    "abstract": "Linear attentions have shown promise for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) `inetuned-conversion of task-specific Transformers into linear versions that recover task performance, and (3) pretrained-conversion of Transformers, such as language models, into linear versions readily finetunable on downstream tasks. However, linear attentions often underperform compared to standard softmax attention. To close this performance gap, we study the behaviors of softmax and linear attentions in various train-from-scratch and finetuned-conversion settings. We find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or spiky) weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple, trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99\\% of standard Transformer performance in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions by up to 6 perplexity points on WikiText-103 when training causal GPT models from scratch, and up to 8.7 GLUE score points when converting finetuned bidirectional BERT models. Hedgehog also enables direct pretrained-conversion, achieving a new state-of-the-art WikiText-103 perplexity of 16.7 for 125M decoder-only Transformers by converting pretrained GPT-2 into a linear attention Transformer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=gLZeEpfVjy": {
    "title": "Understanding and Robustifying Sub-domain Alignment for Domain Adaptation",
    "volume": "review",
    "abstract": "In unsupervised domain adaptation (UDA), aligning source and target domains improves the predictive performance of learned models on the target domain. A common methodological improvement in alignment methods is to divide the domains and align sub-domains instead. These sub-domain-based algorithms have demonstrated great empirical success but lack theoretical support. In this work, we establish a rigorous theoretical understanding of the advantages of these methods that have the potential to enhance their overall impact on the field. Our theory uncovers that sub-domain-based methods optimize an error bound that is at least as strong as non-sub-domain-based error bounds and is empirically verified to be much stronger. Furthermore, our analysis indicates that when the marginal weights of sub-domains shift between source and target tasks, the performance of these methods may be compromised. We therefore implement an algorithm to robustify sub-domain alignment for domain adaptation under sub-domain shift, offering a valuable adaptation strategy for future sub-domain-based methods. Empirical experiments across various benchmarks validate our theoretical insights, prove the necessity for the proposed adaptation strategy, and demonstrate the algorithm's competitiveness in handling label shift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=XNa6r6ZjoB": {
    "title": "Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers",
    "volume": "review",
    "abstract": "An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the *Abstractor*. At the core of the Abstractor is a variant of attention called *relational cross-attention*. The approach is motivated by an architectural inductive bias for relational learning that disentangles relational information from extraneous features about individual objects. This enables explicit relational reasoning, supporting abstraction and generalization from limited data. The Abstractor is first evaluated on simple discriminative relational tasks and compared to existing relational architectures. Next, the Abstractor is evaluated on purely relational sequence-to-sequence tasks, where dramatic improvements are seen in sample efficiency compared to standard Transformers. Finally, Abstractors are evaluated on a collection of tasks based on mathematical problem solving, where modest but consistent improvements in performance and sample efficiency are observed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=OF5x1dzWSS": {
    "title": "Doubly Robust Instance-Reweighted Adversarial Training",
    "volume": "review",
    "abstract": "Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes can be found in the Supplement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=84fOBZlOiV": {
    "title": "Estimating uncertainty from feed-forward network based sensing using quasilinear approximation",
    "volume": "review",
    "abstract": "Artificial neural networks are increasingly integrated into both sensing hardware (e.g., \"smart sensors\") and dedicated decision-making circuits that operate on this information. As this technology is deployed in safety-critical environments (pedestrian-detection, power management, and flight-controls) it is critical to assess the real-time confidence of information built on these networks. However, while stand-alone confidence of sensing (e.g. object detection) neural networks are common, tools are much more limited for integrating such information into formal estimation of latent variables upstream of the sensor. To make this distinction clear, consider the common problem of target-tracking from a mobile camera. The geographic position of the target is a function of the camera position and orientation in addition to position within the image, whereas the neural network only reports confidence in pixel-space. Likewise, optimally leveraging an image-sequence requires consideration of uncertainty in the camera and target dynamics, as well as the sensing neural network. As we will demonstrate, fusing dynamical system models with large sensing networks presents a major computational challenge. Specifically, popular approaches such as first-order (Jacobian) linearization prove inaccurate, whereas nonlinear sampling-based approaches, while effective, are intractable for high-dimensional measurements such as images. In this work, we borrow an analytic approach from control engineering, quasilinear system approximation, to propagate the dynamics of environmental uncertainty through feedforward neural network architectures. The approximation enables direct Bayesian (i.e., Kalman-style) filtering to estimate latent variables, thus obviating the need for taxing sampling-based approaches. Thus, the proposed framework may enable real-time confidence estimation in high-dimensional network-based sensing deployments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FjifPJV2Ol": {
    "title": "SOLVING SCHRODINGER BRIDGE PROBLEM VIA STOCHASTIC ACTION MINIMIZATION",
    "volume": "review",
    "abstract": "The Schrodinger bridge problem is a classical entropy-regularized optimal transport problem that seeks to find optimal diffusion trajectories that transform one probability distribution into another. Although mathematical theory has reached a mature stage, the ongoing research in algorithmic advancements remains a dynamic field, driven by recent innovations in diffusion models. We introduce stochastic Lagrangian and stochastic action as viable alter- native for serving as a direct loss function. We demonstrate the feasibility of incorporating all the vital physical constraints necessary to solve the problem directly into the Lagrangian, providing an intuitive grasp of the loss function and streamlining the training process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.4,
    "authors": []
  },
  "https://openreview.net/forum?id=YCWjhGrJFD": {
    "title": "Training Diffusion Models with Reinforcement Learning",
    "volume": "review",
    "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=Yp01vcQSNl": {
    "title": "DIRECTIONALITY IN GRAPH TRANSFORMERS",
    "volume": "review",
    "abstract": "We study how one can capture directionality in graph transformers, for learning over directed graphs. Most existing graph transformers do not take edge direction into account. We therefore introduce a novel graph transformer architecture that explicitly takes into account the edge directionality. To achieve this, we make use of dual encodings to represent both potential roles, i.e., source or target, of each pair of vertices linked by a directed edge. These dual encodings are learned by leveraging the latent adjacency information extracted from a novel directional attention module, localized with $k$-hop neighborhood information. We also study alternative approaches to incorporating directionality into other graph transformers to enhance their performance on directed graph learning tasks. To evaluate the importance of edge direction, we empirically characterize via randomization whether direction really matters for the downstream task. We propose two new directional graph datasets where direction is intrinsically related to learning. Via experiments on directional graph datasets, we show that our approach yields state-of-the-art results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=OnLAftJkhV": {
    "title": "Latent Conservative Objective Models for Offline Data-Driven Crystal Structure Prediction",
    "volume": "review",
    "abstract": "In computational chemistry, crystal structure prediction (CSP) is an optimization problem that involves discovering the lowest energy stable crystal structure for a given chemical formula. This problem is challenging as it requires discovering globally optimal designs with the lowest energies on complex manifolds. One approach to tackle this problem involves building simulators based on density functional theory (DFT) followed by running search in simulation, but these simulators are painfully slow. In this paper, we study present and study an alternate, data-driven approach to crystal structure prediction: instead of directly searching for the most stable structures in simulation, we train a surrogate model of the crystal formation energy from a database of existing crystal structures, and then optimize this model with respect to the parameters of the crystal structure. This surrogate model is trained to be conservative so as to prevent exploitation of its errors by the optimizer. To handle optimization in the non-Euclidean space of crystal structures, we first utilize a state-of-the-art graph variational auto-encoder (CD-VAE) to convert a crystal structure into a vector-based search space and then optimize a conservative surrogate model of the crystal energy, trained on top of this vector representation. We show that our approach, dubbed LCOMs (latent conservative objective models), performs comparably to the best current approaches in terms of success rate of structure prediction, while also drastically reducing computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=D2eOVqPX9g": {
    "title": "Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning",
    "volume": "review",
    "abstract": "Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-time error analysis. Notably, we establish that FedSARSA converges to a policy that is near-optimal for all agents, with the extent of near-optimality proportional to the level of heterogeneity. Furthermore, we prove that FedSARSA leverages agent collaboration to enable linear speedups as the number of agents increases, which holds for both fixed and adaptive step-size configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jQ596tXT3k": {
    "title": "Explaining the Out-of-Distribution Detection Paradox through Likelihood Peaks",
    "volume": "review",
    "abstract": "Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having high likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass, which can occur if the density is sharply peaked. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur on data confined to low dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Moreover, we provide an efficient method for estimating LID from a normalizing flow model, improving upon existing estimators, and enabling state-of-the-art OOD detection performance with respect to comparable flow-based benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=fe6ANBxcKM": {
    "title": "Federated Q-Learning: Linear Regret Speedup with Low Communication Cost",
    "volume": "review",
    "abstract": "In this paper, we consider federated reinforcement learning for tabular episodic Markov Decision Processes (MDP) where, under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. While linear speedup in the number of agents has been achieved for some metrics, such as convergence rate and sample complexity, in similar settings, it is unclear whether it is possible to design a *model-free* algorithm to achieve linear *regret* speedup with low communication cost. We propose two federated Q-Learning algorithms termed as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the corresponding total regrets achieve a linear speedup compared with their single-agent counterparts, while the communication cost scales logarithmically in the total number of time steps $T$. Those results rely on an event-triggered synchronization mechanism between the agents and the server, a novel step size selection when the server aggregates the local estimates of the state-action values to form the global estimates, and a set of new concentration inequalities to bound the sum of non-martingale differences. This is the first work showing that linear regret speedup and logarithmic communication cost can be achieved by model-free algorithms in federated reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=MeHmwCDifc": {
    "title": "The Trickle-down Impact of Reward Inconsistency on RLHF",
    "volume": "review",
    "abstract": "Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs --- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments --- and their impact on the downstream RLHF model. In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training? We propose **Contrast Instruction** -- a benchmarking strategy for the consistency of RM. Each example in **Contrast Instruction** features a pair of lexically similar instructions with different ground truth responses. A consistent RM is expected to rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on \\contrast{} compared to average humans. To show that RM consistency can be improved efficiently without using extra training budget, we propose two techniques **ConvexDA** and **RewardFusion**, which enhance reward consistency through extrapolation during the RM training and inference stage, respectively. We show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=nZ7rpEp6wj": {
    "title": "Multi-Resolution Learning with DeepONets and Long Short-Term Memory Neural Networks",
    "volume": "review",
    "abstract": "Deep operator networks (DeepONets, DONs) offer a distinct advantage over traditional neural networks in their ability to be trained on multi-resolution data. This property becomes especially relevant in real-world scenarios where high-resolution measurements are difficult to obtain, while low-resolution data is more readily available. Nevertheless, DeepONets alone often struggle to capture and maintain dependencies over long sequences compared to other state-of-the-art algorithms. We propose a novel architecture, named DON-LSTM, which extends the DeepONet with a long short-term memory network (LSTM). Combining these two architectures, we equip the network with explicit mechanisms to leverage multi-resolution data, as well as capture temporal dependencies in long sequences. We test our method on long-time-evolution modeling of multiple non-linear systems and show that the proposed multi-resolution DON-LSTM achieves significantly lower generalization error and requires fewer high-resolution samples compared to its vanilla counterparts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=qVILwUxjLG": {
    "title": "Non-stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling",
    "volume": "review",
    "abstract": "Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Kwm1OyINXt": {
    "title": "Deep probabilistic 3D angular regression for directional dark matter detectors",
    "volume": "review",
    "abstract": "Modern detectors of elementary particles are approaching a fundamental sensitivity limit where individual quanta of charge can be localized and counted in 3D. This enables novel detectors capable of unambiguously demonstrating the particle nature of dark matter by inferring the 3D directions of elementary particles from complex point cloud data. The most complex scenario involves inferring the initial directions of low-energy electrons from their tortuous trajectories. To address this problem we develop and demonstrate the first probabilistic deep learning model that predicts 3D directions using a heteroscedastic von Mises-Fisher distribution that allows us to model data uncertainty. Our approach generalizes the cosine distance loss which is a special case of our loss function in which the uncertainty is assumed to be uniform across samples. We utilize a sparse 3D convolutional neural network architecture and develop approximations to the negative log-likelihood loss which stabilize training. On a simulated Monte Carlo test set, our end-to-end deep learning approach achieves a mean cosine distance of $0.104$ $(26^\\circ)$ compared to $0.556$ $(64^\\circ) $ achieved by a non-machine learning algorithm. We demonstrate that the model is well-calibrated and allows selecting low-uncertainty samples to improve accuracy. This advancement in probabilistic 3D directional learning could significantly contribute to directional dark matter detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=eeaKRQIaYd": {
    "title": "Unsupervised Sign Language Translation and Generation",
    "volume": "review",
    "abstract": "Sign language translation and generation are crucial in facilitating communication between the deaf and hearing communities.However, the scarcity of parallel sign language video-to-text data poses a considerable challenge to developing effective sign language translation and generation systems.Motivated by the success of unsupervised neural machine translation (UNMT), this paper introduces an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. Inspired by UNMT, USLNet comprises two main components: single-modality reconstructing modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences.To address the issues, we propose a sliding window method to align variable-length text with video sequences.To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both text and sign language video in a unified manner.Experimental results on the BBC-Oxford Sign Language datasets (BOBSL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1TKGCrSL7": {
    "title": "Cross-modality debiasing: using language to mitigate sub-population shifts in imaging",
    "volume": "review",
    "abstract": "Sub-population shift is a specific type of domain shift that highlights changes in data distribution within specific sub-groups or populations between training and testing. Sub-population shift accounts for a significant source of algorithmic bias and calls for distributional robustness. Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper, we propose leveraging the connection of robustness among different modalities and reshaping the distributional robustness of one modality with another. Specifically, in the context of the distributional robustness of CLIP, we propose to leverage natural language inputs to debias the image feature representations, to improve worst-case performance on sub-populations. Our extensive empirical studies show that image representations debiased by natural language can achieve significant performance improvement and reduction of performance instability under sub-population shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=edETIhDTwL": {
    "title": "Decompose Time and Frequency Dependencies: Multivariate Time Series Physiological Signal Emotion Recognition",
    "volume": "review",
    "abstract": "In this study, we proposed a transformer based end-to-end solution to capture the relationship between the physiological signals and affective changes. We first convert the physiological signal emotion recognition prediction task to a sequence-to-sequence multivariate time series prediction task. We utilize the state-of-the-art (SOTA) self-attention mechanism to decompose the physiological signals into separate frequency domain and time domain representations, and capture the channel dependencies via Two-Stage Attention (TSA). Meanwhile, we implement the multitask learning framework to better predict the valence and arousal affective states individually. We evaluate our system on the Continuously Annotated Signals of Emotion (CASE) dataset used in the Emotion Physiology and Experience Collaboration (EPiC) challenge, and our proposed system outperform all the challenge participants in all four test scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=PKsTHJXn4d": {
    "title": "Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation",
    "volume": "review",
    "abstract": "Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts; however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, thus making our method independent from the underlying model's representation. For such models, we first learn a behavior representation and subsequently use it to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. We evaluate our method in a multi-agent search-and-rescue environment and demonstrate the effectiveness of our explanations for agents executing various behaviors. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those produced by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=RVaUSKSh9t": {
    "title": "Continual Graph Learning for Thermal Analysis of Composite Materials under Interface Variations",
    "volume": "review",
    "abstract": "Thermal analysis is an important topic in many fields, such as building, machinery, and microelectronics. As the types of materials in a system are increasingly diverse, conventional numerical methods or machine learning-based surrogate models face tremendous challenges in computation cost and accuracy. Furthermore, a realistic system usually suffers from random fabrication variations that induce significant errors in model prediction. To overcome these issues, we propose Graph Neural Networks (GNN) as a framework for thermal analysis of composite materials with diverse thermal conductivity and thermal interface variations. Using chiplets in microelectronics as the study case, we first partition the system into sub-blocks based on their material property. Then we develop a physics-constrained GNN as the aggregator to integrate local models of each sub-block into a system, with the edge to represent the thermal interaction. In the presence of interface variations, we introduce continual adaptation of the GNN model, using a minimum number of training samples. Compared with previous solutions, our GNN model is robust for various material and interface conditions, and efficient in the prediction of hot-spot",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=XJiN1VkgA0": {
    "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the \"dispersion\" of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty metrics, applying them to *selective* NLG where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=e3tveFVmoH": {
    "title": "Stochastic two points method for deep model gradient free optimization",
    "volume": "review",
    "abstract": "Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms standard methods across various model types and scales, with 2$\\times$ speed-up in training over most conducted tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZVi81SH1Ob": {
    "title": "Neural Collapse meets Differential Privacy: Curious behaviors of NoisySGD with Near-Perfect Representation Learning",
    "volume": "review",
    "abstract": "In recent studies, it has been demonstrated that large-scale representation learning through pre-training on gigantic datasets significantly enhances differentially private learning for downstream tasks. By training on Google's proprietary JFT dataset, one can achieve an unprecedented 83% Top 1 accuracy on ImageNet with strong privacy parameters $(0.5,8\\times 10^{-7})$-DP, even given the high dimensionality of the feature space. While the exact behaviors of NoisySGD in these scenarios remain theoretically challenging to analyze, we explore an idealized setting using a layer-peeled model for representation learning, which results in interesting phenomena of the learned features known as neural collapse. Under this setting, we have observed several notable behaviors of NoisySGD. Specifically, we demonstrate that under perfect neural collapse, the misclassification error is unaffected by the dimension of the features. This dimension-independent result holds with any learning rate and even with class imbalance and is not influenced by the nature of the loss functions. Nevertheless, a dimension dependency emerges when introducing minor perturbations in either the feature or model space. To address this dependency under perturbation, we suggest several strategies, such as pre-processing features or employing principal component analysis to reduce feature dimensions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ip5LHJs6QX": {
    "title": "Efficient Modulation for Vision Networks",
    "volume": "review",
    "abstract": "In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the abstracted modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Bene- fiting from the prominent representational ability of modulation mechanism and the efficiency of efficient modulation design, our network can accomplish better accuracy-efficiency trade-offs and set new state-of-the-art performance for efficient networks. When integrating EfficientMod block with the vanilla self-attention block, we obtain the hybrid architecture and further improve the performance without sacrificing the efficiency. We carry out comprehensive experiments to verify EfficientMod's performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than the prior state-of-the-art approach EfficientFormerV2-s2 without any training tricks and is 25% faster on GPU. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Codes and checkpoints are available in the supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fB1iiH9xo7": {
    "title": "Pre-training LiDAR-based 3D Object Detectors through Colorization",
    "volume": "review",
    "abstract": "Accurate 3D object detection and understanding for self-driving cars heavily relies on LiDAR point clouds, necessitating large amounts of labeled data to train. In this work, we introduce an innovative pre-training approach, Grounded Point Colorization (GPC), to bridge the gap between data and labels by teaching the model to colorize LiDAR point clouds, equipping it with valuable semantic cues. To tackle challenges arising from color variations and selection bias, we incorporate color as \"context\" by providing ground-truth colors as hints during colorization. Experimental results on the KITTI and Waymo datasets demonstrate GPC's remarkable effectiveness. Even with limited labeled data, GPC significantly improves fine-tuning performance; notably, on just 20% of the KITTI dataset, GPC outperforms training from scratch with the entire dataset. In sum, we introduce a fresh perspective on pre-training for 3D object detection, aligning the objective with the model's intended role and ultimately advancing the accuracy and efficiency of 3D object detection for autonomous vehicles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eo7kv0sllr": {
    "title": "An Emulator for Fine-tuning Large Language Models using Small Language Models",
    "volume": "review",
    "abstract": "Widely used language models (LMs) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage using more targeted examples of specific behaviors and/or human preferences. While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been rigorously tested. In this paper, we test this hypothesis with a novel methodology for scaling these two stages independently, essentially asking, *What would happen if we combined the knowledge learned by a large model during pre-training with the knowledge learned by a small model during fine-tuning (or vice versa)?* Using an RL-based framework derived from recent developments in learning from human preferences, we introduce *emulated fine-tuning (EFT)*, a principled and practical method for sampling from a distribution that approximates the result of pre-training and fine-tuning at different scales. Our experiments with EFT show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality. Further, we show that EFT enables test-time adjustment of competing behavioral factors like helpfulness and harmlessness without additional training. Finally, we find that a special case of emulated fine-tuning, which we call LM *up-scaling*, avoids resource-intensive fine-tuning of large pre-trained models by ensembling small fine-tuned models with large pre-trained models, essentially 'emulating' the result of fine-tuning the large pre-trained model. Up-scaling consistently improves helpfulness and factuality of widely used pre-trained models like Llama, Llama-2, and Falcon, without additional hyperparameters or training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=wsWGcw6qKD": {
    "title": "Toward Student-oriented Teacher Network Training for Knowledge Distillation",
    "volume": "review",
    "abstract": "How to conduct teacher training for knowledge distillation is still an open problem. It has been widely observed that a best-performing teacher does not necessarily yield the best-performing student, suggesting a fundamental discrepancy between the current teacher training practice and the ideal teacher training strategy. To fill this gap, we explore the feasibility of training a teacher that is oriented toward student performance with empirical risk minimization (ERM). Our analyses are inspired by the recent findings that the effectiveness of knowledge distillation hinges on the teacher's capability to approximate the true label distribution of training inputs. We theoretically establish that ERM minimizer can approximate the true label distribution of training data as long as the feature extractor of the learner network is Lipschitz continuous and is robust to feature transformations. In light of our theory, we propose a teacher training method SoTeacher which incorporates Lipschitz regularization and consistency regularization into ERM. Experiments on benchmark datasets using various knowledge distillation algorithms and teacher-student pairs confirm that SoTeacher can improve student accuracy consistently",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=jE8xbmvFin": {
    "title": "Language Models Represent Space and Time",
    "volume": "review",
    "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process---a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=pAoqRlTBtY": {
    "title": "Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning",
    "volume": "review",
    "abstract": "Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=8OBuqbLb8h": {
    "title": "Fast-ELECTRA for Efficient Pre-training",
    "volume": "review",
    "abstract": "ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity to hyper-parameters and enhances the pre-training stability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNpSUN0uCc": {
    "title": "Maximum Entropy Model Correction in Reinforcement Learning",
    "volume": "review",
    "abstract": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model's next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=DDAtRS5Ngf": {
    "title": "Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings",
    "volume": "review",
    "abstract": "Multi-modal embeddings encode images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call ``adversarial illusions.'' Given an image or a sound, an adversary can perturb it so as to make its embedding close to an arbitrary, adversary-chosen input in another modality. This enables the adversary to align any image and any sound with any text. Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=iPtgY9cJaV": {
    "title": "Identifying Latent State Transition Processes for Individualized Reinforcement Learning",
    "volume": "review",
    "abstract": "In recent years, reinforcement learning (RL) has been increasingly applied to systems that interact with individuals in various domains, such as healthcare, education, and e-commerce. When an RL agent interacts with individuals, individual-specific factors, ranging from personal preferences to physiological nuances, may causally influence state transitions, such as health conditions, learning progress, or user selections. Consequently, different individuals may exhibit different state transition processes. Understanding these individualized state-transition processes is crucial for making individualized policies. In practice, however, identifying these state-transition processes is challenging, especially since individual-specific factors often remain latent. In this paper, we present a practical method that effectively learns these processes from observed state-action trajectories, backed by theoretical guarantees. To our knowledge, this is the first work to provide a theoretical guarantee for identifying the state-transition processes involving latent individual-specific factors. Our experiments on synthetic and real-world datasets demonstrate that our method can effectively identify the latent state-transition processes and help learn individualized RL policies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=0Ce3c9l7G1": {
    "title": "Learning Multi-Agent Communication using Regularized Attention Messages",
    "volume": "review",
    "abstract": "Learning how to communicate in Multi-Agent Reinforcement Learning (MARL) can be key to solve complex cooperative tasks. Recent approaches have shown the advantages of using an efficient communication architecture, tackling problems such as what, when, or whom to communicate. However, these methods still fail to solve some complex scenarios, and some of them do not evaluate the implications of having limited communication channels. In this paper, we propose Attentive Regularized Communication (ARCOMM), a new method for communication in MARL. The proposed method uses an attention module to evaluate the weight of the messages generated by the agents, together with a message regularizer that facilitates learning more meaningful messages, improving the performance of the team. We further analyse how ARCOMM reacts to situations where the messages must be compressed before being sent to other agents. Our results show that the proposed method helps, through the power of communication, to improve the performances of the agents in complex domains when compared to other methods. Furthermore, we show that, although there is a decrease of performance, agents are still capable of learning even with lossy communication. The messages learned by the agents also support the motivations for our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=aN4Jf6Cx69": {
    "title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
    "volume": "review",
    "abstract": "Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 9.0,
    "authors": []
  },
  "https://openreview.net/forum?id=D9rJdtmIG6": {
    "title": "SpaCE: The Spatial Confounding Environment",
    "volume": "review",
    "abstract": "Spatial confounding poses a significant challenge in scientific studies involving spatial data, where unobserved spatial variables can influence both treatment and outcome, possibly leading to spurious associations. To address this problem, we introduce SpaCE: The Spatial Confounding Environment, the first toolkit to provide realistic benchmark datasets and tools for systematically evaluating causal inference methods designed to alleviate spatial confounding. Each dataset includes training data, true counterfactuals, a spatial graph with coordinates, and smoothness and confounding scores characterizing the effect of a missing spatial confounder. It also includes realistic semi-synthetic outcomes and counterfactuals, generated using state-of-the-art machine learning ensembles, following best practices for causal inference benchmarks. The datasets cover real treatment and covariates from diverse domains, including climate, health and social sciences. SpaCE facilitates an automated end-to-end pipeline, simplifying data loading, experimental setup, and evaluating machine learning and causal inference models. The SpaCE project provides several dozens of datasets of diverse sizes and spatial complexity. It is publicly available as a Python package, encouraging community feedback and contributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=TTEwosByrg": {
    "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as \"System Star is better than System Square.\" We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of \\textbf{40\\%} of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be \\textbf{49.6\\%}, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=OkHHJcMroY": {
    "title": "PILOT: An $\\mathcal{O}(1/T)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation",
    "volume": "review",
    "abstract": "Learning an accurate value function for a given policy is a critical step in solving reinforcement learning (RL) problems. So far, however, the convergence speed and sample complexity performances of most existing policy evaluation algorithms remain unsatisfactory, particularly with non-linear function approximation. This challenge motivates us to develop a new path-integrated primal-dual stochastic gradient (PILOT) method, that is able to achieve a fast convergence speed for RL policy evaluation with nonlinear function approximation. To further alleviate the periodic full gradient evaluation requirement, we further propose an enhanced method with an adaptive-batch adjustment called PILOT$^+$. The main advantages of our methods include: i) PILOT allows the use of {\\em{constant}} step sizes and achieves the $\\mathcal{O}(1/K)$ convergence rate to first-order stationary points of non-convex policy evaluation problems; ii) PILOT is a generic {\\em{single}}-timescale algorithm that is also applicable for solving a large class of non-convex strongly-concave minimax optimization problems; iii) By adaptively adjusting the batch size via historical stochastic gradient information, PILOT$^+$ is more sample-efficient empirically without loss of theoretical convergence rate. Our extensive numerical experiments verify our theoretical findings and showcase the high efficiency of the proposed PILOT and PILOT$^+$ algorithms compared with the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2ve0q6cIO": {
    "title": "Graph Neural Networks Gone Hogwild",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) constitute a dominant class of architectures for modelling graph-structured data. Message-passing GNNs in particular appear to be ideal for applications where distributed inference is desired, since node updates can be performed locally. Implementing distributed inference of GNNs on enormous graphs is a conspicuous example of such an application. In this work, we are particularly motivated by the view that GNNs can be interpreted as parametric communication policies between agents which collectively solve a distributed optimization problem (e.g., in robotic swarms or sensor networks). For these applications, node synchrony and central control are undesirable, since they result in communication bottlenecks and reduce fault tolerance and scalability. We examine GNN inference under asynchrony, and find that most GNNs generate arbitrarily incorrect predictions in this regime. A notable exception is GNNs which cast message passing as a fixed point iteration with contractive update functions. We propose a novel GNN architecture, energy GNNs, in which node embeddings are computed by minimizing a scalar-valued convex \"energy\" function. By framing message passing as convex optimization, we unlock a richer class of update functions which preserve robustness under asynchronous execution. We show that, empirically, we outperform other GNNs which are amenable to asynchronous execution on a multitude of tasks across both synthetic and real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=u8L1zzGXRq": {
    "title": "Impact of Molecular Representations on Deep Learning Model Comparisons in Drug Response Predictions",
    "volume": "review",
    "abstract": "Deep learning (DL) plays a crucial role in tackling the complexity and heterogeneity of cancer, particularly in predicting drug response. However, the effectiveness of these models is often hindered by inconsistent benchmarks and disparate data sources. To address the gaps in comparisons, we introduce CoMParison workflow for Cross Validation (CMP-CV), an automated cross-validation framework that trains multiple models with user-specified parameters and evaluation metrics. The effectiveness of DL models in predicting drug responses is closely tied to the methods used to represent drugs at the molecular level. In this contribution, we benchmarked commonly leveraged drug representations (graph, molecular descriptors, molecular fingerprints, and SMILES) to lean and understand the predictive capabilities of the models. We compare the ability of different drug representations to encode different structural properties of the drugs by using prediction errors made by models in different drug descriptor domains. We find that, in terms of the average prediction error over the entire test set, molecular descriptor and encoded SMILES representations perform slightly better than the others. However, we also observe that the rankings of the model performance vary in different regions over the descriptor space studied in this work, emphasizing the importance of domain-based model comparison when selecting a model for a specific application. Our efforts are part of CANcer Distributed Learning Environment (CANDLE), enhancing the model comparison capabilities in cancer research and driving the development of more effective strategies for drug response prediction and optimization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMuPAOY8Oz": {
    "title": "Positional Description Matters for Transformers Arithmetic",
    "volume": "review",
    "abstract": "Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=q38SZkUmUh": {
    "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "volume": "review",
    "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future research, we will release FreshQA after blind review and commit to updating it at regular intervals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.3,
    "authors": []
  },
  "https://openreview.net/forum?id=PEuO8WTolW": {
    "title": "STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning",
    "volume": "review",
    "abstract": "Recently, multi-objective optimization (MOO) problems have received increasing attention due to their wide range of applications in various fields, such as machine learning (ML), operations research, and many engineering applications. However, MOO algorithm design remains in its infancy and many existing MOO methods suffer from unsatisfactory convergence performance. To address this challenge, in this paper, we propose an algorithm called STIMULUS (**ST**ochastic path-**I**ntegrated **MUL**ti-graident rec**U**rsive e**S**timator), a new and robust approach for solving MOO problems. Different from the traditional methods, STIMULUS introduces a simple yet powerful recursive framework for updating stochastic gradient estimates. This methodology improves convergence performance by reducing the variance in multi-gradient estimation, leading to more stable convergence paths. In addition, we introduce an enhanced version of STIMULUS, termed STIMULUS-M, which incorporates the momentum term to further expedite convergence. One of the key contributions of this paper is the theoretical analysis for both STIMULUS and STIMULUS-M, where we establish an $\\mathcal{O}(\\frac{1}{T})$ convergence rate for both methods, which implies a state-of-the-art sample complexity of $O\\left(n+\\sqrt{n}\\epsilon^{-1}\\right)$ under non-convexity settings. In the case where the objectives are strongly convex, we further establish a linear convergence rate of $\\mathcal{O}(e^{-\\mu T})$ of the proposed methods, which suggests an even stronger $\\mathcal{O}\\left(n+ \\sqrt{n} \\ln ({\\mu/\\epsilon})\\right)$ sample complexity. Moreover, to further alleviate the periodic full gradient evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced versions with adaptive batching called STIMULUS$^+$/STIMULUS-M$^+$ and provide their theoretical analysis. Our extensive experimental results verify the efficacy of our proposed algorithms and their superiority over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4eJDMjYZZG": {
    "title": "Language Model Detectors Are Easily Optimized Against",
    "volume": "review",
    "abstract": "The fluency and general applicability of large language models (LLMs) has motivated significant interest in detecting whether a piece of text was written by a language model. While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems. In this paper, we demonstrate a data-efficient attack that fine-tunes language models to confuse existing detectors, leveraging recent developments in reinforcement learning of language models. We use the 'human-ness' score (often just a log probability) of various open-source and commercial detectors as a reward function for reinforcement learning, subject to a KL-divergence constraint that the resulting model does not differ significantly from the original. For a 7B parameter Llama-2 model, fine-tuning for under a day reduces the AUROC of the OpenAI RoBERTa-Large detector from 0.84 to 0.62, while perplexity on OpenWebText increases from 8.7 to only 9.0; with a larger perplexity budget, we reduce AUROC to 0.30 (worse than random), with a perplexity increase to 9.9. Similar to traditional adversarial attacks, we find that this increase in 'detector evasion' generalizes to other detectors not used during training. In light of our empirical results, we advise against continued reliance on LLM-generated text detectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WnqD3EiylC": {
    "title": "The Representation Jensen-Shannon Divergence",
    "volume": "review",
    "abstract": "Statistical divergences quantify the difference between probability distributions, thereby allowing for multiple uses in machine-learning. However, a fundamental challenge of these quantities is their estimation from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose a divergence inspired by the Jensen-Shannon divergence which avoids the estimation of the probability density functions. Our approach embeds the data in an reproducing kernel Hilbert space (RKHS) where we associate data distributions with uncentered covariance operators in this representation space. Therefore, we name this measure the representation Jensen-Shannon divergence (RJSD). We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without an explicit mapping to the RKHS. We provide consistency convergence results for the proposed estimator. Moreover, we demonstrate that this quantity is a lower bound on the Jensen-Shannon divergence, leading to a variational approach to estimate it with theoretical guarantees. We leverage the proposed divergence to train generative networks, where our method mitigates mode collapse and encourages samples diversity. Additionally, RJSD surpasses other state-of-the-art techniques in multiple two-sample testing problems, demonstrating superior performance and reliability in discriminating between distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0chJTSbci": {
    "title": "Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models",
    "volume": "review",
    "abstract": "If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot's own training data. We propose SuSIE, a method that leverages an image editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller attains. Specifically, we fine-tune InstructPix2Pix on robot data such that it outputs a hypothetical future observation given the robot's current observation and a language command. We then use the same robot data to train a low-level goal-conditioned policy to reach a given image observation. We find that when these components are combined, the resulting system exhibits robust generalization capabilities. The high-level planner utilizes its Internet-scale pre-training and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization than conventional language-conditioned policies. We demonstrate that this approach solves real robot control tasks involving novel objects, distractors, and even environments, both in the real world and in simulation. The project website can be found at http://subgoal-image-editing.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=9jMoHuqjfg": {
    "title": "Learning to Reach Goals via Diffusion",
    "volume": "review",
    "abstract": "Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=VmnWoLbzCS": {
    "title": "LUMOS: Towards Language Agents that are Unified, Modular, and Open Source",
    "volume": "review",
    "abstract": "In this paper, we present LUMOS, **L**anguage agents with **U**nified formats, **M**odular design, and **O**pen **S**ource LLMs. LUMOS features a modular architecture consisting of planning, grounding, and execution modules built based on open-source LLMs such as LLAMA-2. The planning module decomposes a task into a sequence of high-level subgoals; the grounding module then grounds the generated subgoals to a series of low-level actions that can then be executed by the execution module. To obtain high-quality annotations for training these modules, we leverage LLMs to convert ground-truth intermediate reasoning steps in existing benchmarks into a unified format that can be used in the LUMOS framework. LUMOS achieves competitive or superior performance compared to the state of the art on a variety of complex interactive tasks. We observe: (1) LUMOS is competitive with the LLM agents that are 2 − 4× larger on maths tasks, and outperforms GPT-4/3.5-based agents on complex QA and web agent tasks; (2) LUMOS shows superior performance against open-source agent baseline formulations including chain-of-thoughts fine-tuning and unmodularized training; (3) LUMOS surpasses larger LLM-based agents on an unseen interactive task, WebShop, and achieves 5-10 reward improvement over domain-specific agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=TB5THwq1sq": {
    "title": "Physics Informed Neurally Constructed ODE Networks (PINeCONes)",
    "volume": "review",
    "abstract": "Recently, there has been a growing interest in using neural networks to approximate the solutions of partial differential equations (PDEs). Physics-informed neural networks (PINNs) have emerged as a promising framework for parameterizing PDE solutions using deep neural networks. However, PINNs often rely on memory-intensive optimizers to attain reasonable accuracy and can encounter training difficulties due to issues such as stiffness in the gradient flow of the loss. To address these challenges, we propose a novel network architecture that combines neural ordinary differential equations (ODEs) with physics-informed constraints in the loss function. In this approach, the dynamics within a neural ODE are expanded to include a system of ODEs whose solution provides the partial derivatives governing our PDE system. We call this architecture PINECONEs: physics-informed neurally constructed ODE networks. We evaluate the approach using simple but canonical PDEs from the literature to illustrate its potential. Our results show that training requires fewer iterations than previous approaches to achieve higher accuracy when using first-order optimization methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.6,
    "authors": []
  },
  "https://openreview.net/forum?id=MqEQbvPvkE": {
    "title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US",
    "volume": "review",
    "abstract": "In policy research, one of the most critical analytic tasks is to estimate the causal effect of a policy-relevant shift to the distribution of a continuous exposure/treatment on an outcome of interest. We call this problem *shift-response function* (SRF) estimation. Existing neural network methods involving robust causal-effect estimators lack theoretical guarantees and practical implementations for SRF estimation. Motivated by a key policy-relevant question in public health, we develop a neural network method and its theoretical underpinnings to estimate SRFs with robustness and efficiency guarantees. We then apply our method to data consisting of 68 million individuals and 27 million deaths across the U.S. to estimate the causal effect from revising the US National Ambient Air Quality Standards (NAAQS) for $\\text{PM}_{2.5}$ from 12 to 9 $\\mu g/m^3$ . This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate, for the first time, the reduction in deaths that would result from this anticipated revision using causal methods for SRFs. Our proposed method, called Targeted Regularization for Exposure Shifts with Neural Networks (TRESNET), contributes to the neural network literature for causal inference in two ways: first, it proposes a targeted regularization loss with theoretical properties that ensure double robustness and achieves asymptotic efficiency specific for SRF estimation; second, it enables loss functions from the exponential family of distributions to accommodate non-continuous outcome distributions (such as hospitalization or mortality counts). We complement our application with benchmark experiments that demonstrate TRESNET's broad applicability and competitiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=FItPCl4uEc": {
    "title": "Efficient Transfer Learning from Arbitrary Pre-Trained Models",
    "volume": "review",
    "abstract": "Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=LCQ7YTzgRQ": {
    "title": "On the Role of Edge Dependency in Graph Generative Models",
    "volume": "review",
    "abstract": "In this work, we introduce a novel evaluation framework for generative models of graphs, emphasizing the importance of model-generated graph *overlap* (Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We delineate a hierarchy of graph generative models categorized into three levels of complexity: edge independent, node independent, and fully dependent models. This hierarchy encapsulates a wide range of prevalent methods. We derive theoretical bounds on the number of triangles and other short-length cycles producible by each level of the hierarchy, contingent on the model overlap. We provide instances demonstrating the asymptotic optimality of our bounds. Furthermore, we introduce new generative models for each of the three hierarchical levels, leveraging dense subgraph discovery (Gionis & Tsourakakis, 2015). Our evaluation, conducted on real-world datasets, focuses on assessing the output quality and overlap of our proposed models in comparison to other popular models. Our results indicate that our simple, interpretable models provide competitive baselines to popular generative models. Through this investigation, we aim to propel the advancement of graph generative models by offering a structured framework and robust evaluation metrics, thereby facilitating the development of models capable of generating accurate and edge-diverse graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=1IaoWBqB6K": {
    "title": "DiffDock-Pocket: Diffusion for Pocket-Level Docking with Sidechain Flexibility",
    "volume": "review",
    "abstract": "When a small molecule binds to a protein, the 3D structure of the protein and its function change. Understanding this process, called molecular docking, can be crucial in areas such as drug design. Recent learning-based attempts have shown promising results at this task, yet lack features that traditional approaches support. In this work, we close this gap by proposing DiffDock-Pocket, a diffusion-based docking algorithm that is conditioned on a binding target to predict ligand poses only in a specific binding pocket. On top of this, our model supports receptor flexibility and predicts the position of sidechains close to the binding site. Empirically, we improve the state-of-the-art in site-specific-docking on the PDBBind benchmark. Especially when using in-silico generated structures, we achieve more than twice the performance of current methods while being more than 20 times faster than other flexible approaches. Although the model was not trained for cross-docking to different structures, it yields competitive results in this task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=tHHzfZSP6T": {
    "title": "How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks",
    "volume": "review",
    "abstract": "Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing simple logical operations. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we aim to assess in this paper \"how capable can a transformer become?\". Specifically, we train Transformer models on a data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) Transformers can learn compositional structures from the training data and generalize to exponentially or even combinatorially many functions; (2) Composing functions by generating intermediate outputs is more effective at generalizing to unseen compositions, compared to generating no intermediate outputs; (3) The training data has a significant impact on the model's ability to compose unseen combinations of functions; (4) The attention layers in the latter half of the Transformer seem critical to compositionality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=QlFlo5533z": {
    "title": "Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation",
    "volume": "review",
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) has emerged as a popular method to protect personally identifiable information in deep learning (DL) applications. Unfortunately, DP-SGD's per-sample gradient clipping and uniform noise addition during training can significantly degrade model utility. To enhance the model's utility, researchers proposed various adaptive/dynamic DP-SGD methods by adapting the noise multiplier and clipping threshold. However, we examined and discovered that these established techniques result in greater privacy leakage or lower accuracy than the traditional DP-SGD method, or a lack of evaluation on a complex data set such as CIFAR100. To address these limitations, we propose an automatic DP-SGD (Auto DP-SGD). Our method automates clipping threshold estimation based on the DL model's total gradient norm and scales the gradients of each training sample instead of simply clipping them without losing gradient information or requiring an additional privacy budget. This helps to improve the algorithm's utility while using a less privacy budget. To further improve accuracy, we introduce automatic noise multiplier decay mechanisms to decrease the noise multiplier after every epoch. Finally, we develop closed-form mathematical expressions using the truncated concentrated differential privacy (tCDP) accountant, which offers a straightforward and tight privacy-bound analysis for automatic noise multiplier and automatic clipping estimation. Through extensive experimentation, we demonstrate that Auto DP-SGD outperforms existing state-of-the-art (SOTA) classification results in privacy and accuracy on various benchmark datasets. We also show that privacy can be improved by lowering the scale factor and using learning rate schedulers without significantly reducing privacy. Moreover, we also explain how to select the best Auto DP-SGD variant without additional privacy leakage. Specifically, Auto DP-SGD, when used with a step noise multiplier (Auto DP-SGD-S), improves accuracy by 3.20\\%, 1.57\\%, 6.73\\%, and 1.42\\% for the MNIST, CIFAR10, CIFAR100, and AG News Corpus datasets, respectively. Furthermore, we achieve a substantial reduction in the privacy budget ($\\epsilon$) of 94.9\\%, 79.16\\%, 67.36\\%, and 53.37\\% for the corresponding data sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cdng6X2Joq": {
    "title": "A New, Physics-Based Continuous-Time Reinforcement Learning Algorithm with Performance Guarantees",
    "volume": "review",
    "abstract": "We introduce a new, physics-based continuous-time reinforcement learning (CT-RL) algorithm for control of affine nonlinear systems, an area that enables a plethora of well-motivated applications. Based on fundamental input/output control mechanisms, our approach uses reference command input (RCI) as probing noise in learning. With known physical dynamics of the environment, and by leveraging on the Kleinman algorithm structure, our RCI-based CT-RL algorithm not only provides theoretical guarantees such as learning convergence, solution optimality, and closed-loop stability, but also well-behaved dynamic system responses with data efficiency during learning. Our results are therefore an advance from the two currently available classes of approaches to CT-RL. The first school of adaptive dynamic programming (ADP) methods features elegant theoretical results stemming from adaptive and optimal control. Yet, they have not been shown effectively synthesizing meaningful controllers. The second school of fitted value iteration (FVI) methods, also the state-of-the-art (SOTA) deep RL (DRL) design, has shown impressive learning solutions, yet theoretical guarantees are still to be developed. We provide several evaluations to demonstrate that our RCI-based design leads to new, SOTA CT-RL results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ogV88XPnK6": {
    "title": "Graph neural processes and their application to molecular functions",
    "volume": "review",
    "abstract": "Neural processes (NPs) are models for meta-learning which output uncertainty estimates. So far, most studies of NPs have focused on low-dimensional datasets of highly-correlated tasks. While these homogeneous datasets are useful for benchmarking, they may not be representative of realistic transfer-learning. In particular, applications in scientific research may prove especially challenging due to the potential novelty of meta-testing tasks. Drug discovery is one such research area that is characterized by sparse datasets of many functions on a shared molecular space. In this paper, we study the application of graph NPs to drug discovery with DOCKSTRING, a diverse dataset of docking scores. Graph NPs show competitive performance in few-shot learning tasks relative to supervised learning baselines common in chemoinformatics, as well as alternative techniques for transfer learning and meta-learning. In order to increase meta-generalization to divergent test functions, we propose fine-tuning strategies that adapt the parameters of NPs. We find that adaptation can substantially increase NPs' regression performance while maintaining good calibration of uncertainty estimates. Finally, we present a Bayesian optimization experiment which showcases the potential advantages of NPs over GPs in molecular applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=kXHEBK9uAY": {
    "title": "Simple Hierarchical Planning with Diffusion",
    "volume": "review",
    "abstract": "Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a \"jumpy\" planning strategy at the high level, which allows it to have a larger receptive field but at a lower computational cost—a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of training and planning speed compared to the non-hierarchical Diffuser as well as other hierarchical planning methods. Moreover, we explore our model's generalization capability, particularly on how our method improves generalization capabilities on compositional out-of-distribution tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=fj2E5OcLFn": {
    "title": "Stochastic Gradient Descent for Gaussian Processes Done Right",
    "volume": "review",
    "abstract": "We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly on the problem or on a reduced-order version of it. However, stochastic gradient descent has recently gained traction in the Gaussian process literature, driven largely by its successes in deep learning. In this paper, we show that this approach when done right---by which we mean using specific insights from the optimisation and kernel communities---is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, conveniently implementable with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies. We then show that the new method is highly competitive: our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apart from conjugate gradients, variational Gaussian process approximations, and a prior version of stochastic gradient descent tailored for Gaussian processes. On a molecular binding affinity prediction task, our method places Gaussian process regression on par in terms of performance with graph neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.4,
    "authors": []
  },
  "https://openreview.net/forum?id=gkfUvn0fLU": {
    "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
    "volume": "review",
    "abstract": "Large language models are typically aligned with human preferences by optimizing reward models (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to *overoptimization*, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally given by the Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=c56TWtYp0W": {
    "title": "GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings",
    "volume": "review",
    "abstract": "Analyzing multivariate time series is important in many domains. However, it has been difficult to learn robust and generalizable representations within multivariate datasets due to complex inter-channel relationships and dynamic shifts. In this paper, we introduce a novel approach for learning spatiotemporal structure and using it to improve the application of transformers to timeseries datasets. Our framework learns a set of group tokens, and builds an instance-specific group embedding (GE) layer that assigns input tokens to a small number of group tokens to incorporate structure into learning. We then introduce a novel architecture, Group-Aware transFormer (GAFormer), which incorporates both spatial and temporal group embeddings to achieve state-of-the-art performance on a number of time-series classification and regression tasks. In evaluations on a number of diverse timeseries datasets, we show that GE on its own can provide a nice enhancement to a number of backbones, and that by coupling spatial and temporal group embeddings, the GAFormer can outperform the existing baselines. Finally, we show how our approach discerns latent structures in data even without information about the spatial ordering of channels, and yields a more interpretable decomposition of spatial and temporal structure underlying complex multivariate datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=90QOM1xB88": {
    "title": "Improved order analysis and design of exponential integrator for diffusion models sampling",
    "volume": "review",
    "abstract": "Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be attributed to the absence of essential order conditions. By reformulating the differential equations in DMs and capitalizing on the theory of exponential integrators, we propose refined EI solvers that fulfill all the order conditions, which we designate as Refined Exponential Solver (RES). Utilizing these improved solvers, RES exhibits more favorable error bounds theoretically and achieves superior sampling efficiency and stability in practical applications. For instance, a simple switch from the single-step DPM-Solver++ to our order-satisfied numerical scheme when NFE$=9$, results in a reduction of numerical defects by 25.2 and FID improvement of 25.4 (16.77 vs 12.51) on a pre-trained ImageNet diffusion model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=OCx7dp58H1": {
    "title": "Setting the Record Straight on Transformer Oversmoothing",
    "volume": "review",
    "abstract": "Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that can oversmooth the input. This causes their performance to quickly saturate as model depth increases. A natural question is: How can Transformers achieve success given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Finally, we describe a simple way to reparameterize the weights of the Transformer update equations to ensure that oversmoothing does not occur. Compared to other solutions for oversmoothing, our approach does not require a new architecture, or any additional hyperparameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=vrhrhGrdXm": {
    "title": "KBFormer: A Transformer-based Diffusion Model of Structured Entities with Heterogeneous Properties",
    "volume": "review",
    "abstract": "We present a generative attention-based architecture that models structured entities comprising different property types, such as numerical, categorical, string, and composite. This architecture handles such heterogeneous data through a mixed continuous-discrete diffusion process over the properties. This flexible framework is capable of modeling entities with arbitrary hierarchical properties, enabling applications to structured KB entities and tabular data. Experiments with a device KB and a nuclear physics dataset demonstrate the model's ability to learn representations useful for entity completion in diverse settings. This has many downstream use cases, including modeling numerical properties with high accuracy - critical for science applications. An additional benefit of the model is its inherent probabilistic nature, enabling predictions accompanied by uncertainties. These critical capabilities are leveraged in a nuclear physics dataset to make precise predictions on various properties of nuclei",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=DP4NkPZOpD": {
    "title": "Bridging Sequence and Structure: Latent Diffusion for Conditional Protein Generation",
    "volume": "review",
    "abstract": "Protein design encompasses a range of challenging tasks, including protein folding, inverse folding, and protein-protein docking. Despite significant progress in this domain, many existing methods address these tasks separately, failing to adequately leverage the joint relationship between protein sequence and three-dimensional structure. In this work, we propose a novel generative modeling technique to capture this joint distribution. Our approach is based on a diffusion model applied on a geometrically-structured latent space, obtained through an encoder that produces roto-translational invariant representations of the input protein complex. It can be used for any of the aforementioned tasks by using the diffusion model to sample the conditional distribution of interest. Our experiments show that our method outperforms competitors in protein docking and is competitive with state-of-the-art for protein inverse folding. Exhibiting a single model that excels on on both sequence-based and structure-based tasks represents a significant advancement in the field and paves the way for additional applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aZCPl3ZvR": {
    "title": "Why is SAM Robust to Label Noise?",
    "volume": "review",
    "abstract": "Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding SAM's label noise robustness requires a departure from characterizing the robustness of minimas lying in ``flatter'' regions of the loss landscape. In particular, the peak performance occurs with early stopping, far before the loss converges. We decompose SAM's robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian. The first can be observed in linear logistic regression where SAM provably upweights the gradient contribution from clean examples. Although this explicit upweighting is also observable in neural networks, when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance. We infer that SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian. We theoretically derive the explicit regularization induced by this Jacobian effect in two layer linear networks. Motivated by our analysis, we see that cheaper alternatives to SAM that explicitly induce these regularization effects largely recover the benefits even in deep networks trained on real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fAGEAEQvRr": {
    "title": "Gradient descent for matrix factorization: Understanding large initialization",
    "volume": "review",
    "abstract": "In deep learning practice, large random initialization is commonly used. Understanding the behavior of gradient descent (GD) with such initialization is both crucial and challenging. This paper focuses on a simplified matrix factorization problem, delving into the dynamics of GD when using large initialization. Leveraging a novel signal-to-noise ratio argument and an inductive argument, we offer a detailed trajectory analysis of GD from the initial point to the global minima. Our insights indicate that even with a large initialization, GD can exhibit incremental learning, which coincides with experimental observations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=xnUIMz5u2s": {
    "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?",
    "volume": "review",
    "abstract": "*Thinking is for Doing.*\" Humans can infer other people's mental states from observations–an ability called Theory-of-Mind (ToM)–and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Qz9BT4mpM": {
    "title": "Predicting the Performance of Foundation Models via Agreement-on-the-line",
    "volume": "review",
    "abstract": "Estimating out-of-distribution performance is critical to safely deploying machine learning models. Recently, Baek et al. showed that the phenomenon ``agreement-on-the-line'' can be a reliable method for predicting OOD accuracy of models in an ensemble consisting largely of CNNs trained from scratch. However, it is now increasingly common to lightly fine-tune foundation models, and it is unclear whether such fine-tuning is sufficient to produce enough diversity in models for such agreement-based methods to work properly. In this paper, we develop methods for reliably applying agreement-on-the-line-based performance estimation to fine-tuned foundation models. In particular, we first study the case of fine-tuning a single foundation model, where we extensively study how different types of randomness (linear head initialization, hyperparameter selection, data subsetting, and data shuffling) contribute to the agreement on the line of the resulting model sets; we find, somewhat surprisingly, that it is typically possible to obtain strong agreement via random initialization of the linear head alone. Next, we study how \\emph{multiple} foundation models, pretrained on different data sets but fine-tuned on the same task, may or may not produce agreement; we show, again rather surprisingly, that the diversity of such models is already sufficient and not too disparate for them to all lie on the same agreement lines. In total, these methods enable reliable and efficient estimation of OOD accuracy for fine-tuned foundation models, without leveraging any labeled OOD data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=xxaEhwC1I4": {
    "title": "Revisiting the Last-Iterative Convergence of Stochastic Gradient Methods",
    "volume": "review",
    "abstract": "In the past several years, the convergence of the last iterate of the Stochastic Gradient Descent (SGD) algorithm has triggered people's great interest due to its good performance in practice but lack of theoretical understanding. For Lipschtiz and convex functions, different works have established the optimal $O(\\log(1/\\delta)\\log T/\\sqrt{T})$ or $O(\\sqrt{\\log(1/\\delta)/T})$ high-probability convergence rates for the final iterate, where $T$ is the time horizon and $\\delta$ is the failure probability. However, to prove these bounds, all the existing works are limited to compact domains, and almost all of them also require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. Besides this important question, there are still lots of theoretical problems lacking an answer. For example, compared with the last iterate convergence of SGD for non-smooth problems, only very few results for smooth optimization have yet been developed. Additionally, the existing results are all limited to a single objective and the standard Euclidean norm. It still remains unclear whether the last-iterative convergence can be provably extended to wider composite optimization and non-Euclidean norms. In this work, to address the issues mentioned above, we revisit the last-iterative convergence of stochastic gradient methods and provide the first unified way to prove the convergence rates both in expectation and in high probability to accommodate general domains, composite objectives, non-Euclidean norms, Lipschitz conditions, smoothness and (strong) convexity simultaneously",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ajG8vLTHh5": {
    "title": "Learning transferrable and interpretable representation for brain network",
    "volume": "review",
    "abstract": "The human brain is a complex, dynamic network, which is commonly studied using functional magnetic resonance imaging (fMRI) and modeled as network of Regions of interest (ROIs) for understanding various brain functions. Recent studies predominantly utilize Graph Neural Networks (GNNs) to learn the brain network representation based on the functional connectivity (FC) profile, typically falling into two main categories. The Fixed-FC approaches, utilize the FC profile which represents the linear temporal relation within the brain network, is limited by failing to capture the informative temporal dynamics of brain activity. On the other hand, the Dynamic-FC approaches, modeling the evolving FC profile over time, often exhibit less satisfactory performance due to challenges in handling the inherent noisy nature of fMRI data. In this study, to address these challenges, we propose Brain Masked Auto-Encoder (BrainMAE) for learning representations directly from fMRI time-series data. Our approach incorporates two essential components—an embedding-based graph attention mechanism and a self-supervised masked autoencoding framework. These components empower our model to capture the rich temporal dynamics of brain activity while maintaining resilience to the inherent noise in fMRI data. Our experiments demonstrate that BrainMAE consistently outperforms several established baseline models by a significant margin in three distinct downstream tasks. Finally, leveraging the model's inherent interpretability, our analysis of model-generated representations reveals intriguing findings that resonate with ongoing research in the field of neuroscience",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=O8ouVV8PjF": {
    "title": "CNN Kernels Can Be the Best Shapelets",
    "volume": "review",
    "abstract": "Shapelets and CNN are two typical approaches to model time series. Shapelets aim at finding a set of sub-sequences that extract feature-based interpretable shapes, but may suffer from accuracy and efficiency issues. CNN performs well by encoding sequences with a series of hidden representations, but lacks interpretability. In this paper, we demonstrate that shapelets are essentially equivalent to a specific type of CNN kernel with a squared norm and pooling. Based on this finding, we propose ShapeConv, an interpretable CNN layer with its kernel serving as shapelets to conduct time-series modeling tasks in both supervised and unsupervised settings. By incorporating shaping regularization, we enforce the similarity for maximum interpretability. We also find human knowledge can be easily injected to ShapeConv by adjusting its initialization and model performance is boosted with it. Experiments show that ShapeConv can achieve state-of-the-art performance on time-series benchmarks without sacrificing interpretability and controllability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=ZlZakr4GYK": {
    "title": "COTIC: Embracing Non-uniformity in Event Sequence Data via Multilayer Continuous Convolution",
    "volume": "review",
    "abstract": "Massive samples of event sequences occur in various domains, including e-commerce, healthcare, and finance. There are two main challenges regarding modeling such data: methodological and computational. The methodological peculiarity for event sequences is their non-uniformity and sparsity. These requirements make time series models unsuitable. The computational challenge arises from a large amount of available data and the significant length of each sequence. Thus, the problem requires complex and efficient models. Existing solutions include large recurrent and transformer neural network architectures. On top of existing blocks, their authors introduce specific intensity functions defined at each moment. However, due to their parametric nature, these continuous-time-aware intensities represent only a limited class of event sequences. We propose the COTIC method based on an efficient continuous convolution neural network suitable for the non-uniform occurrence of events in time. In COTIC, dilations and multi-layer architecture efficiently handle long-term dependencies between events. Furthermore, the model provides intensity dynamics in continuous time --- including self-excitement encountered in practice. Being the first to introduce multiple continuous convolution layers that can handle arbitrary complex dependencies via MLP-modeled convolutions, we obtain these properties. When benchmarked against existing models, the COTIC consistently outperforms them, especially in predicting the next event time and type: it has the average rank of 2.125 vs. 3.688 of the primal competitor. Additionally, its ability to produce effective embeddings showcases its potential for a range of downstream tasks, as produced embeddings are sufficient to solve various downstream tasks, e.g., 0.459 vs. 0.452 baseline accuracy on a 4-label age bin prediction for transactions dataset. The code of the proposed method is available at https://anonymous.4open.science/r/COTIC-F47D/README.md",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YfZMfrpEnl": {
    "title": "Stochastic Vision Transformers with Wasserstein Distance-Aware Attention",
    "volume": "review",
    "abstract": "Self-supervised learning is one of the most promising approaches to acquiring knowledge from limited labeled data. Despite the substantial advancements made in recent years, self-supervised models have posed a challenge to practitioners, as they do not readily provide insight into the model's confidence and uncertainty. Tackling this issue is no simple feat, primarily due to the complexity involved in implementing techniques that can make use of the latent representations learned during pre-training without relying on explicit labels. Motivated by this, we introduce a new stochastic vision transformer that integrates uncertainty and distance awareness into self-supervised learning (SSL) pipelines. Instead of the conventional deterministic vector embedding, our novel stochastic vision transformer encodes image patches into elliptical Gaussian distributional embeddings. Notably, the attention matrices of these stochastic representational embeddings are computed using Wasserstein distance-based attention, effectively capitalizing on the distributional nature of these embeddings. Additionally, we propose a regularization term based on Wasserstein distance for both pre-training and fine-tuning processes, thereby incorporating distance awareness into latent representations. We perform extensive experiments across different tasks such as in-distribution generalization, out-of-distribution detection, dataset corruption, semi-supervised settings, and transfer learning to other datasets and tasks. Our proposed method achieves superior accuracy and calibration, surpassing the self-supervised baseline in a wide range of experiments on a variety of datasets. Our code is in the supplementary material",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=RtOTTdWbZd": {
    "title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment",
    "volume": "review",
    "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also provide a theoretical justification supporting the design of our loss function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=YBSEwwveMr": {
    "title": "Score-Based Multimodal Autoencoders",
    "volume": "review",
    "abstract": "Multimodal Variational Autoencoders (VAEs) represent a promising group of generative models that facilitate the construction of a tractable posterior within the latent space, given multiple modalities. Daunhawer et al. (2022) demonstrate that as the number of modalities increases, the generative quality of each modality declines. In this study, we explore an alternative approach to enhance the generative performance of multimodal VAEs by jointly modeling the latent space of unimodal VAEs using score-based models (SBMs). The role of the SBM is to enforce multimodal coherence by learning the correlation among the latent variables. Consequently, our model combines the superior generative quality of unimodal VAEs with coherent integration across different modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=WPZ2yPag4K": {
    "title": "Fine-Tuning Language Models for Factuality",
    "volume": "review",
    "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. However, language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations', which can harmfully perpetuate myths and misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we leverage two key recent innovations in NLP to fine-tune language models to be more factual without human labeling, targeting more open-ended generation settings than past work. First, several recent works have proposed methods for scoring the factuality of open-ended text derived from consistency with an external knowledge base or simply a large model's confidence scores. Second, the Direct Preference Optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from preference rankings generated by either automated criterion significantly improves the factuality of Llama-2 on held-out topics (percent of generated claims that are correct) compared with existing RLHF procedures or decoding strategies targeted at factuality, showing over 50% and 20-30% error reduction for biographies and medical questions respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=4Hf5pbk74h": {
    "title": "Improving classifier decision boundaries using nearest neighbors",
    "volume": "review",
    "abstract": "In this paper, we show that neural networks are not learning optimal decision boundaries. Decision boundaries go through areas of low training data density. They are impacted by few training samples which can easily lead to overfitting. We show that performing a weighted average of the prediction of a sample and its nearest neighbors' (computed in latent space) leads to a variety of minor favorable outcomes. In our evaluation, we employ various self-trained and pre-trained convolutional neural networks to show that our approach improves (i) resistance to label noise, (ii) robustness against adversarial attacks, (iii) classification accuracy, and to some degree even (iv) interpretability. While improvements are not necessarily large in all four areas, our approach is conceptually simple, i.e., improvements come without any modification to network architecture, training procedure or dataset. Furthermore, they are in stark contrast to prior works that often require trade-offs among the four objectives or provides only non-actionable insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=f3g5XpL9Kb": {
    "title": "LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures",
    "volume": "review",
    "abstract": "Joint embedding (JE) architectures have emerged as a promising avenue for ac- quiring transferable data representations. A key obstacle to using JE methods, however, is the inherent challenge of evaluating learned representations without access to a downstream task, and an annotated dataset. Without efficient and re- liable evaluation, it is difficult to iterate on architectural and training choices for JE methods. In this paper, we introduce LiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the quality of representations within JE archi- tectures. Our metric addresses several shortcomings of recent approaches based on feature covariance rank by discriminating between informative and uninforma- tive features. In essence, LiDAR quantifies the rank of the Linear Discriminant Analysis (LDA) matrix associated with the surrogate SSL task—a measure that intuitively captures the information content as it pertains to solving the SSL task. We empirically demonstrate that LiDAR significantly surpasses naive rank based approaches in its predictive power of optimal hyperparameters. Our proposed cri- terion presents a more robust and intuitive means of assessing the quality of rep- resentations within JE architectures, which we hope facilitates broader adoption of these powerful techniques in various domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=t8D9JxEn0J": {
    "title": "Malcom-PSGD: Inexact Proximal Stochastic Gradient Descent for Communication Efficient Decentralized Machine Learning",
    "volume": "review",
    "abstract": "Recent research indicates that frequent model communication stands as a major bottleneck to the efficiency of decentralized machine learning (ML), particularly for large-scale and over-parameterized neural networks (NNs). In this paper, we introduce \\textsc{Malcom-PSGD}, a new decentralized ML algorithm that strategically integrates gradient compression techniques with model sparsification. \\textsc{Malcom-PSGD} leverages proximal stochastic gradient descent to handle the non-smoothness resulting from the $\\ell_1$ regularization in model sparsification. Furthermore, we adapt vector source coding and dithering-based quantization for compressed gradient communication of sparsified models. Our analysis shows that decentralized proximal stochastic gradient descent with compressed communication has a convergence rate of $\\mathcal{O}\\left(\\ln(t)/\\sqrt{t}\\right)$ assuming a diminishing learning rate and where $t$ denotes the number of iterations. Numerical results verify our theoretical findings and demonstrate that our method reduces communication costs by approximately $75$\\% when compared to the state-of-the-art method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=lOwkOIUJtx": {
    "title": "Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling",
    "volume": "review",
    "abstract": "High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=kz5igjl04W": {
    "title": "Approaching an unknown communication system by latent space exploration and causal inference",
    "volume": "review",
    "abstract": "This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values with methods inspired by causal inference into an approach we call causal disentanglement with extreme values (CDEV) and show that this method yields insights for model interpretability. With this, we can test for what properties of unknown data the model encodes as meaningful, using it to glean insight into the communication system of sperm whales (Physeter macrocephalus), one of the most intriguing and understudied animal communication systems. The network architecture used has been shown to learn meaningful representations of speech; here, it is used as a learning mechanism to decipher the properties of another vocal communication system in which case we have no ground truth. The proposed methodology suggests that sperm whales encode information using the number of clicks in a sequence, the regularity of their timing, and audio properties such as the spectral mean and the acoustic regularity of the sequences. Some of these findings are consistent with existing hypotheses, while others are proposed for the first time. We also argue that our models uncover rules that govern the structure of units in the communication system and apply them while generating innovative data not shown during training. This paper suggests that an interpretation of the outputs of deep neural networks with causal inference methodology can be a viable strategy for approaching data about which little is known and presents another case of how deep learning can limit the hypothesis space. Finally, the proposed approach can be extended to other architectures and datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=FcxwXnYXWh": {
    "title": "Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate",
    "volume": "review",
    "abstract": "Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit local superlinear rate of $\\mathcal{O}((1/\\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is $\\mathcal{O}(\\tau d)$ where $\\tau \\le d$ is the size of the window and $d$ is the problem dimension reducing the $\\mathcal{O}(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for any limited-memory quasi-Newton method. In this work, we close this gap by presenting a Limited-memory Greedy BFGS (LG-BFGS) method that can achieve an explicit non-asymptotic superlinear rate. We incorporate displacement aggregation, i.e., decorrelating projection, in post-processing gradient variations, together with a basis vector selection scheme on variable variations, which $\\textit{greedily}$ maximizes a progress measure of the Hessian estimate to the true Hessian. Their combination allows past curvature information to remain in a sparse subspace while yielding a valid representation of the full history. Interestingly, our established $\\textit{non-asymptotic}$ superlinear convergence rate demonstrates an explicit trade-off between the convergence speed and memory requirement, which to our knowledge, is the first of its kind. Numerical results corroborate our theoretical findings and demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Tigr1kMDZy": {
    "title": "Overthinking the Truth: Understanding how Language Models Process False Demonstrations",
    "volume": "review",
    "abstract": "Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some \"critical layer\", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=VPl472SKaB": {
    "title": "Transforming Smallholder Farmers Support with an AI-Powered FAQbot: A Comparison of Techniques",
    "volume": "review",
    "abstract": "Access to sufficient information on desired agricultural practices, such as planting period, when to apply fertiliser, how to transport grains, etc. is of utmost importance in the agricultural industry as it directly affects farm yields. The responses to these questions are closed domain, therefore leading to the development of a question-answering conversational bot (FAQbot) that can provide the appropriate responses immediately. This study undertakes a comparative analysis of three distinct methodologies for constructing a FAQbot. These approaches encompass the development of a generative-based chatbot employing BERT and GPT-2, the creation of an intent classification model leveraging PyTorch and the Natural Language Toolkit (NLTK) libraries, and the implementation of an information retrieval-based model utilising pre-trained Large Language Models (LLMs) using Langchain. Our methodological framework includes the transformation of a FAQ dataset into formats suitable for chatbot training, specifically CSV and JSON. Notably, the retrieval-based method surpassed the generative-based and intent classification methods by consistently providing precise answers for every question in the database, irrespective of rephrasing or reframing. Keywords: Agriculture, FAQBot, LLMs, Natural Language Processing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=p6hIAEHwSp": {
    "title": "Efficient Subgraph Rule Induction via Tree Folding in Differentiable Logic Programming",
    "volume": "review",
    "abstract": "Differentiable inductive logic programming techniques have proven effective at learning logic rules from noisy datasets; however, existing algorithms incur pernicious trade-offs between rule expressivity and scalability to large problems. Forward-chaining ILP algorithms can learn arbitrary rules, but their memory requirements scale exponentially with problem size. Backwards-chaining ILP algorithms address this limitation but do so with loss of generality by imposing the restrictive constraint that rules must be expressible as ensembles of independent chain-like Horn clauses. In this paper we present FUSE-ILP, a technique that relaxes this chain-like constraint and enables the differentiable evaluation of a restricted class of subgraph-like rules. Our method extends TensorLog-inspired backwards-chaining ILP techniques with branch masking and leaf grouping, which enable tree-like rule evaluation and \"folding\" of these trees into subgraphs. We demonstrate that this formulation allows our algorithm to learn more expressive rules than previous backwards-chaining algorithms while retaining a similar computational cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=DWJr05rymY": {
    "title": "Estimating Unknown Population Sizes Using Hypergeometric Maximum Likelihood",
    "volume": "review",
    "abstract": "The multivariate hypergeometric distribution describes the fundamental process of sampling without replacement from a discrete population of elements divided into multiple categories. Despite the hypergeometric distribution's long history, the literature has not yet addressed the problem of maximum likelihood estimation when both the size of the total population and its constituent categories are unknown. Here, we show that this estimation challenge can be solved by maximizing the hypergeometric likelihood, even in the presence of severe under-sampling. We extend this approach to capture data generating processes where the ground-truth high-dimensional distribution is conditional on a continuous latent variable using the variational autoencoder framework, and validate the resulting model using simulated datasets. In a practical use case, we demonstrate that our method can recover the true number of gene transcripts present in a cell from sparse single-cell genomics data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=dEz3ge8QSo": {
    "title": "Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity",
    "volume": "review",
    "abstract": "Robust Markov Decision Processes (MDPs) and risk-sensitive MDPs are both powerful tools for making decisions in the presence of uncertainties. Previous efforts have aimed to establish their connections, revealing equivalences in specific formulations. This paper introduces a new formulation for risk-sensitive MDPs, which assesses risk in a slightly different manner compared to the classical Markov risk measure \\cite{ruszczynski2010risk}, and establishes its equivalence with a class of regularized robust MDP (RMDP) problems, including the standard RMDP as a special case. Leveraging this equivalence, we further derive the policy gradient theorem for both problems, proving gradient domination and global convergence of the exact policy gradient method under the tabular setting with direct parameterization. This forms a sharp contrast to the Markov risk measure, known to be potentially non-gradient-dominant \\cite{huang2021convergence}. We also propose a sample-based offline learning algorithm, namely the robust fitted-Z iteration (RFZI), for a specific regularized RMDP problem with a KL-divergence regularization term (or equivalently the risk-sensitive MDP with an entropy risk measure). We showcase its streamlined design and less stringent assumptions due to the equivalence and analyze its sample complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=IpJIq3iwMH": {
    "title": "Federated Binary Matrix Factorization using Proximal Optimization",
    "volume": "review",
    "abstract": "Identifying informative components in binary data is an essential task in many research areas, including life sciences, social sciences, natural language processing, and recommendation systems. Boolean matrix factorization (BMF) is a family of methods that performs this task by efficiently factorizing the data into its constituent parts. In real-world settings, the data is often distributed across stakeholders and required to stay private, prohibiting the straightforward application of BMF. To adapt BMF to this context, we approach the problem from a federated-learning perspective, while building on a state-of-the-art continuous binary matrix factorization relaxation to BMF that enables efficient gradient-based optimization. We propose to only share the relaxed component matrices, which are aggregated centrally using a proximal operator that regularizes for binary outcomes. We show the convergence of our federated proximal gradient descent algorithm and provide differential privacy guarantees. Our extensive empirical evaluation demonstrates that our algorithm outperforms, in terms of quality and efficacy, federation schemes of state-of-the-art BMF methods on a diverse set of real-world and synthetic data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=17pVDnpwwl": {
    "title": "Feature Learning in Infinite Depth Neural Networks",
    "volume": "review",
    "abstract": "Empirical studies have consistently demonstrated that increasing the size of neural networks often yields superior performance in practical applications. However, there is a lack of consensus regarding the appropriate scaling strategy, particularly when it comes to increasing the depth of neural networks. In practice, excessively large depths can lead to model performance degradation. In this paper, we introduce Depth-$\\mu$P, a principled approach for depth scaling, allowing for the training of arbitrarily deep architectures while maximizing feature learning and diversity among nearby layers. Our method involves dividing the contribution of each residual block and the parameter update by the square root of the depth. Through the use of Tensor Programs, we rigorously establish the existence of a limit for infinitely deep neural networks under the proposed scaling scheme. This scaling strategy ensures more stable training for deep neural networks and guarantees the transferability of hyperparameters from shallow to deep models. To substantiate the efficacy of our scaling method, we conduct empirical validation on neural networks with depths up to $2^{10}$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=wTtDgucL7h": {
    "title": "Two Facets of SDE Under an Information-Theoretic Lens: Generalization of SGD via Training Trajectories and via Terminal States",
    "volume": "review",
    "abstract": "Stochastic differential equations (SDEs) have been shown recently to well characterize the dynamics of training machine learning models with SGD. This provides two opportunities for better understanding the generalization behaviour of SGD through its SDE approximation. Firstly, viewing SGD as full-batch gradient descent with Gaussian gradient noise allows us to obtain trajectories-based generalization bound using the information-theoretic bound from Xu & Raginsky (2017). Secondly, assuming mild conditions, we estimate the steady-state weight distribution of SDE and use information-theoretic bounds from Xu & Raginsky (2017) and Negrea et al. (2019) to establish terminal-state-based generalization bounds. Our proposed bounds have some advantages, notably the trajectories-based bound outperforms results in Wang & Mao (2022), and the terminal-state-based bound exhibits a fast decay rate comparable to stability-based bounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=iGDWZFc7Ya": {
    "title": "Language Models Linearly Represent Sentiment",
    "volume": "review",
    "abstract": "Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank. We further uncover the mechanisms that involve this direction, highlighting the roles of a small subset of attention heads and neurons. Finally, we discover a phenomenon which we term the summarization motif: sentiment is not solely represented on emotionally charged words, but is additionally summarised at intermediate positions without inherent sentiment, such as punctuation and names. We show that in Stanford Sentiment Treebank zero-shot classification, 76\\% of above-chance classification accuracy is lost when ablating the sentiment direction, nearly half of which (36\\%) is due to ablating the summarized sentiment direction exclusively at comma positions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNzy9bRDvG": {
    "title": "Improved Techniques for Training Consistency Models",
    "volume": "review",
    "abstract": "Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models, and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we borrow Pseudo-Huber losses from robust statistics. Additionally, we introduce a new noise schedule for the consistency training objective, and propose a new curriculum for total discretization steps. Collectively, these modifications enable consistency models to achieve FID scores of 2.62 and 3.91 on CIFAR-10 and ImageNet $64\\times 64$ respectively in a single sampling step. These scores mark a 3.3$\\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.28 and 3.64, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and state-of-the-art generative models on both datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPHcEpGvF8": {
    "title": "Demystifying Poisoning Backdoor Attacks from a Statistical Perspective",
    "volume": "review",
    "abstract": "The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understanding applies to both discriminative and generative models. We also demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=MQ4JJIYKkh": {
    "title": "Concept Alignment as a Prerequisite for Value Alignment",
    "volume": "review",
    "abstract": "Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values---and is even capable of valuing---depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment---agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.2,
    "authors": []
  },
  "https://openreview.net/forum?id=RgELE1dQXx": {
    "title": "Learning to make adherence-aware advice",
    "volume": "review",
    "abstract": "As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.3,
    "authors": []
  },
  "https://openreview.net/forum?id=7VPTUWkiDQ": {
    "title": "Provable Compositional Generalization for Object-Centric Learning",
    "volume": "review",
    "abstract": "Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.3,
    "authors": []
  },
  "https://openreview.net/forum?id=yzfi15eVI7": {
    "title": "iHyperTime: Interpretable Time Series Generation with Implicit Neural Representations",
    "volume": "review",
    "abstract": "Implicit neural representations (INRs) have emerged as a powerful tool that provides an accurate and resolution-independent encoding of data. Their robustness as general approximators has been shown across diverse data modalities, such as images, video, audio, and 3D scenes. However, little attention has been given to leveraging these architectures for time series data. Addressing this gap, we propose an approach for time series generation based on two novel architectures: TSNet, an INR network for interpretable trend-seasonality time series representation, and iHyperTime, a hypernetwork architecture that leverages TSNet for time series generalization and synthesis. Through evaluations of fidelity and usefulness metrics, we demonstrate that iHyperTime outperforms current state-of-the-art methods in challenging scenarios that involve long or irregularly sampled time series, while performing on par on regularly sampled data. Furthermore, we showcase iHyperTime fast training speed, comparable to the fastest existing methods for short sequences and significantly superior for longer ones. Finally, we empirically validate the quality of the model's unsupervised trend-seasonality decomposition by comparing against the established STL method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=07xuZw59uB": {
    "title": "Bridging the Fairness Divide: Achieving Group and Individual Fairness in Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have emerged as a powerful tool for analyzing and learning from complex data structured as graphs, demonstrating remarkable effectiveness in various applications, such as social network analysis, recommendation systems, and drug discovery. However, despite their impressive performance, the fairness problem has increasingly gained attention as a crucial aspect to consider. Existing research on fairness in graph learning primarily emphasizes either group fairness or individual fairness; however, to the best of our knowledge, none of these studies comprehensively address both individual and group fairness simultaneously. In this paper, we propose a new concept of individual fairness within groups and a novel framework named Fairness for Group and Individual (FairGI), which considers both group fairness and individual fairness within groups in the context of graph learning. FairGI employs the similarity matrix of individuals to achieve individual fairness within groups, while leveraging adversarial learning to address group fairness in terms of both Equal Opportunity and Statistical Parity. The experimental results demonstrate that our approach not only outperforms other state-of-the-art models in terms of group fairness and individual fairness within groups, but also exhibits excellent performance in population-level individual fairness, while maintaining comparable prediction accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvFhCUPjtI": {
    "title": "Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs",
    "volume": "review",
    "abstract": "We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with \\eft for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dxm7eil2HT": {
    "title": "RoCA: A Robust Method to Discover Causal or Anticausal Relation by Noise Injection",
    "volume": "review",
    "abstract": "Understanding whether the data generative process is causal or anticausal is important for algorithm design. It helps machine learning practitioners understand whether semi-supervised learning should be employed for real-world learning tasks. In many cases, existing causal discovery methods cannot be adaptable to this task, as they struggle with scalability and are ill-suited for high-dimensional perceptual data such as images. In this paper, we propose a method that detects whether the data generative process is causal or anticausal. Our method is robust to label errors and is designed to handle both large-scale and high-dimensional datasets effectively. Both theoretical analyses and empirical results on a variety of datasets demonstrate the effectiveness of our proposed method in determining the causal or anticausal direction of the data generative process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=f1xnBr4WD6": {
    "title": "Cycle Consistency Driven Object Discovery",
    "volume": "review",
    "abstract": "Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. While these approaches have shown promise in certain scenarios, they still exhibit certain limitations. First, they rely on architectural priors which can be unreliable and usually require meticulous engineering to identify the correct objects. Second, there has been a notable gap in investigating the practical utility of these representations in downstream tasks. To address the first limitation, we introduce a method that explicitly optimizes the constraint that each object in a scene should be associated with a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. By integrating these consistency objectives into various existing slot-based object-centric methods, we showcase substantial improvements in object-discovery performance. These enhancements consistently hold true across both synthetic and real-world scenes, underscoring the effectiveness and adaptability of the proposed approach. To tackle the second limitation, we apply the learned object-centric representations from the proposed method to two downstream reinforcement learning tasks, demonstrating considerable performance enhancements compared to conventional slot-based and monolithic representation learning methods. Our results suggest that the proposed approach not only improves object discovery, but also provides richer features for downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=wQCPHxtzGV": {
    "title": "RF-POLICY: Rectified Flows are Adaptive Decision Makers",
    "volume": "review",
    "abstract": "Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making but comes at the cost of significantly slower inference due to the recursion in the diffusion process. However, in real-world scenarios, states that require multi-modal decision-making are rare, and the huge consumption of diffusion models is not necessary for most cases. It inspires us to design efficient policy generators that can wisely allocate computation for different contexts. To address this challenge, we propose RF-POLICY (Rectified Flow-Policy), an imitation learning algorithm based on Rectified Flow, a recent advancement in flow-based generative modeling~\\citep{liu2022flow}. RF-POLICY adopts probability flow ordinary differential equations (ODEs) for diverse policy generation, with the learning principle of following straight trajectories as much as possible. We uncover and leverage a surprisingly intriguing advantage of these flow-based models over previous diffusion models: their training objective indicates the uncertainty of a certain state, and when the state is uni-modal, they automatically reduce to one-step generators since the probability flows admit straight lines. Therefore, RF-POLICY is naturally an adaptive decision maker, offering rapid inference without sacrificing diversity. Our comprehensive empirical evaluation shows that \\ours{}, to the best of our knowledge, is the first algorithm to achieve high performance across all dimensions, including success rate, behavioral diversity, and inference speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=1qzUPE5QDZ": {
    "title": "Rectifying Group Irregularities in Explanations for Distribution Shift",
    "volume": "review",
    "abstract": "It is well-known that real-world changes constituting distribution shift adversely affect model performance. How to characterize those changes in an interpretable manner is poorly understood. Existing techniques take the form of shift explanations that elucidate how samples map from the original distribution toward the shifted one by reducing the disparity between the two distributions. However, these methods can introduce group irregularities, leading to explanations that are less feasible and robust. To address these issues, we propose Group-aware Shift Explanations (GSE), an explanation method that leverages worst-group optimization to rectify group irregularities. We demonstrate that GSE not only maintains group structures, but can improve feasibility and robustness over a variety of domains by up to 20% and 25% respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=SzV37yefM4": {
    "title": "Contrastive Decoding Improves Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=RVrINT6MT7": {
    "title": "Sufficient conditions for offline reactivation in recurrent neural networks",
    "volume": "review",
    "abstract": "During periods of quiescence, such as sleep, neural activity in many brain circuits resembles that observed during periods of task engagement. However, the precise conditions under which task-optimized networks can autonomously reactivate the same network states responsible for online behavior is poorly understood. In this study, we develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. We demonstrate mathematically that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation based on angular velocity cues. Overall, our work provides theoretical support for modeling offline reactivation as an emergent consequence of task optimization in noisy neural circuits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=N2ggBozsss": {
    "title": "Centroid-Based Learning for Malware Detection and Novel Family Identification",
    "volume": "review",
    "abstract": "Detecting out-of-distribution (OOD) data categories while preserving the accuracy of existing classifications is a pressing challenge in many domains. Conventional methods often falter when tasked with generating or identifying new data classes, especially when dealing with graphical data and the problem of graph isomorphism. In this paper, we present a novel approach, the Graph Centroid Model (GCM), which combines Control Flow Graphs (CFGs) with a Graph Neural Network (GNN) to address this challenge effectively. The GCM assigns embeddings produced by a GNN to partitions that support the classification of both known and new classes, even those absent during training. Our approach quantifies the differences between samples in the embedding space, enabling the identification of multiple distinct representations of familiar classes during training while providing a straightforward mechanism for detecting new classes during testing. This not only improves classification accuracy but also offers intuitive visualizations that provide valuable insights.When applied to a benchmark malware dataset (BODMAS), our method reveals structural commonalities among samples from different malware families while effectively discerning new, previously unseen classes based on their distance from learned representatives in the embedding space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=Abr7dU98ME": {
    "title": "Forward Learning of Graph Neural Networks",
    "volume": "review",
    "abstract": "Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks. However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning neural networks. Examples of such constraints include the storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains neural networks by performing two forward passes over positive and negative data. Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for GNNs, which avoids the constraints imposed by BP via an effective layer-wise local forward training. ForwardGNN extends the original FF to deal with graph data and GNNs, and makes it possible to operate without generating negative inputs (hence no longer forward-forward). Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors. Extensive experiments involving five real-world datasets and three representative GNNs show the effectiveness and generality of the proposed forward graph learning framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.5,
    "authors": []
  },
  "https://openreview.net/forum?id=Rd1pjx84rk": {
    "title": "Size Generalization of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective",
    "volume": "review",
    "abstract": "We investigate size-induced distribution shifts in graphs and assess their impact on the ability of graph neural networks (GNNs) to generalize to larger graphs relative to the training data. Existing literature presents conflicting conclusions on GNNs' size generalizability, primarily due to disparities in application domains and underlying assumptions concerning size-induced distribution shifts. Motivated by this, we take a data-driven approach: we focus on real biological datasets and seek to characterize the types of size-induced distribution shifts. Diverging from prior approaches, we adopt a spectral perspective and identify that spectrum differences induced by size are related to differences in subgraph patterns (e.g., average cycle lengths). We further find that common GNNs cannot capture these subgraph patterns, resulting in performance decline when testing on larger graphs. Based on these spectral insights, we introduce and compare three model-agnostic strategies aimed at making GNNs aware of important subgraph patterns to enhance their size generalizability: self-supervision, augmentation, and size-insensitive attention. Our empirical results reveal that all strategies enhance GNNs' size generalizability, with simple size-insensitive attention surprisingly emerging as the most effective method. Notably, this strategy substantially enhances graph classification performance on large test graphs, which are 2-10 times larger than the training graphs, resulting in an improvement in F1 scores by up to 8%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=P50qJuu4IY": {
    "title": "Self-Supervised Learning with the Matching Gap",
    "volume": "review",
    "abstract": "Contrastive learning (CL) is a fundamental paradigm in self-supervised learning. CL methods rely on a loss that nudges the features of various views from one image to stay closer, while pulling away those drawn from different images. Such a loss favors invariance: feature representations of the same perturbed image should collapse to the same vector, while remaining far enough from those of any other image. Although intuitive, CL leaves room for trivial solutions, and has a documented propensity to collapse representations for very different images. This is often mitigated by using a very large variety of augmentations. In this work, we address this tension by introducing a different loss, the matching gap. Given a set of $n$ images transformed in two different ways, the matching gap is the difference between the mean cost (e.g. a squared distance), in representation space, of the $n$ paired images, and the optimal matching cost obtained by running an optimal matching solver across these two families of $n$ images. The matching gap naturally mitigates the problem of data augmentation invariance, since it can be zero without requiring features from the same image to collapse. We implement the matching gap using the Sinkhorn algorithm and show that it can be easily differentiated using Danskin's theorem. In practice, we show that we can learn competitive features, even without extensive data augmentations: Using only cropping and flipping, we achieve 74.2% top-1 accuracy with a ViT-B/16 on ImageNet-1k, to be compared to 72.9% for I-JEPA (Assran et al., 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Rry1SeSOQL": {
    "title": "COMPARATOR: Reference-free machine translation evaluation by inter-system comparison",
    "volume": "review",
    "abstract": "Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem—producing an absolute translation-quality score. However, this approach has two limitations: i) the scores lack interpretability and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent. In practice, we often care about whether a new MT system is better or worse than some competitors. In addition, reference-free MT evaluation is increasingly practical and necessary. However, these two practical considerations have yet to be jointly explored. In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem. Given the source sentence and a pair of translations, our system predicts which translation is better. In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior performance by merely using indirect supervision from natural language inference and weak supervision from our synthetic data. In the context of reference-free evaluation, our system, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors our system marks state-of-the-art against reference-free as well as reference-based baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=Rh4DmXaf8R": {
    "title": "Multi-timestep models for Model-based Reinforcement Learning",
    "volume": "review",
    "abstract": "In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered environment, highlighting the potential of our approach in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.5,
    "authors": []
  },
  "https://openreview.net/forum?id=TgTJvwMEax": {
    "title": "Embedding Improves Neural Regularizers for Inverse Problems",
    "volume": "review",
    "abstract": "Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering. Recent machine learning techniques based on proximal and diffusion-based methods have shown some promising results. However, as we show in this work, they can also face challenges when applied to some exemplary problems. We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions. The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector. We demonstrate the merit of this approach on several exemplary and common inverse problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.5,
    "authors": []
  },
  "https://openreview.net/forum?id=9wSWiavGwU": {
    "title": "SwapTransformer: Highway Overtaking Tactical Planner Model via Imitation Learning on OSHA Dataset",
    "volume": "review",
    "abstract": "This paper investigates the high-level decision-making problem in highway scenarios regarding lane changing and over-taking other slower vehicles. In particular, this paper aims to improve the Travel Assist feature for automatic overtaking and lane changes on highways. About 9 million samples including lane images and other dynamic objects are collected in simulation. This data; Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this challenge. To solve this problem, an architecture called SwapTransformer is designed and implemented as an imitation learning approach on the OSHA dataset. Moreover, auxiliary tasks such as future points and car distance network predictions are proposed to aid the model in better understanding the surrounding environment. The performance of the proposed solution is compared with a multi-layer perceptron (MLP) and multi-head self-attention networks as baselines in a simulation environment. We also demonstrate the performance of the model with and without auxiliary tasks. All models are evaluated based on different metrics such as time to finish each lap, number of overtakes, and speed difference with speed limit. The evaluation shows that the SwapTransformer model outperforms other models in different traffic densities in the inference phase",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=NV6rn7j5p5": {
    "title": "GEO: Generative Engine Optimization",
    "volume": "review",
    "abstract": "The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), has the potential to generate accurate and personalized responses, and is rapidly replacing traditional search engines like Google and Bing. Generative engines typically satisfy queries by synthesizing information from multiple sources and summarizing them with the help of LLMs. While this shift significantly improves user utility and generative search engine traffic, it results in a huge challenge for the third stakeholder - website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, the right tools should be provided to ensure that creator economy is not severely disadvantaged. To address this, we introduce generative engine optimization (GEO), a novel paradigm to aid content creators in improving their visibility. In this work, we propose several optimizations that can be applied to improve the visibility of content. To evaluate and compare different GEO methods, we propose a benchmark encompassing diverse user queries from multiple domains and settings, along with relevant sources needed to answer those queries. Through rigorous experiments on the proposed benchmark, we demonstrate different GEO methods involving well-designed textual enhancements, are capable of boosting source visibility by up to 40% in Generative engines responses. We find several insights that aid content creators -- for example, adding citations and quotations significantly improves visibility. We also discover that these optimizations are domain dependent, thus requiring a change in the nature of the optimization based on the source. Our work opens a new frontier in the field of information discovery systems, with profound implications for both developers of Generative enginess and content creators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=rINBD8jPoP": {
    "title": "Curriculum reinforcement learning for quantum architecture search under hardware errors",
    "volume": "review",
    "abstract": "The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a solution where a circuit architecture is first fixed, and then the individual gate parameters are optimized in an external loop to solve a task. However, the performance optimization can be intractable, and the overall performance, as well as the optimization, highly depends on the initially fixed circuit's architecture. Several quantum architecture search (QAS) algorithms have been developed to automatically select the best circuit architecture. In the case of parameter optimization, it has been observed that noise effects dramatically influence the optimizer performance and final outcomes, and this is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. In this work, we tackle this issue. To do so, we first significantly improve the computational time to simulate realistic quantum circuits by employing pauli transfer matrix formalism in the Pauli-Liouville basis by fusing gates with their respective noise models and values. Then, we devise a curriculum-based reinforcement learning QAS (CRLQAS) algorithm optimized to tackle the challenges of realistic VQA deployment by introducing (i) a 3-D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation algorithm as an optimizer for faster convergence. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across noiseless and noisy environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=tnBaiidobu": {
    "title": "Does CLIP's generalization performance mainly stem from high train-test similarity?",
    "volume": "review",
    "abstract": "Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's OOD performance, and other properties of the training data must drive CLIP to learn more generalizable representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION (¼ of its original size) on which CLIP can be trained to match its original OOD performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=AnuHbhwv9Q": {
    "title": "Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift",
    "volume": "review",
    "abstract": "Designing deep neural network classifiers that perform robustly on distributions differing from the available training data is an active area of machine learning research. However, out-of-distribution generalization for regression---the analogous problem for modeling continuous targets---remains relatively unexplored. To tackle this problem, we return to first principles and analyze how the closed-form solution for ordinary least squares (OLS) regression is sensitive to covariate shift. We characterize the out-of-distribution risk of the OLS model in terms of the eigenspectrum decomposition of the source and target data. We then use this insight to propose a method for adapting the weights of the last layer of a pre-trained neural regression model to perform better on input data originating from a different distribution. We demonstrate how this lightweight spectral adaptation procedure can improve out-of-distribution performance in a suite of both synthetic and real-world experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=H4A9e8HvIn": {
    "title": "A Unified Approach for Online Continuous DR-Submodular Maximization",
    "volume": "review",
    "abstract": "This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\\alpha$-regret bounds or have better $\\alpha$-regret bounds than the state of the art, where $\\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this optimization area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4Ya9RkEEW": {
    "title": "Fast Sampling via De-randomization for Discrete Diffusion Models",
    "volume": "review",
    "abstract": "Diffusion models have emerged as powerful tools for high-quality data generation, such as image generation. Despite its success in continuous spaces, discrete diffusion models, which apply to domains such as texts and natural languages, remain under-studied and often suffer from slow generation speed. In this paper, we propose a novel de-randomized diffusion process, which leads to an accelerated algorithm for discrete diffusion models. Our technique significantly reduces the number of function evaluations (i.e., calls to the score network), making the sampling process much faster. Furthermore, we introduce a continuous-time (i.e., infinite-step) sampling algorithm that can provide even better sample qualities than its discrete-time (finite-step) counterpart. Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality over existing methods for discrete diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=UOdz9U4fxg": {
    "title": "A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions",
    "volume": "review",
    "abstract": "An important aspect of developing reliable deep learning systems is devising strategies that make these systems robust to adversarial attacks. There is a long line of work that focuses on developing defenses against these attacks, but recently, researchers have begun to study ways to reverse engineer the attack process. This allows us to not only defend against several attack models, but also classify the threat model. However, there is still a lack of theoretical guarantees for the reverse engineering process. Current approaches that give any guarantees are based on the assumption that the data lies in a union of linear subspaces, which is not a valid assumption for more complex datasets. In this paper, we propose a novel framework for reverse engineering of deceptions which supposes that the clean data lies in the range of a GAN. To classify the signal and attack, we jointly solve a GAN inversion problem and a block-sparse recovery problem. The core contribution of this paper is to provide for the first time deterministic linear convergence guarantees for this problem. We also empirically demonstrate the merits of the proposed approach on several nonlinear datasets as compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=8ZW3oLNE0c": {
    "title": "SEArch: A Self-Evolving Framework for Network Architecture Optimization",
    "volume": "review",
    "abstract": "This paper studies a fundamental network optimization problem that finds a network architecture with optimal performance (low losses) under given resource budgets (small parameter size and/or fast inference). Different from existing network optimization approaches such as network pruning, knowledge distillation (KD), and network architecture search (NAS), in this work we introduce a novel self-evolving pipeline to perform network optimization. In this framework, a simple network iteratively and adaptively modifies its structures by using the guidance from the teacher network, until it reaches the resource budget. An attention module is introduced to transfer the knowledge from teacher network to student network. The splitting edge scheme helps the student model find an optimal macro architecture. The proposed framework combines the advantages of pruning, KD, and NAS, and hence, can efficiently generate networks with flexible structure and desirable performance. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrated that our framework achieves state-of-the-art performance in this network architecture optimization task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=5HCnKDeTws": {
    "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
    "volume": "review",
    "abstract": "While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=CZ6XT5phWW": {
    "title": "Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance",
    "volume": "review",
    "abstract": "Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' \\cite{kojima2022large}). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. PRoMTd consistently outperforms traditional zero-shot approaches on all the datasets. Notably, we observe an absolute improvement of 10\\% on the complex MATHS dataset and 5\\% on the code generation task on HumanEval. In addition, we also showed that the rewritten prompt can provide better interpretability of how the LLM resolves each test instance, which can potentially be leveraged as a defense mechanism against adversarial prompting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.3,
    "authors": []
  },
  "https://openreview.net/forum?id=xcMmebCT7s": {
    "title": "Learning to design protein-protein interactions with enhanced generalization",
    "volume": "review",
    "abstract": "Discovering mutations enhancing protein-protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct PPIRef, the largest and non-redundant dataset of 3D protein-protein interactions, enabling effective large-scale learning. Second, we leverage PPIRef to pre-train PPIformer, a new SE(3)-equivariant model, generalizing across diverse protein-binder variants. We fine-tune PPIformer to predict effects of mutations on protein-protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new PPIformer approach by outperforming other state-of-the-art methods on the new non-leaking splits of the standard labeled PPI mutational data and independent case studies optimizing a human antibody against SARS-CoV-2 and increasing staphylokinase thrombolytic activity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=3pgJNIx3gc": {
    "title": "AlphaFold Distillation for Protein Design",
    "volume": "review",
    "abstract": "Inverse protein folding, the process of designing sequences that fold into a specific 3D structure, is crucial in bio-engineering and drug discovery. Traditional methods rely on experimentally resolved structures, but these cover only a small fraction of protein sequences. Forward folding models like AlphaFold offer a potential solution by accurately predicting structures from sequences. However, these models are too slow for integration into the optimization loop of inverse folding models during training. To address this, we propose using knowledge distillation on folding model confidence metrics, such as pTM or pLDDT scores, to create faster and end-to-end differentiable distilled model. This model can then be used as a structure consistency regularizer in training the inverse folding model. Our technique is versatile and can be applied to other design tasks, such as sequence-based protein infilling. Experimental results show that our method outperforms non-regularized baselines, yielding up to 3\\% improvement in sequence recovery and up to 45\\% improvement in protein diversity while maintaining structural consistency in generated sequences. Anonymized code for this work is available at https://anonymous.4open.science/r/AFDistill-28C3",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=ayLov67GxD": {
    "title": "Video2Demo: Grounding Videos in State-Action Demonstrations",
    "volume": "review",
    "abstract": "Vision-language demonstrations provide a natural way for users to teach robots everyday tasks. However, for effective imitation learning, these demonstrations must be perceptually grounded in the robot's states and actions. While prior works train task-specific models to predict state-actions from images, these often require extensive manual annotation and fail to generalize to complex scenes. In this work, we leverage pre-trained instruction-following Vision-Language Models (VLMs) that have shown impressive zero-shot generalization for detailed caption generation. However, VLM captions, while descriptive, fail to maintain the structure and temporal consistency required to track object states over time. We propose a novel approach, Video2Demo, that uses GPT-4 to interactively query a generative VLM to construct temporally coherent state-action sequences. These sequences are in turn fed into a language model to generate robot task code that faithfully imitates the demonstration. We evaluate on a large-scale human activity dataset, EPIC-Kitchens, and show that Video2Demo outperforms pure VLM-based approaches, resulting in accurate robot task code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=77N93tc3o5": {
    "title": "Deep Independent Vector Analysis",
    "volume": "review",
    "abstract": "We introduce a deep multivariate latent variable model, Deep Independent Vector Analysis (DeepIVA), for learning linked and identifiable disentangled representations across multiple data modalities by unifying multidataset independent subspace analysis (MISA) and identifiable variational autoencoders (iVAE). DeepIVA aims to leverage hidden linkage information via the MISA loss to attain latent cross-modal alignment while leveraging the identifiability properties of the iVAE to ensure proper unimodal disentanglement. We propose a more strict set of performance measures, and demonstrate that DeepIVA can successfully recover nonlinearly mixed multimodal sources on multiple linked synthetic datasets compared with iVAE and MISA. We then apply DeepIVA on a large multimodal neuroimaging dataset, and show that DeepIVA can reveal linked nonlinear imaging sources associated with phenotype measures including age and sex",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=LkQoiVp6XG": {
    "title": "G-Local Attention Graph Pooling for Graph Classification",
    "volume": "review",
    "abstract": "Graph pooling is an essential operation in Graph Neural Networks that reduces the size of an input graph while preserving its core structural properties. This compression operation improves the learned representation of the graph, yielding to a performance boost on downstream tasks. Existing pooling methods find a compressed representation considering the Global Topological Structures (e.g., cliques, stars, clusters) or Local information at node level (e.g., top-$k$ informative nodes). However, there is a lack of an effective graph pooling method that integrates both Global and Local properties of the graph. To this end, we propose a two-channel Global-Local Attention Pooling (GLA-Pool) layer that exploits the aforementioned graph properties, generating more robust graph representations. The GLA-Pool can be integrated into any GNN-based architectures. Further, we propose a smart data augmentation technique to enrich small-scale datasets. Exhaustive experiments on eight publicly available graph classification benchmarks, under standard metrics, show that GLA-Pool significantly outperforms thirteen state-of-the-art models on six datasets while being on par for the remaining two. The code will be available at this link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=CgkAGcp9lk": {
    "title": "Compositional Search of Stable Crystalline Structures in Multi-Component Alloys Using Generative Diffusion Models",
    "volume": "review",
    "abstract": "Exploring the vast composition space of multi-component alloys presents a challenging task for both ab initio (first principles) and experimental methods due to the time-consuming procedures involved. This ultimately impedes the discovery of novel, stable materials that may display exceptional properties. Here, the Crystal Diffusion Variational Autoencoder (CDVAE) model is adapted to characterize the stable compositions of a well studied multi-component alloy, NiFeCr, with two distinct crystalline phases known to be stable across its compositional space. To this end, novel extensions to CDVAE were proposed, enhancing the model's ability to reconstruct configurations from their latent space within the test set by approximately 30% . A fact that increases a model's probability of discovering new materials when dealing with various crystalline structures. Afterwards, the new model is applied for materials generation, demonstrating excellent agreement in identifying stable configurations within the ternary phase space when compared to first principles data. Finally, a computationally efficient framework for inverse design is proposed, employing Molecular Dynamics (MD) simulations of multi- component alloys with reliable interatomic potentials, enabling the optimization of materials property across the phase space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=EhrzQwsV4K": {
    "title": "L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation",
    "volume": "review",
    "abstract": "Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store. These components enable L2MAC to generate virtually unbounded code structures, bypassing the constraints of the finite context window while producing code that fulfills complex user-specified requirements. We empirically show that L2MAC succeeds in generating large code bases for system design tasks where other coding methods fall short in implementing user requirements and provide insight into the reasons for this performance gap",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.2,
    "authors": []
  },
  "https://openreview.net/forum?id=ICuUgRLp4C": {
    "title": "Learning High-Order Relationships of Brain Regions",
    "volume": "review",
    "abstract": "Discovering reliable and informative interactions among brain regions from functional magnetic resonance imaging (fMRI) signals is essential in neuroscientific predictions of cognition. Most of the current methods fail to accurately characterize those interactions because they only focus on pairwise connections and overlook the high-order relationships of brain regions. We delve into this problem and argue that these high-order relationships should be maximally informative and minimally redundant (MIMR). However, identifying such high-order relationships is challenging and highly under-explored. Methods that can be tailored to our context are also non-existent. In response to this gap, we propose a novel method named HyBRiD that aims to extract MIMR high-order relationships from fMRI data. HyBRiD employs a Constructor to identify hyperedge structures, and a Weighter to compute a weight for each hyperedge. HyBRiD achieves the MIMR objective through an innovative information bottleneck framework named multi-head drop-bottleneck with theoretical guarantees. Our comprehensive experiments demonstrate the effectiveness of our model. In terms of the quality of hyperedges measured by the CPM metric, our model outperforms the state-of-the-art predictive model by an average of 12.1%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.5,
    "authors": []
  },
  "https://openreview.net/forum?id=NeWiiF6KLB": {
    "title": "Stabilized E(n)-Equivariant Graph Neural Networks-assisted Generative Models",
    "volume": "review",
    "abstract": "Due to its simplicity and computational efficiency, the E(n)-equivariant graph neural network (EGNN) [Satorras, et al., ICML, 2021] has been used as the backbone of equivariant normalizing flows (ENF), equivariant diffusion model (EDM), and beyond for Euclidean equivariant generative modeling. Nonetheless, it has been observed that ENF and EDM can be unstable; in this paper, we investigate the source of their instability by performing a sensitivity analysis of their backpropagation. Based on our theoretical analysis, we propose a regularization to stabilize and improve ENF and EDM. Experiments on benchmark datasets demonstrate that the regularized ENF outperforms the baseline model in terms of stability and computational efficiency by a remarkable margin. Furthermore, our results show that the proposed regularization can stabilize EDM and improve its performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=c93SBwz1Ma": {
    "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger is embedded in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0\\% across the six benchmark tasks on GPT-4. We also demonstrate the interpretability of BadChain by showing that the relationship between the trigger and the backdoor reasoning step can be well-explained based on the output of the backdoored model. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=xNn2nq5kiy": {
    "title": "Plan-based Prompting Improves Literature Review Generation",
    "volume": "review",
    "abstract": "We explore the zero-shot abilities of recent large language models (LLMs) for the task of writing the literature review of a scientific research paper conditioned on its abstract and the content of related papers. We propose and examine a novel strategy for literature review generation with an LLM in which we first generate a plan for the review, and then use it to generate the actual text. While modern LLMs can easily be trained or prompted to condition on all abstracts of papers to be cited to generate a literature review without such intermediate plans, our empirical study shows that these intermediate plans improve the quality of generated literature reviews over vanilla zero-shot generation. Furthermore, we also create a new test corpus consisting of recent arXiv papers (with full content) posted after both open-sourced and closed-sourced LLMs that were used in our study were released. This allows us to ensure that our zero-shot experiments do not suffer from test set contamination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjJcJhIzYx": {
    "title": "Neural Rankers for Code Generation via Inter-Cluster Modeling",
    "volume": "review",
    "abstract": "Code Large Language Models (CodeLLMs) have ushered in a new era of code generation advancements. However, selecting the best solutions from among all possible CodeLLM solutions remains a challenge. Previous methods frequently overlooked the intricate functional similarities and interactions between clusters, resulting in suboptimal results. In this work, we introduce SRank, a novel rerank- ing strategy for selecting the best solution from code generation that focuses on modeling inter-cluster relationship. By quantifying the functional overlap between clusters, our approach provides a better ranking strategy of code solutions. Empir- ical results show that our method achieves a remarkable results on pass@1 score. For instance, on the Human-Eval benchmark, we achieve 69.66% in pass@1 with Codex002, 75.31% for WizardCoder, 53.99% for StarCoder and 60.55% for Code- Gen, which surpass the state-of-the-arts solution ranking methods, such as CodeT and Coder-Reviewer on the same CodeLLMs with significant margin (≈ 6.1% improvement on average). Comparing to the random sampling method, we can achieve an average improvement of ≈ 23.07% on Human-Eval. Even in scenar- ios with limited test inputs, our approach demonstrates robustness and superiority, marking a new benchmark in code generation reranking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=bLhqPxRy3G": {
    "title": "Linear programming using diagonal linear networks",
    "volume": "review",
    "abstract": "Linear programming has played a crucial role in shaping decision-making, resource allocation, and cost reduction in various domains. In this paper, we investigate the application of overparametrized neural networks and their implicit bias in solving linear programming problems. Specifically, our findings reveal that training diagonal linear networks with gradient descent, while optimizing the squared $L_2$-norm of the slack variable, leads to solutions for entropically regularized linear programming problems. Remarkably, the strength of this regularization depends on the initialization used in the gradient descent process. We analyze the convergence of both discrete-time and continuous-time dynamics and demonstrate that both exhibit a linear rate of convergence, requiring only mild assumptions on the constraint matrix. For the first time, we introduce a comprehensive framework for solving linear programming problems using diagonal neural networks. We underscore the significance of our discoveries by applying them to address challenges in basis pursuit and optimal transport problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=M0QHJI9OuF": {
    "title": "TROJFAIR: TROJAN FAIRNESS ATTACKS",
    "volume": "review",
    "abstract": "Deep learning models have been incorporated into high-stakes sectors, including healthcare diagnosis, loan approvals, and candidate recruitment, among others. Consequently, any bias or unfairness in these models can harm those who depend on such models. In response, many algorithms have emerged to ensure fairness in deep learning. However, while the potential for harm is substantial, the resilience of these fair deep learning models against malicious attacks has never been thoroughly explored, especially in the context of emerging Trojan attacks. Moving beyond prior research, we aim to fill this void by introducing \\textit{TrojFair}, a Trojan fairness attack. Unlike existing attacks, TrojFair is model-agnostic and crafts a Trojaned model that functions accurately and equitably for clean inputs. However, it displays discriminatory behaviors - producing both incorrect and unfair results - for specific groups with tainted inputs containing a trigger. TrojFair is a stealthy Fairness attack that is resilient to existing model fairness audition detectors since the model for clean inputs is fair. TrojFair achieves a target group attack success rate exceeding 88.77\\%, with an average accuracy loss less than 0.44\\%. It also maintains a high discriminative score between the target and untarget groups across various datasets and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.7,
    "authors": []
  },
  "https://openreview.net/forum?id=bKzX0m6TEZ": {
    "title": "An Inexact Conditional Gradient Method for Constrained Bilevel Optimization",
    "volume": "review",
    "abstract": "Bilevel optimization is an important class of optimization problems where one optimization problem is nested within another. This framework is widely used in machine learning problems, including meta-learning, data hyper-cleaning, and matrix completion with denoising. In this paper, we focus on a bilevel optimization problem with a strongly convex lower-level problem and a smooth upper-level objective function over a compact and convex constraint set. Several methods have been developed for tackling unconstrained bilevel optimization problems, but there is limited work on methods for the constrained setting. In fact, for those methods that can handle constrained problems, either the convergence rate is slow or the computational cost per iteration is expensive. To address this issue, in this paper, we introduce a novel single-loop projection-free method using a nested approximation technique. Our proposed method has an improved per-iteration complexity, surpassing existing methods, and achieves optimal convergence rate guarantees matching the best-known complexity of projection-free algorithms for solving convex constrained single-level optimization problems. In particular, when the upper-level objective function is convex, our method requires $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ iterations to find an $\\epsilon$-optimal solution. Moreover, when the upper-level objective function is non-convex the complexity of our method is $\\mathcal{O}(\\epsilon^{-2})$ to find an $\\epsilon$-stationary point. We also present numerical experiments to showcase the superior performance of our method compared with state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=samyfu6G93": {
    "title": "NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks",
    "volume": "review",
    "abstract": "Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To train NeuroBack, a new dataset called DataBack containing 120,286 data samples is created. Finally, NeuroBack is implemented as an enhancement to a state-of-the-art SAT solver called Kissat. As a result, it allowed Kissat to solve 5.2% more problems on the recent SAT competition problem set, SATCOMP-2022. NeuroBack therefore shows how machine learning can be harnessed to improve SAT solving in an effective and practical manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=DpFeMH4l8Q": {
    "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models",
    "volume": "review",
    "abstract": "Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=kVj2uyytyg": {
    "title": "Unsupervised Federated Graph Matching with Graphlet Feature Extraction and Separate Trust Region",
    "volume": "review",
    "abstract": "Graph matching in the setting of federated learning is still an open problem. This paper proposes an unsupervised federated graph matching algorithm, UFGM, for inferring matched node pairs on different graphs across clients while maintaining privacy requirement, by leveraging graphlet theory and trust region optimization. First, the nodes' graphlet features are captured to generate pseudo matched node pairs on different graphs across clients as pseudo training data for tackling the dilemma of unsupervised graph matching in federated setting and leveraging the strength of supervised graph matching. An approximate graphlet enumeration method is proposed to sample a small number of graphlets and capture nodes' graphlet features. Theoretical analysis is conducted to demonstrate that the approximate method is able to maintain the quality of graphlet estimation while reducing its expensive cost. Second, we propose a separate trust region algorithm for pseudo supervised federated graph matching while maintaining the privacy constraints. In order to avoid expensive cost of the second-order Hessian computation in the trust region algorithm, we propose two weak quasi-Newton conditions to construct a positive definite scalar matrix as the Hessian approximation with only first-order gradients. We theoretically derive the error introduced by the separate trust region due to the Hessian approximation and conduct the convergence analysis of the approximation method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.0,
    "authors": []
  },
  "https://openreview.net/forum?id=yiyDi50xx6": {
    "title": "GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data",
    "volume": "review",
    "abstract": "Vertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. In this work, we train a graph neural network (GNN) through VFL, where each client owns a part of the node features and a different edge set. This data scenario incurs a significant communication overhead, not only because of the handling of distributed features but also due to neighborhood aggregation in a GNN. Moreover, the training analysis is faced with a challenge caused by the biased stochastic gradients. We propose a model-splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip communication in neighborhood aggregation and in model updates, respectively, greatly reducing communication while enjoying convergence guarantees. We conduct extensive numerical experiments on real-world datasets, showing that GLASU effectively trains a GNN that matches the accuracy of centralized training, while using only a fraction of the time due to communication saving",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJxhZnGU1q": {
    "title": "Strategic Recommendations for Improved Outcomes in Congestion Games",
    "volume": "review",
    "abstract": "Traffic on roads, packets on the Internet, and electricity on power grids share a structure abstracted in congestion games, where self-interested behaviour can lead to socially sub-optimal results. External recommendations may seek to alleviate these issues, but recommenders must take into account the effect that their recommendations have on the system. In this paper, we investigate the effects that dynamic recommendations have on $Q$-learners as they repeatedly play congestion games. To do so, we propose a novel model of recommendation whereby a $Q$-learner receives a recommendation as a state. Thus, the recommender strategically picks states during learning, which we call the Learning Dynamic Manipulation Problem. We define the \\textit{manipulative potential} of these recommenders in repeated congestion games and propose an algorithm for the Learning Dynamic Manipulation Problem designed to drive the actions of $Q$-learners toward a target action distribution. We simulate our algorithm and show that it can drive the system to convergence at the social optimum of a well-known congestion game. Our results show theoretically and empirically that increasing the recommendation space can increase the manipulative potential of the recommender",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=73lu1yw6At": {
    "title": "Complexity of Formal Explainability for Sequential Models",
    "volume": "review",
    "abstract": "This work contributes to formal explainability in AI (FXAI) for sequential models, including Recurrent Neural Networks (RNN), Transformers, and automata models from formal language theory (e.g. finite-state automata). We study two common notions of explainability in FXAI: (1) abductive explanations (a.k.a. minimum sufficient reasons), and (2) counterfactual (a.k.a. contrastive) explanations. To account for various forms of sequential data (e.g. texts, time series, and videos), our models take a sequence of rational numbers as input. We first observe that simple RNN and Transformers suffer from NP-hard complexity (or sometimes undecidability) for both types of explanations. The works on extraction of automata from RNN hinge on the assumption that automata are more interpretable than RNN. Interestingly, it turns out that generating abductive explanations for DFA is computationally intractable (PSPACE-complete), for features that are represented by regular languages. On the positive side, we show that deterministic finite automata (DFA) admit polynomial-time complexity for counterfactual explanations. However, DFA are a highly inexpressive model for classifying sequences of numbers. To address this limitation, we provide two expressive extensions of finite automata, while preserving PTIME explainability and admitting automata learning algorithms: (1) deterministic interval automata, and (2) deterministic register automata with a fixed number of registers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=UqY0SEe5pC": {
    "title": "Analyzing Neural Network Based Generative Diffusion Models via Convexification",
    "volume": "review",
    "abstract": "Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of the score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. We show that the global optimum of the score matching objective can be attained by solving a simple convex program. Specifically, for univariate training data, we establish that the Langevin diffusion process through the learned neural network model converges in the Kullback-Leibler (KL) divergence to either a Gaussian or a Gaussian-Laplace distribution when the weight decay parameter is set appropriately. Our convex programs alleviate issues in computing the Jacobian and also extends to multidimensional score matching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=wgmOXVTGdb": {
    "title": "LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer",
    "volume": "review",
    "abstract": "Graphic layout designs play an essential role in visual communication. Yet handcrafting layout designs is skill-demanding, time-consuming, and non-scalable to batch production. Generative models emerge to make design automation scalable but it remains non-trivial to produce designs that comply with designers' multimodal desires, i.e., constrained by background images and driven by foreground content. We propose LayoutDETR that inherits the high quality and realism from generative modeling, while reformulating content-aware requirements as a detection problem: we learn to detect in a background image the reasonable locations, scales, and spatial relations for multimodal foreground elements in a layout. Our solution sets a new state-of-the-art performance for layout generation on public benchmarks and on our newly-curated ad banner dataset. We integrate our solution into a graphical system that facilitates user studies, and show that users prefer our designs over baselines by significant margins",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=AcGUW5655J": {
    "title": "Constraining Non-Negative Matrix Factorization to Improve Signature Learning",
    "volume": "review",
    "abstract": "Collaborative filtering approaches are fundamental for learning meaningful low-dimensional representations when only association data is available. Among these methods, Non-negative Matrix Factorization (NMF) has gained prominence due to its capability to yield interpretable and meaningful low-dimensional representations. However, one significant challenge for NMF is the vast number of solutions for the same problem instance, making the selection of high-quality signatures a complex task. In response to this challenge, our work introduces a novel approach, Self-Matrix Factorization (SMF), which leverages NMF by incorporating constraints that preserve the relationships inherent in the original data. This is achieved by drawing inspiration from a distinct family of matrix decomposition methods, known as Self-Expressive Models (SEM). In our experimental analyses, conducted on two diverse benchmark datasets, our findings present a compelling narrative. SMF consistently delivers competitive or even superior performance when compared to NMF in predictive tasks. However, what truly sets SMF apart, as validated by our empirical results, is its remarkable ability to consistently generate significantly more meaningful object representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.0,
    "authors": []
  },
  "https://openreview.net/forum?id=zSwH0Wo2wo": {
    "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch",
    "volume": "review",
    "abstract": "Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text. Prior work has introduced automated tools that elicit harmful outputs to identify these risks. While this is a valuable step toward securing models, these approaches rely on a pre-existing way to efficiently classify undesirable outputs. Using a pre-existing classifier does not allow for red-teaming to be tailored to the target model. Furthermore, when failures can be easily classified in advance, red-teaming has limited marginal value because problems can be avoided by simply filtering training data and/or model outputs. Here, we consider red-teaming \"from scratch\" in which the adversary does not begin with a way to classify failures. Our framework consists of three steps: 1) Exploring the model's range of behaviors in the desired context; 2) Establishing a measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) Exploiting the model's flaws using this measure to develop diverse adversarial prompts. We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements. In doing so, we construct the CommonClaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither. We are making code and data available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.2,
    "authors": []
  },
  "https://openreview.net/forum?id=k82MvVIbrC": {
    "title": "Learning Structured Sparse Neural Networks Using Group Envelope Regularization",
    "volume": "review",
    "abstract": "We propose an efficient method to learn both unstructured and structured sparse neural networks during training, utilizing a novel generalization of the sparse envelope function (SEF) used as a regularizer, termed {\\itshape{weighted group sparse envelope function}} (WGSEF). The WGSEF acts as a neuron group selector, which is leveraged to induce structured sparsity. The method ensures a hardware-friendly structured sparsity of a deep neural network (DNN) to efficiently accelerate the DNN's evaluation. Notably, the method is adaptable, letting any hardware specify group definitions, such as filters, channels, filter shapes, layer depths, a single parameter (unstructured), etc. Owing to the WGSEF's properties, the proposed method allows to a pre-define sparsity level that would be achieved at the training convergence, while maintaining negligible network accuracy degradation or even improvement in the case of redundant parameters. We introduce an efficient technique to calculate the exact value of the WGSEF along with its proximal operator in a worst-case complexity of $O(n)$, where $n$ is the total number of group variables. In addition, we propose a proximal-gradient-based optimization method to train the model, that is, the non-convex minimization of the sum of the neural network loss and the WGSEF. Finally, we conduct an experiment and illustrate the efficiency of our proposed technique in terms of the completion ratio, accuracy, and inference latency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.7,
    "authors": []
  },
  "https://openreview.net/forum?id=B4nhr6OJWI": {
    "title": "Instilling Inductive Biases with Subnetworks",
    "volume": "review",
    "abstract": "Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases — preferences for some solutions over others — into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to adopt a specific, generalizable solution to a modular arithmetic task. Second, we demonstrate that Subtask Induction successfully induces a human-like shape bias while increasing data efficiency for convolutional and transformer-based image classification models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=jenyYQzue1": {
    "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
    "volume": "review",
    "abstract": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our data instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=gyJpajLkX2": {
    "title": "ENHANCING MULTIVARIATE TIME SERIES FORECAST- ING WITH MUTUAL INFORMATION-DRIVEN CROSS- VARIABLE AND TEMPORAL MODELING",
    "volume": "review",
    "abstract": "Recent researches have showcased the significant effectiveness of deep learning techniques for multivariate time series forecasting (MTSF). Broadly speaking, these techniques are bifurcated into two categories: Channel-independence and Channel-mixing approaches. While Channel-independence models have generally demonstrated superior outcomes, Channel-mixing methods, especially when dealing with time series that display inter-variable correlations, theoretically promise enhanced performance by incorporating the correlation between variables. However, we contend that the unnecessary integration of information through Channel-mixing can curtail the potential enhancement in MTSF model performance. To substantiate this claim, we introduce the Cross-variable Decorrelation Aware feature Modeling (CDAM) for Channel-mixing approaches. This approach is geared toward reducing superfluous information by minimizing the mutual information between the latent representation of a single univariate sequence and its accompanying multivariate sequence input. Concurrently, it optimizes the joint mutual information shared between the latent representation, its univariate input, and the associated univariate forecast series. Notably, prevailing techniques directly project future series using a single-step forecaster, sidelining the temporal correlation that might exist across varying timesteps in the target series. Addressing this gap, we introduce the Temporal correlation Aware Modeling (TAM). This strategy maximizes the mutual information between adjacent sub-sequences of both the forecasted and target series. By synergizing CDAM and TAM, we sculpt a pioneering framework for MTSF, named as InfoTime. Comprehensive experimental analysis have demonstrated the capability of InfoTime to consistently outpace existing models, encompassing even those considered state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=mWxcEm7jIv": {
    "title": "Training Diffusion Classifiers with Denoising Assistance",
    "volume": "review",
    "abstract": "Score-matching and diffusion models have emerged as state-of-the-art generative models for both conditional and unconditional generation. Classifier-guided diffusion models are created by training a classifier on samples obtained from the forward-diffusion process (i.e., from data to noise). In this paper, we propose denoising-assisted (DA) classifiers wherein the diffusion classifier is trained using both noisy and denoised examples as simultaneous inputs to the model. We differentiate between denoising-assisted (DA) classifiers and noisy classifiers, which are diffusion classifiers that are only trained on noisy examples. Our experiments on Cifar10 and Imagenet show that DA-classifiers improve over noisy classifiers both quantitatively in terms of generalization to test data and qualitatively in terms of perceptually-aligned classifier-gradients and generative modeling metrics. We theoretically characterize the gradients of DA-classifiers to explain improved perceptual alignment. Building upon the observed generalization benefits of DA-classifiers, we propose and evaluate a semi-supervised framework for training diffusion classifiers and demonstrate improved generalization of DA-classifiers over noisy classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=GKxmmAwxj1": {
    "title": "Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules",
    "volume": "review",
    "abstract": "The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strategies, such as maximum likelihood alone, fail while our novel architecture and multi-stage training strategy are able to model the conformational distributions of protein G and HP35",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=FL1VmOgiO8": {
    "title": "Sentiment-Enhanced Stock Price Prediction: A Novel Ensemble Model Approach",
    "volume": "review",
    "abstract": "Stock price prediction remains a formidable challenge within the realm of financial markets, wherein a multitude of models and methodologies have been under exploration to prognosticate the dynamic behaviour of equities. This research endeavour encompasses an exhaustive examination of extant stock prediction systems, entailing a meticulous assessment of their merits and demerits, concurrently pinpointing discernible lacunae and avenues for enhancement. Subsequently, we harnessed the capabilities of BERT, an exemplar in the domain of natural language processing, to conduct sentiment analysis across a heterogeneous corpus of news articles pertinent to the subject stocks. Additionally, an ancillary sub-experiment was conducted to ascertain the relative impact of three distinct categories of news articles, namely headlines, summaries, and a composite amalgamation of the two, on the efficacy of stock price prediction. The outcome of this investigative pursuit was the generation of sentiment scores for each trading date, which were subsequently integrated as input features in the training of a neural network. Through a comparative analysis of various neural network models, including but not limited to RNN, LSTM, GAN, and WGAN-GP, we discerned that the WGAN-GP model exhibited the most favourable predictive performance. Building upon these findings, we introduced the FB-GAN model, an ensemble architecture comprising WGAN-GP, which capitalizes on the fusion of historical stock price data and market sentiment scores for enhanced stock price prediction. Subsequently, a comprehensive evaluation of our approach was undertaken vis-à-vis established models, gauging its performance against five prominent equities, namely Amazon, Apple, Microsoft, Nvidia, and Adobe. In summation, this research makes a compelling case for the integration of BERT-based sentiment analysis within the ambit of stock price prediction. Our initial hypothesis regarding the significant influence of market sentiment on stock price prediction was validated, and our proposed FB-GAN model outperformed all other models. Furthermore, incorporating both the headline and summary of the news article contributed to enhanced stock price prediction compared to utilizing either the headline or summary in isolation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 2.3,
    "authors": []
  },
  "https://openreview.net/forum?id=zFWKKYz2yn": {
    "title": "Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network",
    "volume": "review",
    "abstract": "This paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA (Deterministic Finite Automata) with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are trained completely. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with the number of states very close to ground truth DFA. Among RNN cells, O2RNN produces stable DFA consistently compared to other cells. For Dyck Languages, we observe that although GRU outperforms other RNNs in network performance, the DFA extracted by O2RNN has higher performance and better stability. The stability is computed as the standard deviation of accuracy on test sets on networks trained across $10$ seeds. On Dyck Languages, quantization methods outperformed $L^{*}$ with better stability in accuracy and the number of states. $L^{*}$ often showed instability in accuracy in the order of $16\\% - 22\\%$ for GRU and MIRNN while deviation for quantization methods varied in $5\\% - 15\\%$. In many instances with LSTM and GRU, DFA's extracted by $L^{*}$ even failed to beat chance accuracy ($50\\%$), while those extracted by quantization method had standard deviation in the $7\\%-17\\%$ range. For O2RNN, both rule extraction methods had a deviation in the $0.5\\% - 3\\%$ range",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=THJEa8adBn": {
    "title": "Harnessing Density Ratios for Online Reinforcement Learning",
    "volume": "review",
    "abstract": "The theories of offline and online reinforcement learning, despite having evolved in parallel, have recently started to see unification, and algorithms/concepts in one setting often have natural counterparts in the other. However, the notion of density ratio modeling, an emerging topic in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on a dataset with good coverage, but the core challenge in online RL is to collect such an exploratory dataset without having one to start. In this work we show—perhaps surprisingly—that density ratio-based algorithms have online counterparts. Assuming the mere existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give an algorithm (GLOW) which performs sample-efficient online exploration under value-function and density-ratio realizability. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HYGLOW, for the Hybrid RL setting (Song et al., 2023) in which online RL is augmented with additional offline data. HYGLOW is derived as a special case of a novel meta-algorithm, H2O, which provides a provable black-box reduction from hybrid RL to offline RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=w3YZ9MSlBu": {
    "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training",
    "volume": "review",
    "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic **M**usic und**ER**standing model with large-scale self-supervised **T**raining (**MERT**), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 7.5,
    "authors": []
  },
  "https://openreview.net/forum?id=rDH7dIFn20": {
    "title": "Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits",
    "volume": "review",
    "abstract": "Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\\tilde O\\big(d\\sqrt{\\sum_{t=1}^T\\sigma_t^2} + d\\big)$, where $\\sigma_t$ is the variance of the pairwise comparison at round $t$, $d$ is the dimension of the context vectors, and $T$ is the time horizon. Our regret bound naturally aligns with the intuitive expectation — in scenarios where the comparison is deterministic, the algorithm only suffers from an $\\tilde O(d)$ regret. We perform empirical experiments on synthetic data to confirm the advantage of our method over previous variance-agnostic algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=ueTdErd5Ib": {
    "title": "A Discretization Framework for Robust Contextual Stochastic Optimization",
    "volume": "review",
    "abstract": "We study contextual stochastic optimization problems. Optimization problems have uncertain parameters stemming from unknown, context-dependent, distributions. Due to the inherent uncertainty in these problems, one is often interested not only in minimizing expected cost, but also to be robust and protect against worst case scenarios. We propose a novel method that combines the learning stage with knowledge of the downstream optimization task. The method prescribes decisions which aim to maximize the likelihood that the cost is below a (user-controlled) threshold. The key idea is (1) to discretize the feasible region into subsets so that the uncertain objective function can be well approximated deterministically within each subset, and (2) devise a secondary optimization problem to prescribe decisions by integrating the individual approximations determined in step (1). We provide theoretical guarantees bounding the underlying regret of decisions proposed by our method. In addition, experimental results demonstrate that our approach is competitive in terms of average regret and yields more robust solutions than other methods proposed in the literature, including up to 20 times lower worst-case cost on a real-world electricity generation problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=uCMxeZCp2T": {
    "title": "Nature-Inspired Local Propagation",
    "volume": "review",
    "abstract": "The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections. On the opposite, intelligent processes in nature arises without the need for such collections, but simply by online processing of the environmental information. In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality. This paper shows that such a feature arises from a pre-algorithmic view of learning that is inspired by related studies in Theoretical Physics. We show that the algorithmic interpretation of the derived \"laws of learning\", which takes the structure of Hamiltonian equations, reduces to Backpropagation when the the speed of propagation goes to infinity. This opens the doors to machine learning studies based on full on-line information processing that are based the replacement of Backpropagation with the proposed spatiotemporal local algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=npf3gREtf7": {
    "title": "Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3× more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2× fewer ICL examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=87XbxDnPqj": {
    "title": "Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction",
    "volume": "review",
    "abstract": "In computed tomography (CT), the forward model consists of a linear Radon transform followed by an exponential nonlinearity based on the attenuation of light according to the Beer–Lambert Law. Conventional reconstruction often involves inverting this nonlinearity as a preprocessing step and then solving a convex inverse problem. However, this nonlinear measurement preprocessing required to use the Radon transform is poorly conditioned in the vicinity of high-density materials, such as metal. This preprocessing makes CT reconstruction methods numerically sensitive and susceptible to artifacts near high-density regions. In this paper, we study a technique where the signal is directly reconstructed from raw measurements through the nonlinear forward model. Though this optimization is nonconvex, we show that gradient descent provably converges to the global optimum at a geometric rate, perfectly reconstructing the underlying signal with a near minimal number of random measurements. We also prove similar results in the under-determined setting where the number of measurements is significantly smaller than the dimension of the signal. This is achieved by enforcing prior structural information about the signal through constraints on the optimization variables. We illustrate the benefits of direct nonlinear CT reconstruction with cone-beam CT experiments on synthetic and real 3D volumes. We show that this approach reduces metal artifacts compared to a commercial reconstruction of a human skull with metal dental crowns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=SmZD7yxpPC": {
    "title": "GlycoNMR: A Carbohydrate-Specific NMR Chemical Shift Dataset for Machine Learning Research",
    "volume": "review",
    "abstract": "Molecular representation learning (MRL) is a powerful contribution by machine learning to chemistry as it converts molecules into numerical representations, which serves as fundamental for diverse biochemical applications, such as property prediction and drug design. While MRL has had great success with proteins and general biomolecules, it has yet to be explored for carbohydrates in the growing fields of glycoscience and glycomaterials (the study and design of carbohydrates). This under-exploration can be primarily attributed to the limited availability of comprehensive and well-curated carbohydrate-specific datasets and a lack of machine learning (ML) techniques tailored to meet the unique problems presented by carbohydrate data. Interpreting and annotating carbohydrate data is generally more complicated than protein data, and requires substantial domain knowledge. In addition, existing MRL methods were predominately optimized for proteins and small biomolecules, and may not be effective for carbohydrate applications without special modifications. To address this challenge, accelerate progress in glycoscience and glycomaterials, and enrich the data resources of the ML community, we introduce GlycoNMR. GlycoNMR contains two laboriously curated datasets with 2,609 carbohydrate structures and 211,543 annotated nuclear magnetic resonance (NMR) atomic-level chemical shifts that can be used to train ML models for precise atomic-level prediction. NMR data is one of the most appealing starting points for developing ML techniques to facilitate glycoscience and glycomaterials research, as NMR is the preeminent technique in carbohydrate structure research, and biomolecule structure is among the foremost predictors of functions and properties. We tailored a set of carbohydrate-specific features and adapted existing MRL models to effectively tackle the problem of predicting NMR shifts. For illustration, we benchmark these modified MRL models on the GlycoNMR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=kKXIYUi8ff": {
    "title": "DynamicsDiffusion: Generating and Rare Event Sampling of Molecular Dynamic Trajectories Using Diffusion Models",
    "volume": "review",
    "abstract": "Molecular dynamics simulations are fundamental tools for quantitative molecular sciences. However, these simulations are computationally demanding and often struggle to sample rare events crucial for understanding spontaneous organization and reconfiguration in complex systems. To improve general speed and the ability to sample rare events in a directed fashion, we propose a method called $\\textit{DynamicsDiffusion}$ based on denoising diffusion probabilistic models (DDPM) to generate molecular dynamics trajectories from noise. The generative model can then serve as a surrogate to sample rare events. We leverage the properties of DDPMs, such as conditional generation, the ability to generate variations of trajectories, and those with certain conditions, such as crossing from one state to another, using the 'inpainting' property of DDPMs, which became only applicable when generating whole trajectories and not just individual conformations. To our knowledge, this is the first deep generative modeling for generating molecular dynamics trajectories. We hope this work will motivate a new generation of generative modeling for the study of molecular dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=gisAooH2TG": {
    "title": "RePLan: Robotic Replanning with Perception and Language Models",
    "volume": "review",
    "abstract": "Advancements in large language models (LLMs) have demonstrated their potential in facilitating high-level reasoning, logical reasoning and robotics planning. Recently, LLMs have also been able to generate reward functions for low-level robot actions, effectively bridging the interface between high-level planning and low-level robot control. However, the challenge remains that even with syntactically correct plans, robots can still fail to achieve their intended goals. This failure can be attributed to imperfect plans proposed by LLMs or to unforeseeable environmental circumstances that hinder the execution of planned subtasks due to erroneous assumptions about the state of objects. One way to prevent these challenges is to rely on human-provided step-by-step instructions, limiting the autonomy of robotic systems. Vision Language Models (VLMs) have shown remarkable success in tasks such as visual question answering and image captioning. Leveraging the capabilities of VLMs, we present a novel framework called RePLan that enables real-time replanning capabilities. This framework utilizes the physical grounding provided by a VLM's understanding of the world's state to adapt robot actions when the initial plan fails to achieve the desired goal. We test our approach within two long-horizon task domains, a wooden cabinet puzzle and a larger-scale kitchen environment. We find that RePLan enables a robot to successfully adapt to unforeseen obstacles while accomplishing open-ended, long-horizon goals, while baseline models cannot",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=agPpmEgf8C": {
    "title": "Predictive auxiliary objectives in deep RL mimic learning in the brain",
    "volume": "review",
    "abstract": "The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain-- that of an auxiliary learning system that benefits representation learning in other regions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 8.0,
    "authors": []
  },
  "https://openreview.net/forum?id=AP779Zy70y": {
    "title": "GATE: How to Keep Out Intrusive Neighbors",
    "volume": "review",
    "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neighborhood aggregation, as we show experimentally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over-smoothing by addressing its root cause of unnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non-)linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neighbors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to analyze a model's ability to utilize the appropriate amount of neighborhood aggregation, which could be of independent interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.4,
    "authors": []
  },
  "https://openreview.net/forum?id=AcoXPIPh4A": {
    "title": "Risk Bounds of Accelerated SGD for Overparameterized Linear Regression",
    "volume": "review",
    "abstract": "Accelerated stochastic gradient descent (ASGD) is a workhorse in deep learning and often achieves better generalization performance than SGD. However, existing optimization theory can only explain the faster convergence of ASGD, but cannot explain its better generalization. In this paper, we study the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. We establish an instance-dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. Our analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD. Our result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when our analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=MY8SBpUece": {
    "title": "A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks",
    "volume": "review",
    "abstract": "Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component---spike---in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the loss, we demonstrate that these non-linear features can enhance learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.5,
    "authors": []
  },
  "https://openreview.net/forum?id=iARAKITHTH": {
    "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
    "volume": "review",
    "abstract": "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using pre-trained LLMs. The method, called *Binoculars*, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate *Binoculars* on a number of text sources and in varied situations. On news documents *Binoculars* detect 95\\% of synthetic samples at a false positive rate of 0.01%, given 512 tokens of text from either humans or ChatGPT, matching highly competitive commercial detectors tuned specifically to detect ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=q4cfN6PGY7": {
    "title": "Towards Deep Viticultural Representations: Joint Region and Grape Variety Embeddings",
    "volume": "review",
    "abstract": "The creation of embeddings, representations, or features for abstract or non-numeric variables is a prerequisite to utilize these variables in machine learning models; this is also the case for viticulture (growing grapes for wine). Viticultural regions and grape varieties are variables for which deep representations are currently not available. Regions are somewhat definable by their approximate longitude and latitude, average elevation, or averages of climate variables. Each of these 'raw' features contributes valuable information about the region but it does not easily define a metric for agro-ecological proximity between regions. Grape varieties have much fewer 'raw' features; one example may be their genetic markers, which, however, are still categorical in nature. Analysis of lineage is possible but does not necessarily provide useful features to the viticulturists as grape attributes are not necessarily inferable by their lineage such as dominant wine style or suitability for a particular region. Therefore, here we present a self-supervised approach to learning joint regional and varietal embeddings using joint variational autoencoder (VAE) networks. This is based on the assumption that regions that grow similar proportions of similar grape varieties are more similar to each other than those that do not, or that grape varieties that often occur together may have similar viticultural characteristics (e.g. climate requirements, aromas, disease resistance). We thereby overcome the lack of detailed data and create deep embeddings for 1557 grape varieties (e.g. Merlot, Riesling, Chardonnay etc.) and 595 viticulturally important regions (e.g. Piemonte, Bourgogne, Mosel etc.). We examine the embeddings, their usability for downstream tasks as well as whether the joint autoencoder network may be used as a varietal suitability ranking system. We show our embeddings to outperform 'raw' features on downstream tasks and results indicating potential of the autoencoder networks as data-based recommender systems. This is also, to our knowledge, the first work to apply joint VAEs to purely categorical data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9t8dQ30kU": {
    "title": "Task structure and nonlinearity jointly determine learned representational geometry",
    "volume": "review",
    "abstract": "The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and on the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric saturation of ReLU, which leads feature neurons to specialize for different regions of input space. Feature neurons in Tanh networks, by contrast, tend to inherit the task label structure. Consequently, when the target outputs are low dimensional, Tanh networks generate neural representations that are more disentangled than those obtained with a ReLU nonlinearity. Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=RnYd44LR2v": {
    "title": "OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift",
    "volume": "review",
    "abstract": "Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear way, under many distribution shifts. The latter enables the prediction of OOD robustness from ID robustness. Based on this, we are able to predict the upper limit of OOD robustness for existing robust training schemes. The results suggest that achieving OOD robustness requires designing novel methods beyond the conventional ones. Last, we discover that extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness. Noticeably, the discovered training schemes, compared to the baseline, exhibit dramatically higher robustness under threat shift while keeping high ID robustness, demonstrating new promising solutions for robustness against both multi-attack and unforeseen attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.8,
    "authors": []
  },
  "https://openreview.net/forum?id=BSqVfAFJWz": {
    "title": "The Distributional Reward Critic Architecture for Reinforcement Learning Under Confusion Matrix Reward Perturbations",
    "volume": "review",
    "abstract": "We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed especially for that setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WnqRR915j": {
    "title": "Llemma: An Open Language Model for Mathematics",
    "volume": "review",
    "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known openly released models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.7,
    "authors": []
  },
  "https://openreview.net/forum?id=IWpLQfZ8Xg": {
    "title": "Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention",
    "volume": "review",
    "abstract": "Self-attention has been widely used in various machine learning models, such as vision transformers. The standard dot-product self-attention is arguably the most popular structure, and there is a growing interest in understanding the mathematical properties of such attention mechanisms. This paper presents a fine-grained local sensitivity analysis of the standard dot-product self-attention. Despite the well-known fact that dot-product self-attention is not (globally) Lipschitz, we develop new theoretical local bounds quantifying the effect of input feature perturbations on the attention output. Utilizing mathematical techniques from optimization and matrix theory, our analysis reveals that the local sensitivity of dot-product self-attention to $\\ell_2$ perturbations can actually be controlled by several key quantities associated with the attention weight matrices and the unperturbed input. We empirically validate our theoretical findings through several examples, offering new insights for achieving low sensitivity in dot-product self-attention against $\\ell_2$ input perturbations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vmSEVL19f": {
    "title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards",
    "volume": "review",
    "abstract": "We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of reward functions, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=NgaLU2fP5D": {
    "title": "Predictive, scalable and interpretable knowledge tracing on structured domains",
    "volume": "review",
    "abstract": "Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (\"knowledge tracing\"; KT), and the prerequisite structure of the learning domain (\"knowledge mapping\"). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and interaction data. Evaluated on three datasets from online learning platforms, PSI-KT achieves superior multi-step **p**redictive accuracy and **s**calable inference in continual-learning settings, all while providing **i**nterpretable representations of learner-specific traits and the prerequisite structure of knowledge that causally supports learning. In sum, predictive, scalable and interpretable knowledge tracing with solid knowledge mapping lays a key foundation for effective personalized learning to make education accessible to a broad, global audience",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.8,
    "authors": []
  },
  "https://openreview.net/forum?id=eoTCKKOgIs": {
    "title": "Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift",
    "volume": "review",
    "abstract": "A key challenge of modern machine learning systems is to achieve Out-of-Distribution (OOD) generalization --- generalizing to target data whose distribution differs from those of source data. Despite its significant importance, the fundamental question of ``what are the most effective algorithms for OOD generalization'' remains open even under the standard setting of covariate shift. This paper addresses this fundamental question by proving that, surprisingly, classical Maximum Likelihood Estimation (MLE) purely using source data (without any modification) achieves the *minimax* optimality for covariate shift under the *well-specified* setting. This result holds for a very large class of parametric models, including but not limited to linear regression, logistic regression, and phase retrieval, and does not require any boundedness condition on the density ratio. This paper further complement the study by proving that for the *misspecified setting*, MLE can perform poorly, and the Maximum Weighted Likelihood Estimator (MWLE) emerges as minimax optimal in specific scenarios, outperforming MLE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.2,
    "authors": []
  },
  "https://openreview.net/forum?id=UTLv72uDlS": {
    "title": "Scaling Safe Learning-based Control to Long-Horizon Temporal Tasks",
    "volume": "review",
    "abstract": "This paper introduces a model-based approach for training parameterized policies for an autonomous agent operating in a highly nonlinear (albeit deterministic) environment. We desire the trained policy to ensure that the agent satisfies specific task objectives and safety constraints, both expressed in Signal Temporal Logic. We show that this learning problem reduces to the problem of training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agent's task objectives. This poses a challenge: RNNs are susceptible to vanishing and exploding gradients, and naive gradient descent-based strategies to solve long-horizon task objectives thus suffer from the same problems. To tackle this challenge, we introduce a novel gradient approximation algorithm based on the idea of gradient sampling, and a smooth computation graph that provides a neurosymblic encoding of STL formulas. We show that these two methods combined improve the quality of the stochastic gradient, enabling scalable backpropagation over long time horizon trajectories. We demonstrate the efficacy of our approach on various motion planning applications requiring complex spatio-temporal and sequential tasks ranging over thousands of time steps",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.2,
    "authors": []
  },
  "https://openreview.net/forum?id=pAVJKp3Dvn": {
    "title": "Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks",
    "volume": "review",
    "abstract": "This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. Finally, we introduce an effective initialization method for the proposed scheme. Our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.7,
    "authors": []
  },
  "https://openreview.net/forum?id=B4XM9nQ8Ns": {
    "title": "HyperSINDy: Deep Generative Modeling of Nonlinear Stochastic Governing Equations",
    "volume": "review",
    "abstract": "The discovery of governing differential equations from data is an open frontier in machine learning. The {\\em sparse identification of nonlinear dynamics} (SINDy) \\citep{brunton_discovering_2016} framework enables data-driven discovery of interpretable models in the form of sparse, deterministic governing laws. Recent works have sought to adapt this approach to the stochastic setting, though these adaptations are severely hampered by the curse of dimensionality. On the other hand, Bayesian-inspired deep learning methods have achieved widespread success in high-dimensional probabilistic modeling via computationally efficient approximate inference techniques, suggesting the use of these techniques for efficient stochastic equation discovery. Here, we introduce {\\em HyperSINDy}, a framework for modeling stochastic dynamics via a deep generative model of sparse, nonlinear governing equations whose parametric form is discovered from data. HyperSINDy employs a variational encoder to approximate the distribution of observed states and derivatives. A hypernetwork \\citep{ha_hypernetworks_2016} transforms samples from this distribution into the coefficients of a differential equation whose sparse form is learned simultaneously using a trainable binary mask \\citep{louizos_learning_2018}. Once trained, HyperSINDy generates stochastic dynamics via a differential equation whose coefficients are driven by a Wiener process. In experiments HyperSINDy accurately recovers ground truth stochastic governing equations, with stochasticity scaled to match that of the data. Finally, HyperSINDy provides uncertainty quantification that scales to high-dimensional systems, retaining computational efficiency and interpretability. Taken together, HyperSINDy offers a promising framework for model discovery and uncertainty quantification in real-world systems, integrating sparse equation discovery methods with advances in statistical machine learning and deep generative modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 6.0,
    "authors": []
  },
  "https://openreview.net/forum?id=IoDhZOgteu": {
    "title": "DEXR: A Unified Approach Towards Environment Agnostic Exploration",
    "volume": "review",
    "abstract": "The exploration-exploitation dilemma poses pivotal challenges in reinforcement learning (RL). While recent advances in curiosity-driven techniques have demonstrated capabilities in sparse reward scenarios, they necessitate extensive hyperparameter tuning on different types of environments and often fall short in dense reward settings. In response to these challenges, we introduce the novel \\textbf{D}elayed \\textbf{EX}ploration \\textbf{R}einforcement Learning (DEXR) framework. DEXR adeptly curbs over-exploration and optimization instabilities issues of curiosity-driven methods, and can efficiently adapt to both dense and sparse reward environments with minimal hyperparameter tuning. This is facilitated by an auxiliary exploitation-only policy that streamlines data collection, guiding the exploration policy towards high-value regions and minimizing unnecessary exploration. Additionally, this exploration policy yields diverse, in-distribution data, and bolsters training robustness with neural network structures. We verify the efficacy of DEXR with both theoretical validations and comprehensive empirical evaluations, demonstrating its superiority in a broad range of environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 4.8,
    "authors": []
  },
  "https://openreview.net/forum?id=W2tCmRrj7H": {
    "title": "A Flexible Generative Model for Heterogeneous Tabular EHR with Missing Modality",
    "volume": "review",
    "abstract": "Realistic synthetic electronic health records (EHRs) can be leveraged to acceler- ate methodological developments for research purposes while mitigating privacy concerns associated with data sharing. However, the training of Generative Ad- versarial Networks remains challenging, often resulting in issues like mode col- lapse. While diffusion models have demonstrated progress in generating qual- ity synthetic samples for tabular EHRs given ample denoising steps, their perfor- mance wanes when confronted with missing modalities in heterogeneous tabular EHRs data. For example, some EHRs contain solely static measurements, and some contain only contain temporal measurements, or a blend of both data types. To bridge this gap, we introduce FLEXGEN-EHR– a versatile diffusion model tai- lored for heterogeneous tabular EHRs, equipped with the capability of handling missing modalities in an integrative learning framework. We define an optimal transport module to align and accentuate the common feature space of hetero- geneity of EHRs. We empirically show that our model consistently outperforms existing state-of-the-art synthetic EHR generation methods both in fidelity by up to 3.10% and utility by up to 7.16%. Additionally, we show that our method can be successfully used in privacy-sensitive settings, where the original patient-level data cannot be shared",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 5.6,
    "authors": []
  },
  "https://openreview.net/forum?id=VyMW4YZfw7": {
    "title": "Simplifying GNN Performance with Low Rank Kernel Models",
    "volume": "review",
    "abstract": "We revisit recent spectral GNN approaches to semi-supervised node classification (SSNC). We posit that many of the current GNN architectures may be over-engineered. Instead, simpler, traditional methods from nonparametric estimation, applied in the spectral domain, could replace many deep-learning inspired GNN designs. These conventional techniques appear to be well suited for a variety of graph types reaching state-of-the-art performance on many the common SSNC benchmarks. Additionally, we show that recent performance improvements in GNN approaches may be partialy attributed to shifts in evaluation conventions. Lastly, an ablative study is conducted on the various hyperparameters associated with GNN spectral filtering techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 3.0,
    "authors": []
  },
  "https://openreview.net/forum?id=79rfgv3jw4": {
    "title": "Designing Skill-Compatible AI: Methodologies and Frameworks in Chess",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d5DGVHMdsC": {
    "title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jos5c7vJPP": {
    "title": "Exchangeable Dataset Amortization for Bayesian Posterior Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mOeklnLf4": {
    "title": "FroSSL: Frobenius Norm Minimization for Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RaqZX9LSGA": {
    "title": "Tree Search-Based Policy Optimization under Stochastic Execution Delay",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lJYAkDVnRU": {
    "title": "Context-Aware Meta-Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oWKPZ1Hcsm": {
    "title": "Efficient Offline Reinforcement Learning: The Critic is Critical",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ZUKLCxwBo": {
    "title": "A simple and interpretable model of grokking modular arithmetic tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOm1RYdHHu": {
    "title": "SAFHE: Defending Against Backdoor and Gradient Inversion Attacks in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lr806pdNZa": {
    "title": "LLM Censorship: The Problem and its Limitations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o0C2v4xTdS": {
    "title": "CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4QNyUqBIu": {
    "title": "Graph Neural Modeling of Network Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GviaQcGnx": {
    "title": "Constrained Parameter Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJYIgEteHX": {
    "title": "Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse Training Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5SurcLh24": {
    "title": "Unifying Model-Based and Model-Free Reinforcement Learning with Equivalent Policy Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gDDW5zMKFe": {
    "title": "FIITED: Fine-grained embedding dimension optimization during training for recommender systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=caW7LdAALh": {
    "title": "Beyond Accuracy: Evaluating Self-Consistency of Code LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNl1UsUUX2": {
    "title": "Improving Generalization for Small Datasets with Data-Aware Dynamic Reinitialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjpfCOV4ru": {
    "title": "Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vngVydDWft": {
    "title": "From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFKIfhvdmZ": {
    "title": "Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgDDyoWQt3": {
    "title": "Feasibility with Language Models for Open-World Compositional Zero-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o2IEmeLL9r": {
    "title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrR3rMxqqv": {
    "title": "Memorization Capacity of Multi-Head Attention in Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rjLgCkJH79": {
    "title": "A SIMILARITY-AGNOSTIC REINFORCEMENT LEARNING APPROACH FOR LEAD OPTIMIZATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W3VsHuga3j": {
    "title": "Modeling Boundedly Rational Agents with Latent Inference Budgets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rFZtyj5kBz": {
    "title": "Certifiably Byzantine-Robust Federated Conformal Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkE2js9qRe": {
    "title": "Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MEGQGNUfPx": {
    "title": "The Effectiveness of Random Forgetting for Robust Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gxhRR8vUQb": {
    "title": "Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p34fRKp8qA": {
    "title": "Lie Group Decompositions for Equivariant Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByW9j60mvV": {
    "title": "RL Algorithms are Information-State Policies in the Bayes-Adaptive MDP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QiJuMJl0QS": {
    "title": "Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHjE5v5MB7": {
    "title": "To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFVQaqkf8Z": {
    "title": "Revisiting Non-separable Binary Classification and its Applications in Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HmKav4WZ9w": {
    "title": "Basis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=voLFfrWzFI": {
    "title": "Task Generalization in Decision-Focused Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wrqAn3AJA1": {
    "title": "Everybody Needs a Little HELP: Explaining Graphs via Hierarchical Concepts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONhwvkaIe6": {
    "title": "Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ntUmktUfZg": {
    "title": "Generate to Discriminate: Expert Routing for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bHOcs4PBgR": {
    "title": "Flatter, Faster: Scaling Momentum for Optimal Speedup of SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SUUrkC3STJ": {
    "title": "VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4NJFfrqoq": {
    "title": "Optimistic Bayesian Optimization with Unknown Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7aPLHwsLr": {
    "title": "DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pEGSdJu52I": {
    "title": "Calibrated Chaos: Variance Between Runs of Neural Network Training is Harmless and Inevitable",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VaZa8zj0Yw": {
    "title": "Lyfe Agents: generative agents for low-cost real-time social interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fpoAYV6Wsk": {
    "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7em7Jl0qMm": {
    "title": "Fourier Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkeoEFlF0E": {
    "title": "NEURAL ADDITIVE TENSOR DECOMPOSITION FOR SPARSE TENSORS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OOxotBmGol": {
    "title": "Large Language Models to Enhance Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sojpn00o8z": {
    "title": "Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=55uj7mU7Cv": {
    "title": "Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UhcXE3o1R3": {
    "title": "Apollo: Zero-shot MultiModal Reasoning with Multiple Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SqMVI1GFnp": {
    "title": "Lie Neurons: A General Adjoint-Equivariant Neural Network for Semisimple Lie Algebras",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlbFGQYsJr": {
    "title": "Learning Dynamics on Manifolds with Neural Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yuy6cGt3KL": {
    "title": "Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLhEQWQYtb": {
    "title": "Parameter Estimation of Long Memory Stochastic Processes with Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOviNImhfq": {
    "title": "Effective Graph Representation Learning via Smoothed Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vMBNUCZ4cS": {
    "title": "Graph Neural Tangent Kernel and Graph Neural Network Gaussian Processes for Node Classification/ Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSYhE2hLWG": {
    "title": "SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6bAfAcuuZD": {
    "title": "Emergence of Surprise and Predictive Signals from Local Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WIzzXCVYiH": {
    "title": "GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5mtwoRNzjm": {
    "title": "Optimization without retraction on the random generalized Stiefel manifold for canonical correlation analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sNtDKdcI1f": {
    "title": "A Long Way To Go: Investigating Length Correlations in RLHF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jsfesDZDq": {
    "title": "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8LBS1nixTJ": {
    "title": "HashOrder: Accelerating Graph Processing Through Hashing-based Reordering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UJkgGbLfWA": {
    "title": "Guiding Language Models Reasoning with Planning Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0wAim2F8A": {
    "title": "PrivilegedDreamer: Explicit Imagination of Privileged Information for Adaptation in Uncertain Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GgEAdqYPNA": {
    "title": "Investigating the Benefits of Projection Head for Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YO4EE3SPB": {
    "title": "A Variational Perspective on Solving Inverse Problems with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqIH0ObdqL": {
    "title": "Can Large Language Models Infer Causation from Correlation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pmrc0nEvxf": {
    "title": "MemStranding: Adversarial attacks on temporal graph neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Of2nEDc4s7": {
    "title": "Anisotropy helps: improved statistical and computational complexity of the mean-field Langevin dynamics under structured data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zr96FfaUGR": {
    "title": "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wRkfniZIBl": {
    "title": "Splicing Up Your Predictions with RNA Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fapHf9fmqp": {
    "title": "Unnormalized Density Estimation with Root Sobolev Norm Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCUvAc1GTg": {
    "title": "Network Alignment with Transferable Graph Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dtxc7mlKRg": {
    "title": "Class-Conditional Conformal Prediction for Imbalanced Data via Top-$k$ Classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uU0Adp7Sfo": {
    "title": "Competitive-Collaborative GAN with Performance Guarantee",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=66e22qCU5i": {
    "title": "Certified Copy: A Resistant Backdoor Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ffcHGwb4KF": {
    "title": "SPADE: Sparsity-Guided Debugging for Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5CBxA1l5RO": {
    "title": "TimewarpVAE: Simultaneous Time-Warping and Representation Learning of Trajectories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tEAF9LBdgu": {
    "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jX2DT7qDam": {
    "title": "Jointly-Learned Exit and Inference for a Dynamic Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQY2v6VtGe": {
    "title": "Confidential-DPproof: Confidential Proof of Differentially Private Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1XarNmzbgG": {
    "title": "Understanding of Server-Assisted Federated Learning with Incomplete Client Participation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ns8SXMJ2ic": {
    "title": "Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational Quantum Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GfXF04YYvu": {
    "title": "Enhancing Group Fairness in Federated Learning through Personalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtl4XnJYBh": {
    "title": "Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tS3gexmfeT": {
    "title": "Fusion Token: Enhancing Compression and Efficiency in Language Model Tokenization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJWT0692hw": {
    "title": "SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfmZh91tDI": {
    "title": "Layer-wise linear mode connectivity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h05eQniJsQ": {
    "title": "Understanding Certified Training with Interval Bound Propagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GnOLWS4Llt": {
    "title": "Offline RL with Observation Histories: Analyzing and Improving Sample Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WEQS3oUPs3": {
    "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UvRjDCYIHw": {
    "title": "Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6AA1NZhLd": {
    "title": "Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAqdG2dy5s": {
    "title": "Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0bMmZ3fkCk": {
    "title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lxlMFlzZO9": {
    "title": "DS-Prover: A Dynamic Sampling Based Approach for Neural Theorem Proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WWlxFtR5sV": {
    "title": "An operator preconditioning perspective on training in physics-informed machine learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VUR7STEajx": {
    "title": "M-BioBERTa: Modular RoBERTa-based Model for Biobank-scale Unified Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjZMGKB2dU": {
    "title": "A neuro-symbolic framework for answering conjunctive queries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pCEgna6Qco": {
    "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0bjIoHD45G": {
    "title": "Closing the gap on tabular data with Fourier and Implicit Categorical Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyuQKk7sX2": {
    "title": "(Dynamic) Prompting might be all you need to repair Compressed LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhLBhx5BVY": {
    "title": "Instance Segmentation with Supervoxel Based Topological Loss Function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJhgguibXu": {
    "title": "Using Approximate Models for Efficient Exploration in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHgu98u8Sc": {
    "title": "$\\nu$-ensembles: Improving deep ensemble calibration in the small data regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3j5bsiwRv6": {
    "title": "Sparse Refinement for Efficient High-Resolution Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0fSNU64FV7": {
    "title": "Sorting Out Quantum Monte Carlo",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d2YjPbSpDZ": {
    "title": "Understanding the Theoretical Generalization Performance of Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzyZ4wzKlM": {
    "title": "Expressive Losses for Verified Robustness via Convex Combinations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VZP2Dr9KX": {
    "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XWfjugkXzN": {
    "title": "On Sampling Information Sets to Learn from Imperfect Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUOesbrlw4": {
    "title": "Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vbebD7QRxP": {
    "title": "Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LXVswInHOo": {
    "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hr4HTShC6l": {
    "title": "Detecting Shortcuts using Mutual Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z62Xc88jgF": {
    "title": "Neural functional a posteriori error estimates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WroPkTLiAJ": {
    "title": "FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RvfPnOkPV4": {
    "title": "What's In My Big Data?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0D6mUZTWoF": {
    "title": "A Topology-aware Graph Coarsening Framework for Continual Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5RielfrDkP": {
    "title": "Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HFG7LcCCwK": {
    "title": "Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3NLRksLiZ": {
    "title": "Reservoir Transformer at Infinite Horizon: the Lyapunov Time and the Butterfly Effect",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fgKjiVrm6u": {
    "title": "REFACTOR: Learning to Extract Theorems from Proofs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=clU5xWyItb": {
    "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JR20YOE0H": {
    "title": "On Feature Diversity in Energy-based Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fj7Fzm5lWL": {
    "title": "Let's do the time-warp-attend: Learning topological invariants of dynamical systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWEqTLCHrw": {
    "title": "Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ySS7hH1smL": {
    "title": "Sparse MoE with Language Guided Routing for Multilingual Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UNv8RzIf5x": {
    "title": "Class-Wise Generalization Error: An Information-Theoretic Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zWqr3MQuNs": {
    "title": "Detecting Pretraining Data from Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V5tdi14ple": {
    "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lR3rk7ysXz": {
    "title": "On Diffusion Modeling for Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tvwf4Vsi5F": {
    "title": "Defending Against Transfer Attacks From Public Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FwdnG0xR02": {
    "title": "Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic Contrast Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tjn2YZSHUv": {
    "title": "Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZZbdJ4wff": {
    "title": "PRO: Pseudo-label Regularized Optimization on Unlabeled Test Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bDcaz87WCZ": {
    "title": "Recent Link Classification on Temporal Graphs Using Profile Builder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v3K5TVP8kZ": {
    "title": "AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xDaj4pRna": {
    "title": "Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rzEPbs4Wg": {
    "title": "Improving Generalization and Safety of Deep Neural Networks with Masked Anchoring",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ccxD4mtkTU": {
    "title": "Can LLM-Generated Misinformation Be Detected?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z8q8kBxC5H": {
    "title": "Sharp results for NIEP and NMF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bkdWThqE6q": {
    "title": "A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rwmWd2rjP1": {
    "title": "Molecule Relaxation by Reverse Diffusion with Time Step Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2VAi5F9BOJ": {
    "title": "PLPP: PROMPT LEARNING WITH PERPLEXITY FOR VISION-LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z91rwXnJsw": {
    "title": "Interactive Semantic Map Representation for Skill-based Visual Object Navigation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTEGyKf0dZ": {
    "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRoBig6ov1": {
    "title": "High-Order Tensor Recovery with A Tensor $U_1$ Norm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EDXkkUAIFW": {
    "title": "One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iI7hZSczxE": {
    "title": "Disentangling Time Series Representations via Contrastive based $l$-Variational Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aj1wftldeR": {
    "title": "D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eP6ZSy5uRj": {
    "title": "Endowing Protein Language Models with Structural Knowledge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GlpawHh80l": {
    "title": "Improved algorithm and bounds for successive projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AfnsTnYphT": {
    "title": "Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MtzHEqqUm0": {
    "title": "In-Depth Comparison of Regularization Methods For Long-Tailed Learning in Trajectory Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMwD6pGYB4": {
    "title": "A Distributional Analogue to the Successor Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zLwCT9srfo": {
    "title": "H-Rockmate: Hierarchical Approach for Efficient Re-materialization of Large Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0TZs6WOs16": {
    "title": "Hyperbolic Embeddings in Sequential Self-Attention for Improved Next-Item Recommendations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0tWTxYYPnW": {
    "title": "Understanding Hidden Context in Preference Learning: Consequences for RLHF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kvByNnMERu": {
    "title": "Estimating Shape Distances on Neural Representations with Limited Samples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OqlmgmS4Wr": {
    "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NTWtNjlThd": {
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TwB6N055Ub": {
    "title": "Reduced-Rank Online Gaussian Process Modeling With Uncertain Inputs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMv6zKYYUs": {
    "title": "Learning semilinear neural operators: A unified recursive framework for prediction and data assimilation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wsab3NhIwC": {
    "title": "Resource Efficient Self-Supervised Learning for Speech Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vkgAaCI3F": {
    "title": "Balancing Stability and Plasticity in Continual Learning: the readout-decomposition of activation change (RDAC) framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HOf3K763zg": {
    "title": "Beyond Differentiability: Neurosymbolic Learning with Black-Box Programs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kfpaq5CJPy": {
    "title": "Leveraging image representations for bounded adversarial attacks and robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PH0L3ABwM2": {
    "title": "SEER: Towards Efficient Preference-based Reinforcement Learning via Aligned Experience Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6werMQy1uz": {
    "title": "Rethinking the Buyer's Inspection Paradox in Information Markets with Language Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9v5uZPWZoV": {
    "title": "Not Just Pretty Pictures: Toward Interventional Data Augmentation Using Text-to-Image Generators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IrZTJ7t2GW": {
    "title": "Fair Adversarial Training: on the Adversarial Attack and Defense of Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjYNFZEjc7": {
    "title": "Head Information Bottleneck: An Evaluation Method for Transformer Head Contributions in Speech Task",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IEduRUO55F": {
    "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s90VIdza2K": {
    "title": "f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7suavRDxe8": {
    "title": "Plausibly Deniable Encryption with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IORAqe04sO": {
    "title": "The crossover strategy based on the cellular automata for genetic Algorithms with binary chromosomes population",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPvufoBAIs": {
    "title": "Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBwxmTbY6Z": {
    "title": "Tensor Time-Series Forecasting and Anomaly Detection with Augmented Causality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IOp3Qgep9V": {
    "title": "Towards Adversarially Robust Condensed Dataset by Curvature Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dONpC9GL1o": {
    "title": "Closing the Curious Case of Neural Text Degeneration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bsKMPAFHO7": {
    "title": "Mediator Interpretation and Faster Learning Algorithms for Linear Correlated Equilibria in General Sequential Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OinvjdvPjp": {
    "title": "xVal: A Continuous Number Encoding for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LokR2TTFMs": {
    "title": "3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VrHiF2hsrm": {
    "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JEYWfmz2TU": {
    "title": "Robot Learning from Demonstration: Enhancing Plan Execution with Failure Detection Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e4xS9ZarDr": {
    "title": "Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TTrzgEZt9s": {
    "title": "Distributionally Robust Optimization with Bias & Variance Reduced Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTY1RtowlY": {
    "title": "Competition Priors for Object-Centric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1GUTzm2a4v": {
    "title": "Greedy PIG: Adaptive Integrated Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OR4Jo158Dd": {
    "title": "Synthesizing Programmatic Policy for Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iWi2mL8qoc": {
    "title": "Multi-Scale Window based Transformer Network for High Quality Image Inpainting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hDzjO41IOO": {
    "title": "Tweedie Moment Projected Diffusions for Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=grQ97sPU5T": {
    "title": "Spectral Highways: Injecting Homophily into Heterophilic Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=viNQSOadLg": {
    "title": "Biological Sequence Editing with Generative Flow Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EsjoMaNeVo": {
    "title": "Steering No-Regret Learners to Optimal Equilibria",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tzD9HVgeVx": {
    "title": "AgentMixer: Multi-Agent Correlated Policy Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J7hbPeOZ39": {
    "title": "Dynamic Assortment Selection and Pricing with Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jla53ILAha": {
    "title": "Implicit regularization of multi-task learning and finetuning in overparameterized neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1zt8GWZ9sc": {
    "title": "Quack: Automatic Jailbreaking Large Language Models via Role-playing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hqUznsPMLn": {
    "title": "ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xRiZddh5Pb": {
    "title": "Learning from A Single Graph is All You Need for Near-Shortest Path Routing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gBLEHzKOfF": {
    "title": "Generative Entropic Neural Optimal Transport To Map Within and Across Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UZS6D7GfP1": {
    "title": "Human-in-the-loop Detection of AI-generated Text via Grammatical Patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fM1ETm3ssl": {
    "title": "Towards Meta-Models for Automated Interpretability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gppLqZLQeY": {
    "title": "Efficient Subgraph GNNs by Learning Effective Selection Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YIls9HEa52": {
    "title": "Parsing neural dynamics with infinite recurrent switching linear dynamical systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h7DGnWGeos": {
    "title": "Active Retrosynthetic Planning Aware of Route Quality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mBzsKsrXf9": {
    "title": "ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zb3b6oKO77": {
    "title": "How do Language Models Bind Entities in Context?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L9G9nR8fMF": {
    "title": "LayerAct: Advancing CNNs with BatchNorm through Layer-direction Normalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbVWug9f2h": {
    "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HfXDrAzFvG": {
    "title": "Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEAVTKUEpJ": {
    "title": "SARI: SIMPLISTIC AVERAGE AND ROBUST IDENTIFICATION BASED NOISY PARTIAL LABEL LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2UnCj3jeao": {
    "title": "Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzo7N2XkJ2": {
    "title": "Corrupting Unbounded Unlearnable Datasets with Pixel-based Image Transformations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQALhPTAfj": {
    "title": "Navigating Scaling Laws: Accelerating Vision Transformer's Training via Adaptive Strategies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aM7US5jKCd": {
    "title": "Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IU4L7wiwxw": {
    "title": "Pushing Gradient towards Zero: A Novel Pruning Method for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S24zdyiWDT": {
    "title": "Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkL8djXrMM": {
    "title": "Neural Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riYNe4jnKV": {
    "title": "Calibration-then-Calculation: A Variance Reduced Metric Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wdEHqQWTG4": {
    "title": "Robust Reinforcement Learning for Portfolio Management via Competition and Cooperation Strategies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uZfjFyPAvn": {
    "title": "Implicit Neural Representations and the Algebra of Complex Wavelets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sP1tCl2QBk": {
    "title": "Fiber Monte Carlo",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gMsZBhwiM4": {
    "title": "ICA model estimation using an optimized version of genetic algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQe9tHd0k8": {
    "title": "Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20KYsQ8Q4Z": {
    "title": "High-dimensional Bayesian Optimization with Group Testing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQwV7Y8qwa": {
    "title": "Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7GCRhebJEr": {
    "title": "Robustness via learned Bregman divergence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SEPaEuPwpr": {
    "title": "SOI: Scaling down computational complexity by estimating partial states of the model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lwtaEhDx9x": {
    "title": "Elephants Never Forget: Testing Language Models for Memorization of Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C42FkKhAUC": {
    "title": "IMPROVING ADVERSARIAL TRAINING WITH MARGIN- WEIGHTED PERTURBATION BUDGET",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HYyRwm367m": {
    "title": "Object-Centric Semantic Vector Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5LhYYajlqV": {
    "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0t1O8ziRZp": {
    "title": "Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9k27IITeAZ": {
    "title": "ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H5XZLeXWPS": {
    "title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=04UvXg4CvW": {
    "title": "EPIC: Compressing Deep GNNs via Expressive Power Gap-Induced Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9qtswuW5ux": {
    "title": "Unsupervised graph neural networks with recurrent features for solving combinatorial optimization problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RwwM7pKGWv": {
    "title": "Towards Dynamic EHR Phenotyping: A Generative Clustering Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FH7lfTfjcm": {
    "title": "ADELT: Transpilation Between Deep Learning Frameworks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWwYyxF3w9": {
    "title": "Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IBACinPJG5": {
    "title": "MIRAGE: Modelling Interpretable Multivariate Time Series Forecasts with Actionable Ground Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ztpy1gsUpT": {
    "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VbR6K7TQV4": {
    "title": "Learning the Latent Noisy Data Generative Process for Label-Noise Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkjcdmz8Ro": {
    "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jpu1Gd3F1r": {
    "title": "Data Imputation by Pursuing Better Classification: A Supervised Learning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RE0aibEQ1J": {
    "title": "IG-Net: Image-Goal Network for Offline Visual Navigation on A Large-Scale Game Map",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ajRRisV1n1": {
    "title": "Learning the Hidden Set Locally",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jsvvPVVzwf": {
    "title": "What Makes a Good Prune? Optimal Unstructured Pruning for Maximal Cosine Similarity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kjZlzuVJF0": {
    "title": "Boosting Multi-Agent Reinforcement Learning via Transition-Informed Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqEvdOS1Hs": {
    "title": "Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=unxTEvHOW7": {
    "title": "EXPLEME: A Study in Meme Interpretability, Diving Beyond Input Attribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vY9nzQmQBw": {
    "title": "Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFS9Pm7bsM": {
    "title": "Adversarial Latent Feature Augmentation for Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YrTI2Zu0dd": {
    "title": "An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T9w5ttdqLV": {
    "title": "Towards Complete Expressiveness Capacity of Mixed Multi-Agent Q Value Function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IjJU2BRSCV": {
    "title": "Differentiable Tree Search in Latent State Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YH9tnuUYds": {
    "title": "Model-based Reinforcement Learning for Parameterized Action Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdaPky8MUn": {
    "title": "Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAGtjl7HOw": {
    "title": "Explaining Kernel Clustering via Decision Trees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCl0TLboP1": {
    "title": "Improving Offline RL by Blending Heuristics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ctXZJLBbyb": {
    "title": "Understanding Heterophily for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K4fd38VWHt": {
    "title": "Assessing Robustness via Score-based Adversarial Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TS8PXBN6B6": {
    "title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZiHI6raor0": {
    "title": "CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQuF0jslCc": {
    "title": "Online Fractional Knapsack With Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eqz5aXtQv1": {
    "title": "STUPD: A Synthetic Dataset for Spatial and Temporal Relation Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T0FuEDnODP": {
    "title": "Cooperative Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NI0RsRuFsW": {
    "title": "How Hard is Trojan Detection in DNNs? Fooling Detectors With Evasive Trojans",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5TlHjMVrNG": {
    "title": "Evaluating Robustness to Unforeseen Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U6Qulbv2qT": {
    "title": "Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4r2ybzJnmN": {
    "title": "Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HmL2Buf0Ur": {
    "title": "Can Copyright be Reduced to Privacy?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GPKTIktA0k": {
    "title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O0vy7hHqyU": {
    "title": "Fake News Detection via an Adaptive Feature Matching Optimization Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LKx4rubqkO": {
    "title": "Metric Learning for Detection of Large Language Model Generated Texts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOBvr1PxFd": {
    "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qRbkTbe8JT": {
    "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z3mPLBLfGY": {
    "title": "Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzkpLkd1S8": {
    "title": "Improving Robustness in Vision Transformers with Nullspace Noise Augmented Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WTh6EnJXWQ": {
    "title": "DeepROCK: Error-controlled interaction detection in deep neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8S7eGD15b6": {
    "title": "Subspace Grid-sweep: ML Defense Evaluation via Constrained Brute-force Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2l7g7zwC4z": {
    "title": "Embedding File Structure for Tabular File Preparation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Jwpw4qKkb": {
    "title": "Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=af2c8EaKl8": {
    "title": "Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PXXuLvIH5r": {
    "title": "From Matching to Mixing: A Graph Interpolation Approach for SAT Instance Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YIWe2amtrV": {
    "title": "Are LLMs Aware that Some Questions are not Open-ended?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rUH2EDpToF": {
    "title": "Generative Marginalization Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3i7iNGxw6r": {
    "title": "Where Does In-context Machine Translation Happen in Large Language Models?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j5EbZEyK9I": {
    "title": "Mo' Data Mo' Problems: How Data Composition Compromises Scaling Properties",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSh5ePa0ph": {
    "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RyUvzda8GH": {
    "title": "A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jo36Mzwuvf": {
    "title": "Gaussian Process-Based Corruption-resilience Forecasting Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DjIsNDEOYX": {
    "title": "Scalable Monotonic Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s2NjWfaYdZ": {
    "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bwZ9xh178a": {
    "title": "Exploiting Negative Samples: A Catalyst for Cohort Discovery in Healthcare Analytics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N1gmpVd4iE": {
    "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DxM73sxtna": {
    "title": "Private Overparameterized Linear Regression without Suffering in High Dimensions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKK1jXEWja": {
    "title": "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vueANsev2R": {
    "title": "Investigating the chaotic dynamics produced by deep reinforcement learning controllers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x5LvBK43wg": {
    "title": "PROGRAM: PROtotype GRAph Model based Pseudo-Label Learning for Test-Time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xC8xh2RSs2": {
    "title": "Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on HuggingFace",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XCMbagV0No": {
    "title": "A Language-Agent Approach to Formal Theorem-Proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6klub5OXr": {
    "title": "An Extensive Analysis on the Underlying Premises Behind Deep Reinforcement Learning Algorithm Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkSjjs4o5d": {
    "title": "A Differentially Private Clustering Algorithm for Well-Clustered Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jeMZi2Z9xe": {
    "title": "Follow-the-Perturbed-Leader for Adversarial Bandits: Heavy Tails, Robustness, and Privacy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ge0GEOvifh": {
    "title": "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d94x0gWTUX": {
    "title": "Tool-Augmented Reward Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rpwES4pe9W": {
    "title": "Refined Tensorial Radiance Field: Harnessing coordinate based networks for novel view synthesis from sparse inputs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EW8ZExRZkJ": {
    "title": "Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXTr9hIvJ1": {
    "title": "Reweighted Solutions for Weighted Low Rank Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XOnya9gSdF": {
    "title": "Consistent algorithms for multi-label classification with macro-at-$k$ metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=50vyPuz0iv": {
    "title": "Iteratively Refined Behavior Regularization for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ul1cjLB98Y": {
    "title": "A Theory of Unimodal Bias in Multimodal Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAY6ORIvn5": {
    "title": "How well does Persistent Homology generalize on graphs?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d4uL2MSe0z": {
    "title": "Dynamic Layer Tying for Parameter-Efficient Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLBPjECC5D": {
    "title": "Unlearning via Sparse Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GSBHKiw19c": {
    "title": "Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N581Nje6fH": {
    "title": "Long Horizon Episodic Decision Making for Cognitively Inspired Robots",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NgtEafc8NZ": {
    "title": "Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kO8AxyGBxG": {
    "title": "UNITE:Universally Trustworthy GNN Via Subgraph Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymjI8feDTD": {
    "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IlNVkYUSfF": {
    "title": "Resonator-Gated RNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEhTnQZB3D": {
    "title": "Learning with Language Inference and Tips for Continual Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAMoxm86KV": {
    "title": "Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rUx0zQFwD1": {
    "title": "Quantum Speedups in Linear Programming via Sublinear Multi-Gibbs Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=icTZCUbtD6": {
    "title": "Dissecting sample hardness: Fine-grained analysis of Hardness Characterization Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tPjVRmHqCg": {
    "title": "Curiosity Driven Protein Sequence Generation via Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D9SA02esgh": {
    "title": "MorphOcc: An Implicit Generative Model of Neuronal Morphologies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OfAO2mes1": {
    "title": "Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7fxzVTSgZC": {
    "title": "Offline Imitation Learning without Auxiliary High-quality Behavior Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLRo4qhg6t": {
    "title": "HIWE: Scene Importance Weighted Encoding For Fast Neural Radiance Field Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8uFGTNXIF": {
    "title": "Simplifying Referred Visual Search with Conditional Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hz2zhaZPXm": {
    "title": "Towards Foundation Models for Learning on Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8zJevzvk64": {
    "title": "Schrodinger Bridge to Bridge Generative Diffusion Method to Off-Policy Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rO62BY3dYc": {
    "title": "Pruning via Ranking (PvR): A unified structured pruning approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5e0yWSNGIc": {
    "title": "Exposing the Silent Hidden Impact of Certified Training in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vlQ56aWJhl": {
    "title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5VD7dS3cZX": {
    "title": "Rethinking the Solution to Curse of Dimensionality on Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mssRRt6OPE": {
    "title": "Relevance-based embeddings for efficient relevance retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ch7WqGcGmb": {
    "title": "Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOPJantEd": {
    "title": "Wigner kernels: body-ordered equivariant machine learning without a basis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X0fDR10B7c": {
    "title": "Predictive Coding beyond Correlations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2inBuwTyL2": {
    "title": "Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=reBq1gmlhS": {
    "title": "Learning Differentially Private Rewards from Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3YaCghZNt": {
    "title": "Lemur: Integrating Large Language Models in Automated Program Verification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kRdcwzEL5J": {
    "title": "CUS3D: A New Comprehensive Urban-Scale Semantic Segmentation 3D Benchmark Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=liuqDwmbQJ": {
    "title": "ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5Tp4WwZl8": {
    "title": "Error Feedback Shines when Features are Rare",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6yv8UHVJn4": {
    "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UMOlFJzLfL": {
    "title": "A Precise Characterization of SGD Stability Using Loss Surface Geometry",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D1w3huGGpu": {
    "title": "Compositional Interfaces for Compositional Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WpQbM1kBuy": {
    "title": "Prodigy: An Expeditiously Adaptive Parameter-Free Learner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xt9Bu66rqv": {
    "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86NGO8qeWs": {
    "title": "CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ro3EBZiKhT": {
    "title": "HiLoRL: A Hierarchical Logical Model for Learning Composite Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LYS3RhIYCq": {
    "title": "Scaling Laws for Imitation Learning in Single-Agent Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0q880yOgY": {
    "title": "Can Language Agents Approach the Performance of RL? An Empirical Study On OpenAI Gym",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSrDaGWTnv": {
    "title": "Contrastive Representations Make Planning Easy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7gVX2LxE7A": {
    "title": "SpecAR-Net: Spectrogram Analysis and Representation Network for Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fnO5h1CFyh": {
    "title": "Learning Successor Representations with Distributed Hebbian Temporal Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7LrHqcOyh": {
    "title": "DNCs require more planning steps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ulaUJFd96G": {
    "title": "Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uw5U7FfTRf": {
    "title": "BaDLoss: Backdoor Detection via Loss Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ePOjNlOjLC": {
    "title": "Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QbXo9pbJpp": {
    "title": "Improved Invariant Learning for Node-level Out-of-distribution Generalization on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NPViqdhTIi": {
    "title": "Parameter-Free Molecular Classification and Regression with Gzip",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DfPtC8uSot": {
    "title": "Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcSDt7H8kI": {
    "title": "Boosting Reinforcement Learning with Extremum Experiences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFLZFxoLnE": {
    "title": "Modify Training Direction in Function Space to Reduce Generalization Error",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oXjnwQLcTA": {
    "title": "Score Models for Offline Goal-Conditioned Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTU2X9mUBe": {
    "title": "LaDe: The First Comprehensive Last-mile Express Dataset from Industry",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=krx55l2A6G": {
    "title": "Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=igfDXfMvm5": {
    "title": "USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk0ykeYCfP": {
    "title": "Analyzing the Effects of Emulating on the Reinforcement Learning Manifold",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4y3GDTFv70": {
    "title": "A Latent Space Theory for Emergent Abilities in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S4wo3MnlTr": {
    "title": "A trainable manifold for accurate approximation with ReLU Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkhVrIllKg": {
    "title": "Federated Learning Under Second-Order Data Heterogeneity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAMPOUF227": {
    "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XN6ZPINdSg": {
    "title": "COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3UIvWeAKs": {
    "title": "Learning Node Selection via Tripartite Graph Representation in Mixed Integer Linear Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0akLDTFR9x": {
    "title": "Contrastive Difference Predictive Coding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpiOUOaqh3": {
    "title": "PARAMETER OPTIMIZATION FOR EPIDEMIOLOGICAL MODEL WITH GENETIC ALGORITHM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ytGU2iit80": {
    "title": "From Fourier to Neural ODEs: Flow matching for modeling complex systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ISfY3YMXxU": {
    "title": "INRet: A General Framework for Accurate Retrieval of INRs for Shapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pogJXugbN8": {
    "title": "BAFFLE: A Baseline of Backpropagation-Free Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NiefAhgJqH": {
    "title": "Bayesian Exploration Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dtFN6T4aMU": {
    "title": "MAST: A Sparse Training Framework for Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u7LFI98JI6": {
    "title": "GraphDeepONet: Learning to simulate time-dependent partial differential equations using graph neural network and deep operator network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNLjU5C5dK": {
    "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWzUA9zeAg": {
    "title": "Effective Data Augmentation With Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5T46w5X3Go": {
    "title": "Theoretical Analysis on the Generalization Power of Overfitted Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bo6GpQ3B9a": {
    "title": "Out-Of-Domain Unlabeled Data Improves Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYXtXLYHpR": {
    "title": "Towards Transparent Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TCSoDjtSZL": {
    "title": "Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=61mnwO4Mzp": {
    "title": "Denoising Diffusion Variational Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkvdAYhyqm": {
    "title": "Explaining black box text modules in natural language with language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fw1oizreEF": {
    "title": "Convexifying Transformers: Improving optimization and understanding of transformer networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DyclWshWvf": {
    "title": "Causal-based Analysis on Credibility of Feedforward Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BlkxbI6vzl": {
    "title": "A Fast and Provable Algorithm for Sparse Phase Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNZJyEDxy4": {
    "title": "MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNFKm4AStf": {
    "title": "Representation Disentanglement via Regularization by Causal Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNvubydSB5": {
    "title": "HiGen: Hierarchical Graph Generative Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iP8ig954Uz": {
    "title": "HART: Efficient Adaptation via Regularized Autoregressive Parameter Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8BAkNCqpGW": {
    "title": "A Policy Gradient Method for Confounded POMDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xoZ29eXUk7": {
    "title": "A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. ‘Ending the HIV Epidemic' initiative",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yoVq2BGQdP": {
    "title": "Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tAcEidZ1Y2": {
    "title": "Self-supervision Meets Bootstrap Estimation: New Paradigm for Unsupervised Reconstruction with Uncertainty Quantification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M2oUA4XBq4": {
    "title": "Learning to ignore: Single Source Domain Generalization via Oracle Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHv6wcBb0z": {
    "title": "Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ndR8Ytrzhh": {
    "title": "Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJEcAnFPqC": {
    "title": "Toward a Mechanistic Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MdHDUsP2lt": {
    "title": "Information-Theoretic World Model learning for Denoised Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xLRAQiqd9I": {
    "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zQo5oUvia": {
    "title": "Retrieval-Based Reconstruction For Time-series Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1IbewSnqq": {
    "title": "FedQV: Leveraging Quadratic Voting in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Itc7v0pnA": {
    "title": "Quantile-Free Regression: A Flexible Alternative to Quantile Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DbRfXmzwjc": {
    "title": "MAGNet: Motif-Agnostic Generation of Molecules from Shapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HqQctXKI7W": {
    "title": "Casting Light on Large Generative Networks: Taming Epistemic Uncertainty in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7oYpj8BOLW": {
    "title": "BackBench: Are Vision Language Models Resilient to Object-to-Background Context?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oi6BhzIu7R": {
    "title": "REAL: Rectified Adversarial Sample via Max-Min Entropy for Test-Time Defense",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JdWpIe70FL": {
    "title": "Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fGAIgO75dG": {
    "title": "CoLiDE: Concomitant Linear DAG Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDQ9ZzsgDL": {
    "title": "PromptNER : Prompting For FewShot Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CgpiO0DRrk": {
    "title": "Video Caching at Data-drifting Network Edge: A KD-based Cross-domain Collaborative Solution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LzPWWPAdY4": {
    "title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikmuHqugN7": {
    "title": "Scaling Convex Neural Networks with Burer-Monteiro Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vogtAV1GGL": {
    "title": "Simple mechanisms for representing, indexing and manipulating concepts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r42tSSCHPh": {
    "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Jl0sjmZx9": {
    "title": "Large Multimodal Model for Real-World Radiology Report Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y21ZO6M86t": {
    "title": "PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I7FPVqlwSe": {
    "title": "Reward Translation via Reward Machine in Semi-Alignable MDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LLho5X6xV": {
    "title": "UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model in Data Science",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qi88abxiE4": {
    "title": "Large-Scale Spectral Graph Neural Networks via Laplacian Sparsification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q9vYgjcvrX": {
    "title": "SASS: Self-Alignment with Semi-Supervised Instruction Data Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JCn0kmS8W": {
    "title": "WavJourney: Compositional Audio Creation with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hCrFG9cyuC": {
    "title": "PolyVoice: Language Models for Speech to Speech Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IOEEDkla96": {
    "title": "Adversarial Feature Map Pruning for Backdoor",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7HdtLgsvys": {
    "title": "Tube Loss: A Novel Approach for High Quality Prediction Interval Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=awHTL3Hpto": {
    "title": "Expressivity of ReLU-Networks under Convex Relaxations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDfq0RRkuz": {
    "title": "WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oO6FsMyDBt": {
    "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsjNCPqziJ": {
    "title": "Learning Latent Causal Semantics from Text: An Empirical Study of Next-Token Predictors Trained on Programs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YqyTXmF8Y2": {
    "title": "Emerging Pixel-level Semantic Knowledge in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wXpSidPpc5": {
    "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3husFxdHI1": {
    "title": "Duality of Information Flow: Insights in Graphical Models and Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LixGd92Wri": {
    "title": "GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HBEjrlu7Aa": {
    "title": "Object-level Data Augmentation for Visual 3D Object Detection in Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YEPlTU5mZC": {
    "title": "Implicit Gaussian process representation of vector fields over arbitrary latent manifolds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IGzaH538fz": {
    "title": "GraphGuard: Provably Robust Graph Classification against Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfaPgIQTul": {
    "title": "Learning HJB Viscosity Solutions with PINNs for Continuous-Time Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wMXH8tTQE3": {
    "title": "ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cYksYKbf6K": {
    "title": "Imagine Within Practice: Conservative Rollout Length Adaptation for Model-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=61DYdiyQqk": {
    "title": "Two Heads Are Better Than One: Exploiting Both Sequence and Graph Models in AMR-To-Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xVbke7yC07": {
    "title": "Enhancing Tropical Cyclone Formation Prediction Using Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gbrHZq07mq": {
    "title": "Logical Languages Accepted by Transformer Encoders with Hard Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Kf1AIdeyt": {
    "title": "Balancing Information Preservation and Computational Efficiency: L2 Normalization and Geodesic Distance in Manifold Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apQukvJHFE": {
    "title": "Lightweight uncertainty modelling using function space particle optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qNrJJZAKI3": {
    "title": "FairSeg: A Large-scale Medical Image Segmentation Dataset for Fairness Learning with Fair Error-Bound Scaling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G1Hlubz1fR": {
    "title": "Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9gyDdCKTDJ": {
    "title": "Gaitor: Learning a Unified Representation for Continuous Gait Transition and Terrain Traversal for Quadruped Robots",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IiimxXqxNP": {
    "title": "Efficient and scalable reinforcement learning via hypermodel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anG8cNYQAs": {
    "title": "INCYDE: A large scale cyclone detection and intensity estimation dataset using satellite infrared imagery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8DLVrWL78S": {
    "title": "Streamlining Generative Models for Structure-Based Drug Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nu9mOSq7eH": {
    "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTqmVbBa8R": {
    "title": "Generative Modeling of Individual Behavior at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rAX55lDjtt": {
    "title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Koh0i2u8qX": {
    "title": "Mitigating Severe Robustness Degradation on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p4S5Z6Sah4": {
    "title": "Traveling Waves Encode The Recent Past and Enhance Sequence Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I0wEUVzbNY": {
    "title": "A Critical Study of What Pre-trained Code Models (do not) Learn",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E64ZqVCr72": {
    "title": "Active Domain Adaptation Of Medical Images Using Feature Disentanglement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YkEW5TabYN": {
    "title": "Perturbed examples reveal invariances shared by language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FT4gAPFsQd": {
    "title": "How Sparse Can We Prune A Deep Network: A Geometric Viewpoint",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6IjN7oxjXt": {
    "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5ouuojPGe": {
    "title": "Red Pill or Blue Pill? Thresholding Strategies for Neural Network Monitoring",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CWoIj2XJuT": {
    "title": "Unbalanced Diffusion Schrödinger Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rKPK2Rn6y8": {
    "title": "Autonomous Tree-search Ability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHYkcQzdae": {
    "title": "A Novel Approach for Micro-Expression Recognition Incorporating Vertical Attention and Position Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaAtQwhnM2": {
    "title": "Perturb and Learn: Energy-Based Modelling in Discrete Spaces without MCMC",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hB2hXtxIPH": {
    "title": "Greedy Sequential Execution: Solving Homogeneous and Heterogeneous Cooperative Tasks with a Unified Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U2ZIgcrg7Z": {
    "title": "ZOOPFL: EXPLORING BLACK-BOX FOUNDATION MODELS FOR PERSONALIZED FEDERATED LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLevOah0CJ": {
    "title": "Hindsight PRIORs for Reward Learning from Human Preferences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONnZVUrFBT": {
    "title": "Communication-Efficient Algorithm for Asynchronous Multi-Agent Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mbPvdO2dxb": {
    "title": "Meta-Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nN1bEm8cna": {
    "title": "Are Spiking Neural Networks more expressive than Artificial Neural Networks?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kzGuiRXZrQ": {
    "title": "Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zNzVhX00h4": {
    "title": "Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DUkYDXqxKp": {
    "title": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mRw9BuNO9i": {
    "title": "Effortless Cross-Platform Video Codec: A Codebook-Based Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RzOm9oOSzm": {
    "title": "Unveiling Linear Mode Connectivity of Re-basin from Neuron Distribution Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xkf2EBj4w3": {
    "title": "Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqHaLnans2": {
    "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fszrlQ2DuP": {
    "title": "Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AMCaG2TAeg": {
    "title": "Causal Influence-Aware Counterfactual Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XT2yAa6Bbp": {
    "title": "Sinkhorn Output Perturbations: Structured Pseudo-Label Noise in Semi-Supervised Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcHty5VvkQ": {
    "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49ZYkhEGmv": {
    "title": "Scalabale AI Safety via Doubly-Efficient Debate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyTPWfXtcc": {
    "title": "Global Optimality for Non-linear Constrained Restoration Problems via Invexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MiMxv6ijvC": {
    "title": "CARENET : A NOVEL ARCHITECTURE FOR LOW DATA REGIME MIXING CONVOLUTIONS AND ATTENTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0c2IaQhHk": {
    "title": "Exploring the State and Action Space in Reinforcement Learning with Infinite-Dimensional Confidence Balls",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DHCp41nv1M": {
    "title": "Seeing Video Through Optical Scattering Media using Spatio-Temporal Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TeeyHEi25C": {
    "title": "Value function estimation using conditional diffusion models for control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jYHRP6nj9Q": {
    "title": "CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OGtnhKQJms": {
    "title": "Multi-View Causal Representation Learning with Partial Observability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqovbCMrOp": {
    "title": "On the Recoverability of Causal Relations from Temporally Aggregated I.I.D Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anek0q7QPL": {
    "title": "Exploring the Combined Power of Covariance and Hessian Matrices Eigenanalysis for Binary Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DkYQHewNcp": {
    "title": "Unsupervised Detection of Recurrent Patterns in Neural Recordings with Constrained Filters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iriEqxFB4y": {
    "title": "DOS: Diverse Outlier Sampling for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7n360rsYAq": {
    "title": "Towards Dynamic Trend Filtering through Trend Points Detection with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ws0F5NTzGw": {
    "title": "AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cTOL99p5HL": {
    "title": "Ghost in the Minecraft: Hierarchical Agents for Minecraft via Large Language Models with Text-based Knowledge and Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nddtu94uX": {
    "title": "PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r6NMqADLGQ": {
    "title": "How To Train Your Covariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MY0qlcFcUg": {
    "title": "Denoising Task Routing for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=48CXLrx7K3": {
    "title": "Revealing Unintentional Information Leakage in Low-Dimensional Facial Portrait Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQrHpTllXa": {
    "title": "CABINET: Content Relevance-based Noise Reduction for Table Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJDlRzQh7x": {
    "title": "Neural Networks and Solomonoff Induction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dcjtMYkpXx": {
    "title": "Reward Model Ensembles Help Mitigate Overoptimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TyFrPOKYXw": {
    "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S4zpk61r6G": {
    "title": "DiffMaSIF: Score-Based Diffusion Models for Protein Surfaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTL534o7Ot": {
    "title": "Programmable Synthetic Data Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKGQDDTuvZ": {
    "title": "Frequency-Aware Transformer for Learned Image Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMFSUjxMIl": {
    "title": "CircuitNet 2.0: An Advanced Dataset for Promoting Machine Learning Innovations in Realistic Chip Design Environment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEdg9JvO8X": {
    "title": "BDQL: Offline RL via Behavior Diffusion Q-learning without Policy Constraint",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OHll7EfuSi": {
    "title": "Weight-Based Performance Estimation for Diverse Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdTe8S0Mkl": {
    "title": "Humans vs ChatGPT: Uncovering the Non-trivial Distinctions by Evaluating Parallel Responses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyQO9RPhwN": {
    "title": "Geometry-Guided Conditional Adaption for Surrogate Models of Large-Scale 3D PDEs on Arbitrary Geometries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wprSv7ichW": {
    "title": "Benchmarking Algorithms for Federated Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T23HYw6lta": {
    "title": "Forget-Me-Not: Making Backdoor Hard to be Forgotten in Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q9jQPA6zPK": {
    "title": "Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IzrLkbq1dc": {
    "title": "Analyzing Local Representations of Self-supervised Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=veIzQxZUhF": {
    "title": "Deep concept removal",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HAMBmtKLc8": {
    "title": "Graph Neural Networks on Symmetric Positive Definite Manifold",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dk10QugVHb": {
    "title": "Causal analysis of social bias in CLIP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=epFk8e470p": {
    "title": "Deep Models modelled after human brain boost performance in action classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mDe5o24BM": {
    "title": "HFDream: Improving 3D Generation via Human-Assisted Multi-view Text-to-Image Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xvhjRjoFCN": {
    "title": "BiXT: Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kwn9ySjbc1": {
    "title": "Variable resolution: improving scene visual question answering with a limited pixel budget",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3wpuxd7u9": {
    "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PczQtTsTIX": {
    "title": "Cross$Q$: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LY1eOfqU16": {
    "title": "A Soft Labeling Approach for Fairness-aware Learning Under Partially Annotated Sensitive Attributes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scFfMOOGD8": {
    "title": "Learnable Invisible Backdoor for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yhvtZdqBNm": {
    "title": "Pruning Attention Heads with Almost-sure Sparsity Targets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8WH6ZlDad6": {
    "title": "EWoK: Tackling Robust Markov Decision Processes via Estimating Worst Kernel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aup1BV78Gq": {
    "title": "A New Type of Associative Memory Network with Exponential Storage Capacity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjivA1SLZ6": {
    "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZ6r9GMT1n": {
    "title": "Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=02f3mUtqnM": {
    "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3cuJwmPxXj": {
    "title": "Identifying Representations for Intervention Extrapolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmq67R2PIu": {
    "title": "DockGame: Cooperative Games for Multimeric Rigid Protein Docking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yZBpnKpBCw": {
    "title": "Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ck4SG9lnrQ": {
    "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j5JvZCaDM0": {
    "title": "Feasibility-Guided Safe Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tKu7NNu0Yq": {
    "title": "DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EraNITdn34": {
    "title": "Unlocking the Transferability of Tokens in Deep Models for Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S5EqslEHnz": {
    "title": "Do Generated Data Always Help Contrastive Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1P92J25hdf": {
    "title": "Going Deeper with General and Specific Inductive Bias for Real-Time Stereo Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JRbs3i9Ei": {
    "title": "Machine Learning for PROTAC Engineering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2LhCPowI6i": {
    "title": "Self-Supervised Pseudodata Filtering for Improved Replay with Sub-Optimal Generators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vst5P4Pve2": {
    "title": "Towards Global Interaction Efficiency of Graph Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kMp8zCsXNb": {
    "title": "ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJ0dzPJq1F": {
    "title": "Blending Imitation and Reinforcement Learning for Robust Policy Improvement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L9kwewFGQZ": {
    "title": "Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0uUASYeXav": {
    "title": "Graphical Object-Centric Actor-Critic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsnR0YO4Fq": {
    "title": "Exploring Weight Balancing on Long-Tailed Recognition Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tuzTN0eIO5": {
    "title": "Zero Bubble Pipeline Parallelism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ynguffsGfa": {
    "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zh047FhXqI": {
    "title": "Effective Offline Environment Reconstruction when the Dataset is Collected from Diversified Behavior Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqtbL7j2JW": {
    "title": "You Only Submit One Image to Find the Most Suitable Generative Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSxE03S4fp": {
    "title": "Learn to Achieve Out-of-the-Box Imitation Ability from Only One Demonstration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mWf3RGc6HG": {
    "title": "Revisiting Ternary Neural Networks towards Asymmetric Thresholds and Uniform Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3NBqtrQat": {
    "title": "Learning Object-Centric Representation via Reverse Hierarchy Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TCJbcjS0c2": {
    "title": "LASER: Linear Compression in Wireless Distributed Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3OCarOfxx": {
    "title": "Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2bF381xEke": {
    "title": "MapSelect: Sparse & Interpretable Graph Attention Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMjflI1aL0": {
    "title": "Imbalanced data robust online continual learning based on evolving class aware memory selection and built-in contrastive representation learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qW9GVa3Caa": {
    "title": "Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vtyasLn4RM": {
    "title": "CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eKGEsFdpin": {
    "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LYG6tBlEX0": {
    "title": "H-GAP: Humanoid Control with a Generalist Planner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HRkyLbBRHI": {
    "title": "Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5CgCJY2po": {
    "title": "Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VeFmnRmoaW": {
    "title": "MetroGNN: Metro Network Expansion with Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfHISoWo2m": {
    "title": "Meta-Learning Nonlinear Dynamical Systems with Deep Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwtMhMSybu": {
    "title": "Unlocking the Power of Representations in Long-term Novelty-based Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FNCFiXKYoq": {
    "title": "MAAD Private: Multi-Attribute Adversarial Debiasing with Differential Privacy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oUF3uGIVo": {
    "title": "Exploring High-Order Message-Passing in Graph Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcZ9VadFd5": {
    "title": "Emergence of Equivariance in Deep Ensembles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8JizpeY4y": {
    "title": "Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBqowcUwFP": {
    "title": "L(M)V-IQL: Multiple Intention Inverse Reinforcement Learning for Animal Behavior Characterization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQgm3UvGNY": {
    "title": "Synergistic Information Retrieval: Interplay between Search and Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T1Y2KmVtUn": {
    "title": "Differentiable Sensor Layouts for End-to-End Learning of Task-Specific Camera Parameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EG68RSznLT": {
    "title": "Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ro4CgvfUKy": {
    "title": "Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UpgRVWexaD": {
    "title": "Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mkdwvl3Y8L": {
    "title": "Discovering Knowledge-Critical Subnetworks in Neural Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTFdNLHE7k": {
    "title": "Kernelised Normalising Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HdAoLSBYXj": {
    "title": "Topic modeling as multi-objective optimization with Setwise Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9DvDRTTdlu": {
    "title": "ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LxCPyLREX5": {
    "title": "Federated Learning under Label Shifts with Guarantees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8FHWkY0SwF": {
    "title": "Learning Personalized Causally Invariant Representations for Heterogeneous Federated Clients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dn87xnULwF": {
    "title": "Maximally Expressive GNNs for Outerplanar Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lWXedJyLuL": {
    "title": "A Unified Causal View of Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJs4oE4m9Q": {
    "title": "Deep Orthogonal Hypersphere Compression for Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhNXGWVH1N": {
    "title": "LeanFlex-GKP: Advancing Hassle-Free Structured Pruning with Simple Flexible Group Count",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Op1XmdxFk8": {
    "title": "ProtoReg: Prioritizing Discriminative Information for Fine-grained Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7v3tkQmtpE": {
    "title": "Rethinking Decision Transformer via Hierarchical Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R7rZUSGOPD": {
    "title": "PAE: Reinforcement Learning from External Knowledge for Efficient Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2h3m61LFWL": {
    "title": "Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EOTgj37XNM": {
    "title": "Classifiers are Forgetful! Balancing the Mutual Causal Effects in Class-Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nrDRBhNHiB": {
    "title": "A multiobjective continuation method to compute the regularization path of deep neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgmCGI2IpI": {
    "title": "An Efficient Query Strategy for Active Learning via Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gny0PVtKz2": {
    "title": "ConvFormer: Revisiting Token-mixers for Sequential User Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7iCUSBlOgh": {
    "title": "Toward Generalizability of Graph-based Imputation on Bio-Medical Missing Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rHzapPnCgT": {
    "title": "Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d2TOOGbrtP": {
    "title": "Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XaqaitclOA": {
    "title": "Investigating the Ability of PINNs To Solve Burgers' PDE Near Finite-Time BlowUp",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ZWxBU9sYG": {
    "title": "How to Craft Backdoors with Unlabeled Data Alone?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pa6SiS66p0": {
    "title": "Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xd46Q82QEO": {
    "title": "Exploring Pointwise Similarity of Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tB7p0SM5TH": {
    "title": "GraSP: Simple yet Effective Graph Similarity Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JGP1GlTnLF": {
    "title": "Learning from Distinction: Mitigating backdoors using a low-capacity model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1dY11GyZdp": {
    "title": "Signed-Binarization: Unlocking Efficiency Through Repetition-Sparsity Trade-Off",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dxI1HLatWw": {
    "title": "Generalized Temporal Difference Learning Models for Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N134PpnlKs": {
    "title": "Twinned Interventional Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1yll8U12GT": {
    "title": "Enhancing Decision Tree Learning with Deep Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CJnyR3M6Oh": {
    "title": "Sparse hyperbolic representation learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i43XCU54Br": {
    "title": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x6gnuUXpxM": {
    "title": "Constructing Sparse Neural Architecture with Deterministic Ramanujan Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFQe52N40m": {
    "title": "Online Feature Updates Improve Online (Generalized) Label Shift Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8p3fu56lKc": {
    "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yYylDyLnzt": {
    "title": "Dantzig-Wolfe Decomposition and Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KIq6p9iv2q": {
    "title": "Towards Perpetually Trainable Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KskgLM728l": {
    "title": "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZS6lgCLr2B": {
    "title": "Tackling Byzantine Clients in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dVq2StlcnY": {
    "title": "Interpretable and Generalizable Graph Neural Networks via Subgraph Multilinear Extension",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=om5z1n0mXA": {
    "title": "Rethinking the Effectiveness of Graph Classification Datasets in Benchmarks for Assessing GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MBIGXMT0qC": {
    "title": "Multi-Scale Protein Language Model for Unified Molecular Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJ2PQ9QaDF": {
    "title": "Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8euJaTveKw": {
    "title": "Prometheus: Inducing Evaluation Capability in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fMzO6vcmhy": {
    "title": "QORA: Zero-Shot Transfer via Interpretable Object-Relational Model Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lxc4nBkJuq": {
    "title": "Dissecting Gradient Masking and Denoising in Diffusion Models for Adversarial Purification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xuY33XhEGR": {
    "title": "ClimODE: Climate Forecasting With Physics-informed Neural ODEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=THUBTfSAS2": {
    "title": "Querying Easily Flip-flopped Samples for Deep Active Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DKfcxPxunu": {
    "title": "Multi-Task Learning for Routing Problem with Zero-Shot Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDb2JQZsFH": {
    "title": "Attention-based Iterative Decomposition for Tensor Product Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=er7VhmqZEA": {
    "title": "NOISY MULTI-VIEW CONTRASTIVE LEARNING FRAMEWORK FOR ENHANCING TOP-K RECOMMENDATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzAuFCKiov": {
    "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SKfBx2rv2c": {
    "title": "Feasible Algorithmic Recourse Without Explicit Structure Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RR70yWYenC": {
    "title": "Efficient Instance-Optimal Finite-Sum Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2iGiSHmeAN": {
    "title": "BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BIglOUjfXX": {
    "title": "Forked Diffusion for Conditional Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6WtaLj8O1": {
    "title": "Fully Hyperbolic Representation Learning on Knowledge Hypergraph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=95joD3Yc5t": {
    "title": "Generative Semantic Communication: Diffusion Models Beyond Bit Recovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FFvCjbhpDq": {
    "title": "The Role of Forgetting in Fine-Tuning Reinforcement Learning Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ki39vo5x1T": {
    "title": "Federated Offline Policy Learning with Heterogeneous Observational Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5jWsW08zUh": {
    "title": "Some Intriguing Aspects about Lipschitz Continuity of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OVPoEhbsDm": {
    "title": "Thermodynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNfniUgXJt": {
    "title": "WASSERSTEIN-GUIDED SYMBOLIC REGRESSION: MODEL DISCOVERY OF NETWORK DYNAMICS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZqKxMHcAg": {
    "title": "Evaluating Language Models Through Negotiations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JKpk2p4O99": {
    "title": "Towards robust unlearnable examples via deep hiding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JshLcbPI9J": {
    "title": "Deep Backtracking Counterfactuals for Causally Compliant Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JSS9rKHySk": {
    "title": "On the Role of General Function Approximation in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZS4m74kZpH": {
    "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEzY6fRUMH": {
    "title": "State Chrono Representation for Enhancing Generalization in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EwAGztBkJ6": {
    "title": "On the Generalization of Gradient-based Neural Network Interpretations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KqTzfiNjWU": {
    "title": "Restorer Guided Diffusion Models for Variational Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8T7m27VC3S": {
    "title": "3D Dense Captioning beyond Nouns: A Middleware for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYWsyTuiRp": {
    "title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9sVJ17zvB": {
    "title": "VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SIZWiya7FE": {
    "title": "Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3vnnLeajU": {
    "title": "Controlling Vision-Language Models for Universal Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6W35Wcs077": {
    "title": "Decomposition Ascribed Synergistic Learning for Unified Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ox2ATRM90I": {
    "title": "Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvJxTjTQtq": {
    "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U9NHClvopO": {
    "title": "SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition of Multi Token Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9Vs5NGDpk": {
    "title": "Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70A6oo3Il2": {
    "title": "AdaFlood: Adaptive Flood Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zUDbPgskDS": {
    "title": "Crystals with Transformers on Graphs, for predictions of crystal material properties",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iPWxqnt2ke": {
    "title": "Identifying Policy Gradient Subspaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sDmjlpphdB": {
    "title": "Mixture-of-Experts in Prompt Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvZDQvjULn": {
    "title": "A bi-objective perspective on controllable language models: reward dropout improves off-policy control performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2D8aW4son": {
    "title": "Capturing The Channel Dependency Completely Via Knowledge-Episodic Memory For Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lv9KZ5qCSG": {
    "title": "Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=13D1zn0mpd": {
    "title": "Effective and Parameter-Efficient Reusing Fine-Tuned Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zmJDzPh1Dm": {
    "title": "Nemesis: Normalizing the soft-prompt vectors of vision-language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXu7oYPOhg": {
    "title": "Memory-efficient particle filter recurrent neural network for object localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j4VMrwgn1M": {
    "title": "Training Graph Transformers via Curriculum-Enhanced Attention Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bm1JVsVZVu": {
    "title": "Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a24gfxA7jD": {
    "title": "Physics Informed Distillation for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VDkye4EKVe": {
    "title": "Discovering Minimal Reinforcement Learning Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=We6kIyBOMp": {
    "title": "Delayed Spiking Neural Network and Exponential Time Dependent Plasticity Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mIQ2puu82H": {
    "title": "DIFFNAT: IMPROVING DIFFUSION IMAGE QUALITY USING NATURAL IMAGE STATISTICS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Kgnvknvwd": {
    "title": "A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WpMHBIYsUf": {
    "title": "Homeomorphic Model Transformation for Boosting Performance and Efficiency in Object Detection Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jzdQPKgIWA": {
    "title": "Learning to Explore with In-Context Policy for Fast Peer Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zAdUB0aCTQ": {
    "title": "AgentBench: Evaluating LLMs as Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDve8qGH3M": {
    "title": "Simple CNN for Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ai4L058yoO": {
    "title": "Is Feature Extraction the most informative dimensionality reduction technique? Revisiting Unsupervised Feature Selection from a Dynamic Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ym0ubZrsmm": {
    "title": "Image Background Serves as Good Proxy for Out-of-distribution Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvjz7Bm3Ea": {
    "title": "ChronoGAM: An End-to-End One-Class Time Series Gaussian Mixture Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhoehDVFeJ": {
    "title": "Efficient Meshy Neural Fields for Animatable Human Avatars",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YEhQs8POIo": {
    "title": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XCVuT5Stl5": {
    "title": "SENSITIVITY-INFORMED REGULARIZATION FOR OFFLINE BLACK-BOX OPTIMIZATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5COCYDObes": {
    "title": "Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhYNXVcZYz": {
    "title": "SketchEdit: Editing Freehand Sketches At The Stroke-Level",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pdJXYfJjz9": {
    "title": "EXPLORING RAIN-/DETAIL-AWARE REPRESENTATION FOR INSTANCE-SPECIFIC IMAGE DE-RAINING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4kJfWZChJI": {
    "title": "Generalization or Specificity? Spectral Meta Estimation and Ensemble (SMEE) with Domain-specific Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ecbRyZZmKG": {
    "title": "Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Go33RnNiVH": {
    "title": "$\\beta$-DQN: Diverse Exploration via Learning a Behavior Function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wDd4Zcnc08": {
    "title": "HP$^3$-NS: Hybrid Perovskite Property Prediction Using Nested Subgraph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Wi0Ys33Nm": {
    "title": "Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xU0XRbn3b5": {
    "title": "Privacy at Interpolation: Precise Analysis for Random and NTK Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i6JcQpiFdR": {
    "title": "Guaranteed Trust Region Optimization via Two-Phase KL Penalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4aJg9e4nvF": {
    "title": "What do vision transformers learn? A visual exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A0DI5v6m8O": {
    "title": "Black-Box Gradient Matching for Reliable Offline Black-Box Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HM2E7fnw2U": {
    "title": "Mitigating Mode Collapse in Sequential Disentanglement via an Architecture Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIx1YXVWZb": {
    "title": "Understanding Addition in Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=36L7W3ri4U": {
    "title": "Beating Price of Anarchy and Gradient Descent without Regret in Potential Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9OevMUdods": {
    "title": "Do Large Language Models Know about Facts?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bA5o5eZplk": {
    "title": "New recipes for graph anomaly detection: Forward diffusion dynamics and graph generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A6juYCULJO": {
    "title": "Abstractive Summarization through the PRISM of Decoding Strategies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypAT2ixD4X": {
    "title": "In defense of parameter sharing for model-compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RzY9qQHUXy": {
    "title": "Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wgb8tuu5BI": {
    "title": "Decoupling Intrinsic and Measurement Trends: A Crucial Consideration in Time Series Causal Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wCUw8t63vH": {
    "title": "Spectral learning of shared dynamics between generalized-linear processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCNqgUFTHI": {
    "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9bmTbVaA2A": {
    "title": "Bootstrapping Variational Information Pursuit with Foundation Models for Interpretable Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yqAToOgxgf": {
    "title": "An old dog can learn (some) new tricks: A tale of a three-decade old architecture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJNCnkDRkY": {
    "title": "Generative Pre-Trained Speech Language Model with Efficient Hierarchical Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=92yrETgM6G": {
    "title": "Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lWe3GBRem8": {
    "title": "Offline RL for Online RL: Decoupled Policy Learning for Mitigating Exploration Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eVpjeCNsR6": {
    "title": "EraseDiff: Erasing Data Influence in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ep85sakT3": {
    "title": "Contextual Bandits with Online Neural Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tr0KidwPLc": {
    "title": "Evaluating Large Language Models at Evaluating Instruction Following",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A81iom2Y41": {
    "title": "Be Your Own Neighborhood: Detecting Adversarial Example by the Neighborhood Relations Built on Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pJBSzGmb9a": {
    "title": "On the Global Convergence of Natural Actor-Critic with Neural Network Parametrization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTssMmhC2X": {
    "title": "How to Fine-Tune Vision Models with SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oxjeePpgSP": {
    "title": "Backdoor Contrastive Learning via Bi-level Trigger Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JpyWPfzu0b": {
    "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VyWv7GSh5i": {
    "title": "A Novel Variational Lower Bound For Inverse Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hz9TMobz2q": {
    "title": "Push: Concurrent Probabilistic Programming for Bayesian Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E78OaH2s3f": {
    "title": "CAS: A Probability-Based Approach for Universal Condition Alignment Score",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tnAPOvvNzZ": {
    "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TDxtP8nxkh": {
    "title": "NAP2: Neural Networks Hyperparameter Optimization Using Weights and Gradients Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5GX6s5TpmV": {
    "title": "The Certification Paradox: Certifications Admit Better Evasion Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IsGsv8qEHp": {
    "title": "Human-oriented Representation Learning for Robotic Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByR3NdDSZB": {
    "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tsE5HLYtYg": {
    "title": "SafeDreamer: Safe Reinforcement Learning with World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5BCFlnfE1g": {
    "title": "Demystifying CLIP Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z7usV2BlEE": {
    "title": "Making Large Language Models Better Reasoners with Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DiWRG9JTWZ": {
    "title": "MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ufvwhR3XmN": {
    "title": "A Joint Spectro-Temporal Relational Thinking Based Acoustic Modeling Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5sixirvG0I": {
    "title": "Whittle Index with Multiple Actions and State Constraint for Inventory Management",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vZ8UjP2Mz": {
    "title": "Exploring the Generalization Capabilities of AID-based Bi-level Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G1DoOVM3xZ": {
    "title": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HHbRxoDTxE": {
    "title": "Looped Transformers are Better at Learning Learning Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mF3cTns4pe": {
    "title": "Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qaKRfobbTg": {
    "title": "Learning Thresholds with Latent Values and Censored Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7uPprVelO": {
    "title": "GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lebNJk3ul9": {
    "title": "A space-continuous implementation of Proper Orthogonal Decomposition by means of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WtHKqtHVXo": {
    "title": "Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hac6DzbMa7": {
    "title": "Continual Learning with Orthogonal Weights and Knowledge Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qwq4cpLtoX": {
    "title": "Exploring the Relationship Between Model Architecture and In-Context Learning Ability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TfbpnxTJt3": {
    "title": "Federated Learning with Local Openset Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2zoi9YI21Y": {
    "title": "Towards a Self-Made Model: Zero-Shot Self-Supervised Purification for Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cLqCZ740vw": {
    "title": "Benchmarking Smoothness and Reducing High-Frequency Oscillations in Continuous Control Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZVmYg3LvS": {
    "title": "Improved Function Space Variational Inference with Informative Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b8hRudcKQ3": {
    "title": "Performance Adjustment for Federated Learning Marketplace",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOMy6J8epf": {
    "title": "A counterfactual-based approach to prevent crowding in intelligent subway systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nJnky5K944": {
    "title": "Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WSsP7W8tqN": {
    "title": "Grokking Tickets: Lottery Tickets Accelerate Grokking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7iVaVidha": {
    "title": "OfflineLight: An Offline Reinforcement Learning Model for Traffic Signal Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scxDIx6StY": {
    "title": "Adaptive Temperature Enhanced Dual-level Hypergraph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cqrv7Sve7g": {
    "title": "Offline Reward Inference on Graph: A New Thinking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8F6bws5JBy": {
    "title": "Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5bNYf0CqxY": {
    "title": "Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gf4KZIqLHD": {
    "title": "A Change of Heart: Backdoor Attacks on Security-Centric Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NY3HzOOL3u": {
    "title": "Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2DJUXmHZ2O": {
    "title": "Generalizing Poincaré Policy Representations in Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ERTp3iQWPW": {
    "title": "A Framework for PromptOps in GenAI Application Development Lifecycle",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NY3wMJuaLf": {
    "title": "Fake It Till Make It: Federated Learning with Consensus-Oriented Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hRbLHpLAy4": {
    "title": "RetPur: Diffusion Purification Model for Defending Hash Retrieval Target Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FeqxK6PW79": {
    "title": "Analyzing Deep Transformer Models for Time Series Forecasting via Manifold Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FNq3nIvP4F": {
    "title": "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FiQRgzKl64": {
    "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P2Fjm0nIit": {
    "title": "NeRF Compression via Transform Coding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vEEWhGjx0M": {
    "title": "Adversarial Attacks on Combinatorial Multi-Armed Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDdSRaOiyb": {
    "title": "Explaining Time Series via Contrastive and Locally Sparse Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CThn4xaLDT": {
    "title": "E(3) Equivariant Scalar Interaction Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nATTIkte9f": {
    "title": "LMO-DP: Accurately Fine-Tuning Language Models with Stronger Differential Privacy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NX0eNGXezp": {
    "title": "Semi-HyperGraph Benchmark: Enhancing Flexibility of Hypergraph Learning with Datasets and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2Lnqs4eMJ": {
    "title": "Optimal Neural Network Approximation for High-Dimensional Continuous Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qhAx0fU9YE": {
    "title": "When Does Bias Transfer in Transfer Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nkUQPOwYy0": {
    "title": "Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wxClzZdjqP": {
    "title": "LLM4GCL: CAN LARGE LANGUAGE MODEL EM-POWER GRAPH CONTRASTIVE LEARNING?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJvbOSXRUq": {
    "title": "GnnX-Bench: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNGxg8shA1": {
    "title": "Noise Robust Graph Learning under Feature-Dependent Graph-Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jrm33chK71": {
    "title": "Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Ay23yeuz0": {
    "title": "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o8tjamaJ80": {
    "title": "Adversarial AutoMixup",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pBxeZ6pVUD": {
    "title": "Grounded Object-Centric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KEpR8hFzvO": {
    "title": "Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=htEL8LrrVe": {
    "title": "Communication Bounds for the Distributed Experts Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c8UABqZfld": {
    "title": "Spatial Matching Loss Function for Mass Segmentation on Whole Mammography Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ts95eXsPBc": {
    "title": "Spatially-Aware Transformers for Embodied Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3GDKJSQnW2": {
    "title": "Pivotal Prompt Tuning for Video Dynamic Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7U5QE9T4hI": {
    "title": "Learning to Extrapolate and Adjust: Two-Stage Meta-Learning for Concept Drift in Online Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xAqcJ9XoTf": {
    "title": "On the Stability of Expressive Positional Encodings for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sawjxRnVpF": {
    "title": "Curvature-Informed SGD via General Purpose Lie-Group Preconditioners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dRel8fuUK4": {
    "title": "Low-Cost High-Power Membership Inference by Boosting Relativity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5oXp5Kvq5": {
    "title": "A Causal Ordering Prior for Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HiTg16qhxp": {
    "title": "Dynamic Neural Response Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Let8OMe20n": {
    "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VzPGV19Bnp": {
    "title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvtmdK69KQ": {
    "title": "Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQGUDc9tC8": {
    "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xCawdgA8Qr": {
    "title": "Leveraging Behavioral Cloning for Representation Alignment in Cross-Domain Policy Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vW1SkPl4kp": {
    "title": "Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSwu81S33z": {
    "title": "Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7mR83Q12cJ": {
    "title": "Counterfactual Data Augmentation with Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B6pQxqUcT8": {
    "title": "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RR8y0WKrFv": {
    "title": "Ensemble Distillation for Unsupervised Constituency Parsing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KOUAayk5Kx": {
    "title": "Defying Multi-model Forgetting: Orthogonal Gradient Learning to One-shot Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rnxam2SRgB": {
    "title": "Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SdUUyqakLl": {
    "title": "Exploit Gradient Skew to Circumvent Byzantine Defenses for Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=02Ug9N8DCI": {
    "title": "GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nr0w6CH7v4": {
    "title": "Quality Diversity through Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AssIuHnmHX": {
    "title": "Understanding Length Generalization by Thinking Like Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zZFGliCl9": {
    "title": "Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X41c4uB4k0": {
    "title": "Training-free Multi-objective Diffusion Model for 3D Molecule Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mvGa1ikBD3": {
    "title": "Graph Neural Networks with Directional Encodings for Anisotropic Elasticity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WSd408I9M": {
    "title": "Generative AI in healthcare: A trustworthy approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bNt7oajl2a": {
    "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CGBfHtFRM": {
    "title": "Mean Field Langevin Actor-Critic: Faster Convergence and Global Optimality beyond Lazy Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z9AZsU1Tju": {
    "title": "Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hF8jnnexSB": {
    "title": "The Power of Minimalism in Long Sequence Time-series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7avlrpzWqo": {
    "title": "Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2RGQwJEcAC": {
    "title": "Visual Transformer with Differentiable Channel Selection: An Information Bottleneck Inspired Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bDooTVT4t2": {
    "title": "Universally Amplifying Randomized Smoothing for Certified Robustness with Anisotropic Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ntSP0bzr8Y": {
    "title": "PowerGPT: Foundation Model for Power Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uj2Wjv0pMY": {
    "title": "Put on your detective hat: What's wrong in this video?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=639DcBewcJ": {
    "title": "Low-Rank Robust Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYDOCManeN": {
    "title": "Representation-space diffusion models for generating periodic materials",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaZAKvjLro": {
    "title": "Semi-supervised Long-tailed Recognition using Alternate Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9m02ib92Wz": {
    "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RfCGvKBmMq": {
    "title": "Representation Matching Information Bottleneck for Text Matching in Asymmetrical Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iUD9FklwQf": {
    "title": "G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I2mIxuXA72": {
    "title": "Understanding Domain Generalization: A Noise Robustness Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNCnZwcH5Z": {
    "title": "Non-negative Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kT0vIJA8CT": {
    "title": "Can Differentiable Decision Trees Learn Interpretable Reward Functions?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2cG3mQqop": {
    "title": "Image Clustering Conditioned on Text Criteria",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNlntv7A9X": {
    "title": "SoftPhy: Soft-Body Physical Concept Learning and Reasoning from Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xHmCdSArUC": {
    "title": "Correlated Noise Provably Beats Independent Noise for Differentially Private Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6bcAD6g688": {
    "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mH3yfzIPsL": {
    "title": "XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpyBQn6gJY": {
    "title": "Regularized Optimal Transport for Temporal Trajectory Analysis in Single-Cell Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuflmOaxb7": {
    "title": "Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43cYe4oogi": {
    "title": "Understanding Expressivity of Neural KG Reasoning from Rule Structure Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxgSlyirUZ": {
    "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OhTzuWzO6Q": {
    "title": "A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RSincg5RBe": {
    "title": "Hierarchical Graph Latent Diffusion Model for Molecule Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5kAfAC7hO": {
    "title": "Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xrFTey4pY6": {
    "title": "Interactive Model Correction with Natural Language",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNShbDSxKH": {
    "title": "Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RqUMWdDg52": {
    "title": "FireAct: Toward Language Agent Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IcVNBR7qZi": {
    "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ov9RiAkxE": {
    "title": "Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFJGFrMvYj": {
    "title": "TCIG: Two-Stage Controlled Image Generation with Quality Enhancement through Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yINucFNbcZ": {
    "title": "Improving the efficiency of conformal predictors via test-time augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mnHpxTxnYg": {
    "title": "Black-Box Privacy Attacks Against GANs via Detector Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qoHeuRAcSl": {
    "title": "Grounding Language Plans in Demonstrations Through Counter-Factual Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A7t7z6g6tM": {
    "title": "Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5o9G4XF1LI": {
    "title": "Goodhart's Law in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xCRr9DrolJ": {
    "title": "Score Regularized Policy Optimization through Diffusion Behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKV45Y0rFz": {
    "title": "Conservative Prediction via Data-Driven Confidence Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gBV21wK07P": {
    "title": "3D Autoencoding Diffusion Model for Molecule Interpolation and Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPloNoDJZn": {
    "title": "Robustifying and Boosting Training-Free Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9528xxcT7h": {
    "title": "Two Heads are Better than One: Towards Better Adversarial Robustness by Combining Transduction and Rejection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L9U5MJJleF": {
    "title": "Concept Bottleneck Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vrS1zwekw": {
    "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duBCwjb68o": {
    "title": "Latent Consistency Models: Synthesizing High-Resolution Images with Few-step Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V01FPV3SNY": {
    "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rriucj4UmC": {
    "title": "Reconstruction of Cortical Surfaces with Spherical Topology from Infant Brain MRI via Recurrent Deformation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPOcBR9h2a": {
    "title": "DLCNet: Enabling Long-Range Convolution with Data Dependency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nT8ouPui8": {
    "title": "On Memorization in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kDoKXaucJV": {
    "title": "Sparse-Guard: Sparse Coding-Based Defense against Model Inversion Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1iKydVG6pL": {
    "title": "Discovering Mathematical Formulas from Data via LSTM-guided Monte Carlo Tree Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UP387Adir": {
    "title": "Weakly Supervised Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2JF8mJRJ7M": {
    "title": "Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6uUmpPvqUU": {
    "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wure6HljpJ": {
    "title": "CoSDA: Continual Source-Free Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DFTHW0MyiW": {
    "title": "Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eY7sLb0dVF": {
    "title": "Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yxKZGQLzOP": {
    "title": "Generating Pragmatic Examples to Train Neural Program Synthesizers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HSKaGOi7Ar": {
    "title": "Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=svSWP21tdp": {
    "title": "Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pe2lo3QOvo": {
    "title": "Making RL with Preference-based Feedback Efficient via Randomization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oxEER3kZ9M": {
    "title": "On the Possibilities of AI-Generated Text Detection: A Sample Complexity Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OlwW4ZG3Ta": {
    "title": "Reflective Policy Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EBUoTvVtMM": {
    "title": "User Inference Attacks on Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L7LwHpjMTQ": {
    "title": "CLIP as Multi-Task Multi-Kernel Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFWG9Cy3WK": {
    "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AY9KyTGcnk": {
    "title": "Adaptive Regret for Bandits Made Possible: Two Queries Suffice",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekdurSMmbH": {
    "title": "Universal Off-Policy Selection for Human-Centric Systems via Participant Sub-grouping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jw8EoY1FvF": {
    "title": "Delayed Local-SGD for Distributed Learning with Linear Speedup",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xle26hcxHh": {
    "title": "AudoFormer: An Efficient Transformer with Consistent Auxiliary Domain for Source-free Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SHUQtRK0eU": {
    "title": "Generalized Activation via Multivariate Projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zkVm3JqJzs": {
    "title": "Conformal Prediction for Deep Classifier via Label Ranking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eS0qCQDrkG": {
    "title": "Towards Efficient Trace Estimation for Optimal Transport in Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyH5ijgx9C": {
    "title": "Efficient Stagewise Pretraining via Progressive Subnetworks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvSwR4IvLO": {
    "title": "Can AI-Generated Text be Reliably Detected?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iAW2EQXfwb": {
    "title": "Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOmqfJovQ6": {
    "title": "Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QgwAYFrh9t": {
    "title": "Learning Hierarchical Polynomials with Three-Layer Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oKGDfMrD4A": {
    "title": "Exploring Adversarial Robustness of Graph Neural Networks in Directed Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=usmP3muXMI": {
    "title": "Minimizing Chebyshev Risk Magically Mitigates the Perils of Overfitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bZMyHBSnEI": {
    "title": "Deep Equilibrium Multimodal Fusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y8DClN5ODu": {
    "title": "Demonstration Distillation for Efficient In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70xhiS0AQS": {
    "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1VWS7ZRm6": {
    "title": "On Transferring Expert Knowledge from Tabular Data to Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i2Phucne30": {
    "title": "On Bias-Variance Alignment in Deep Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDEWIMoiNK": {
    "title": "Mobile Object Rearrangement with Learned Localization Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJ0Cfj4Ex9": {
    "title": "Learning Grounded Action Abstractions from Language",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BEH4mGo7zP": {
    "title": "Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ta2ctBXj1J": {
    "title": "CityGPT: Generative Transformer for City Layout of Arbitrary Building Shape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDxGthJkSD": {
    "title": "Hybrid Classification-Regression Adaptive Loss for Dense Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=viC3cpWFTN": {
    "title": "Clip21: Error Feedback for Gradient Clipping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qH8ADnIVII": {
    "title": "Dynamic Demonstrations Controller for In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H396R79GiQ": {
    "title": "A unique M-pattern for micro-expreesion spotting in long videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=atQqW27RMQ": {
    "title": "GENIU: A Restricted Data Access Unlearning for Imbalanced Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k2lkeCCfRK": {
    "title": "GFLOWNET TRAINING BY POLICY GRADIENTS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SirD4KYNRr": {
    "title": "Invariant Attention: Provable Clustering Under Transformations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUNzEQMWU7": {
    "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tj4a1JY03u": {
    "title": "Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YcM6ofShwY": {
    "title": "BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxpswbhr1r": {
    "title": "In-context Convergence of Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3cE6NKYy8x": {
    "title": "Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J9wzKfgZVK": {
    "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thbtoAkCe9": {
    "title": "$\\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVTN7Zs0ml": {
    "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i5da6iedW8": {
    "title": "FedBiOT: a solution for federated large language model fine-tuning with intellectual property protection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s5hSp7EdL3": {
    "title": "The Human-AI Substitution game: active learning from a strategic labeler",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UfBIxpTK10": {
    "title": "The Discovery of Binding Modes Requires Rethinking Docking Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqVgBbNCm9": {
    "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YGTSLDAPqb": {
    "title": "Connect Later: Improving Fine-Tuning for Robustness with Targeted Augmentations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEUN4FCCrS": {
    "title": "Value Bonuses using Ensemble Errors for Exploration in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzqrANCF4g": {
    "title": "Language Model Beats Diffusion - Tokenizer is key to visual generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3oTPsORaDH": {
    "title": "Improving Generalization in Equivariant Graph Neural Networks with Physical Inductive Biases",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2WchST43h": {
    "title": "A Sublinear Adversarial Training Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7gLfQT52Nn": {
    "title": "Proper Laplacian Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSBB2nRaoj": {
    "title": "Bi-Directional Goal-Conditioning on Single Policy Function for State Space Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=poFAoivHQk": {
    "title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Trg9qb0d5U": {
    "title": "Fantastic DNN-Classifier Identification without Testing Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OeQE9zsztS": {
    "title": "Spectrally Transformed Kernel Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vJGKYWC8j8": {
    "title": "Continual Traffic Forecasting via Mixture of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xw29VvOMmU": {
    "title": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNwXif6AWA": {
    "title": "Periodic Set Transformer: Material Property Prediction from Continuous Isometry Invariants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YxvmODVWny": {
    "title": "RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ViNe1fjGME": {
    "title": "Deep Temporal Graph Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rOpK0ToM3o": {
    "title": "V-Former: Offline RL with Temporally-Extended Actions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MEztAJjcYZ": {
    "title": "Enhancing Clinical Note Summarization: Iterative Reflexions with Small-model Supervision and Error2Correct Demonstrations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHGxChm1l5": {
    "title": "Compositional VLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cHy00K3Och": {
    "title": "GRADSIMCORE: GRADIENT SIMILARITY BASED REPRESENTATIVE INSTANCES AS CORESET",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOGqJ6Z1sA": {
    "title": "Treatment Effects Estimation By Uniform Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDSj6S8WJe": {
    "title": "Demystifying Linear MDPs and Novel Dynamics Aggregation Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDIqMB4mMg": {
    "title": "PostRainBench: A Comprehensive Benchmark and A New Model for Precipitation Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjDROBU93g": {
    "title": "DISTA: DENOISING SPIKING TRANSFORMER WITH INTRINSIC PLASTICITY AND SPATIOTEMPORAL ATTENTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W2HJKGnb5y": {
    "title": "POPULATION DESCENT: A NATURAL-SELECTION BASED HYPER-PARAMETER TUNING FRAMEWORK",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kJ0qp9Xdsh": {
    "title": "Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L76lvHZqeS": {
    "title": "A Unified Framework of Theoretically Robust Contrastive Loss against Label Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u4FiXrH09F": {
    "title": "Implicit Neural Network on Dynamic Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SMZGQu6lld": {
    "title": "LLM-Prop: Predicting Physical And Electronic Properties of Crystalline Solids From Their Text Descriptions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ndRkLsoQ1Q": {
    "title": "Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuZujQ9LJV": {
    "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3GunDQNKFJ": {
    "title": "Learning-Retrieval-Revision For Large Language Model Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WrEFIbrVg9": {
    "title": "Non-asymptotic Analysis of Stochastic Gradient Descent under Local Differential Privacy Guarantee",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OyIzNLAQfE": {
    "title": "Adaptive Continual Learning: Rapid Adaptation and Knowledge Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4olqbTBt1Y": {
    "title": "DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sRyGgkdQ47": {
    "title": "Making Batch Normalization Great in Federated Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFI3wFM9yN": {
    "title": "Communication-Efficient Federated Non-Linear Bandit Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Nn2BLV7SB": {
    "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KbetDM33YG": {
    "title": "Online GNN Evaluation Under Test-time Graph Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BAX3NXJ6vU": {
    "title": "Escaping Saddle Point Efficiently in Minimax and Bilevel Optimizations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NltzxpG0nz": {
    "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C371MUzjBl": {
    "title": "DAG-Based Column Generation for Adversarial Team Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4aywmeb97I": {
    "title": "Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgvOSEMEQS": {
    "title": "Lightweight Unsupervised Federated Learning with Pretrained Vision Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J0qgRZQJYX": {
    "title": "An Axiomatic Approach to Model-Agnostic Concept Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOgLmcJxxF": {
    "title": "Sample-Efficient Training for Score-Based Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xx05gm7oQw": {
    "title": "Debias your VLM with Counterfactuals: A Unified Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjhUtloBZU": {
    "title": "Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bl8u7ZRlbM": {
    "title": "(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vE8Vn6DM0y": {
    "title": "Aligning Brains into a Shared Space Improves Their Alignment to Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IRcv4yFX6z": {
    "title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oZf2SlXEY": {
    "title": "Distribution Calibration For Few-Shot Learning by Bayesian Relation Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TilcG5C8bN": {
    "title": "Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2rZGCbRRd": {
    "title": "Extracting Post-Treatment Covariates for Heterogeneous Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bDZCBjVgKW": {
    "title": "Fast Post-training Analysis of NeRFs Using A Simple Visibility Prediction Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJHUYWviZ6": {
    "title": "On Socially Fair Regression and Low-Rank Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UqEI76CKgO": {
    "title": "Amphibian: A Meta-Learner for Rehearsal-Free Fast Online Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I09JonzQJV": {
    "title": "Counterfactual Fairness With the Human in the Loop",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vqHTUTod9": {
    "title": "Can Language Models be Instructed to Protect Personal Information?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRhRDpsCO2": {
    "title": "MATT: Random Local Implicit Purification for Defending Query-based Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CfXh93NDgH": {
    "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t84UBRhhvp": {
    "title": "Text Descriptions are Compressive and Invariant Representations for Visual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ipjMIHVJt": {
    "title": "DASFormer: Self-supervised Pretraining for Earthquake Monitoring",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQStRgYfuJ": {
    "title": "Topology-aware Embedding Memory for Learning on Expanding Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wabp68RoSP": {
    "title": "Active Prompting with Chain-of-Thought for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=usrChqw6yK": {
    "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DQCZiKb3Uy": {
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iQIQT88prm": {
    "title": "Adversarial Machine Unlearning: A Stackelberg Game Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7vzyqs8UbA": {
    "title": "LMCC-MBC: Metric-Constrained Model-Based Clustering with Wasserstein-2 Distance of Gaussian Markov Random Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mhu9iNGKqP": {
    "title": "Optimizing Layerwise Polynomial Approximation for Efficient Private Inference on Fully Homomorphically Encryption: A Dynamic Programming Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SWRFC2EupO": {
    "title": "Language Reward Modulation for Pretraining Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lKxL5zkssv": {
    "title": "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=plmBsXHxgR": {
    "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2GJm8yT2jN": {
    "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMPHKOmQNb": {
    "title": "Protein Discovery with Discrete Walk-Jump Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Te5v4EcFGL": {
    "title": "PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HjfvnxaU5k": {
    "title": "Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71kocBuhNO": {
    "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xVlcbh0poD": {
    "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Y26tFG3WF": {
    "title": "Inducing Precision in Lagrangian Neural Networks : Proof of concept application on Chaotic systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GURqUuTebY": {
    "title": "DreamFlow: High-quality text-to-3D generation by Approximating Probability Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTRwljRgiv": {
    "title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=98g9NdJPxm": {
    "title": "Theoretically Understanding Data Reconstruction Leakage in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8jdwkUNXb": {
    "title": "Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9Xb6fADe4": {
    "title": "Towards Greener and Sustainable Airside Operations: A Deep Reinforcement Learning Approach to Pushback Rate Control for Mixed-Mode Runways",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rO8QOHrCeA": {
    "title": "Grounding Code Generation with Input-Output Specifications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iX1RjVQODj": {
    "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HxHrRUHMOD": {
    "title": "Accurate Differential Operators for Neural Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JDp3AQ2elP": {
    "title": "Revisiting Familiar Places in an Infinite World: Continuing RL in Unbounded State Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h8GeqOxtd4": {
    "title": "Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v675Iyu0ta": {
    "title": "Interpretability Illusions in the Generalization of Simplified Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b9aCXHhdbv": {
    "title": "Pipeline Parallelism Optimization with Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4abltTZ2f": {
    "title": "Batched Low-Rank Adaptation of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTddgL0lTQ": {
    "title": "ToolTalk: Evaluating Tool Usage in a Conversational Setting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJZL5w4Iez": {
    "title": "Investigating the effective dimensionality of a model using a thermodynamic learning capacity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=84Hk01tFKq": {
    "title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xTFgpfIMOt": {
    "title": "Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cI7WAadODh": {
    "title": "An Invex Relaxation Approach for Minimizing Polarization from Fully and Partially Observed Initial Opinions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMsmo01TaI": {
    "title": "The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5zwrpqYIK5": {
    "title": "Outlier-Robust Orthogonal Regression on Manifolds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m0x0rv6Iwm": {
    "title": "Time-Varying Propensity Score to Bridge the Gap between the Past and Present",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yr4RgiZ7P5": {
    "title": "Does resistance to style-transfer equal Shape Bias? Evaluating shape bias by distorted shape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XVhm3X8Fum": {
    "title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J7AwIJvR3d": {
    "title": "Discovering Divergences between Language Models and Human Brains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHzzAU7Qf9": {
    "title": "Soft Merging of Experts with Adaptive Routing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jLIUfrAcMQ": {
    "title": "Debiasing Attention Mechanism in Transformer without Demographics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=22OTbutug9": {
    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YH3tFtwuzb": {
    "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BjG6McP5nA": {
    "title": "Improving Gradient-guided Nested Sampling for Posterior Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mM7VurbA4r": {
    "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rM9VJPB20F": {
    "title": "Like Oil and Water: Group Robustness Methods and Poisoning Defenses Don't Mix",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IYxDy2jDFL": {
    "title": "Improved Active Learning via Dependent Leverage Score Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WyEdX2R4er": {
    "title": "Visual Data-Type Understanding does not emerge from scaling Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNrFpDPMyo": {
    "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I0gwsdSgsk": {
    "title": "Memory Efficient Neural Processes via Constant Memory Attention Block",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z8Wva86JLB": {
    "title": "GEOFFair: a GEOmetric Framework for Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FvfhHucpLd": {
    "title": "DIVERSITY OF THOUGHT IMPROVES REASONING ABILITIES OF LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ORUiqcLpV6": {
    "title": "CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8jKuUHsndT": {
    "title": "Re-evaluating Retrosynthesis Algorithms with Syntheseus",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6n1fKMMC1": {
    "title": "An Efficient Tester-Learner for Halfspaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NGVljI6HkR": {
    "title": "Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fe8CzLTMG1": {
    "title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-Temporal Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCjVArCAZ1": {
    "title": "Is Pre-training Truly Better Than Meta-Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESSqkWnApz": {
    "title": "Fast and Reliable Generation of EHR Time Series via Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmoknhh7CH": {
    "title": "Harmonic Prior Flow Matching for Multi-Ligand Docking and Binding Site Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=506Sxc0Adp": {
    "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAEBTnTGo3": {
    "title": "JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EpVe8jAjdx": {
    "title": "Privileged Sensing Scaffolds Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w9tc699w3Z": {
    "title": "Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZWdgOvmAA": {
    "title": "LumiNet: The Bright Side of Perceptual Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C36v8541Ns": {
    "title": "The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uf5EAGmkrN": {
    "title": "Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xnhvVtZtLD": {
    "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1XReHUSUp9": {
    "title": "Monsters in the Dark: Sanitizing Hidden Threats with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAKcnjzQI3": {
    "title": "MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0BqyZSWfzo": {
    "title": "One-shot Empirical Privacy Estimation for Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3UWuFoksGb": {
    "title": "Learning Planning Abstractions from Language",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y3qpL2Ioys": {
    "title": "Towards Neural Architecture Search through Hierarchical Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvUq3cxpDF": {
    "title": "Learning to Act without Actions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oaTkYHPINY": {
    "title": "Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OQccFglTb5": {
    "title": "FT-SHIELD: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8eO0CiZJV": {
    "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EnXJfQqy0K": {
    "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TTonmgTT9X": {
    "title": "Fast Hyperboloid Decision Tree Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IzqZbNMZ0M": {
    "title": "Private Zeroth-Order Nonsmooth Nonconvex Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EsiU7bNabf": {
    "title": "Approximate Clustering for Extracting Task Relationships in Multi-Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pB9XVRGVu0": {
    "title": "GeRA: Label-Efficient Geometrically Regularized Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FAPahXyVh": {
    "title": "OptiMUS: Optimization Modeling Using mip Solvers and large language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=68k0KcHFrW": {
    "title": "Stochastic Unrolled Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNSjteBJd9": {
    "title": "Who Leaked the Model? Tracking IP Infringers in Accountable Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gZRfDWLlGY": {
    "title": "Exact Path Kernels Naturally Decompose Model Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xHDeA8Noi": {
    "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yRrPfKyJQ2": {
    "title": "Conversational Drug Editing Using Retrieval and Domain Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iHcTLIor0m": {
    "title": "Poly-View Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jhu4dQv5rY": {
    "title": "Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MVe2dnWPCu": {
    "title": "A Probabilistic Framework for Modular Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bLpUtGyf9g": {
    "title": "Boundary Denoising for Video Activity Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cWiEN1plhJ": {
    "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PCm1oT8pZI": {
    "title": "Safe and Robust Watermark Injection with a Single OoD Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6L1CJQ2PE": {
    "title": "Massive Editing for Large Language Model via Meta Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RRKggDJxo2": {
    "title": "Real-time learning of decay trajectory of Higgs boson using reservoir-in-reservoir architecture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RwI7ZEfR27": {
    "title": "BrainLM: A foundation model for brain activity recordings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOb0xFwdpr": {
    "title": "On Sarcasm Detection with OpenAI GPT-based Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VTF8yNQM66": {
    "title": "SWE-bench: Can Language Models Resolve Real-world Github Issues?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CtiFwPRMZX": {
    "title": "A simple connection from loss flatness to compressed representations in neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XMaPp8CIXq": {
    "title": "Always-Sparse Training with Guided Stochastic Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1NHgmKqOzZ": {
    "title": "Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9FsiXZxZt": {
    "title": "Object centric architectures enable efficient causal representation learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tI3eqOV6Yt": {
    "title": "Adaptivity and Modularity for Efficient Generalization Over Task Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=760br3YEtY": {
    "title": "($\\texttt{PEEP}$) $\\textbf{P}$redicting $\\textbf{E}$nzym$\\textbf{e}$ $\\textbf{P}$romiscuity with its Molecule Mate – an Attentive Metric Learning Solution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J1djqLAa6N": {
    "title": "Efficient Score Matching with Deep Equilibrium Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONhLaNbxVV": {
    "title": "Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jYsowwcXV1": {
    "title": "A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMvMwNvs4R": {
    "title": "Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=97Dl82avFs": {
    "title": "Alt-Text with Context: Improving Accessibility for Images on Twitter",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X2gjYmy77l": {
    "title": "Taming AI Bots: Controllability of Neural States in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YPfmglNRU": {
    "title": "Defining Expertise: Applications to Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxPqibGUPR": {
    "title": "VibeSpace: Automatic vector embedding creation for arbitrary domains and mapping between them using large language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBpSkFGVQU": {
    "title": "Depth-Guided Self-Supervised Learning: Seeing the World in 3D",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSD3MloKe6": {
    "title": "Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sb0ojNl7F6": {
    "title": "End-Effector-Elbow: A New Action Space for Robot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGHJAyR8w0": {
    "title": "Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dALYqPm9gW": {
    "title": "Recurrent Linear Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZErAetdMu": {
    "title": "Time Series Modeling at Scale: A Universal Representation Across Tasks and Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5eLgTLusaR": {
    "title": "Loco3D: Indoor Multiuser Locomotion 3D Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z3L59iGALM": {
    "title": "Massively Scalable Inverse Reinforcement Learning for Route Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O1lR4vSw5x": {
    "title": "RECURSIVE NEURAL ORDINARY DIFFERENTIAL EQUATIONS FOR PARTIALLY OBSERVED SYSTEM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rzF0R6GOd4": {
    "title": "Neural SDF Flow for 3D Reconstruction of Dynamic Scenes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mliQ2huFrZ": {
    "title": "Class Probability Matching with Calibrated Networks for Label Shift Adaption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJHnSg783t": {
    "title": "DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VLFhbOCz5D": {
    "title": "Tangent Transformers for Composition,Privacy and Removal",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yV6wwEbtkR": {
    "title": "Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIu5lyNXjT": {
    "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wUaOVNv94O": {
    "title": "AUTOMATIC NEURAL SPATIAL INTEGRATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzpWBbnwiJ": {
    "title": "Universal Guidance for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=osoWxY8q2E": {
    "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Io0Q37X5fP": {
    "title": "Counterfactual Generative Models for Time-Varying Treatments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KsUh8MMFKQ": {
    "title": "Thin-Shell Object Manipulations With Differentiable Physics Simulations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L4nOxziGf9": {
    "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eojWsJQ2fe": {
    "title": "Prompt Engineering a Prompt Engineer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWuYsSD94h": {
    "title": "A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ssOs9BBxa": {
    "title": "A Competition Winning Deep Reinforcement Learning Agent in microRTS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHKS0oXuku": {
    "title": "Jensen-Shannon Divergence Based Novel Loss Functions for Bayesian Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i4eDGZFcva": {
    "title": "Reward Centering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zap3nZhRIQ": {
    "title": "Three ways that non-differentiability affects neural network training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VKEJKKLvr": {
    "title": "A GRAPH-BASED REPRESENTATION LEARNING APPROACH FOR BREAST CANCER RISK PREDICTION USING GENOTYPE DATA",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jnZtTUdWyi": {
    "title": "Adaptive Invariant Representation Learning for Non-stationary Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REKRLIXtQG": {
    "title": "Supermodular Rank: Set Function Decomposition and Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tGGWOijvq": {
    "title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0kaVlC5ue": {
    "title": "Spectral Neural Networks: Approximation Theory and Optimization Landscape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fZZ4ubttru": {
    "title": "GenBot: Generative Simulation Empowers Automated Robotic Skill Learning at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=smy4DsUbBo": {
    "title": "Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LyNsMNNLjY": {
    "title": "Large Language Model Routing with Benchmark Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0kvrymILfy": {
    "title": "Making Predictors More Reliable with Selective Recalibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g0mlwqs8pi": {
    "title": "Adaptive Federated Learning with Auto-Tuned Clients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=27YiINkhw3": {
    "title": "ToolDec: Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71mqtQdKB9": {
    "title": "Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7erlRDoaV8": {
    "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6tazBqPem3": {
    "title": "Capacity Analysis of Vector Symbolic Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GOiEdLIgVF": {
    "title": "Saliency-Guided Hidden Associative Replay for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SznHfMwmjG": {
    "title": "Measuring Feature Sparsity in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uavy4DLrXR": {
    "title": "($\\texttt{PASS}$) Visual Prompt Locates Good Structure Sparisty through a Recurent HyperNetwork",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8CtXin7mZ": {
    "title": "A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2gwo9cjOEz": {
    "title": "Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zamGHHs2u8": {
    "title": "If there is no underfitting, there is no Cold Posterior Effect",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vaf4sIrRUC": {
    "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g16vmAtJ8x": {
    "title": "On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against ``Truly Anonymous Synthetic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHmmnNvU2U": {
    "title": "Weighted Risk Invariance for Density-Aware Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ky2JYPKkml": {
    "title": "Towards Explainable and Efficient Multi-Modality Learning: Domain-Agnostic Concept Space Paired with Domain-Specific Projection Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DQTxr8JtPX": {
    "title": "Detecting Influence Structures in Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zeobgjmUCc": {
    "title": "Using Machine Learning Models to Predict Genitourinary Involvement Among Gastrointestinal Stromal Tumour Patients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8gZtt8nrpI": {
    "title": "Diffusion Models With Learned Adaptive Noise Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dsd04MYKax": {
    "title": "Sum-of-Parts Models: Faithful Attributions for Groups of Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCfz492fM8": {
    "title": "CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HCCkCjClO0": {
    "title": "Online Weight Approximation for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIt0sJsZw9": {
    "title": "Clustering Entity Specific Embeddings Towards a Prescribed Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aiPcdCFmYy": {
    "title": "Sinkhorn Distributional Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LY3ukUANko": {
    "title": "On input-dependence and recall in convolutional language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYyu3jpk8z": {
    "title": "Open-Domain Text Evaluation via Contrastive Distribution Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ta26LtNq2r": {
    "title": "Learning to Reject for Balanced Error and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzZjyYee6L": {
    "title": "Don't Reinvent the Steering Wheel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2kvDzdC5rh": {
    "title": "IntentGPT: Few-Shot Intent Discovery with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tj3xLVuE9f": {
    "title": "On the Foundations of Shortcut Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sehRvaIPQQ": {
    "title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFfMsKjqaH": {
    "title": "Interpreting Categorical Distributional Reinforcement Learning: An Implicit Risk-Sensitive Regularization Effect",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xhCZD9hiiA": {
    "title": "Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ze7DOLi394": {
    "title": "On the Joint Interaction of Models, Data, and Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DASh78rJ7g": {
    "title": "Plugin estimators for selective classification with out-of-distribution detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kOBkxFRKTA": {
    "title": "Dynamic Sparse Training with Structured Sparsity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fkrYDQaHOJ": {
    "title": "Efficient Dynamics Modeling in Interactive Environments with Koopman Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ROxsH4rMe4": {
    "title": "Systolic Array Acceleration of Spiking Neural Networks with Application-Independent Split-Time Temporal Coding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=53gU1BASrd": {
    "title": "Evaluating and Finetuning Models For Financial Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tcFcKyJgRM": {
    "title": "HeaP: Hierarchical Policies for Web Actions using LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvRZ68ObgW": {
    "title": "Controlling language over-optimization by targeting reward distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aKivEaIbN2": {
    "title": "Graph is All You Need? Lightweight Data-agnostic Neural Architecture Search without Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MFCjgEOLJT": {
    "title": "Learning interpretable control inputs and dynamics underlying animal locomotion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUBLhhVM1l": {
    "title": "Tight Rates in Supervised Outlier Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gTWaUlxxWi": {
    "title": "On the Effectiveness of One-Shot Federated Ensembles in Heterogeneous Cross-Silo Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMVFYTgj0H": {
    "title": "Continual Reinforcement Learning by Reweighting Bellman Targets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ut9aUpFZFr": {
    "title": "COINs: Model-based Accelerated Inference for Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oAMArMMQxb": {
    "title": "Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xh0XzueyCJ": {
    "title": "Plug-And-Play Controllable Graph Generation With Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GicZtgSlJW": {
    "title": "Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dl34rOnbqJ": {
    "title": "Actions-to-Action: Inductive Attention for Egocentric Video Action Anticipation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UB03wcP8RH": {
    "title": "Multitask Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itNHdOzZig": {
    "title": "Encodings for Prediction-based Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KqkizXgXU": {
    "title": "Curiosity-driven Red-teaming for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkETBJRKH7": {
    "title": "A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a745RnSFLT": {
    "title": "Understanding prompt engineering may not require rethinking generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Naiy1jf8UA": {
    "title": "MGDC-UNet: Multi-group Deformable Convolution for Medical Image Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GwBTlCIGs5": {
    "title": "Addressing Sample Inefficiency in Multi-View Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YikB42Oyaw": {
    "title": "MoReDrop: Dropout Without Dropping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dLrhRIMVmB": {
    "title": "Topological data analysis on noisy quantum computers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DuQkqSe9en": {
    "title": "Adversarial Imitation Learning via Boosting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2uHTuvDkLZ": {
    "title": "Physics-aware Causal Graph Network for Spatiotemporal Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6pHf8AiO7": {
    "title": "Pruning neural networks using FishLeg estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xXtD9P2lvH": {
    "title": "Directed Graph Generation with Heat Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mjsP8RYAw": {
    "title": "Unsupervised Fact Verification by Language Model Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0w42S2Gp70": {
    "title": "LipSim: A Provably Robust Perceptual Similarity Metric",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhY1XSYqO4": {
    "title": "Deep Variational Multivariate Information Bottleneck - A Framework for Variational Losses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d98CzL5h0i": {
    "title": "Learning to Generate Better than your Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rhaQbS3K3R": {
    "title": "Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtOydkE1Ku": {
    "title": "TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbOSJ3CATY": {
    "title": "A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4SrzKsJocx": {
    "title": "Simultaneous Dimensionality Reduction: A Data Efficient Approach for Multimodal Representations Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4mwbPjOwb": {
    "title": "Simple-TTS: End-to-End Text-to-Speech Synthesis with Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNtcoAM5Gy": {
    "title": "BaFTA: Backprop-Free Test-Time Adaptation for Zero-shot Vision Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AMDKqZcZbi": {
    "title": "Rapid Learning without Catastrophic Forgetting in the Morris Water Maze",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q20O1J9ujh": {
    "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsztjXcvUf": {
    "title": "A Primal-Dual Approach to Solving Variational Inequalities with General Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLADT8Wrhn": {
    "title": "TiC-CLIP: Continual Training of CLIP Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dLoAdIKENc": {
    "title": "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rNvyMAV8Aw": {
    "title": "Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XK7kyCVjqr": {
    "title": "Alignment-Enhancing Parallel Code Generation for Semi-Supervised Code Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DayPQKXaQk": {
    "title": "Constrained Decoding for Cross-lingual Label Projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hSyW5go0v8": {
    "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=piWvNRR0Ym": {
    "title": "Towards Minimal Targeted Updates of Language Models with Targeted Negative Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x5txICnnjC": {
    "title": "Synaptic Weight Distributions Depend on the Geometry of Plasticity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CK5Hfb5hBG": {
    "title": "Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ijK5hyxs0n": {
    "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SFCHv2G33F": {
    "title": "Protein Language Models Enable Accurate Cryptic Ligand Binding Pocket Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=06lrITXVAx": {
    "title": "Dropout Enhanced Bilevel Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dgmcE0RsTi": {
    "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n3kFlvVhJM": {
    "title": "Adder: Adapted Dense Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljVCPV7jK3": {
    "title": "Fairness Under Demographic Scarce Regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VoLDkQ6yR3": {
    "title": "Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tfyLS1cB5W": {
    "title": "Encoding Ontologies with Holographic Reduced Representations for Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUzWmFdglP": {
    "title": "Privacy Amplification for Matrix Mechanisms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsxeNvYqCj": {
    "title": "Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yol6nUVIJD": {
    "title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iIT02bAKzv": {
    "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qup9xD8mW4": {
    "title": "Behaviour Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLPzL6HWNl": {
    "title": "Improving LoRA in Privacy-preserving Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZXwP3H5t6": {
    "title": "Faster and Accurate Neural Networks with Semantic Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hB7SlfEmze": {
    "title": "PhyloGFN: Phylogenetic inference with generative flow networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TskzCtpMEO": {
    "title": "Training Bayesian Neural Networks with Sparse Subspace Variational Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eoB6JmdmVf": {
    "title": "Speech language models lack important brain-relevant semantics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Ua4hKiAJX": {
    "title": "Locality-Aware Graph Rewiring in GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nz6xYntfJ": {
    "title": "AlignDiff: Aligning Diffusion Models for General Few-Shot Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QeX0YFt4iW": {
    "title": "Multi-modality Adversarial Attacks on Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4FUa5dxiiA": {
    "title": "Risk-Sensitive Variational Model-Based Policy Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnVtsfyvap": {
    "title": "Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C6zFUEvgiU": {
    "title": "Feedback-guided Data Synthesis for Imbalanced Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSaN4gxuEf": {
    "title": "Adapting to Distribution Shift by Visual Domain Prompt Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S7j1sNVIm9": {
    "title": "Locally Adaptive Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Je5SHCKpPa": {
    "title": "Multimodal Patient Representation Learning with Missing Modalities and Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h4pNROsO06": {
    "title": "Improved sampling via learned diffusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fj5SqqXfn1": {
    "title": "Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OLi39lZS9Y": {
    "title": "Learning to Solve New sequential decision-making Tasks with In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1tZbq88f27": {
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yAcLwJu9qs": {
    "title": "Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkBg5D2lgT": {
    "title": "Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SIojR1ruNQ": {
    "title": "TIGERScore: Building Explainable Metric for All Text Generation Task",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZyhUXpEee": {
    "title": "SpaFL: Communication-Efficient Federated Learning with Sparse Models and Low Computational Overhead",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUZ2S0JVJP": {
    "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AweVGJeW47": {
    "title": "Smoothing for exponential family dynamical systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HE9eUQlAvo": {
    "title": "What Data Benefits My Classifier?\" Enhancing Model Performance and Interpretability through Influence-Based Data Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQt0MwXA81": {
    "title": "Do LLMs exhibit human-like response biases? A case study in survey design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6tqgL8VluV": {
    "title": "Towards Establishing Guaranteed Error for Learned Database Operations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTHfNGI3zT": {
    "title": "Quantifying the Plausibility of Context Reliance in Neural Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p79lnC36CO": {
    "title": "Automatic Calibration Diagnosis: Interpreting Probability Integral Transform (PIT) Histograms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9j1RD9LlWH": {
    "title": "Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4CxQmp9wc": {
    "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nqlymMx42E": {
    "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MO632iPq3I": {
    "title": "Differentiable Euler Characteristic Transforms for Shape Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hp4yOjhwTs": {
    "title": "Causally Aligned Curriculum Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RG98EkuHdT": {
    "title": "Transforming Transformers for Resilient Lifelong Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6EQbYM0CIX": {
    "title": "Conditional Generative Modeling for High-dimensional Marked Temporal Point Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L7KDMsqWl9": {
    "title": "HHD-Ethiopic: A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YGWGhdik6O": {
    "title": "Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTl1ABwM4n": {
    "title": "Improving length generalization in transformers via task hinting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KBGbEncHZF": {
    "title": "ARE YOU CERTAIN THAT IT IS A DEEPFAKE? DETECTION, GENERATION, AND SOURCE DETECTION FROM AN UNCERTAINTY PERSPECTIVE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gWHiS8Z867": {
    "title": "Routing with Rich Text Queries via Next-Vertex Prediction Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zgHamUBuuO": {
    "title": "Sparling: Learning Latent Representations With Extremely Sparse Activations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YUefWMfPoc": {
    "title": "How to fix a broken confidence estimator: Evaluating post-hoc methods for selective classification with deep neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v63GWletn8": {
    "title": "Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lr0byX2aNO": {
    "title": "Counterfactual Fairness on Graphs: Augmentations, Hidden Confounders, and Identifiability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v1VvCWJAL8": {
    "title": "Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNLVvdHyAw": {
    "title": "Detecting Language Model Attacks With Perplexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vt5mnLVIVo": {
    "title": "Grokking as the transition from lazy to rich training dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t5LXyWbs5p": {
    "title": "Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RLC0J2N9n": {
    "title": "SynBench: Evaluating Pretrained Representations for Image Classification using Synthetic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qcigbR1UYA": {
    "title": "Performance Bounds for Active Binary Testing with Information Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GE0N1htnu": {
    "title": "RINGER: Conformer Ensemble Generation of Macrocyclic Peptides with Sequence-Conditioned Internal Coordinate Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M0xK8nPGvt": {
    "title": "Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KgaBScZ4VI": {
    "title": "Language Model Cascades: Token-Level Uncertainty And Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3JjJezzVkT": {
    "title": "The Marginal Value of Momentum for Small Learning Rate SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mnWvUZIXt": {
    "title": "Towards Principled Representation Learning from Videos for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w49jlMWDSA": {
    "title": "GIST: Generating Image-Specific Text for Fine-grained Object Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duLr8BIzro": {
    "title": "A Fast and Effective Alternative to Graph Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3tjTJeXyA7": {
    "title": "Revitalizing Channel-dimension Fourier Transform for Image Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gyfXuRfxW2": {
    "title": "Learning Polynomial Problems with $SL(2, \\mathbb{R})$-Equivariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wYvuY60SdD": {
    "title": "Mixture of Weak and Strong Experts on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zz61cEY84L": {
    "title": "Meta-Learning Strategies through Value Maximization in Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oNzf7u5lT": {
    "title": "Pylic: Leveraging Source Code for Planning in Structured Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nji0ztL5rP": {
    "title": "Best Arm Identification for Stochastic Rising Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NU9AYHJvYe": {
    "title": "Optimal Sample Complexity of Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uS85FzjNDR": {
    "title": "A Unified Framework for Heterogeneous Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6K81ILDnuv": {
    "title": "Learning from Integral Losses in Physics Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLkj91HIZU": {
    "title": "Transformers can optimally learn regression mixture models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S83ldgJZLh": {
    "title": "A Structured Pruning Algorithm for Model-based Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iCNOK45Csv": {
    "title": "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l9GaXJnMJ8": {
    "title": "Fast Stochastic Kernel Approximation by Dual Wasserstein Distance Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6pPYRXKPpw": {
    "title": "Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyXWIJ99nh": {
    "title": "Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=odY3PkI5VB": {
    "title": "Reconciling Spatial and Temporal Abstractions for Goal Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6NO5UVWvo6": {
    "title": "Annotation by Clicks: A Point-Supervised Contrastive Variance Method for Medical Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oju2Qu9jvn": {
    "title": "Estimating Conditional Mutual Information for Dynamic Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jjA4O1vJRz": {
    "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PvJnX3dwsD": {
    "title": "Quadratic models for understanding neural network dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TdyfmCM8iR": {
    "title": "Latent Concept-based Explanation of NLP Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxPTSDp9xK": {
    "title": "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fud9JxIiEq": {
    "title": "Improved DDIM Sampling with Moment Matching Gaussian Mixtures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTYuRVrdK3": {
    "title": "Evaluating Representation Learning on the Protein Structure Universe",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aemqy6Hjdj": {
    "title": "Enhancing Compositional Generalization via Compositional Feature Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ViPtjIVzUw": {
    "title": "T-MARS: Improving Visual Representations by Circumventing Text Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUtxNAKpdV": {
    "title": "Nougat: Neural Optical Understanding for Academic Documents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vVoWRFV5Y4": {
    "title": "Solving the Quadratic Assignment Problem With Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6aRMQVlPVE": {
    "title": "Rank-adaptive spectral pruning of convolutional layers during training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3OzQhhPLyW": {
    "title": "Meta-Value Learning: a General Framework for Learning with Learning Awareness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STUGfUz8ob": {
    "title": "When can transformers reason with abstract symbols?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dLMPOY0HW": {
    "title": "When Do MLPs Excel in Node Classification? An Information-Theoretic Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IcR1OOFzxm": {
    "title": "Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=79FVDdfoSR": {
    "title": "A Characterization Theorem for Equivariant Networks with Point-wise Activations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiKHRTqaUD": {
    "title": "Handling Cost and Constraints with Off-Policy Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2qLSkTuqrb": {
    "title": "Translating cognitive models into neural and statistical descriptions of real-world multi-agent foraging behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAkjVCtRO2": {
    "title": "Variational quantization for state space models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iShM3YolRY": {
    "title": "On the Tool Manipulation Capability of Open-sourced Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ysue5S6cVS": {
    "title": "Confidence-driven Sampling for Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fvse7bMkAs": {
    "title": "Risk Assessment and Statistical Significance in the Age of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LVFoynuAQn": {
    "title": "A universal metric of dataset similarity for multi-source learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ph04CRkPdC": {
    "title": "Think before you speak: Training Language Models With Pause Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EWcybWr3MR": {
    "title": "Unlocking Tuning-free Generalization: Minimizing the PAC-Bayes Bound with Trainable Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IuXR1CCrSi": {
    "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Scc7Nl7lg": {
    "title": "Revealing Vision-Language Integration in the Brain with Multimodal Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaFrlUcAn3": {
    "title": "Debiasing Language Models Using Energy-Guided Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FM5xfcaR2Y": {
    "title": "Post-hoc bias scoring is optimal for fair classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWlSNtViSA": {
    "title": "Coupling Fairness and Pruning in a Single Run: a Bi-level Optimization Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxITGFPVWh": {
    "title": "Sharpness-Aware Data Poisoning Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pK7V0glCdj": {
    "title": "BOtied: Multi-objective Bayesian optimization with tied multivariate ranks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3d0OmYTNui": {
    "title": "Privately Aligning Language Models with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHBfxhZu1u": {
    "title": "YaRN: Efficient Context Window Extension of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wk77w7DG1N": {
    "title": "Evaluating and Improving Generation Consistency of Large Language Models via A Divide-Conquer-Reasoning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EE75tyB5Ay": {
    "title": "On the Generalization of Training-based ChatGPT Detection Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybiwT2yP1c": {
    "title": "BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aJl5aK9n7e": {
    "title": "What Improves the Generalization of Graph Transformer? A Theoretical Dive into Self-attention and Positional Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLfdJEuXkR": {
    "title": "UGSL: A Unified Framework for Benchmarking Graph Structure Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kuj5gVp5GQ": {
    "title": "Accelerating Sinkhorn algorithm with sparse Newton iterations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EmUVpfrXWN": {
    "title": "Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cDInj7WMQm": {
    "title": "UGC: UNIVERSAL GRAPH COARSENING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rR03qFesqk": {
    "title": "Functional Interpolation for Relative Positions improves Long Context Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLOaCQfZe9": {
    "title": "Dream to Adapt: Meta Reinforcement Learning by Latent Context Imagination and MDP Imagination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfEqSWpMfj": {
    "title": "Word Importance Explains How Prompts Affect Language Model Outputs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLOgB6oDnd": {
    "title": "KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4Hcegjzph": {
    "title": "Pre-training with Random Orthogonal Projection Image Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPpkFqMX6V": {
    "title": "Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkJiNn2QDF": {
    "title": "FeatUp: A Model-Agnostic Framework for Features at Any Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Giwj9cgAIl": {
    "title": "Mechanistic Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=12zKEh2APn": {
    "title": "PROSE: Predicting Operators and Symbolic Expressions using Multimodal Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q53QLftNkA": {
    "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nshk5YpdWE": {
    "title": "Lagrangian Flow Networks for Conservation Laws",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J2TZgj3Tac": {
    "title": "Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDuQNUQN6q": {
    "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F76bwRSLeK": {
    "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w7LU2s14kE": {
    "title": "Linearity of Relation Decoding in Transformer Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Gvs64deOj": {
    "title": "Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3RfGSbXUt8": {
    "title": "Option Boosting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUtGjQEDd4": {
    "title": "Generative Modeling with Phase Stochastic Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbuyDzZTPt": {
    "title": "OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yiMB2DOjsR": {
    "title": "Chain of Log-Concave Markov Chains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zww4Xqmk38": {
    "title": "Tree-based Ensemble Learning for Out-of-distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YTKShuSOhI": {
    "title": "Demonstrating the capacity of a Path-Based variational inference formulation for robust hidden Markov modelling of complex and noisy binary trees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxHgpC6FNv": {
    "title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lkIRFglmTp": {
    "title": "Resolving Partial Observability in Decision Processes via the Lambda Discrepancy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RxhOEngX8s": {
    "title": "Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=unE3TZSAVZ": {
    "title": "Breaking Neural Network Scaling Laws with Modularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20oxNYWQl9": {
    "title": "Sensitivity Sampling for Coreset-Based Data Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zRMXQMyyM8": {
    "title": "DISCRET: a self-interpretable framework for treatment effect estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EArTDUmILF": {
    "title": "VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kKxvFpvV04": {
    "title": "Towards Exact Computation of Inductive Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJ1w6MzVZw": {
    "title": "Large Pre-trained time series models for cross-domain Time series analysis tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qyp3Rni2g1": {
    "title": "Efficiency Pentathlon: A Standardized Benchmark for Efficiency Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qTui9aQ3VW": {
    "title": "How Robust Are Energy-Based Models Trained With Equilibrium Propagation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXpH3D0TVP": {
    "title": "The Journey, Not the Destination: How Data Guides Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNQjN5HzXt": {
    "title": "AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qUVP6IDc5J": {
    "title": "Eliciting Attributions from LLMs with Minimal Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owokKCrGYr": {
    "title": "Quality-Diversity through AI Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WM0OogPTx": {
    "title": "Learning from Sparse Offline Datasets via Conservative Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ivokwVKY4o": {
    "title": "Formal Verification for Neural Networks with General Nonlinearities via Branch-and-Bound",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qqu5mMgIBV": {
    "title": "Castor: Causal Temporal Regime Structure Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DL7JWbdGr3": {
    "title": "PEMs: Pre-trained Epidemic Time-Series Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJDFhkwQg6": {
    "title": "Federated contrastive GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JigPd5Pm5": {
    "title": "Informed weight initialization of Graph Neural Networks and its effect on Oversmoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5EtSvYUU0v": {
    "title": "Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vg7dECgAw2": {
    "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qr4ECbGcSj": {
    "title": "On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auUngos7eR": {
    "title": "Implicit Maximum a Posteriori Filtering via Adaptive Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHXCByvrLd": {
    "title": "Rethinking Optimal Transport in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=93LoCyww8o": {
    "title": "The HIM Solution for Legged Locomotion: Minimal Sensors, Efficient Learning, and Substantial Agility",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rAHcTCMaLc": {
    "title": "S$2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qhkEOCcVX9": {
    "title": "A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oDGkq0AleM": {
    "title": "Anomaly Detection with Variance Stabilized Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9cumTvvlHG": {
    "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xhEN0kJh4q": {
    "title": "Robust Model-Based Optimization for Challenging Fitness Landscapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4AEBLHuA6": {
    "title": "Solving High Frequency and Multi-Scale PDEs with Gaussian Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yjX303Smre": {
    "title": "Reinforcement Learning of Diverse Skills using Mixture of Deep Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XMJBrvRDI8": {
    "title": "Hierarchically branched diffusion models leverage dataset structure for class-conditional generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKHmjlpViu": {
    "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7QAX3zRh0": {
    "title": "Towards guarantees for parameter isolation in continual learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nf4Lm6fXN8": {
    "title": "Replay across Experiments: A Natural Extension of Off-Policy RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzE7EG7S4i": {
    "title": "High-Dimensional Geometric Streaming for Nearly Low Rank Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dapU3n7yfp": {
    "title": "Automatically Eliciting Toxic Outputs from Pre-trained Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2SuA42Mq1c": {
    "title": "BMAD: Benchmarks for Medical Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YkCjojDG3l": {
    "title": "PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EcDO5EXFdH": {
    "title": "SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nrctFaenIZ": {
    "title": "GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S5yOuNfSA0": {
    "title": "Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQrFaQC3kj": {
    "title": "Dataset Fairness: Achievable Fairness On Your Data With Utility Guarantees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fx8AJDQRVB": {
    "title": "Image Super-Resolution via Latent Diffusion: A Sampling-Space Mixture of Experts and Frequency-Augmented Decoder Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YOKnEkIuoi": {
    "title": "Conditional Variational Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CH6DQGcI3a": {
    "title": "Revisiting DeepFool: generalization and improvement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hj9ZuNimRl": {
    "title": "Better Neural PDE Solvers Through Data-Free Mesh Movers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sq5gkjC9jv": {
    "title": "Topological Expressive Power of ReLU Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wIFvdh1QKi": {
    "title": "Metric Space Magnitude for Evaluating Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUz8QXAgFV": {
    "title": "Bridging Autoregressive and Masked Modeling for Enhanced Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MXI8aSgl53": {
    "title": "NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ydlfehfvge": {
    "title": "Mitigating Estimation Errors By Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JsRZEGZ7L": {
    "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xIHi5nxu9P": {
    "title": "Subtractive Mixture Models via Squaring: Representation and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PbGs8PGoCn": {
    "title": "Stateless Mean-Field Games: A Framework for Independent Learning with Large Populations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZWHYyfPwY": {
    "title": "Don't trust your eyes: on the (un)reliability of feature visualizations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CBGdLyJXBW": {
    "title": "Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W0zgCR6FIE": {
    "title": "Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKALcMvCdm": {
    "title": "Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ipWSxcmgsx": {
    "title": "Optimizing the trade-off between utility and performance in interpretable sleep classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r0BcyqWAcj": {
    "title": "Loci-Segmented: Improving Scene Segmentation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3rjenIOfx": {
    "title": "A Conceptual Framework for Analyzing Social Representation in Unstructured Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqTipMg9CZ": {
    "title": "Contextual Molecule Representation Learning from Chemical Reaction Knowledge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wCRTEOIdmf": {
    "title": "Towards Subgraph Isomorphism Counting with Graph Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5XUlfPcQnG": {
    "title": "A Calibrated Simulation for Offline Training of Reinforcement Learning Agents to Optimize Energy and Emission in Office Buildings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmXXKxQpOR": {
    "title": "On the Provable Advantage of Unsupervised Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uKB4cFNQFg": {
    "title": "BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1IIiQnLRe8": {
    "title": "Diversity Modeling for Semantic Shift Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QxItoEAVMb": {
    "title": "TorchRL: A data-driven decision-making library for PyTorch",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dm4qrBuFKH": {
    "title": "Training Binary Neural Networks in a Binary Weight Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdOaaDzDD6": {
    "title": "Bandits with Ranking Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqYjAQ5diD": {
    "title": "Factorized Neural Radiance Field with Depth Covariance Function for Dense RGB Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AIbQ3HDDHU": {
    "title": "Training and inference of large language models using 8-bit floating point",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uJPWeZffgl": {
    "title": "Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sysX9XMGdF": {
    "title": "Understanding and Tackling Over-Dilution in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7pVIFJW2Hp": {
    "title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N7rEyHTZO9": {
    "title": "SSC Layer - A replacement for convolutional layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64t9er38Zs": {
    "title": "Learning Deep O($n$)-Equivariant Hyperspheres",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ledQ1BCrwc": {
    "title": "GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VNyIVrKrqv": {
    "title": "Constrained Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Piod76RSrx": {
    "title": "Slicing Mutual Information Generalization Bounds for Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5hAMmCU0bK": {
    "title": "Towards Robust Offline Reinforcement Learning under Diverse Data Corruption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T16M4SzH1v": {
    "title": "Distributional Bellman Operators over Mean Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GdTOzdAX5A": {
    "title": "On the Identifiability of Switching Dynamical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KknWbD5j95": {
    "title": "SoundStorm: Efficient Parallel Audio Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYp2KwjCWx": {
    "title": "Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r125wFo0L3": {
    "title": "Large Trajectory Models are Scalable Motion Predictors and Planners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUy4mGPmUy": {
    "title": "Optimization Framework of Transfer Learning and its Feasibility",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=up6hr4hIQH": {
    "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39HaKNXpsu": {
    "title": "Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sx7BIiPzys": {
    "title": "Variational Bayesian Last Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hOMVq57Ce0": {
    "title": "Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jc0FssXh2R": {
    "title": "Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIJFG7wW2d": {
    "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gIiz7tBtYZ": {
    "title": "Neural Optimal Transport with General Cost Functionals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7gUrYE50Rb": {
    "title": "EQA-MX: Embodied Question Answering using Multimodal Expression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnAeyjtMFM": {
    "title": "When Witnesses Defend: A Witness Graph Topological Layer for Adversarial Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Re5KnZcXhf": {
    "title": "Constrained Variational Generation for Generalizable Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEDTZxwJjT": {
    "title": "DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZDEwhAlCO": {
    "title": "ILPO-NET: convolution network for the recognition of arbitrary volumetric patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apA6SSXx2e": {
    "title": "A Topological Perspective on Demystifying GNN-Based Link Prediction Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5liV2xUdJL": {
    "title": "Time-Efficient Reinforcement Learning with Stochastic Stateful Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=whFQe4MRIY": {
    "title": "MI-NeRF: Learning a Single Face NeRF from Multiple Identities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0y0yOpI4wx": {
    "title": "General-Purpose In-Context Learning by Meta-Learning Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZlQRiFmq7Y": {
    "title": "Retrieval-based Disentangled Representation Learning with Natural Language Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6eA8Srlqd": {
    "title": "Scalable Long Range Propagation on Continuous-Time Dynamic Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PN0SuVRMxa": {
    "title": "Structured Packing in LLM Training Improves Long Context Utilization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2Kdq5biZx": {
    "title": "Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mnipav175N": {
    "title": "Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy8upuD6Bw": {
    "title": "Emergent Communication with Conversational Repair",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGUyAuuTYZ": {
    "title": "Bridging the Gap between Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lROh08eK6n": {
    "title": "Efficient Network Embedding in the Exponentially Large Quantum Hilbert Space: A High-Dimensional Perspective on Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UyPmWupphV": {
    "title": "Hyperion: Fused Multi-Trial and Gradient Descent for Joint Hyperparameter and Neural Architecture Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=czpx02orl7": {
    "title": "Learning Abstract World Models for Value-preserving Planning with Options",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D96juYQ2NW": {
    "title": "Coresets for Clustering with Noisy Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CW2aryHm95": {
    "title": "Policy Learning For Video Streaming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pzir15nPfc": {
    "title": "Contextual Vision Transformers for Robust Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lOWCkhr4g": {
    "title": "Unsupervised ASR via Cross-Lingual Pseudo-Labeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XdSYtriYfI": {
    "title": "Federated Ensemble-Directed Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bH6T0Jjw5y": {
    "title": "Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RPhoFFj0jg": {
    "title": "ResBit: Residual Bit Vector for Categorical Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j511LaqEeP": {
    "title": "Non-Exchangeable Conformal Risk Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pqgDqYinDZ": {
    "title": "Learning From Multi-Expert Demonstrations: A Multi-Objective Inverse Reinforcement Learning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kn7tWhuetn": {
    "title": "On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jId5PXbBbX": {
    "title": "Provably Efficient UCB-type Algorithms For Learning Predictive State Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kBNIx4Biq4": {
    "title": "Lifting Architectural Constraints of Injective Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UH4HinPK9d": {
    "title": "Provably Accurate ODE Forecasting Through Explicit Trajectory Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxBoUKhcBJ": {
    "title": "LM-Switch: Transforming Word Embedding Space for Flexible Language Model Steering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxebDHZ7b7": {
    "title": "TRAM: Bridging Trust Regions and Sharpness Aware Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ptCIlV24YZ": {
    "title": "Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=khAE1sTMdX": {
    "title": "Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iyMixbK9M2": {
    "title": "The Extrapolation Power of Implicit Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lIYxAcxY1B": {
    "title": "Understanding Sparse Feature Updates in Deep Networks using Iterative Linearisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkJOCga62u": {
    "title": "Orbit-Equivariant Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aBUidW4Nkd": {
    "title": "Object-Centric Learning with Slot Mixture Module",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhanPLSHRt": {
    "title": "EXCOST: Semi-Supervised Classification with Exemplar-Contrastive Self-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhPUSofMgr": {
    "title": "Text-Aware Diffusion Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lm7MRcsFiS": {
    "title": "Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jD1sU2vLOn": {
    "title": "Learning Counterfactually Invariant Predictors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oYjW8QxuC": {
    "title": "Pi-DUAL: Using privileged information to distinguish clean from noisy labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xlQrAm3LE4": {
    "title": "DiffSim: Aligning Diffusion Model and Molecular Dynamics Simulation for Accurate Blind Docking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Oiee202rd": {
    "title": "More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSvOIT5Ai2": {
    "title": "Interpretable Concept Discovery and Learning from Pretrained Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rr4OccbgJi": {
    "title": "A Lennard-Jones Layer for Distribution Normalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8SPSIfR2e0": {
    "title": "Dissecting Language Models: Machine Unlearning via Selective Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjNfLdxr3A": {
    "title": "ELoRA: Efficient Low-Rank Adaptation with Random Matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v9Pguuamfp": {
    "title": "Explaining Emergent In-Context Learning as Kernel Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4Yd9i5FFm": {
    "title": "Asymmetric Momentum: A Rethinking of Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Zbg38nA0J": {
    "title": "Explaining grokking through circuit efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WKDwIaF7y": {
    "title": "Lookahead Sharpness-Aware Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wLbL3lJNTL": {
    "title": "Joint Representations for Reinforcement Learning with Multiple Sensors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0sbIEkIutN": {
    "title": "From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bb21JPnhhr": {
    "title": "AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DGez4B2a6Y": {
    "title": "A Plug-and-Play Image Registration Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZTkLDRmkg": {
    "title": "BENO: Boundary-embedded Neural Operators for Elliptic PDEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8aD5pUcVX": {
    "title": "What Makes for Good Visual Tokenizers for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUeYSTIhpE": {
    "title": "DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwO71a8wET": {
    "title": "Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4GfEOQlBoc": {
    "title": "Disentangling the Link Between Image Statistics and Human Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdwrCm5Msr": {
    "title": "MapLearn: Indoor Mapping using Audio",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nTNElfN4O5": {
    "title": "3D Interacting Hands Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQqZVayz07": {
    "title": "Aligning Agents like Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TPAj63ax4Y": {
    "title": "Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NddKiWtdUm": {
    "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vE1e1mLJ0U": {
    "title": "The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rrCF6WasY8": {
    "title": "Distributed DPHelmet: Differentially Private Non-interactive Convex Blind Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykEixGIJYb": {
    "title": "Incentivized Truthful Communication for Federated Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ep9y5OrFmS": {
    "title": "What Apples Tell About Oranges: Connecting Pruning Masks and Hessian Eigenspaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UDNJdVjhyg": {
    "title": "Learning Graph Representations via Graph Entropy Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pC3WJHf51j": {
    "title": "Large-scale training of foundation models for wearable biosignals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aFMiKm9Qcx": {
    "title": "The Central Spanning Tree Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oq5EF8parZ": {
    "title": "Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OIsahq1UYC": {
    "title": "Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfAqPxPsAj": {
    "title": "Language Conditioned Equivariant Grasp",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j20nMRUWK9": {
    "title": "Adaptive Knowledge Transfer for Generalized Category Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7FHrZuKogW": {
    "title": "Contractive Systems Improve Graph Neural Networks Against Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eMNN0wIyVw": {
    "title": "On Trajectory Augmentations for Off-Policy Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ylHLVq0psd": {
    "title": "Rethinking the Noise Schedule of Diffusion-Based Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JSlTXa6WE6": {
    "title": "Efficient Certification of Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yz0Strbex6": {
    "title": "A Note on Some Statistical Properties of Signature Transform Under Stochastic Integrals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rsg1mvUahT": {
    "title": "Federated Wasserstein Distance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gENfMmUIkT": {
    "title": "A Pipeline-Based Approach for Object Detection on Resource Constrained Internet of Things Devices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WoP9veDwUp": {
    "title": "Variance-Reduced Meta-Learning via Laplace Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UulwvAU1W0": {
    "title": "Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WR9M6AA4LT": {
    "title": "Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Diffusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oashk4fDD9": {
    "title": "Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kA7vZQG34x": {
    "title": "Adversarial Imitation Learning from Visual Observations using Latent Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=17BA0Tl2Id": {
    "title": "Meta-Referential Games to Learn Compositional Learning Behaviours",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GY1fKFXG5i": {
    "title": "Non-Vacuous Generalization Bounds for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zz594UBNOH": {
    "title": "Clifford Group Equivariant Simplicial Message Passing Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5HpZZbgdeK": {
    "title": "Efficient calibration as a binary top-versus-all problem for classifiers with many classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NxoFmGgWC9": {
    "title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hsf2pDv2Qw": {
    "title": "RL Simplex: Bringing Computational Efficiency in Linear Programming via Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TkP2RtR4hr": {
    "title": "Regulating the level of manipulation in text augmentation with systematic adjustment and advanced sentence-embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ohamFnX14": {
    "title": "The (co)limit of metabeliefs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jFox1iMWUa": {
    "title": "CAUSAL NEURAL NETWORKS FOR CONTINUOUS TREATMENT EFFECT ESTIMATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6iBe17Y16": {
    "title": "On Using Admissible Bounds for Learning Forward Search Heuristics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWHf7lg8zM": {
    "title": "MultiContrievers: Analysis of Dense Retrieval Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EDPxCjXzSb": {
    "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wYmvN3sQpG": {
    "title": "Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ltZ9ianMth": {
    "title": "RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyuCmLLluu": {
    "title": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8vJSIsLhC": {
    "title": "SMPE: A Framework for Multi-Dimensional Permutation Equivariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VNHsZPZ5rJ": {
    "title": "Targeted Model Inversion: Distilling Style Encoded in Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PXD3FAVHJT": {
    "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rzBskAEmoc": {
    "title": "CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h5lqXPd9JN": {
    "title": "Model-Decoupling-Based Federated Learning with Consistency via Knowledge Distillation Using Conditional Generator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q00CO1Tm6M": {
    "title": "Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATEawsFUj4": {
    "title": "GAIA: Data-driven Zero-shot Talking Avatar Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KbvKjpqYQR": {
    "title": "Equivariant Quantum Graph Neural Network for Mixed-Integer Linear Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKPzWyaOsK": {
    "title": "Are machines automating morality?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUiEgloner": {
    "title": "Adaptive Learning of Quantum Hamiltonians",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MnMWa94t12": {
    "title": "DyST: Towards Dynamic Neural Scene Representations on Real-World Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89l6VLPrin": {
    "title": "Graph layouts and graph contrastive learning via neighbour embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VIEbRFp6s3": {
    "title": "Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skcTCdJz0f": {
    "title": "Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djmLZkEw1L": {
    "title": "IMPLICIT STACKED AUTOREGRESSIVE MODEL FOR WEATHER FORECASTING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bdJaYLiOxi": {
    "title": "Radar Spectra-language Model for Automotive Scene Parsing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zvB14IF6D": {
    "title": "DORSal: Diffusion for Object-centric Representations of Scenes $\\textit{et al.}$",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GsNp4ob8BY": {
    "title": "Mark My Words: Repurposing LLMs for Specialized Domains via Ability Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ulMXGO1fdH": {
    "title": "Estimating Post-Synaptic Effects for Online Training of Feed-Forward SNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s5ZAs0UkRr": {
    "title": "ODEdit: Blind Face Restoration through Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PoBB8n52oi": {
    "title": "SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WEoyWdsI9f": {
    "title": "Quantifying and Defending against the Privacy Risk in Logit-based Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8Qg1IIMaR": {
    "title": "Fool Your Large (Vision and) Language Models with Embarrassingly Simple Permutations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bIHyMpzeuI": {
    "title": "Sparse MoE as a New Treatment: Addressing Forgetting, Fitting, Learning Issues in Multi-Modal Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HsJzGWvg7K": {
    "title": "Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MACKSU3xed": {
    "title": "PeriodNet:Lightweight And Efficient Time Series Prediction Model Based On Periodic Characteristics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYyzypZrgU": {
    "title": "DOMAIN-GROUNDING OF NEURAL NETWORKS FOR SPATIOTEMPORAL REASONING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvq4Nh8eZB": {
    "title": "Protecting Sensitive Data through Federated Co-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3y2TfP966N": {
    "title": "T-Rep: Representation Learning for Time Series using Time-Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gk75gOjtQh": {
    "title": "Variational Inference with Singularity-Free Planar Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vy5aRVSbNo": {
    "title": "Looping LOCI: Developing Object Permanence from Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7P2mK3x3o": {
    "title": "Computing high-dimensional optimal transport by flow neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KiK4MNkuiQ": {
    "title": "Clustering with Geometric Modularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8S14xeFQAY": {
    "title": "Segmenting the Unknown: Discrete Diffusion Models for Non-Deterministic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x36mCqVHnk": {
    "title": "Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5juXkyorf": {
    "title": "Closed-Form Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDMyhAxok3": {
    "title": "MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible and Diverse Neuronal Morphology Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpWRCiw8g5": {
    "title": "JOSENet: A Joint Stream Embedding Network for Violence Detection in Surveillance Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5vXDQ65dzH": {
    "title": "ParFam - Symbolic Regression Based on Continuous Global Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2dVrgLpsF": {
    "title": "On partial prototype collapse in clustering-based self-supervised learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LqRGsGWOTX": {
    "title": "Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IB1HqbA2Pn": {
    "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8ibi56aM6": {
    "title": "SINGLE-IMAGE COHERENT RECONSTRUCTION OF OBJECTS AND HUMANS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WdvT2UgsTK": {
    "title": "Enhancing the Cross-Size Generalization for Solving Vehicle Routing Problems via Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lf8QQ2KMgv": {
    "title": "Is Memorization Actually Necessary for Generalization?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1VeQ6VBbev": {
    "title": "Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DT8ipHAAVz": {
    "title": "End-to-End Training of Unsupervised Trees: KAURI and DOUGLAS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATQSDgYwqA": {
    "title": "Diffusion Random Feature Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mOTiVzTgF2": {
    "title": "ResiDual: Transformer with Dual Residual Connections",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7LZjuA4AB2": {
    "title": "Ask Your Distribution Shift if Pre-Training is Right for You",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vXxardq6db": {
    "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B37UmlxsaP": {
    "title": "Revealing The Intrinsic Ability of Generative Text Summarizers for Outlier Paragraph Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O04DqGdAqQ": {
    "title": "Ada-Instruct: Adapting Instruction Generators For Complex Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MloaGA6WwX": {
    "title": "Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g4I3Wzv3fw": {
    "title": "Revisiting the Static Model in Robust Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ee4QXtVDVm": {
    "title": "Subword embedding from bytes against embedding-based attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d3xKPQVjSc": {
    "title": "Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1oqedRt6Z7": {
    "title": "Convolutional Deep Kernel Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RJDjSXNuAZ": {
    "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IL9o1meezQ": {
    "title": "Random Walk Diffusion For Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HobyL1B9CZ": {
    "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DEJIDCmWOz": {
    "title": "On the Reliability of Watermarks for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q0IZQMojwv": {
    "title": "Objectives Are All You Need: Solving Deceptive Problems Without Explicit Diversity Maintenance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDaLmkdSKU": {
    "title": "Near-Optimal Solutions of Constrained Learning Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lUYY2qsRTI": {
    "title": "Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9F0xInGNBF": {
    "title": "VIDEOPROMPTER: AN ENSEMBLE OF FOUNDATIONAL MODELS FOR ZERO-SHOT VIDEO UNDERSTANDING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b7bilXYHgG": {
    "title": "Counterfactual Fairness for Predictions using Generative Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CCo8ElCT7v": {
    "title": "Comprehensive Comparison between Vision Transformers and Convolutional Neural Networks for Face Recognition Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RNfX0ah0K": {
    "title": "Leave-one-out Distinguishability in Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBEQIxXDDD": {
    "title": "TopoFormer: Topology-aware Transformer for Reactive Motion Prediction in Close Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pj3ErOxlLo": {
    "title": "NaviFormer: A Deep Reinforcement Learning Transformer-like Model to Holistically Solve the Navigation Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7cuUZxwFS": {
    "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESq3U7z6FD": {
    "title": "EHI: End-to-end learning of Hierarchical Index for Efficient Dense Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3IUunLy8s": {
    "title": "Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3y1K6buO8c": {
    "title": "Brain decoding: toward real-time reconstruction of visual perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nM2AHzqUj": {
    "title": "Linear Log-Normal Attention with Unbiased Concentration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pSf8rrn49H": {
    "title": "Copyright Plug-in Market for The Text-to-Image Copyright Protection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3J7foqnJkA": {
    "title": "Understanding Parameter Saliency via Extreme Value Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2wFXD2upSQ": {
    "title": "A Demon at Work: Leveraging Neuron Death for Efficient Neural Network Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nil8G449BI": {
    "title": "Block-local learning with probabilistic latent representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7OB810eaP": {
    "title": "Non-ergodicity in reinforcement learning: robustness via ergodic transformations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MvHiijJL3": {
    "title": "Model Explanation Disparities as a Fairness Diagnostic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wiYV0KDAE6": {
    "title": "Diffusion Models for Tabular Data Imputation and Synthetic Data Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JlSyXwCEIQ": {
    "title": "CodeIt: Abstract Reasoning with Iterative Policy-Guided Program Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sY5N0zY5Od": {
    "title": "DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=le1UUMd45T": {
    "title": "Solving Multiobjective Combinatorial Optimization via Learn to Improve Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3QR230r11w": {
    "title": "Multi-Fidelity Active Learning with GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=25VG15SnkH": {
    "title": "United We Train, Divided We Fail! Representation Learning for Time Series by Pretraining from 75 Datasets at Once",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJEd8PkdNz": {
    "title": "Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0MyyXyGfn": {
    "title": "Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDgw3yX2aO": {
    "title": "FedGT: Identification of Malicious Clients in Federated Learning with Secure Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6tUsZeVs7": {
    "title": "Energy-guided Entropic Neural Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FPpLTTvzR0": {
    "title": "IDEA: Invariant Causal Defense for Graph Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RN2lIjrtSR": {
    "title": "ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CwAY8b8i97": {
    "title": "Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9WD9KwssyT": {
    "title": "Zipformer: A faster and better encoder for automatic speech recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TWVMVPx2wO": {
    "title": "Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DFQCJmHPoe": {
    "title": "Adversarial latent representation for positive unlabeled learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Akd36hG9z": {
    "title": "Enhancing Offline Reinforcement Learning with an Optimal Supported Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vN9fpfqoP1": {
    "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WOiOzHG2zD": {
    "title": "TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UMwn5l37gU": {
    "title": "Non-uniform Noise Injection For Enhancing DNN Adversarial Robustness And Efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKPh4CLmYp": {
    "title": "Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m5m3nugttY": {
    "title": "UniVis: A Universal Framework for Computer Vision Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PbpJnyewVM": {
    "title": "Zero-shot Cross-task Preference Alignment for Offline RL via Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qODvxQ8TXW": {
    "title": "Masks, Signs, And Learning Rate Rewinding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cUSNs8nGaV": {
    "title": "GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gDlsMWost9": {
    "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GYAvwLviup": {
    "title": "Aligning brain functions boosts the decoding of videos in novel subjects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XyrB1Ay44j": {
    "title": "Quantifying and Enhancing Multi-modal Robustness with Modality Preference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xi7UoErFRt": {
    "title": "FedGP: Buffer-based Gradient Projection for Continual Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dxJKLozjQl": {
    "title": "Data Distribution Valuation with Incentive Compatibility",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgrZluxFC7": {
    "title": "Adversarial Machine Learning in Latent Representations of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVuZa1bgOs": {
    "title": "Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=otU31x3fus": {
    "title": "Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AcSChDWL6V": {
    "title": "Transformers vs. Message Passing GNNs: Distinguished in Uniform",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvSMIsztka": {
    "title": "Faster Approximation of Probabilistic and Distributional Values via Least Squares",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1pTlvxIfuV": {
    "title": "A Reparameterized Discrete Diffusion Model for Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yisfNWUEsD": {
    "title": "SCALE: Synergized Collaboration of Asymmetric Language Translation Engines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zgQ0PHeGnL": {
    "title": "Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SqNi6Se1NT": {
    "title": "A Bayesian Framework for Clustered Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fd8MBEOirN": {
    "title": "OpenPatch: a 3D patchwork for Out-Of-Distribution detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pA4s793lcB": {
    "title": "Improved Algorithms for Replicable Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mavWQw7DnC": {
    "title": "Explaining recommendation systems through contrapositive perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTTZFKrlGV": {
    "title": "Gradual Domain Adaptation via Gradient Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhINOCrrqI": {
    "title": "AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YzJT0Y67Go": {
    "title": "HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YmQyEdLIkU": {
    "title": "Adversarial Attacks as Near-Zero Eigenvalues in The Empirical Kernel of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a4O528mek9": {
    "title": "Learning Multi-modal Representations Under Incomplete Data Via Dual Level Alignments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nliDYxirqq": {
    "title": "EduGym: An Environment Suite for Reinforcement Learning Education",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=akKNGGWegr": {
    "title": "Spatio-Temporal Graph Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlY7WQ2hWS": {
    "title": "Incentive-Aware Federated Learning with Training-Time Model Rewards",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7PmO5boQ9": {
    "title": "DynaEval: A Dynamic Interaction-based Evaluation Framework for Assessing LLMs in Real-world Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AwX6ON5A0V": {
    "title": "On Gaussian Mixture Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7TOs9gjAg1": {
    "title": "Removing Biases from Molecular Representations via Information Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXjXeTsqgW": {
    "title": "Sequential Condition Evolved Interaction Knowledge Graph for Traditional Chinese Medicine Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JtTPYBKqt": {
    "title": "Neural Architecture Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QibPzdVrRu": {
    "title": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pf2hEdu8B": {
    "title": "Rethinking the Uniformity Metric in Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmqOhBC4a5": {
    "title": "Maximum Entropy Heterogeneous-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFqVifIr6E": {
    "title": "Rethinking Semantic Few-Shot Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VWGyUZ9dOX": {
    "title": "Data augmentation guided Decouple Knowledge Distillation for low-resolution fine-grained image classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x17qiTPDy5": {
    "title": "DiffFlow: A Unified SDE for Score-Based Diffusion Models and Generative Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AKZtQO81GQ": {
    "title": "Evaluating model bias requires characterizing model mistakes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBSmr51fCR": {
    "title": "URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99tKiMVJhY": {
    "title": "Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Xx0mKoCMd": {
    "title": "ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1wlmtPGLq": {
    "title": "TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oc4ji1iCjQ": {
    "title": "Catch the Shadow: Automatic Shadow Variables Generation for Treatment Effect Estimation under Collider Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Jer2DQt9V": {
    "title": "The Unreasonable Effectiveness of Pretraining in Graph OOD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a4DBEeGfQq": {
    "title": "StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mxCX2bSV0Z": {
    "title": "Using Forwards-Backwards Models to Approximate MDP Homomorphisms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BBD6KXIGJL": {
    "title": "Hybrid Directional Graph Neural Network for Molecules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=816T4ab9Z5": {
    "title": "Perfect Alignment May be Poisonous to Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oJ1tx3fXDA": {
    "title": "Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkH1l00p6u": {
    "title": "When Treatment Effect Estimation Meets Collider Bias: A Dual Counterfactual Generative Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KqbCvIFBY7": {
    "title": "Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pE6gWrASQm": {
    "title": "On Adversarial Training without Perturbing all Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KBo7Z5aTV0": {
    "title": "Diving Segmentation Model into Pixels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rBu7dR7rm": {
    "title": "Unified Long-Term Time-Series Forecasting Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AfhNyr73Ma": {
    "title": "General Stability Analysis for Zeroth-Order Optimization Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yVJd8lKyVX": {
    "title": "Hybrid Sharing for Multi-Label Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e1vqloonRy": {
    "title": "Symmetric Single Index Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdPvGRvoBC": {
    "title": "An improved analysis of per-sample and per-update clipping in federated learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ey3GhWXQ97": {
    "title": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nkCWKkSLyb": {
    "title": "Benchmarking Diffusion Based Text-Guided Image Editing Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VtmBAGCN7o": {
    "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FddFxi08J3": {
    "title": "On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yMMIWHbjWS": {
    "title": "On convex decision regions in deep network representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWVC5FVidc": {
    "title": "Unbiased Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EriR6Ec69a": {
    "title": "Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dt3rcTC8Sw": {
    "title": "Enhancing Mutual Information Estimation in Self-Interpretable Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VB2WkqvFwF": {
    "title": "The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4vPVBh3fhz": {
    "title": "PAC Prediction Sets Under Label Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PtB6l1vNtk": {
    "title": "PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Unz9zYdjTt": {
    "title": "FedNovel: Federated Novel Class Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X1p0eNzTGH": {
    "title": "How the Level Sampling Process impacts Zero-Shot Generalisation in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JWVWUlobv": {
    "title": "4D Tensor Multi-task Continual Learning for Disease Dynamic Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KSjPaXtxP8": {
    "title": "Memorization in Self-Supervised Learning Improves Downstream Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M3QXCOTTk4": {
    "title": "The Curse of Diversity in Ensemble-Based Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUcdXA8hAa": {
    "title": "Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sK2A7Ve2co": {
    "title": "Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5rEkR8OgU": {
    "title": "Implicit Intermediate Supervision for Learning Complex Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bbCL5aRjUx": {
    "title": "Multilinear Operator Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zhHVyLY4K": {
    "title": "Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r65xfUb76p": {
    "title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W8S8SxS9Ng": {
    "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vy42bYs1Wo": {
    "title": "Off-Policy Primal-Dual Safe Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EwMhfwiAuv": {
    "title": "Localized Text-to-Image Generation For Free via Cross Attention Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T8RiH35Hy6": {
    "title": "Understanding Community Bias Amplification in Graph Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vyGp9Mty2t": {
    "title": "Implicit Neural Representations for Joint Sparse-View CT Reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KkrDUGIASk": {
    "title": "An Extensible Framework for Open Heterogeneous Collaborative Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q6HYM1EMu8": {
    "title": "LARG2, Language-based Automatic Reward and Goal Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R4gqcDRJ9l": {
    "title": "TopoFR: A Closer Look at Topology Alignment on Face Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsJGd0xfgv": {
    "title": "Quantum Architecture Search with Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=StkLULT1i1": {
    "title": "Learning a Diffusion Model Policy from Rewards via Q-Score Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4flIscNE6": {
    "title": "Meta-Collaboration in Distillation: Pooled Learning from Multiple Students",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V1GM9xDvIY": {
    "title": "Neural structure learning with stochastic differential equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OTMPdMH9JL": {
    "title": "Neural Eigenfunctions Are Structured Representation Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wPhbtwlCDa": {
    "title": "STARC: A General Framework For Quantifying Differences Between Reward Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fibxvahvs3": {
    "title": "GAIA: a benchmark for General AI Assistants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYMeXb6PAw": {
    "title": "Adaptive Compression of the Latent Space in Variational Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KC2MViQASx": {
    "title": "Mutual Information Estimation via $f$-Divergence and Data Derangement Based Learning Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AU2gS9ut61": {
    "title": "BrainPy: a differentiable brain simulator bridging brain simulation and brain-inspired computing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbRWdSxTPY": {
    "title": "SQS: Speech Quality Assessment in the Data Annotation Context",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LGzTtvisL3": {
    "title": "FLea: Improving federated learning on scarce and label-skewed data via privacy-preserving feature augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvbeD9Ttkx": {
    "title": "FOSI: Hybrid First and Second Order Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jMJ9IRWmH9": {
    "title": "Privacy Preserving API Fine-tuning for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0zIKlb0prF": {
    "title": "MPPN: Multi-Resolution Periodic Pattern Network For Long-Term Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KP4xJQcG3H": {
    "title": "Lagrangian Proximal Gradient Descent for Learning Convex Optimization Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VQ7Q6qdp0P": {
    "title": "Fine-tuning can cripple foundation models; preserving features may be the solution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3jATpVEGv": {
    "title": "GraphAgent: Exploiting Large Language Models for Interpretable Learning on Text-attributed Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B8FA2ixkPN": {
    "title": "GML-NeRF: Gate-guided Mutual Learning Framework for Neural Rendering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vb3O9jxTLc": {
    "title": "Lost in Translation: Conceptual Blind Spots in Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Va2IQ471GR": {
    "title": "Convergence of SVGD in KL divergence via approximate gradient flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tWOUmBHRv": {
    "title": "Offline Tracking with Object Permanence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwU9scoU4A": {
    "title": "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DjzvJCRsVf": {
    "title": "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ywGSgEmOYb": {
    "title": "Fine-Tuning Is All You Need to Mitigate Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vQqJJzL2Jf": {
    "title": "Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lvf7GnaLru": {
    "title": "Unraveling the Key Components of OOD Generalization via Diversification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzTpTRVtrP": {
    "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNgY6ODeMp": {
    "title": "Cross-modality Interpretable image classification via Concept Decomposition Vector of Visual Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5THrhXDV3": {
    "title": "Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AY6aM13gGF": {
    "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=87YOFayjcG": {
    "title": "JudgeLM : Fine-tuned Large Language Models are Scalable Judges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H03dW4TysQ": {
    "title": "Experts on Demand: Dynamic Routing for Personalized Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrBVFXwAmi": {
    "title": "Q-TAPE: A Task-Agnostic Pre-Trained Approach for Quantum Properties Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlH6VB5sJN": {
    "title": "A Parallel Multi-compartment Spiking Neuron For Multi-scale Sequential Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juStNETXI5": {
    "title": "Tiny-StyleWizard: Unleashing the Potential of Small Language Models in Complex Style Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0oIkKERYhH": {
    "title": "DOG: Discriminator-only Generation Beats GANs on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BIveOmD1Nh": {
    "title": "Learning Scalar Fields for Molecular Docking with Fast Fourier Transforms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8RPghUs3W": {
    "title": "Analytic DAG Constraints for Differentiable DAG Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5oJlyJXUxK": {
    "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fh8EYKFKns": {
    "title": "The Alignment Problem from a Deep Learning Perspective: A Position Paper",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9o7KuFcsps": {
    "title": "Unified Anomaly Detection via Multi-Scale Contrasted Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=79tJB1eTmb": {
    "title": "Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJJcs3zbmi": {
    "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YSJW69CFQ": {
    "title": "Enhancing Machine Learning System Reliability in Healthcare through Uncertainty Estimation and Multi-Modal Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WcOohbsF4H": {
    "title": "Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tBROYsEz9G": {
    "title": "How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s8cMuxI5gu": {
    "title": "Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3Foe1fDjh": {
    "title": "Expected Probabilistic Hierarchies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PFUrgJtfs0": {
    "title": "Lost in Transformation: Current roadblocks for Transformers in 3D medical image segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7QnIKlC1N": {
    "title": "GTMGC: Using Graph Transformer to Predict Molecule's Ground-State Conformation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=llXCyLhOY4": {
    "title": "Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhlwoC1XaN": {
    "title": "From Zero to Turbulence: Generative Modeling for 3D Flow Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YclZqtwf9e": {
    "title": "Slingshot Perturbation to Learning in Monotone Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fagTLzHFBs": {
    "title": "LDINet: Latent Decomposition and Interpolation for Single Image FMO Deblatting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vq75kRCYuY": {
    "title": "SOLO: Surrogate Online Learning at Once for Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4YK1e3Ehdy": {
    "title": "Understanding Deep Neural Networks as Dynamical Systems: Insights into Training and Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AlkANue4lm": {
    "title": "Non-Redundant Graph Neural Networks with Improved Expressiveness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oa758mIOcP": {
    "title": "A Structured Matrix Method for Nonequispaced Neural Operators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s25i99RTCg": {
    "title": "Multi-modal Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yvxDJ8eyBu": {
    "title": "MuseCoco: Generating Symbolic Music from Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q9R10ZKd8z": {
    "title": "PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tMzPZTvz2H": {
    "title": "Generalization of Deep ResNets in the Mean-Field Regime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tIsYpRxMvr": {
    "title": "Good Better Best: Self-Motivated Imitation Learning For Noisy Demonstrations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pxI5IPeWgW": {
    "title": "ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6GySuKTJcd": {
    "title": "Energy-Guided Continuous Entropic Barycenter Estimation for General Costs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zj12nzlQbz": {
    "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5j6wtOO6Fk": {
    "title": "Hieros: Hierarchical Imagination on Structured State Space Sequence World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MZsKW0CraD": {
    "title": "Beyond Labeling Oracles: What does it mean to steal ML models?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExiBN1ZWJn": {
    "title": "Denoising Graph Dissipation Model Improves Graph Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlhjUkC7vH": {
    "title": "DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tD4NOxYTfg": {
    "title": "The Convergence of Variance Exploding Diffusion Models under the Manifold Hypothesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yx7TnC6AAp": {
    "title": "Towards Provably Efficient Learning of Extensive-Form Games with Imperfect Information and Linear Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlfTLt0zvd": {
    "title": "An Efficient Multi-Task Transformer for 3D Face Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zrxlSviRqC": {
    "title": "Learning energy-based models by self-normalising the likelihood",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WsHaBoucSG": {
    "title": "Emergent Language based Dialog for Collaborative Multi-agent Navigation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUveo5k1GF": {
    "title": "Improving equilibrium propagation without weight symmetry through Jacobian homeostasis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SGQ9aDvObu": {
    "title": "DIFAIR: Towards learning differenciated and interpretable representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WtNgFrPn8y": {
    "title": "Safe Online Bid Optimization with Return On Investment and Budget Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjCDNssXKU": {
    "title": "Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wcKGK0tRHD": {
    "title": "The Trifecta: Three simple techniques for training deeper Forward-Forward networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYXzKTljwx": {
    "title": "Uniform Localized Convergence and Sharper Generalization Bounds for Minimax Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3k6raldhEd": {
    "title": "A Best-of-Both-Worlds Algorithm for MDPs with Long-Term Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGQBpkIEuu": {
    "title": "Revisiting Data Augmentation in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HiClR4rwJf": {
    "title": "Value Factorization for Asynchronous Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKnzPdyeJu": {
    "title": "Structural Inference with Dynamics Encoding and Partial Correlation Coefficients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mFBR2ksIwY": {
    "title": "MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SCQfYpdoGE": {
    "title": "Prediction without Preclusion: Recourse Verification with Reachable Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Djw0XhjHZb": {
    "title": "Simplicial Representation Learning with Neural $k$-Forms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cVUOnF7iVp": {
    "title": "Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NzxCMe88HX": {
    "title": "Toward effective protection against diffusion-based mimicry through score distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KW3aAxkhE1": {
    "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LqaEEs3UxU": {
    "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IPayPEGwdE": {
    "title": "Learning Good Interventions in Causal Contextual Bandits with Adaptive Context",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wY67ZDQTE": {
    "title": "Cauchy-Schwarz Divergence Information Bottleneck for Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7sMR09VNKU": {
    "title": "Learning System Dynamics from Sensory Input under Optimal Control Principles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZTFUtldbC": {
    "title": "MeMo: Meaningful, Modular Controllers Via Information Bottlenecks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=14rn7HpKVk": {
    "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g8oaZRhDcf": {
    "title": "Copy Suppression: Comprehensively Understanding an Attention Head",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rc3RP9OoEJ": {
    "title": "In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzmKEpze8e": {
    "title": "Kalman Filter Online Learning from non-Stationary Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYjPk8mqAV": {
    "title": "Magnushammer: A Transformer-Based Approach to Premise Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bZHz9WYs9z": {
    "title": "Molecule Generation by Heterophilious Triple Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uLOFyiruin": {
    "title": "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojAc7y2P4K": {
    "title": "Dispatching Ambulances using Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49Tn5mfTy5": {
    "title": "Uncertainty Quantification Using a Codebook of Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ggu3cWldTy": {
    "title": "Unified Mirror Descent: Towards a Big Unification of Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u11Jwd3opH": {
    "title": "Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L8UNn7Llt4": {
    "title": "Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZtBP6DZah": {
    "title": "Contrastive Grouping-based Invariant Learning for Generalizable Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8cVivO5k5": {
    "title": "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZnnHDrBXD": {
    "title": "Tree-based Action-Manipulation Attack Against Continuous Reinforcement Learning with Provably Efficient Support",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HSUSo9p8X5": {
    "title": "Stochastic Subgoal Representation for Hierarchical Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CupHThqQl3": {
    "title": "It's About Time: Temporal References in Emergent Communication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HT2dAhh4uV": {
    "title": "Learning to Compose: Improving Object Centric Learning by Injecting Compositionality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CVldG5ohCy": {
    "title": "Adam through a Second-Order Lens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqsYs05Ri7": {
    "title": "Estimation of Concept Explanations Should be Uncertainty Aware",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8hc2UvwTaL": {
    "title": "FLAIM: AIM-based Synthetic Data Generation in the Federated Setting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8Lj9eoGl8": {
    "title": "Proximal Curriculum with Task Correlations for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRbnZs2WY4": {
    "title": "SelfClean: A Self-Supervised Data Cleaning Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LnxviiZ1xi": {
    "title": "MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wD8L86iCvD": {
    "title": "FINE-GRAINED AUDIO-VISUAL JOINT REPRESENTATIONS FOR MULTIMODAL LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rfSfDSFrRL": {
    "title": "Gated recurrent neural networks discover attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNzL1nglNB": {
    "title": "Label-encoding Risk Minimization under Label Insufficient Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R1crLHQ4kf": {
    "title": "Leveraging characteristics of the output distribution for identifying adversarial audio examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WhZoCLRWYJ": {
    "title": "Light Schrödinger Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKkiX32Zw1": {
    "title": "Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNMsieEiAc": {
    "title": "Prompt2Rec : Prompt based user and item Re-characterizing method for Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEoBvQWzHo": {
    "title": "Dirichlet-based Uncertainty Quantification for Personalized Federated Learning with Improved Posterior Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eCGpNGDeNu": {
    "title": "Reward-Free Curricula for Training Robust World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7UhxsmbdaQ": {
    "title": "Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uf4Hr5qU6L": {
    "title": "PreCoT: Problem Representation Enhances Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GaBg3pgXfX": {
    "title": "MusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical Representation of Symbolic Music",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bGJZXb26lo": {
    "title": "DITTO: Offline Imitation Learning with World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsNyDvNQTc": {
    "title": "Leveraging Uncertainty Estimates To Improve Classifier Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jzzEHTBFOT": {
    "title": "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GAXedKmbFZ": {
    "title": "Disco-Bench: A Context-Aware Evaluation Benchmark for Language Modelling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mqCt76eiNt": {
    "title": "Advantage-Aware Policy Optimization for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FTpdQBoBd0": {
    "title": "Enhancing Fine-Tuning Performance of Large-Scale Text-to-Image Models on Specialized Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2UlHeyyC0": {
    "title": "Retrieval-augmented Vision-Language Representation for Fine-grained Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvXK8Xk9Jk": {
    "title": "DEEP NEURAL NETWORK INITIALIZATION WITH SPARSITY INDUCING ACTIVATIONS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=324zEJCo3a": {
    "title": "Local Vs. Global Interpretability: A Computational Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQUbpAHbIZ": {
    "title": "Post-Nonlinear Causal Relationship with Finite Samples: A Maximal Correlation Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ic1Z7Qe9xH": {
    "title": "Elastic Load Balancing for Dynamic LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jwzm44fsJ8": {
    "title": "Multilingual Code Retrieval Without Paired Data: New Datasets and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZnmofqLWMQ": {
    "title": "Zero-shot Image Restoration via Diffusion Inversion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrYiwlDRQO": {
    "title": "PeFLL: Personalized Federated Learning by Learning to Learn",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUe1YqEgd6": {
    "title": "Unsupervised motion segmentation in one go: Smooth long-term model over a video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FYKVPOHCpE": {
    "title": "Improving Non-Transferable Representation Learning by Harnessing Content and Style",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W4rbphLht": {
    "title": "A Semi-smooth, Self-shifting, and Singular Newton Method for Sparse Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ciBFYxzpBT": {
    "title": "PASTA: Pretrained Action-State Transformer Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HaLxcQ6OOm": {
    "title": "Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xx0ITyHp3u": {
    "title": "Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BEyEziZ4R6": {
    "title": "DP-SGD Without Clipping: The Lipschitz Neural Network Way",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3PWYAlAQxv": {
    "title": "Neural Networks Trained by Weight Permutation are Universal Approximators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LlG0jR7Yjh": {
    "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B21c9hT1D7": {
    "title": "High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nfIAEJFiBZ": {
    "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W3GLNImfS": {
    "title": "Human Feedback is not Gold Standard",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pLvh9DTyoE": {
    "title": "Integrating Visual Cues via Prompting for Low-Resource Multimodal Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vpJMJerXHU": {
    "title": "ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMkvcg1i1u": {
    "title": "Abstract Interpretation of ReLU Neural Networks with Optimizable Polynomial Relaxations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3FHMoKZcS": {
    "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NukRlEUICA": {
    "title": "AFFINE INVARIANCE IN CONTINUOUS-DOMAIN CONVOLUTIONAL NEURAL NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOFLn0pMoe": {
    "title": "High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YrXHEb2qMb": {
    "title": "Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IAWIgFT71j": {
    "title": "Assessing Large Language Models on Climate Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PORUmWsgBN": {
    "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MU6jInwj7p": {
    "title": "LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jU3zRzUBiD": {
    "title": "Compensating for Nonlinear Reduction with Linear Computations in Private Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=if2vRbS8Ew": {
    "title": "First-order ANIL provably learns representations despite overparametrisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jGuXGNcK6O": {
    "title": "The Fundamental Limits of Least-Privilege Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZGNWW7xZ6Q": {
    "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=567BjxgaTp": {
    "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1VcKvdYbUM": {
    "title": "APBench: A Unified Benchmark for Availability Poisoning Attacks and Defenses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CetUU9FSt": {
    "title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86zAUE80pP": {
    "title": "CPPO: Continual Learning for Reinforcement Learning with Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKuimaBj4I": {
    "title": "Learning Optimal Contracts: How to Exploit Small Action Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fIKRJeLH7W": {
    "title": "Proper Backward Connection Placement Boosts Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9cQtXpRshE": {
    "title": "AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=am7BPV3Cwo": {
    "title": "Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=17ZbByq95E": {
    "title": "Memory-Efficient Backpropagation through Large Linear Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uizIvVBY8P": {
    "title": "Continual Supervised Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuoeYIGaSW": {
    "title": "$\\texttt{PREMIER-TACO}$ is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BI1N3lTWtn": {
    "title": "A Multi-Level Framework for Accelerating Training Transformer Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7q7s5fXEpP": {
    "title": "Stealthy Imitation: Reward-guided Environment-free Policy Stealing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pa4hecILrt": {
    "title": "Incremental Successive Halving for Hyperparameter Optimization with Budget Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tc3duzOHa8": {
    "title": "RODEO: Robust Out-of-Distribution Detection Via Exposing Adaptive Outliers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izO4mxI9nU": {
    "title": "HAICO-CN: Human-AI Collaboration By Cluster-wise Noisy-Label Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oQKKlzxV1o": {
    "title": "Online Information Acquisition: Hiring Multiple Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sZZ3R0lV9f": {
    "title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxmvbzw76R": {
    "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y8OaqdX5Xt": {
    "title": "Planning with Theory of Mind for Few-Shot Adaptation in Sequential Social Dilemmas",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IkIqzDI7ie": {
    "title": "M$^4$LE: A Multi-Ability Multi-Range Long Context Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MzjiMxlWab": {
    "title": "Learning Multi-Faceted Prototypical User Interests",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11oqo92x2Z": {
    "title": "Detection and Segmentation of Solar Farms in Satellite Imagery: A Study of Deep Neural Network Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0V311Uh8q1": {
    "title": "Algorithmic Stability Unleashed: Generalization Bounds with Unbounded Losses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c4QgNn9WeO": {
    "title": "LMEye: An Interactive Perception Network for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZWTHU7AsQ": {
    "title": "Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBIJRIYTqa": {
    "title": "Bandits with Replenishable Knapsacks: the Best of both Worlds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QFo2wxQEW6": {
    "title": "Autonomous Catheterization with Open-source Simulator and Expert Trajectory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YRXDl6I3j5": {
    "title": "Tall Tales at Different Scales: Evaluating Scaling Trends For Deception in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2xLkpkh0s": {
    "title": "FARSE-CNN: Fully Asynchronous, Recurrent and Sparse Event-Based CNN",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1jIKhMJ8y": {
    "title": "Learning Embeddings for Sequential Tasks Using Population of Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SvEMbCMgb5": {
    "title": "$R^2$: Range Regularization for Model Compression and Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gsZAtAdzkY": {
    "title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5qi6fnnw7": {
    "title": "Conservative World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwMfg9PfPs": {
    "title": "Out-of-Variable Generalisation for Discriminative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OWpp0TjdTt": {
    "title": "Semi-supervised batch learning from logged data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVsiTj9aOO": {
    "title": "Improved Variational Bayesian Phylogenetic Inference using Mixtures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39cPKijBed": {
    "title": "Training Unbiased Diffusion Models From Biased Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tPEwSYPtAC": {
    "title": "Towards Robust Out-of-Distribution Generalization Bounds via Sharpness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f43Kxj0FaW": {
    "title": "Meta-Prior: Meta learning for Adaptive Inverse Problem Solvers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JfqN3gu0i7": {
    "title": "The optimality of kernel classifiers in Sobolev space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jiQg5IvuYF": {
    "title": "Corgi$^2$: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dW7FRwi1eA": {
    "title": "Learning a Reusable Meta Denoiser for Learning with Noisy Labels on Multiple Target Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eOCvA8iwXH": {
    "title": "Neural Fourier Transform: A General Approach to Equivariant Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3GurO0kRue": {
    "title": "On Harmonizing Implicit Subpopulations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jdynlBj3b0": {
    "title": "Can Class-Priors Help Single-Positive Multi-Label Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pRpMAD3udW": {
    "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOwDQl8haC": {
    "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9k4Yvb75ED": {
    "title": "EquiAV: Single-modal Equivariance Promotes Audio-Visual Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2boLXjsHsB": {
    "title": "Multi-Objective Reinforcement Learning for Forward-Backward Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rKMQhP6iAv": {
    "title": "Personas as a way to Model Truthfulness in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RTLjdy6Ntk": {
    "title": "FL-GNN: A Fuzzy-logic Graph Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Y7r6xueJJ": {
    "title": "Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKhNBulNMh": {
    "title": "Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BURvGotSLz": {
    "title": "Is Training Necessary for Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8eWha27jw": {
    "title": "Accelerating Federated Learning with Quick Distributed Mean Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itGkF993gz": {
    "title": "MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUO1HXz4an": {
    "title": "Negative Label Guided OOD Detection with Pretrained Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgayrNSbri": {
    "title": "Close the Gap: Lightweight Image Captioning via Retrieval Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE6WxgrOWP": {
    "title": "Chain of Images for Intuitively Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IhOeYKqnfp": {
    "title": "Continual Memory Neurons",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jHkUDyEO9": {
    "title": "Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZrY38sUYWs": {
    "title": "Feature Map Matters in Out-of-distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wYmcfur889": {
    "title": "Data Prediction Denoising Models: The Pupil Outdoes the Master",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZRzlhfMqHt": {
    "title": "Periodic and Random Sparsity for Multivariate Long-Term Time-Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QI7tVrh2c": {
    "title": "Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tj6Wcx7gVk": {
    "title": "Probabilistically Rewired Message-Passing Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HhVns87e74": {
    "title": "Towards a Better Theoretical Understanding of Independent Subnetwork Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6ajk22thA": {
    "title": "HGMD: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qIn2IgMWYg": {
    "title": "Iterative Search Attribution for Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duZANm2ABX": {
    "title": "BadEdit: Backdooring Large Language Models by Model Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUnFOgAy1D": {
    "title": "Efficient Differentiable Approximation of the Generalized Low-rank Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TBLe2BHBsr": {
    "title": "Dilated convolution neural operator for multiscale partial differential equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7ZEcoSdXQ": {
    "title": "Incentivizing Data Collection from Heterogeneous Clients in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qxLVaYbsSI": {
    "title": "Robust Training of Federated Models with Extremely Label Deficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D3JpYSn7dL": {
    "title": "An Instance-Level Framework for Multi-tasking Graph Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktJAF3lxbi": {
    "title": "On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHLVmV88Zy": {
    "title": "Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1ZEMXxSz1": {
    "title": "Segment Anything Model is a Good Teacher for Local Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmQAKu1wCe": {
    "title": "Temporal Flexibility in Spiking Neural Networks: A Novel Training Method for Enhanced Generalization Across Time Steps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qGaIMO8dqD": {
    "title": "Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sW95puhphh": {
    "title": "DECENTRALIZED MULTI-AGENT REINFORCEMENT LEARNING VIA ANTICIPATION SHARING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3w6xuXDOdY": {
    "title": "A Study of Generalization in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xwKt6bUkXj": {
    "title": "Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47hDbAMLbc": {
    "title": "OPTIMAL ROBUST MEMORIZATION WITH RELU NEURAL NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iAYIRHOYy8": {
    "title": "Neural Contractive Dynamical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpXGPCBOTX": {
    "title": "Sparsistency for inverse optimal transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yV6fD7LYkF": {
    "title": "ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HexshmBu0P": {
    "title": "A Recipe for Watermarking Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pEKJl5sflp": {
    "title": "Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dlIMcmlAdk": {
    "title": "Noise-free Score Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BO3aRwGzq0": {
    "title": "DINAR: Fine-Grained Privacy Preserving Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWS4iOkhXv": {
    "title": "Scalable Lipschitz Estimation for CNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z0ojN315Uf": {
    "title": "Differentially Private Principal Component Analysis for Vertically Partitioned Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VhQUwxIHER": {
    "title": "Small Variance, Big Fairness: A Path to Harmless Fairness without Demographics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pB1FeRSQxh": {
    "title": "Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sPUrdFGepF": {
    "title": "Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qe49ybvvPs": {
    "title": "Diverse Projection Ensembles for Distributional Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6r9t0HtqQ": {
    "title": "KEFI: Kernel-based Feature Identification for Generalizable Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESmvnmZ9fT": {
    "title": "SCoRF: Single-stage convolutional radiance fields for effective 3D scene representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVl1KO5K76": {
    "title": "Momentum-SAM: Sharpness Aware Minimization without Computational Overhead",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hy84B74XFt": {
    "title": "Towards Interpretable Controllability in Object-Centric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MK7TEe7SJ3": {
    "title": "USTAM: UNIFIED SPATIO-TEMPORAL ATTENTION MIXFORMER FOR VISUAL OBJECT TRACKING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nY9nITZQjc": {
    "title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iGHPVbttMs": {
    "title": "The Cyclical Chaos And Its Equilibrium",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vKViCoKGcB": {
    "title": "Intriguing Properties of Data Attribution on Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5jBLcVmhe": {
    "title": "SoftTreeMax: Exponential Variance Reduction in Policy Gradient via Tree Expansion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H6XiAoyugv": {
    "title": "Robust Backdoor Attack with Visible, Semantic, Sample-specific and Compatible Triggers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekz1hN5QNh": {
    "title": "Fully Hyperbolic Convolutional Neural Networks for Computer Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKGpe1792U": {
    "title": "RGLA: Reverse Gradient Leakage Attack using Inverted Cross-Entropy Loss Function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FdUloEgBSE": {
    "title": "Text-guided Diffusion Model for 3D Molecule Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PCXvcULwiI": {
    "title": "Benchmarking Structural Inference Methods for Interacting Dynamical Systems with Synthetic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tzh6xAJSll": {
    "title": "Scaling Laws for Associative Memories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMZc3KqjEb": {
    "title": "Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjXjkxhSdE": {
    "title": "Enhancing One-Shot Pruned Generative Pre-training Language Models through Sparse-Dense-Sparse Mechanism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1BuWv9poWz": {
    "title": "Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fVxIEHGnVT": {
    "title": "An interpretable error correction method for enhancing code-to-code translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgCz44ebFe": {
    "title": "May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYsLU5TEEo": {
    "title": "Counterfactual Image Generation for adversarially robust and interpretable Classifiers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLLZhbBSOU": {
    "title": "RLIF: Interactive Imitation Learning as Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K13qUXDsTS": {
    "title": "Bidirectional-Reachable Hierarchical RL with Mutually Responsive Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyCPspuM5L": {
    "title": "PowerGraph: A power grid benchmark dataset for graph neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DyBcEiIs5J": {
    "title": "Boosting Adverse Weather Crowd Counting via Multi-queue Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6jFjYmahxu": {
    "title": "DiffSound: Differentiable Modal Sound Simulation for Inverse Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBAWiEjogY": {
    "title": "ProteiNexus: Illuminating Protein Pathways through Structural Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXjz7p4qha": {
    "title": "Rotation Invariant Quantization for Model Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MVmT6uQ3cQ": {
    "title": "The Need for Speed: Pruning Transformers with One Recipe",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWS4MueyzC": {
    "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xI4yNlkaqh": {
    "title": "Towards 3D Molecule-Text Interpretation in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CtOA9aN8fr": {
    "title": "Effective pruning of web-scale datasets based on complexity of concept clusters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngp5jzx5oK": {
    "title": "Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLTgo1dcIl": {
    "title": "Stochastic Extragradient with Flip-Flop Shuffling & Anchoring: Provable Improvements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FsVxd9CIlb": {
    "title": "AttEXplore: Attribution for Explanation with model parameters eXploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PAfnMGXief": {
    "title": "BRUSLEATTACK: QUERY-EFFICIENT SCORE-BASED SPARSE ADVERSARIAL ATTACK",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N23A4ybMJr": {
    "title": "Win-Win: Training High-Resolution Vision Transformers from Two Windows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bDkisS75zy": {
    "title": "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fn655mJ4bv": {
    "title": "SOInter: A Novel Deep Energy-Based Interpretation Method for Explaining Structured Output Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gMLQwKDY3N": {
    "title": "A Private Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUM39YTRxH": {
    "title": "Text2Reward: Dense Reward Generation with Language Models for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OzAGE2W9yz": {
    "title": "Improving Neural Program Induction by Reflecting on Failures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m52uU0dVbH": {
    "title": "Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8y5vlBuRll": {
    "title": "Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VPx3Jw2MSk": {
    "title": "Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cLIvvqf3Wk": {
    "title": "Attribute-Guided Diffusion for Unsupervised Few-Shot Font Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3tM1l5tSbv": {
    "title": "Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6tNkN6ate": {
    "title": "Interpretable Diffusion via Information Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuIiLSktO4": {
    "title": "Algorithms for Caching and MTS with reduced number of predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=crF9dk4poo": {
    "title": "Interpretable Deep Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T218mLyMHg": {
    "title": "Spectrum-guided Multi-view Graph Fusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0uzEweZB1": {
    "title": "FrAug: Frequency Domain Augmentation for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CIGhcJYJH": {
    "title": "Two-timescale Extragradient for Finding Local Minimax Points",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UJeIujVxMn": {
    "title": "FedEBA+: Towards Fair and Effective Federated Learning via Entropy-based Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JxQyat11M": {
    "title": "Zero-Shot Visual Classification with Guided Cropping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILYjDvUM6U": {
    "title": "Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BUSZQWbRaR": {
    "title": "Generalized Convergence Analysis of Tsetlin Machines: A Probabilistic Approach to Concept Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KzMMv0OygD": {
    "title": "TeG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Task Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lK0WxHeups": {
    "title": "Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GQGNLEHmdl": {
    "title": "AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QcMdPYBwTu": {
    "title": "Scalable and Effective Implicit Graph Neural Networks on Large Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EMVct15bl5": {
    "title": "A qualitative theory of dynamical systems for assessing stability in ResNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qofh48zW3T": {
    "title": "Distributional Distance Classifiers for Goal-Conditioned Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oXYZJXDdo7": {
    "title": "Retrieval is Accurate Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d18RgYF6Y7": {
    "title": "Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NdcQQ82mfy": {
    "title": "Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qwxe8WKSgy": {
    "title": "BTBS-LNS: A Binarized-Tightening, Branch and Search Approach of Learning Large Neighborhood Search Policies for MIP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DkhYlWZq84": {
    "title": "Protein Captioning: Bridging the Gap between Protein Sequences and Natural Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3IyC5lQTSi": {
    "title": "Fairness Through Matching for better group fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c85tdYOOju": {
    "title": "Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V3j5d0GQgH": {
    "title": "FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uSX6IbpGZ9": {
    "title": "Trend/Seasonality based Causal Structure for Time Series Counterfactual Outcome Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk7yW3vmSq": {
    "title": "Conceptual Graph Counterfactuals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5HGPR6fg2S": {
    "title": "Normalized Space Alignment: A Versatile Metric for Representation Space Discrepancy Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gK7HIepdn7": {
    "title": "MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKf6JtXtoy": {
    "title": "MAP IT to Visualize Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=070DFUdNh7": {
    "title": "GraphGPT: Graph Learning with Generative Pre-trained Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjwZHuQ3cm": {
    "title": "LIMANS: Linear Model of the Adversarial Noise Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=spvaV5LELF": {
    "title": "Measuring Vision-Language STEM Skills of Neural Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkpVgxHQ1S": {
    "title": "Latent Diffusion Counterfactual Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PqjQmLNuJt": {
    "title": "DUAL DENOISING LOGICAL REASONING FOR INDUC\u0002TIVE KNOWLEDGE GRAPH COMPLETION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wvHfsiWXUR": {
    "title": "Tell, Don't Show: Internalized Reasoning influences how LLMs generalize",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PaOuEBMvTG": {
    "title": "Multiple Object Stitching for Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dXRWP4n15q": {
    "title": "$\\sigma$-zero: Gradient-based Optimization of \\\\$\\ell_0$-norm Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RIbNmx984": {
    "title": "On Double-Descent in Reinforcement Learning with LSTD and Random Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GhYXocT75t": {
    "title": "Forward-Backward Reasoning in Large Language Models for Mathematical Verification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N5ID99rsUq": {
    "title": "Stability and Generalization in Free Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRmmIdqvZp": {
    "title": "Barycentric Alignment of Mutually Disentangled Modalities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zW1tyw3UFu": {
    "title": "Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MLShfiJ3CB": {
    "title": "Towards Reliable Backdoor Attacks on Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrtGfTGaGe": {
    "title": "The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHAr7ZA1OL": {
    "title": "Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tAmfM1sORP": {
    "title": "Large Language Models can Learn Rules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XZGklkaOsL": {
    "title": "Unified Medical Image Pre-training in Language-Guided Common Semantic Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycF7mKfVGO": {
    "title": "Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PCTqol2hvy": {
    "title": "Characterizing ResNet's Universal Approximation Capability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3P87ptzvTm": {
    "title": "Optimal Multiple Transport with Applications to Visual Matching, Model Fusion and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNiI3aGcE6": {
    "title": "Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2fSyBPBfBs": {
    "title": "Bilevel Optimization without Lower-Level Strong Convexity from the Hyper-Objective Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwjDyJfe3m": {
    "title": "Benchmarks for Reinforcement Learning with Biased Offline Data and Imperfect Simulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjeqMvQpen": {
    "title": "Transformer Fusion with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TkdMRKvZDJ": {
    "title": "Phrase Grounding-based Style Transfer for Single-Domain Generalized Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nkKWY5JjtZ": {
    "title": "Exact Mean Square Linear Stability Analysis for SGD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMoifbuxjB": {
    "title": "Towards Meta-Pruning via Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=libLqoInAd": {
    "title": "Reliable Classifications with Guaranteed Confidence using the Dempster-Shafer Theory of Evidence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWvKBCYh4S": {
    "title": "MoLE: Mixture of LoRA Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tth2qXY7RU": {
    "title": "Super Floating-Point (SuFP): Efficient To All. Multi-Region Piecewise Quantization using Scalable Bias with Hardware Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I8pdQLfR77": {
    "title": "Improving MLP Module in Vision Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NJ6nyv3XWH": {
    "title": "Leveraging Graph Neural Networks to Boost Fine-Grained Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T11rD8k578": {
    "title": "Calibrated on Average, but not Within Each Slice: Few-shot Calibration for All Slices of a Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=adSGeugiuj": {
    "title": "On the Posterior Distribution in Denoising: Application to Uncertainty Quantification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jeNWwtIX71": {
    "title": "Provable Domain Generalization via Information Theory Guided Distribution Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T2ToleSDk6": {
    "title": "Learning Constraints from Offline Dataset via Inverse Dual Values Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BLGQ3oqldb": {
    "title": "LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eiC4BKypf1": {
    "title": "Turning large language models into cognitive models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vI95kcLAoU": {
    "title": "Skip-Attention: Improving Vision Transformers by Paying Less Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=evk6pPJqMy": {
    "title": "Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1qDRwhe379": {
    "title": "Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phBS6YpTzC": {
    "title": "Benchmarking and Improving Generator-Validator Consistency of Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u7559ZMvwY": {
    "title": "Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a6SntIisgg": {
    "title": "LogoRA: Local-Global Representation Alignment for Robust Time Series Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MLBdiWu4Fw": {
    "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbFBh3LMKi": {
    "title": "Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gg7cXo3S8l": {
    "title": "Dictionary Contrastive Forward Learning via Adaptive Label Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXBsK3GsL6": {
    "title": "Soft iEP: On the Exploration Inefficacy of Gradient Based Strong Lottery Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2XfOm3RJa": {
    "title": "How Large Language Models Implement Chain-of-Thought?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmM4Ecm4HJ": {
    "title": "Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3wGi5m2YHY": {
    "title": "FlowHash: Accelerating Audio Search with Balanced Hashing via Normalizing Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hnrB5YHoYu": {
    "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFJVpeYcnv": {
    "title": "Bandit Learning in Matching: Unknown Preferences On Both Sides",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PnR1MNen7u": {
    "title": "Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mNYF0IHbRy": {
    "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzAJbTClAz": {
    "title": "FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4qWt62BdN": {
    "title": "DSparsE: Dynamic Sparse Embedding for Knowledge Graph Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tveiUXU2aa": {
    "title": "SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUtTtiNksb": {
    "title": "FFCA-Net: Stereo Image Compression via Fast Cascade Alignment of Side Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EWTFMkTdkT": {
    "title": "Invariance-based Learning of Latent Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=znjaiy1Z9q": {
    "title": "AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F1TKzG8LJO": {
    "title": "Robotic Task Generalization via Hindsight Trajectory Sketches",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=90yw2uM6J5": {
    "title": "Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYv3OMboTF": {
    "title": "ExcelFormer: Making Neural Network Excel in Small Tabular Data Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rL7xsg1aRn": {
    "title": "Masked Structural Growth for 2x Faster Language Model Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rc7dAwVL3v": {
    "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kq5avXrkpY": {
    "title": "Federated Optimization Algorithms with Random Reshuffling and Gradient Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JiIKjcwrr": {
    "title": "Robust Self-supervised Learning in Heterogeneous Graph Based on Feature-Topology Balancing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZVXqiaI4U": {
    "title": "Attribute Based Interpretable Evaluation Metrics for Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KojubHBr8": {
    "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jw0qHTfdhv": {
    "title": "Learning to Generate Predictor for Long-Term Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4iQuByhNie": {
    "title": "ContextNER: Contextual Phrase Generation at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=258EqEA05w": {
    "title": "A Simple Data Augmentation for Feature Distribution Skewed Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Tl99XWXC": {
    "title": "Efficient Transfer Learning in Diffusion Models via Adversarial Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXv0zQ1umU": {
    "title": "Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6Ux5OCGmW": {
    "title": "RSAM: Learning on Manifolds with Riemannian Sharpness-Aware Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xwZhyKynCB": {
    "title": "${\\rm EFO}_k$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cfnevfQDsx": {
    "title": "Converging and Stabilizing Generative Adversarial Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yeKQXf08Db": {
    "title": "Dynamic Continuous Hyperparameter Tuning for Generalized Linear Contextual Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXWTXXtHNl": {
    "title": "Label-Noise Robust Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PJwAkg0z7h": {
    "title": "EasyTPP: Towards Open Benchmarking Temporal Point Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ec2rYpP42y": {
    "title": "Solving Inverse Problem With Unspecified Forward Operator Using Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1pSL2cXWoz": {
    "title": "ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hI18CDyadM": {
    "title": "Adaptive Window Pruning for Efficient Local Motion Deblurring",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mY9aGiMn0": {
    "title": "Sparser, Better, Deeper, Stronger: Improving Sparse Training with Exact Orthogonal Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W2d3LZbhhI": {
    "title": "A unified sampling framework for solver searching of Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpTitXWGNO": {
    "title": "xCodeEval: An Execution based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sZACTXpSc6": {
    "title": "ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uw8xvFqVAE": {
    "title": "A representation-learning game for classes of prediction tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZIbUx5dzfZ": {
    "title": "ORBIS: Open Dataset Can Rescue You From Dataset Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NdbUfhttc1": {
    "title": "Learning to Optimize for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pGL4P2kg6V": {
    "title": "Towards Interpretable Continual Learning Through Controlling Concepts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1MRfyGLCcU": {
    "title": "Graph Representation Learning enhanced Semi-supervised Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdoD5OYHPJ": {
    "title": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8jH6rjw8c": {
    "title": "Fairness Improves Learning from Noisily Labeled Long-Tailed Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDuqo9KTzh": {
    "title": "Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxe0hQ5mxp": {
    "title": "Elephant Neural Networks: Born to Be a Continual Learner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ChNy95ovpF": {
    "title": "DebateGPT: Fine-tuning Large Language Models with Multi-agent Debate Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RFjhxXrTlX": {
    "title": "Learning Unorthogonalized Matrices for Rotation Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vULHgaoASR": {
    "title": "MissDiff: Training Diffusion Models on Tabular Data with Missing Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bYQkOPvgDw": {
    "title": "Probabilistic Graphical Model for Robust Graph Neural Networks against Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=loYSzjSaAK": {
    "title": "Submodular Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7GkdjhupsV": {
    "title": "InfoAug: Mutual Information Informed Augmentation for Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hyjfjf8GA0": {
    "title": "A Case Study for the Behaviors of Generalists and Specialists in Competitive Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wwotGBxtC3": {
    "title": "Data-Efficient Molecular Generation with Hierarchical Textual Inversion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F6QqOdLzsx": {
    "title": "Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ouNI9pkf9g": {
    "title": "Quantifying Anonymity in Score-Based Generators with Adversarial Fingerprinting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kPrxk6tUcg": {
    "title": "Neuron-Enhanced AutoEncoder Matrix Completion: Theory and Practice",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIv15j44gR": {
    "title": "Estimating Heterogeneous Treatment Effect with Delayed Response",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=viJlKbTfbb": {
    "title": "What If You Were Not There? Learning Causally-Aware Representations of Multi-Agent Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OUeIBFhyem": {
    "title": "$\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1eynu9DVf": {
    "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l3s3HwJYDm": {
    "title": "Opponent Modeling based on Sub-Goal Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bpkhu2ExxU": {
    "title": "Stochastic Modified Equations and Dynamics of Dropout Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJLD9MquPH": {
    "title": "Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hDKLrK8AwP": {
    "title": "Towards Readable Scalable Vector Graphic Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uLCtVTzFhg": {
    "title": "Contrastive Positive Unlabeled Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7OwML7fwl8": {
    "title": "Fairness without Sensitive attributes via Noise and Uncertain Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=npoi2fr882": {
    "title": "3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jd5GokdySz": {
    "title": "Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWNJFD1l8M": {
    "title": "Transferring Learning Trajectories of Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S7T0slMrTD": {
    "title": "Resolving Knowledge Conflicts in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2EN8MCHiz": {
    "title": "Understanding Vision and Language Representations under the Lens of Intrinsic Dimension",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lCLdLlXAvt": {
    "title": "Average Sensitivity of Hierarchical Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RNgZTA4CTP": {
    "title": "Best Possible Q-Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IxpTsFS7mh": {
    "title": "VQ-TR: Vector Quantized Attention for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BuFNoKBiMs": {
    "title": "Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1CK45cqkEh": {
    "title": "Unsupervised Order Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vEgLnT9avP": {
    "title": "ResolvNet: A Graph Convolutional Network with multi-scale Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mrBd4hyWlP": {
    "title": "CRL-NET: ACCELERATED MAGNETIC RESONANCE IMAGING RECONSTRUCTION THROUGH COIL REPRESENTATION LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anzIzGZuLi": {
    "title": "Making Pre-trained Language Models Great on Tabular Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YRJDZYGmAZ": {
    "title": "Domain Prompt Matters a Lot in Multi-Source Few-Shot Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ur25Xxvzsg": {
    "title": "Transferable Deep Clustering Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F4bmOrmUwc": {
    "title": "Fixed Non-negative Orthogonal Classifier: Inducing Zero-mean Neural Collapse with Feature Dimension Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rn3qJGOitY": {
    "title": "Linear Attention via Orthogonal Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6r0BOIb771": {
    "title": "Sequential Bayesian Continual Learning with Meta-Learned Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qgLyKwXVDs": {
    "title": "FreeLM: Fine-Tuning-Free Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ug8wDSimNK": {
    "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKDwsJmrDJ": {
    "title": "Collaborating Heterogeneous Natural Language Processing Tasks via Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJiOQw1fkF": {
    "title": "Enhanced Model-agnostic Training of Deep Tabular Generation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j8hdRqOUhN": {
    "title": "Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHVIxJGwr4": {
    "title": "Learning to Branch with Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pk0iUCNVPa": {
    "title": "Polynomial-based Self-Attention for Table Representation learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EFGwiZ2pAW": {
    "title": "SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tG5mpAM7ZK": {
    "title": "Extending to New Domains without Visual and Textual Oracles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kg0IDQF3wp": {
    "title": "LegoMT2: Non-Blocking Federated Learning for Massive Multilingual Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7x0XVlCpX": {
    "title": "MaGIC: Multi-modality Guided Image Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kz3yckpCN5": {
    "title": "The False Promise of Imitating Proprietary Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=psEswR8Jz4": {
    "title": "AmortizedPeriod: Attention-based Amortized Inference for Periodicity Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikwEDva1JZ": {
    "title": "How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycv2z8TYur": {
    "title": "EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NeKjkOWvwd": {
    "title": "Rethinking the OoD Generalization for Deep Neural Network: A Frequency Domain Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eMHn77ZKOp": {
    "title": "Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tr3fZocrI6": {
    "title": "Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkkHqoerLV": {
    "title": "Alice Benchmarks: Connecting Real World Object Re-Identification with the Synthetic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6c4gv0E9sF": {
    "title": "SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6imHU4Ebu": {
    "title": "Large Language Models as Generalizable Policies for Embodied Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jg8y1buQ3r": {
    "title": "LLM-driven Hateful Meme Detection via Cross-modal Memorizing and Self-rejection Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9pKtcJcMP3": {
    "title": "Video Language Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JuMFjSkpD": {
    "title": "Fair Attribute Classification via Distance Covariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mao3y822aM": {
    "title": "NanoLM: An Affordable LLM Study Benchmark via Accurate Loss Prediction Across Scales",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzxKLZNbrQ": {
    "title": "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ffTRtzXkIW": {
    "title": "PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QmZKc7UZCy": {
    "title": "LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rpH9FcCEV6": {
    "title": "An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PyHRUMxKbT": {
    "title": "InfoNet: An Efficient Feed-Forward Neural Estimator for Mutual Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tlsdsb6l9n": {
    "title": "Mol-Instructions - A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtT6o5tfHu": {
    "title": "Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iP31fDtrPR": {
    "title": "Learning Directed Graphical Models with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gCjeBKuDlc": {
    "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gTRtDFm8Di": {
    "title": "Reduce, Reuse, and Recycle: Navigating Test-Time Adaptation with OOD-Contaminated Streams",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByXWN19vWP": {
    "title": "Confident Sinkhorn Allocation for Pseudo-Labeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o83eu4H9Mb": {
    "title": "Information Retention via Learning Supplemental Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3NmO9lY4Jn": {
    "title": "Don't Play Favorites: Minority Guidance for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E60SIDItyT": {
    "title": "Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BMw4Cm0gGO": {
    "title": "C-MCTS: Safe Planning with Monte Carlo Tree Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1RqSVIf3c": {
    "title": "Bayesian Preference Elicitation for Personalized Prefactual Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U17KoLrXE8": {
    "title": "ObjectNet Captions: Models are not superhuman captioners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTwwtlEfTF": {
    "title": "Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n6mLhaBahJ": {
    "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zix86UbMGh": {
    "title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imuVEKaU3b": {
    "title": "Noise-guided Unsupervised Outlier Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n7Sr8SW4bn": {
    "title": "Mayfly: a Neural Data Structure for Graph Stream Summarization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhMEkBSdIV": {
    "title": "LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with Class Taxonomies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IrdbUQ1zTw": {
    "title": "Reinforcement Learning-based Layer-wise Aggregation for Personalized Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u6jbcaCHqO": {
    "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wAsjsSe0U6": {
    "title": "Visual Semantic Learning via Early Stopping in Inverse Scale Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1SktNMHA7": {
    "title": "Rethinking Label Smoothing as a Tool for Embedding Perturbation Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11WAKGH8uv": {
    "title": "FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=776lhoaulC": {
    "title": "Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HqmpIud9Uq": {
    "title": "Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbWtOYIzIK": {
    "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HFtrXBfNru": {
    "title": "Temporal Generalization Estimation in Evolving Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yN4Wv17ss3": {
    "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUVvUf9SZB": {
    "title": "On Learning with a Concurrent Verifier: Convexity, Improving Bounds, and Complex Requirements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZgECfyHXF": {
    "title": "On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c5pwL0Soay": {
    "title": "METRA: Scalable Unsupervised RL with Metric-Aware Abstraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mXpNp8MMr5": {
    "title": "On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RTI6MLwWbs": {
    "title": "Physics-infused Intention Network for Crowd Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6N8TW504aa": {
    "title": "Graphical Multioutput Gaussian Process with Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQYpgReSRk": {
    "title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CRkvR8TJkk": {
    "title": "A Game-theoretic Approach to Personalized Federated Learning Based on Target Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lzpHNyhIbr": {
    "title": "CropNet: An Open Large-Scale Dataset with Multiple Modalities for Climate Change-aware Crop Yield Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AB077M4TY": {
    "title": "Dynamic Training Guided by Training Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsnWEcuymH": {
    "title": "Accelerating Simulation-Based Influence Maximization via Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pAsQSWlDUf": {
    "title": "Soft Contrastive Learning for Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qA4foxO5Gf": {
    "title": "Efficient Integrators for Diffusion Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSDszJ2uIV": {
    "title": "Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYVQHR5IAq": {
    "title": "Enhancing Personal Decentralized Federated Learning through Model Decoupling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0pMPCmEfN": {
    "title": "Splitted Wavelet Differential Inclusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H6pf70GZVU": {
    "title": "YoooP: You Only Optimize One Prototype per Class for Non-Exemplar Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V163iNHVi7": {
    "title": "Feynman-Kac Operator Expectation Estimator: An Innovative Method for Enhancing MCMC Efficiency and Reducing Variance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWAhEjXjeG": {
    "title": "Provably Robust Conformal Prediction with Improved Efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xlayxj2fWp": {
    "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=41CYtxM2jQ": {
    "title": "Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=giU9fYGTND": {
    "title": "FedImpro: Measuring and Improving Client Update in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=quf7D5agqa": {
    "title": "Deep Reinforcement Learning from Weak Hierarchical Preference Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8mGHjwFZV": {
    "title": "How to Guess a Gradient",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p4eG8rCa0b": {
    "title": "Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W9zRGhLq7": {
    "title": "A New Theoretical Perspective on Data Heterogeneity in Federated Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EODzbQ2Gy4": {
    "title": "Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMNkj4ER7V": {
    "title": "A Unified Framework for Bayesian Optimization under Contextual Uncertainty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DS5qRs0tQz": {
    "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7zY781bMDO": {
    "title": "Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbv0sBIZh9": {
    "title": "Diffusion Models for Multi-Task Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oKn9c6ytLx": {
    "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HAJzPND6Xt": {
    "title": "Efficient Realistic Avatar Generation via Model Compression and Enhanced Rendering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZG3RaNIsO8": {
    "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yR5QbFv4Xb": {
    "title": "Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9t7MqZtCR": {
    "title": "Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdwVOREDZM": {
    "title": "WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFf9m4v7oC": {
    "title": "Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E1NxN5QMOE": {
    "title": "Enhancing Group Fairness in Online Settings Using Oblique Decision Forests",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CdjnzWsQax": {
    "title": "Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnsPXGPcgI": {
    "title": "MetaDist: An Infrastructure for Automatic Parallelism via ShardCombine Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Lp6qU9hzV": {
    "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXc5aXeoc8": {
    "title": "Diffusion Sampling with Momentum for Mitigating Divergence Artifacts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKIwtXHg4D": {
    "title": "ProGO: Probabilistic Global Optimizer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KSvRZFCy7s": {
    "title": "Differentially Private Low-dimensional Synthetic Data from High-dimensional Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKEuFKfCKA": {
    "title": "A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJ7XuW5JaH": {
    "title": "Posterior Probability-Based Label Recovery Attack in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVsXDLIt45": {
    "title": "Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgwgrxBYOI": {
    "title": "Deep PDE Solvers for Subgrid Modelling and Out-of-Distribution Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YHUGlwTzFB": {
    "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxfKIYfHyx": {
    "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjGJFkU3xL": {
    "title": "Doubly Robust Proximal Causal Learning for Continuous Treatments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MREQ0k6qvD": {
    "title": "One-hot Generalized Linear Model for Switching Brain State Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dUTwqiEked": {
    "title": "RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WA2mZrDTAH": {
    "title": "ZegOT: Zero-shot Segmentation Through Optimal Transport of Pixels to Text Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8htNAnMSyP": {
    "title": "Neural Auto-designer for Enhanced Quantum Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dr4qD9bzZd": {
    "title": "Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sIcPMMhl9W": {
    "title": "The Phase Transition Phenomenon of Shuffled Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T8Rf1CRbHQ": {
    "title": "Error-Feedback Meets Stochastic Approximation with Two Time Scales",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g8sGBSQjYk": {
    "title": "On the Parameterization of Second-Order Optimization Effective towards the Infinite Width",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXMTK2eltf": {
    "title": "GPT-Driver: Learning to Drive with GPT",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aG3EARrrd1": {
    "title": "OSRT: An Online Sparse Approximation Model for Scattered Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNJEyP4Jv2": {
    "title": "Understanding and Improving Adversarial Attacks on Latent Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C9uv8qR7RX": {
    "title": "SiT: Symmetry-invariant Transformers for Generalisation in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKkFJgAdlW": {
    "title": "Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C53xlgEqVh": {
    "title": "Vec-Tok Speech: Speech Vectorization and Tokenization for Neural Speech Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cnn60wwTe1": {
    "title": "Which mode is better for federated learning? Centralized or Decentralized",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkYCuGM7E2": {
    "title": "Large Language Models as Decision Makers for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GEcwtMk1uA": {
    "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DesYwmUG00": {
    "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYhglxSj8j": {
    "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YM0aPHTDe8": {
    "title": "Finite-Time Analysis of Federated Temporal Difference Learning with Linear Function Approximation under Environment and Computation Heterogeneity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTf4DGAzus": {
    "title": "$\\textbf{\\textit{M}}^\\textbf{\\textit{3}}$: Towards Robust Multi-Modal Reasoning via Model Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4WWqhD9Mw": {
    "title": "Holmex: Human-Guided Spurious Correlation Detection and Black-box Model Fixing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQscjhKXIF": {
    "title": "Class-Incremental Continual Learning for Multi-View Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FZfWQFrdBT": {
    "title": "Split and Merge Proxy: pre-training protein inter-chain contact prediction by mining rich information from monomer data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZAlb8fX5y": {
    "title": "KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Z8rZlKpNT": {
    "title": "Normalizing Flows For Out of Distribution Detection via Latent Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ALVwQjZRS8": {
    "title": "Coeditor: Leveraging Repo-level Diffs for Code Auto-editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rsY6J3ZaTF": {
    "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCDT918ZkI": {
    "title": "Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJjHQS2DyE": {
    "title": "Conditional Support Alignment for Domain Adaptation with Label Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cB9bAFGFAA": {
    "title": "FedSRC: Federated Learning with Self-Regulating Clients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qAoxvePSlq": {
    "title": "DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3qo1pJHabg": {
    "title": "LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3bmjHYX42n": {
    "title": "Leveraging Human Revisions for Improving Text-to-Layout Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ll8PmgD0IB": {
    "title": "Divide and Orthogonalize: Efficient Continual Learning with Local Model Space Projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWcnvZ3qMb": {
    "title": "FITS: Modeling Time Series with $10k$ Parameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S6Dn3uyM2p": {
    "title": "Differentially Private One Permutation Hashing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXeSwrVgjN": {
    "title": "Benchmarking Deletion Metrics with the Principled Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x3LxHdZX0f": {
    "title": "PUMA: Secure Inference of LLaMA-7B in Five Minutes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=URCfZ2NgaR": {
    "title": "Structural Knowledge Informed Continual Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0EVNrBQh6": {
    "title": "Investigating Human-Identifiable Features Hidden in Adversarial Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o0oroLuPLZ": {
    "title": "Sp-R-IP: A Decision-Focused Learning Strategy for Linear Programs that Avoids Overfitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ooThrz2NvC": {
    "title": "CICD-Coder: Chinese EMRs Based ICD Coding With Multi-axial Supported Clinical Evidence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcWwhF8cTZ": {
    "title": "Gradient norm as a powerful proxy to out-of-distribution error estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bpheRCxzb4": {
    "title": "Measuring Information in Text Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ys3uPmZGOR": {
    "title": "Enhancing Neural Network Performance with Leader-Follower Architecture and Local Error Signals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3QLkwU40EE": {
    "title": "SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9NKRfhKgzI": {
    "title": "Adversarially Robust and Privacy-Preserving Representation Learning via Information Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XjlZJJFyla": {
    "title": "Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6tDPefQyvB": {
    "title": "Rotation-Equivariance and Position Encodings for Enhancing Local Descriptors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=09iOdaeOzp": {
    "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JnaN0Crlz": {
    "title": "Enhancing Adversarial Robustness on Categorical Data via Attribution Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KszBlT26pl": {
    "title": "FP-IRL: Fokker-Planck-based Inverse Reinforcement Learning --- A Physics-Constrained Approach to Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1BmveEMNbG": {
    "title": "Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbKcFZ6Dbp": {
    "title": "The Dark Side of the Hyperbolic Moon",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6vtGG0WMne": {
    "title": "Regulating Imbalanced Deep Models with User-Specified Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g1S72T3FGc": {
    "title": "Neural Active Learning Beyond Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ly10tMV6cD": {
    "title": "Structure-Rich Text Benchmark for Knowledge Inference Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9UGAUQjibp": {
    "title": "Quantized Local Independence Discovery for Fine-Grained Causal Dynamics Learning in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qwYKE3VB2h": {
    "title": "From Graphs to Hypergraphs: Hypergraph Projection and its Remediation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N8N0hgNDRt": {
    "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=COO51g41Q4": {
    "title": "Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UoBymIwPJR": {
    "title": "Query-Policy Misalignment in Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Klj7QG0NO": {
    "title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0SOhDO7xI0": {
    "title": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pVKEFtGkM6": {
    "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rFCGiFTVyY": {
    "title": "FedSKU: Defending Backdoors in Federated Learning Through Selective Knowledge Unlearning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v5PBcUSP2o": {
    "title": "Score-based Neural Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TS8HoIWAPQ": {
    "title": "Feature-aligned N-BEATS with Sinkhorn divergence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70PPJo3DwI": {
    "title": "Towards Out-of-federation Generalization in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wriKDQqiOQ": {
    "title": "On the Effect of Batch Size in Byzantine-Robust Distributed Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LebzzClHYw": {
    "title": "Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6okaSfANzh": {
    "title": "Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uQBW7ELXfO": {
    "title": "Unpaired Image-to-Image Translation via Neural Schrödinger Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jesfGs3fIc": {
    "title": "Set-based Neural Network Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0S0CgZEYxR": {
    "title": "Examining the Achilles' Heel of CLIP Models: The Worst-Performing Categories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zI6mMl7UmW": {
    "title": "Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4XNuQ5qft": {
    "title": "Optimum Shifting to Stabilize Training and Improve Generalization of Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q4FmJPQwuJ": {
    "title": "CrossTVR: Multi-Grained Re-Ranker for Text Video Retrieval with Frozen Image Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0aR1s9YxoL": {
    "title": "Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fW7DOHDQvF": {
    "title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=farT6XXntP": {
    "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M0MF4t3hE9": {
    "title": "Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTXaJmWXKu": {
    "title": "Continual Nonlinear ICA-Based Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r5sikTJ94y": {
    "title": "Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qbf1hy8b7m": {
    "title": "Scaling Supervised Local Learning with Augmented Auxiliary Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9DXXMXnIGm": {
    "title": "Elucidating the design space of classifier-guided diffusion generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qo21ZlfNu6": {
    "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SnlDQ5pL6L": {
    "title": "Spatial-Temporal Mutual Distillation for Lightweight Sleep Stage Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IL71c1z7et": {
    "title": "Fleet Policy Learning via Weight Merging and An Application to Robotic Tool-Use",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TAtAYUf1aq": {
    "title": "Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLgbjzKJkk": {
    "title": "CO-MOT: Boosting End-to-end Transformer-based Multi-Object Tracking via Coopetition Label Assignment and Shadow Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g6eCbercEc": {
    "title": "InfoCon: Concept Discovery with Generative and Discriminative Informativeness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eBTtShIjxu": {
    "title": "Prompt Tuning Is All We Need?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JCseZixaI7": {
    "title": "Meta Koopman Decomposition for Time Series Forecasting Under Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hbsvyhznr4": {
    "title": "AutoJoin: Efficient Adversarial Training against Gradient-Free Perturbations for Ro- bust Maneuvering via Denoising Autoencoder and Joint Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fq1wNrC2ai": {
    "title": "Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLO9VaAb3B": {
    "title": "Alphazero-like Tree-Search can guide large language model decoding and training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z3dfuRcGAK": {
    "title": "Revisit and Outstrip Entity Alignment: A Perspective of Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLR7ElwD51": {
    "title": "Learning Scalable Causal Discovery Policies with Adversarial Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oFNpRlPxyQ": {
    "title": "MSPipe: Minimal Staleness Pipeline for Efficient Temporal GNN Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6vF0ZJGor4": {
    "title": "ImplicitSLIM and How it Improves Embedding-based Collaborative Filtering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKlxDx2SoS": {
    "title": "Prompt Learning with Quaternion Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xThb6APBoG": {
    "title": "Adapting Retrieval Models to Task-Specific Goals using Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7etoNfU9uF": {
    "title": "SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PIl69UIAWL": {
    "title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A4YlfnbaSD": {
    "title": "Overcoming the Stability Gap in Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fWk5Qx0exc": {
    "title": "Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KmvYOALQnm": {
    "title": "Improving Sample Efficiency in Off-policy RL with Low-dimensional Policy Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MZzlUyU2ih": {
    "title": "DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xm8okHEC3G": {
    "title": "Boosting Dataset Distillation with the Assistance of Crucial Samples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=esh9JYzmTq": {
    "title": "Assessing the Impact of Distribution Shift on Reinforcement Learning Performance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDU6p4Fydz": {
    "title": "Enhancing Parameter Efficiency in Summarization via Expertise Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3Cu426njo": {
    "title": "Meta-Learning Priors Using Unrolled Proximal Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQDFsuG9HP": {
    "title": "Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZttUMTiPL": {
    "title": "Uncertainty Quantification via Stable Distribution Propagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jDy2Djjrge": {
    "title": "LAURAGPT: LISTEN, ATTEND, UNDERSTAND, AND REGENERATE AUDIO WITH GPT",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VBsoluxR2": {
    "title": "MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vMNpv5OBGb": {
    "title": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hAYHmV1gM8": {
    "title": "FedWon: Triumphing Multi-domain Federated Learning Without Normalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TpD2aG1h0D": {
    "title": "Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VVGO0kuuY": {
    "title": "Learning Causal Dynamics Models in Object-Oriented Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftKqt3Di3H": {
    "title": "Text-Free Federated Transformers Knowledge Distillation Without GAN",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVb0g26xyH": {
    "title": "Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tzpXhoNel1": {
    "title": "GRepsNet: A Simple Equivariant Network for Arbitrary Matrix Groups",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0bdvNsgcF": {
    "title": "A-Loc: Efficient Alternating Iterative Methods for Locating the $k$ Largest/Smallest Elements in a Factorized Tensor",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aaYBsuGRne": {
    "title": "Understanding In-context Learning with a Pelican Soup Hypothesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yC2waD70Vj": {
    "title": "Inverse Approximation Theory for Nonlinear Recurrent Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RRayv1ZPN3": {
    "title": "TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=McfYbKnpT8": {
    "title": "L2P-MIP: Learning to Presolve for Mixed Integer Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SiUhAbb3LH": {
    "title": "Continual Learning Knowledge Graph Embeddings for Dynamic Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EPNEazJoAg": {
    "title": "Exploring the cloud of feature interaction scores in a Rashomon set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSZ2GmGvUV": {
    "title": "EigenGuard: Backdoor Defense in Eigenspace",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QcgvtqxRhI": {
    "title": "BOSS: Diversity-Difficulty Balanced One-Shot Subset Selection for Data-Efficient Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fPEWP3we45": {
    "title": "AED: Adaptable Error Detection for Few-shot Imitation Policy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvDQtdMnOl": {
    "title": "Long-Short-Range Message-Passing: A Fragmentation-Based Framework to Capture Non-Local Atomistic Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZaEdLM4Gn": {
    "title": "TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtCROgWC2o": {
    "title": "Hierarchical Approach to Explaining Poisoned AI Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9QVqYBvCD8": {
    "title": "Asking Before Acting: Gather Information in Embodied Decision-Making with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SA19ijj44B": {
    "title": "A Study of Bayesian Neural Network Surrogates for Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=plebgsdiiV": {
    "title": "Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgDICX1h50": {
    "title": "Large Language Models as Analogical Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aGH43rjoe4": {
    "title": "Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ceATjGPTUD": {
    "title": "Large Language Models are Efficient Learners of Noise-Robust Speech Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H56qXKaNpF": {
    "title": "POSITION EMBEDDING INTERPOLATION IS ALL YOU NEED FOR EFFICIENT IMAGE-TO-IMAGE VIT",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5H8WGLQm5": {
    "title": "UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qvoe4wXWFi": {
    "title": "NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TMYxJIcdgS": {
    "title": "What Makes ImageNet Look Unlike LAION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExZ5gonvhs": {
    "title": "GPS-SSL: Guided Positive Sampling to Inject Prior into Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h9TTpQdGKJ": {
    "title": "Learning Transferable Robust Representations for Few-shot Learning via Multi-view Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6HwamHLDa6": {
    "title": "A Multi-In-Single-Out Network for Video Frame Interpolation without optical flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ArYyAmDGQ": {
    "title": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uFHDOSfuGS": {
    "title": "An Entropic Risk Measure for Robust Counterfactual Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eWLOoaShEH": {
    "title": "Learning to Model the World with Language",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eT6oLkm1cm": {
    "title": "Annealing Self-Distillation Rectification Improves Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLOxzCa6DS": {
    "title": "Differentially Private Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7iuFxx9Ccx": {
    "title": "Resource Efficient Test-Time Training with Slimmable Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VGLU5N1AD2": {
    "title": "Incentivized Black-Box Model Sharing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0gDQgwjoX0": {
    "title": "Stochastic Gradient Discrete Langevin Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBmMgvaEtO": {
    "title": "Stochastic Adaptive Sequential Black-box Optimization for Diffusion Targeted Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIcYTbpO38": {
    "title": "Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ASppt1L3hx": {
    "title": "Cooperative Minibatching in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Unb5CVPtae": {
    "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DmD1wboID9": {
    "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RMgqvQGTwH": {
    "title": "Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBP36xQhZl": {
    "title": "Forward Gradient Training of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PLoWVP7Mjc": {
    "title": "Embarrassingly Simple Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P1ANzoGg3W": {
    "title": "H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i5JfdnCob7": {
    "title": "Optimal Kernel Choice for Score Function-based Causal Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHF35XVjmm": {
    "title": "MADiff: Offline Multi-agent Learning with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0u9uvPdRgV": {
    "title": "Semi-supervised Diffusion Solver for Travelling Salesman Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhXymWfdcN": {
    "title": "Domain Generalization Deep Graph Transformation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MIuimtOu0T": {
    "title": "Towards Fair Knowledge Distillation using Student Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxLMnvnZv0": {
    "title": "CoMNet: Where Biology Meets ConvNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RemfXx7ebP": {
    "title": "Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bUgni8nH8Z": {
    "title": "Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sGVmr7KHfn": {
    "title": "Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikRVG8awyS": {
    "title": "RFold: RNA Secondary Structure Prediction with Decoupled Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qV83K9d5WB": {
    "title": "Large Language Models as Tool Makers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sx038qxjek": {
    "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ItPYVON0mI": {
    "title": "ACHIEVING DYNAMIC ACCURACY IN MACHINE-LEARNED CG POTENTIALS BY MODULATING POTENTIAL ENERGY LANDSCAPE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOtZlKkvdz": {
    "title": "Are Large Language Models Post Hoc Explainers?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JDud6zbpFv": {
    "title": "Sample-Efficient Quality-Diversity by Cooperative Coevolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPONXmVmZ6": {
    "title": "Multi-Agent Interpolated Policy Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3z60EWfh1p": {
    "title": "Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWF4KWeNgb": {
    "title": "Hierarchical Gaussian Mixture Normalizing Flows Modeling for Multi-Class Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yuGqe96t9O": {
    "title": "Stoichiometry Representation Learning with Polymorphic Crystal Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qL9gogRepu": {
    "title": "Zero and Few-shot Semantic Parsing with Ambiguous Inputs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIEW6M9YoV": {
    "title": "Graph Generation with $K^2$-trees",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VtpANKeHeJ": {
    "title": "Strategic Classification with Unforeseeable Outcomes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yldBrD4nYB": {
    "title": "CI-VAE: a Generative Deep Learning Model for Class-Specific Data Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7jWiBAWG0b": {
    "title": "Learning Guarantees for Non-convex Pairwise SGD with Heavy Tails",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnYsWoe1ST": {
    "title": "Self-Supervision is Not All You Need: In Defense of Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2DldCIjAdX": {
    "title": "LayerNAS: Neural Architecture Search in Polynomial Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o3BxOLoxm1": {
    "title": "Manifold Preserving Guided Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sNFLN3itAd": {
    "title": "Neural Common Neighbor with Completion for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZu4gUGN13": {
    "title": "Latent Intuitive Physics: Learning to Transfer Hidden Physics from a 3D Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x4OPJ7lHVU": {
    "title": "Privacy-Preserving In-Context Learning for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=24CZaossxH": {
    "title": "PyTorch Geometric High Order: A Unified Library for High Order Graph Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itJj6p7ssr": {
    "title": "Hardware-Friendly Post-Training Quantization: Input- and Output-Channelwise Scale and Offset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUpC8KTvdV": {
    "title": "Masked Distillation Advances Self-Supervised Transformer Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WipsLtH77t": {
    "title": "Adaptive Self-training Framework for Fine-grained Scene Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WvVyG8qBCt": {
    "title": "DPFormer: Learning Differentially Private Transformer on Long-Tailed Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FAH0SgQzO": {
    "title": "FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKAUvMePUN": {
    "title": "Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVEoydFOl9": {
    "title": "Towards Foundation Models for Knowledge Graph Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MiRPBbQNHv": {
    "title": "COCO-Periph: Bridging the Gap Between Human and Machine Perception in the Periphery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9QV7Q9gKl9": {
    "title": "DIFUSCO-LNS: Diffusion-Guided Large Neighbourhood Search for Integer Linear Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VGEeER6W9": {
    "title": "Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OdpIjS0vkO": {
    "title": "More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJFt8ZRQ9a": {
    "title": "IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJbsmB8UMx": {
    "title": "SALMON: Self-Alignment with Principle-Following Reward Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSWGqY2qNJ": {
    "title": "Indeterminate Probability Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B6t5wy6g5a": {
    "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2sCcTMWPc2": {
    "title": "TimelyGPT: Recurrent Convolutional Transformer for Long Time-series Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wKNKnXjCfT": {
    "title": "Efficient Identification of Direct Causal Parents via Invariance and Minimum Error Testing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5a79AqFr0c": {
    "title": "ControlVideo: Training-free Controllable Text-to-video Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tfp4FxWCC8": {
    "title": "Topo-Diffusion: Topological Diffusion Model for Image and Point Cloud Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=23b9KSNQTX": {
    "title": "RETSim: Resilient and Efficient Text Similarity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mpqMVWgqjn": {
    "title": "KW-Design: Pushing the Limit of Protein Deign via Knowledge Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLJg4wgBPu": {
    "title": "GPT Is Becoming a Turing Machine: Here Are Some Ways to Program It",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruk0nyQPec": {
    "title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmqTuFMk68": {
    "title": "Trainable Transformer in Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i8bdPSmOwk": {
    "title": "Momentum-driven Noise-free Guided Conditional Sampling for Denoising Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0r0GphlIL": {
    "title": "Improving Convergence and Generalization Using Parameter Symmetries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gywQnORzJX": {
    "title": "NPEFF: Non-Negative Per-Example Fisher Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KFjCFxiGk4": {
    "title": "Certified Deductive Reasoning with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2uhY4pXrb": {
    "title": "ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6fFd8QaPVx": {
    "title": "OneBNet: Binarized Neural Networks using Decomposed 1-D Binarized Convolutions on Edge Device",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a1AMdN8pSD": {
    "title": "Neural implicit mapping via nested neighborhoods: real-time rendering of neural SDFs with textures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhMrGCMIRL": {
    "title": "Fusing Models with Complementary Expertise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=85Af6AcMo5": {
    "title": "SciRE-Solver: Accelerating Diffusion Models Sampling by Score-integrand Solver with Recursive Difference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y3dqBDnPay": {
    "title": "HyperRep: Hypergraph-Based Self-Supervised Multimodal Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fJNnerz6iH": {
    "title": "Magnitude Invariant Parametrizations Improve Hypernetwork Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=he4CPgU44D": {
    "title": "Active Continual Learning: On Balancing Knowledge Retention and Learnability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VAmVEghgoC": {
    "title": "Detecting Out-of-distribution with Insights from Neural Collapse",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FhQSGhBlqv": {
    "title": "A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g7ohDlTITL": {
    "title": "Flow Matching on General Geometries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KuPixIqPiq": {
    "title": "Teaching Large Language Models to Self-Debug",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pj52xO5ysY": {
    "title": "Interpretable word-level context-based sentiment analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KF3Q79t8B": {
    "title": "Learning An Efficient-And-Rigorous Neural Multigrid Solver",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y3BuJotSKl": {
    "title": "Adversarial Defense using Targeted Manifold Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O0FOVYV4yo": {
    "title": "A LOCAL POLYAK-ŁOJASIEWICZ AND DESCENT LEMMA OF GRADIENT DESCENT FOR OVERPARAMETERIZED LINEAR MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A2KKgcYYDB": {
    "title": "Global Convergence Rate of Deep Equilibrium Models with General Activations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xU7VWHIo1i": {
    "title": "Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RjYKTQ0L0W": {
    "title": "Achieving Human Parity in Content-Grounded Datasets Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xuxYaBMd9F": {
    "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YuYxoaL7YX": {
    "title": "Learning an Inventory Control Policy with General Inventory Arrival Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xkXdE81mOK": {
    "title": "Federated Recommendation with Additive Personalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ia9fKO1Vjq": {
    "title": "Identifiable Latent Polynomial Causal Models through the Lens of Change",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nKvGCUoiuW": {
    "title": "MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wNere1lelo": {
    "title": "Certifying LLM Safety against Adversarial Prompting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kIP0duasBb": {
    "title": "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iVaxLjVRQN": {
    "title": "Balancing Fairness and Accuracy in Data-Restricted Binary Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHZVrhQuO1": {
    "title": "Linking Finite-Time Lyapunov Exponents to RNN Gradient Subspaces and Input Sensitivity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LxruQOI93v": {
    "title": "Just How Flexible are Neural Networks in Practice?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rN3fh43D30": {
    "title": "Enhancing Length Extrapolation in Sequential Models with Pointer-Augmented Neural Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bRLed9prWC": {
    "title": "Future Language Modeling from Temporal Document History",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dnqPvUjyRI": {
    "title": "SemiReward: A General Reward Model for Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PbvbLyqT6": {
    "title": "Dynamic Discounted Counterfactual Regret Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ad87VjRqUw": {
    "title": "Ghost on the Shell: An Expressive Representation of General 3D Shapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2eIembMRQJ": {
    "title": "Active Teacher Selection for Reinforcement Learning from Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bnPALC6S4l": {
    "title": "Towards general neural surrogate PDE solvers with specialized neural accelerators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvyYFSxdgB": {
    "title": "DATS: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ltutP1Iwqq": {
    "title": "Investigating Feature Alignment Under An Infant-Inspired Visual Distribution Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKqMmKlmA7": {
    "title": "Modulate Your Spectrum in Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YbZxT0SON4": {
    "title": "Improving Intrinsic Exploration by Creating Stationary Objectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pev2ufTzMv": {
    "title": "Why Sanity Check for Saliency Metrics Fails?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mZn2Xyh9Ec": {
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gFBTNDNDUG": {
    "title": "A Computational Framework for Solving Wasserstein Lagrangian Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=228XQpErvW": {
    "title": "Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gU58d5QeGv": {
    "title": "Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUELOzcTk8": {
    "title": "Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZyAqjAjJW": {
    "title": "LDReg: Local Dimensionality Regularized Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lOsF9k1sxW": {
    "title": "Fisher Information Guided Backdoor Purification Via Naive Exploitation of Smoothness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GACjMj1MS1": {
    "title": "Empirical Likelihood for Fair Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7fwzPsn1lJ": {
    "title": "LLark: A Multimodal Foundation Model for Music",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AL1fq05o7H": {
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=88FcNOwNvM": {
    "title": "Compositional Image Decomposition with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3NnfJnbJT2": {
    "title": "GIO: Gradient Information Optimization for Training Dataset Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAwaaLJNCk": {
    "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vl3F3s8OMg": {
    "title": "Can Euclidean Symmetry Help in Reinforcement Learning and Planning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7nz6ljg9Y": {
    "title": "The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XwiA1nDahv": {
    "title": "Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pjtIEgscE3": {
    "title": "Probabilistic Adaptation of Black-Box Text-to-Video Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SdBApv9iT4": {
    "title": "Horizon-Free Regret for Linear Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kBTzlxM2J1": {
    "title": "Faithful Rule Extraction for Differentiable Rule Learning Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0yz1pmVfE": {
    "title": "A Cooperative-Game-Theoretical Model for Ad Hoc Teamwork",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5u72wkdH3": {
    "title": "SYRAC: Synthesize, Rank, and Count",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwlm6Po61I": {
    "title": "Delving into LLMs' visual understanding ability using SVG to bridge image and text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1XDG1Z5Nhk": {
    "title": "Sparse Backpropagation for MoE Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CY9f6G89Rv": {
    "title": "High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tkmO6bXT54": {
    "title": "Mining latent labels for imbalance classification: a regrouping perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wG12xUSqrI": {
    "title": "Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZSEgJGPxu": {
    "title": "SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wm4WlHoXpC": {
    "title": "Scalable Diffusion for Materials Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dm8e7gsH0d": {
    "title": "Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPFsIbF3V6": {
    "title": "Guess & Sketch: Language Model Guided Transpilation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hEl2HpiH3g": {
    "title": "FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QaODpeRaOK": {
    "title": "Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmBKIecDE9": {
    "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cuAxSHcsSX": {
    "title": "On Differentially Private Federated Linear Contextual Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6I7UsvlDPj": {
    "title": "LaMPP: Language Models as Probabilistic Priors for Perception and Action",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDZumshwym": {
    "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KJzwUyryyl": {
    "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=daEqXJ0yZo": {
    "title": "Generative Human Motion Stylization in Latent Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2NpAw2QJBY": {
    "title": "Neural Neighborhood Search for Multi-agent Path Finding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IcbC9F9xJ7": {
    "title": "A General Single-Cell Analysis Framework via Conditional Diffusion Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwNj5TP9gm": {
    "title": "Evidential Conservative Q-Learning for Dynamic Recommendations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpw7V0P136": {
    "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mqukp3Lsnt": {
    "title": "Space-Time Attention with Shifted Non-Local Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lIwp1C1eSK": {
    "title": "Compositional Instruction Following with Language Models and Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2tVHNRZuCs": {
    "title": "Enable Lanuguage Models to Implicitly Learn Self-Improvement From Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AS7vaVU8d": {
    "title": "Learning Personalized Story Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5AbtYdHlr3": {
    "title": "Stochastic Safe Action Model Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m50eKHCttz": {
    "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aMfdN4ZQVx": {
    "title": "Training-free Deep Concept Injection Enables Language Models for Crossmodal Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLJznSp6X3": {
    "title": "ReLoRA: High-Rank Training Through Low-Rank Updates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4uaogMQgNL": {
    "title": "UpFusion: Novel View Diffusion from Unposed Sparse View Observations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPwQj4Mf3u": {
    "title": "Hopfield Encoding Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qi5Xa2cOZg": {
    "title": "Learning with Language-Guided State Abstractions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oM7Jbxdk6Z": {
    "title": "Multimodal Molecular Pretraining via Modality Blending",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o87xfYKQC1": {
    "title": "Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical Invariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZfatbNdLV": {
    "title": "Generative modeling for RNA splicing code predictions and design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVEu295o70": {
    "title": "Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DjeQ39QoLQ": {
    "title": "Robustifying State-space Models for Long Sequences via Approximate Diagonalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hwXUmwJAq5": {
    "title": "UGradSL: Machine Unlearning Using Gradient-based Smoothed Label",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U4RoAyYGJZ": {
    "title": "On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbJqRGNYCf": {
    "title": "JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YZrg56G0JV": {
    "title": "Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVMPfEGT2w": {
    "title": "Provable Offline Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pbLjYjjWqd": {
    "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmg7t8b4s0": {
    "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g1fE8fSOm5": {
    "title": "Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrjLHbqiYs": {
    "title": "Quantifying Interactions in Semi-supervised Multimodal Learning: Guarantees and Applications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTBXeXdbMf": {
    "title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1CPta0bfN2": {
    "title": "Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKKwQvepx0": {
    "title": "Explainable, Steerable Models with Natural Language Parameters and Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ozX92bu8VA": {
    "title": "The Truth Is In There: Improving Reasoning with Layer-Selective Rank Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LNTSrJjBe": {
    "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qBWhjsNPEY": {
    "title": "DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s6bKLlF4Pe": {
    "title": "Provable Knowledge Transfer using Successor Feature for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mmjnr0G8ZY": {
    "title": "Multi-Resolution Diffusion Models for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jg8yHRPU7l": {
    "title": "Mechanism of clean-priority learning in early stopped neural networks of infinite width",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NmaXXAiJJC": {
    "title": "COMPRESSION AND ACCELERATION OF DEEP NEURAL NETWORKS: A VECTOR QUANTIZATION APPROACH",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QDrG0ALevs": {
    "title": "Advantage-Conditioned Diffusion: Offline RL via Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wcka3bd7P4": {
    "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FIGXAxr9E4": {
    "title": "CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RtAct1E2zS": {
    "title": "On Error Propagation of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXGph215fL": {
    "title": "The Update Equivalence Framework for Decision-Time Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aaBnFAyW9O": {
    "title": "Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnYaF3vv3G": {
    "title": "LabelDP-Pro: Learning with Label Differential Privacy via Projections",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkKQjaS9GX": {
    "title": "SDM-RL: Steady-State Divergence Maximization for Robust Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tpk0p9QBM6": {
    "title": "Computing Low-Entropy Couplings for Large-Support Distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YgMdDQB09U": {
    "title": "AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mdCet7vVv": {
    "title": "Maestro: Uncovering Low-Rank Structures via Trainable Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mR5pknv2oP": {
    "title": "Chain-of-Thought Reasoning is a Policy Improvement Operator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SLA7VOqwwT": {
    "title": "Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQzdtBiKie": {
    "title": "Fractal Patterns May Unravel the Intelligence in Next-Token Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kce6LTZ5vY": {
    "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrWuDiW4Qm": {
    "title": "MetaPhysiCa: Improving OOD Robustness in Physics-informed Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3N5JJfqMX": {
    "title": "Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dsUB4bst9S": {
    "title": "Teaching Arithmetic to Small Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KI9NqjLVDT": {
    "title": "ReMasker: Imputing Tabular Data with Masked Autoencoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iStX5y0Ttg": {
    "title": "Towards Universal Robust Federated Learning via Meta Stackelberg Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lpyxWITF2c": {
    "title": "Topology Matters in Fair Graph Learning: a Theoretical Pilot Study",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ogxrdvFdx5": {
    "title": "ReLU soothes NTK conditioning and accelerates optimization for wide neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0i6Z9N5MLY": {
    "title": "Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6YZmkpivVH": {
    "title": "TpopT: Efficient Trainable Template Optimization on Low-Dimensional Manifolds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lrQlLqQase": {
    "title": "A Dynamical View of the Question of Why",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMFqEror1b": {
    "title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CZ50WgfCG": {
    "title": "Learning Reusable Dense Rewards for Multi-Stage Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTFPO9FHL3": {
    "title": "Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware Tokenization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvveGAbkVx": {
    "title": "Fair Classifiers that Abstain without Harm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3gOYtv1xV": {
    "title": "Carrying over Algorithm in Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ODSgo2m8aE": {
    "title": "Aligning Relational Learning with Lipschitz Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUoBaW8KH1": {
    "title": "Rethinking the Smoothness of Node Features Learned by Graph Convolutional Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBZBPXdJlC": {
    "title": "Listen, Think, and Understand",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAR9xu8WM6": {
    "title": "Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nh5tSrqTpe": {
    "title": "Don't Pre-train, Teach Your Small Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkF7NZGVr5": {
    "title": "Curvature Explains Loss of Plasticity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Q8TZWAHv4": {
    "title": "GOAt: Explaining Graph Neural Networks via Graph Output Attribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9kG7TwgLYu": {
    "title": "Time Fairness in Online Knapsack Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gdm87rRjep": {
    "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=okYdj8Ysru": {
    "title": "A Lie Group Approach to Riemannian Normalization for SPD Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q20kiEt1oW": {
    "title": "Strategies and impact of learning curve estimation for CNN-based image classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AEi2wyAMyb": {
    "title": "Bi-Level Optimization for Pseudo-Labeling Based Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NW31gAylIm": {
    "title": "Text-driven Prompt Generation for Vision-Language Models in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yJdj2QQCUB": {
    "title": "Graph Positional and Structural Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mutJBk3ILg": {
    "title": "Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DTwpuoaea4": {
    "title": "PAGER: A Framework for Failure Analysis of Deep Regression Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99hq9VMkbg": {
    "title": "Fisher-aware Quantization for DETR Detectors with Critical-category Objectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gLtHsY0zCC": {
    "title": "T-Measure: A Measure for Model Transferabilty",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HodMKbJkl3": {
    "title": "SGD batch saturation for training wide neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fht65Wm5JC": {
    "title": "Borda Regret Minimization for Generalized Linear Dueling Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFWuX1Fhtj": {
    "title": "On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yCYnKMHX3u": {
    "title": "MultiLayerDiffusion: Composing Global Contexts and Local Details in Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JTBe1WG3Ws": {
    "title": "FLIRT: Feedback Loop In-context Red Teaming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ErllmwXym": {
    "title": "Interpreting and improving diffusion models using the Euclidean distance function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZEZ0CPmoSI": {
    "title": "Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyWKb7Ltcx": {
    "title": "Intrinsic Riemannian Classifiers on the Deformed SPD Manifolds: A Unified Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ti0kjqFx7D": {
    "title": "Editable Graph Neural Network for Node Classifications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtZ7vCt5QY": {
    "title": "Causal-StoNet: Causal Inference for High-Dimensional Complex Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K1mcPiDdOJ": {
    "title": "Conditional Information Bottleneck Approach for Time Series Imputation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2drC319yHQ": {
    "title": "RepoFusion: Training Code Models to Understand Your Repository",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ng7OYC3PT8": {
    "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljwoQ3cvQh": {
    "title": "The Optimal Constant Solution: Predictable Extrapolation in Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hJEMTDOwKx": {
    "title": "Language Models as Semantic Indexers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqDeICpLFo": {
    "title": "Global minima, recoverability thresholds, and higher-order structure in GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v3XXtxWKi6": {
    "title": "RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TVDUVpgu9s": {
    "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=flg9EB6ikY": {
    "title": "Selective Prediction via Training Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afQuNt3Ruh": {
    "title": "Entropy Coding of Unordered Data Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdYqGkSr9S": {
    "title": "Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KZpDGD4Nh": {
    "title": "Neurosymbolic Grounding for Compositional Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8pYNdmwGAO": {
    "title": "EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yF19SY1i8M": {
    "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BSePKWwTUj": {
    "title": "Multiobjective Stochastic Linear Bandits under Lexicographic Ordering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qa0ULgosc9": {
    "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AialDkY6y3": {
    "title": "Deep Graph Predictions using Dirac-Bianconi Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRO4PfCiwb": {
    "title": "OS-net: Orbitally Stable Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rV9cp7KRH": {
    "title": "Incentivized Collaborative Learning: Architectural Design and Insights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QFYVVwiAM8": {
    "title": "Adaptive Sharpness-Aware Pruning for Robust Sparse Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8RGmICUkJ": {
    "title": "SiBBlInGS: Similarity-driven Building Block Inference using Graphs across States",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojrVw6GFFD": {
    "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDRoonpLkD": {
    "title": "Revisiting GNNs for Boolean Satisfiability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bkNx3O0sND": {
    "title": "MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAs4LdaP9Y": {
    "title": "Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ShjMHfmPs0": {
    "title": "Self-Consuming Generative Models Go MAD",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1m5uqUpO9": {
    "title": "A Local Graph Limits Perspective on Sampling-Based GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zt8bb6vC4m": {
    "title": "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=awWpHnEJDw": {
    "title": "The Hidden Language of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yC2jjvYVzb": {
    "title": "Confidence-Based Model Selection: When to Take Shortcuts in Spurious Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WO4BCqEyWc": {
    "title": "Augmentation-aware Self-Supervised Learning with Conditioned Projector",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tGQirjzddO": {
    "title": "Reasoning with Latent Diffusion in Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=biNhA3jbHc": {
    "title": "Learning Sequence Attractors in Recurrent Networks with Hidden Neurons",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6CBQYxXvr": {
    "title": "Project and Probe: Sample-Efficient Adaptation by Interpolating Orthogonal Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P1aobHnjjj": {
    "title": "Implicit bias of SGD in $L_2$-regularized linear DNNs: One-way jumps from high to low rank",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0gT4A0jNV": {
    "title": "Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3jq8f8t3d": {
    "title": "Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dyG2oLJYyX": {
    "title": "DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y5Gseybzp": {
    "title": "Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7tJxajC3G": {
    "title": "Federated Causal Discovery from Heterogeneous Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKXvPDekud": {
    "title": "CellPLM: Pre-training of Cell Language Model Beyond Single Cells",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0H6DFoZZXZ": {
    "title": "Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7iiF79kI3": {
    "title": "CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZndRcfyNI": {
    "title": "Principled Architecture-aware Scaling of Hyperparameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wAyTOazvN0": {
    "title": "Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vc52HZwNwe": {
    "title": "Gradient-free Proxy for Efficient Language Model Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SsmT8aO45L": {
    "title": "Provable Robust Watermarking for AI-Generated Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yd7idEYzNv": {
    "title": "EGALA: Efficient Gradient Approximation for Large-scale Graph Adversarial Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C4BikKsgmK": {
    "title": "Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LQ6LQ8f4y8": {
    "title": "CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R3Tf7LDdX4": {
    "title": "Memory-Consistent Neural Networks for Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwhHSOHMTM": {
    "title": "Learning dynamic representations of the functional connectome in neurobiological networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZFMiHfZwIf": {
    "title": "Skill or Luck? Return Decomposition via Advantage Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLqkCvjHRD": {
    "title": "Coarse-Tuning Models of Code with Reinforcement Learning Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7gDENzTzw1": {
    "title": "Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YHihO8Ka3O": {
    "title": "FedGT: Federated Node Classification with Scalable Graph Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sn7CYWyavh": {
    "title": "Whole-song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L7jtdGhWzT": {
    "title": "Toward $\\textbf{F}$aithfulness-guided $\\textbf{E}$nsemble $\\textbf{I}$nterpretation of Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2oTVrlcp3": {
    "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bM6LUC2lec": {
    "title": "MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g9diuvxN6D": {
    "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDhq1icpO8": {
    "title": "Conditional Instrumental Variable Regression with Representation Learning for Causal Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRgT26JlAx": {
    "title": "Learning with Temporal Label Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aOPTDchLBz": {
    "title": "ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ewIfVxCzbo": {
    "title": "DPO-Diff: On Discrete Prompt Optimization of Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jsQPjIaNNh": {
    "title": "Illuminating Protein Function Prediction through Inter-Protein Similarity Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzQSR56JZr": {
    "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jhPvuc7kxB": {
    "title": "Look, Remember and Reason: Grounded Reasoning in Videos with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1M8yDTa0Pp": {
    "title": "Cross-Model Semi-Supervised Prompt Learning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TyZhiK6fDf": {
    "title": "Co-Learning Empirical Games & World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkibCOnsEv": {
    "title": "Structured Inverse-Free Natural Gradient: Memory-Efficient & Numerically-Stable KFAC for Large Neural Nets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PXNrncg2DF": {
    "title": "SOHES: Self-supervised Open-world Hierarchical Entity Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vq8BCZYAdj": {
    "title": "Multi-fidelity Deep Symbolic Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d0BXudm2S4": {
    "title": "Natural Counterfactuals With Necessary Backtracking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PKUy1VJqwV": {
    "title": "Graph Representation Learning with Multi-granular Semantic Ensemble",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dwademPdV1": {
    "title": "Understanding Unfairness via Training Concept Influence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kYXZ4FT2b3": {
    "title": "Grid Cell-Inspired Fragmentation and Recall for Efficient Map Building",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7sASqAmGaO": {
    "title": "Augmenting Negative Representation for Continual Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZG2AiVMj1I": {
    "title": "IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mCOBKZmrzD": {
    "title": "EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sDlMJVXXeV": {
    "title": "Neural varifolds: an aggregate representation for quantifying geometry of point clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWSTBrmRqD": {
    "title": "DOMINO: A Dual-System for Multi-step Visual Language Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uV39mPKRGw": {
    "title": "Concept Matching: Clustering-based Federated Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4DoSULcfG6": {
    "title": "Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXTmAdGjlg": {
    "title": "Adaptive Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JfjduOxrTY": {
    "title": "Understanding Graph Transformers by Generalized Propagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zCJFTA19K4": {
    "title": "Token Alignment via Character Matching for Subword Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xLoxMvO695": {
    "title": "Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wqi85OBVLE": {
    "title": "Reward Adaptation Via Q-Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cdUpf6t6LZ": {
    "title": "Robust NAS benchmark under adversarial training: assessment, theory, and beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aq35gl2c1k": {
    "title": "Critical Learning Periods Emerge Even in Deep Linear Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eEslYpY6Yq": {
    "title": "On the Equivalence of Graph Convolution and Mixup",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzUhfQ74c5": {
    "title": "Conformal Language Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NialiwI2V6": {
    "title": "MOTOR: A Time-To-Event Foundation Model For Structured Medical Records",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3l0piOrGU": {
    "title": "Representation Deficiency in Masked Language Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OI3RoHoWAN": {
    "title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=chVYVLJIAh": {
    "title": "$\\lambda$-AC: Effective decision-aware reinforcement learning with latent models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE7PY7e4tr": {
    "title": "Neural Network Expressive Power Analysis Via Manifold Topology",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=34STseLBrQ": {
    "title": "Polynomial Width is Sufficient for Set Representation with High-dimensional Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XwBIcywWM": {
    "title": "Learning Variational Neighbor Labels for Test-Time Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tcx84iyqaC": {
    "title": "Reward Collapse in Aligning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIPLdFy3vp": {
    "title": "Parametric Augmentation for Time Series Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxcEqwl9es": {
    "title": "Microenvironment Probability Flows as Proficient Protein Engineers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQLDXQ3IG8": {
    "title": "Robustness Guarantees for Adversarial Training on Non-Separable Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DJZDgMOLXQ": {
    "title": "Prediction Error-based Classification for Class-Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CNL2bku4ra": {
    "title": "Test-Time Training on Nearest Neighbors for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ax2yRhCQr1": {
    "title": "Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bC50ZOyPQm": {
    "title": "READ: Recurrent Adaptation of Large Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ft1mr3WlGM": {
    "title": "Improved Probabilistic Image-Text Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Cw3yFqPDX": {
    "title": "Buffered Asynchronous Federated Learning with Local Differential Privacy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nR1EEDuov7": {
    "title": "Securing Deep Generative Models with Universal Adversarial Signature",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NoeLQU4J2O": {
    "title": "Soon Filter: Advancing Feed-Forward Neural Architectures for Inference at the Edge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iy0WQ0c75x": {
    "title": "Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYwYYwsbSo": {
    "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=abL5LJNZ49": {
    "title": "SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lpxcCD7WbQ": {
    "title": "Task adaptation by biologically inspired stochastic comodulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NOz4YbdHl9": {
    "title": "Confession Networks: Boosting Accuracy and Improving Confidence in Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yui55YzCao": {
    "title": "Shape-aware Graph Spectral Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IAkflJmNrC": {
    "title": "Polarity-Aware Semantic Retrieval with Fine-Tuned Sentence Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iH49a0yxgF": {
    "title": "DUDE: Deep Unsupervised Domain adaptation using variable nEighbors for physiological time series analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s56xikpD92": {
    "title": "BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSVtmmzeRB": {
    "title": "Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BYUdBlaNqk": {
    "title": "System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUGFpDCu3W": {
    "title": "What does GPT store in its MLP weights? A case study of long-range dependencies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MO5PiKHELW": {
    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LAEd3kHao9": {
    "title": "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y0GJXRungR": {
    "title": "Is Self-Repair a Silver Bullet for Code Generation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8OxL034uEr": {
    "title": "MgNO: Efficient Parameterization of Linear Operators via Multigrid",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ev10F9TWML": {
    "title": "Dissecting Neural Network Robustness Proofs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uFbWHyTlPn": {
    "title": "Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PoDkdFQIu3": {
    "title": "A Linear Algebraic Framework for Counterfactual Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMLQB4EZE1": {
    "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmXedvwrT1": {
    "title": "Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cXbnGtO0NZ": {
    "title": "Latent 3D Graph Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XLjlLQz2y2": {
    "title": "Spectral Greedy Coresets for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OUkZXbbwQr": {
    "title": "Reward Design for Justifiable Sequential Decision-Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rfz3K3yyU4": {
    "title": "Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5xKixQzhDE": {
    "title": "Calibrated Dataset Condensation for Faster Hyperparameter Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qL6brrBDk2": {
    "title": "SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnsLGUCynE": {
    "title": "3D Diffuser Actor: Multi-task 3D Robot Manipulation with Iterative Error Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TqYbAWKMIe": {
    "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tOzCcDdH9O": {
    "title": "Matryoshka Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9bCeJGUJi": {
    "title": "Curriculum Dynamic Graph Invariant Learning under Distribution Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LLbHdII8Pg": {
    "title": "Two Birds with One Stone: Protecting DNN Models Against Unauthorized Inference and Domain Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kJFIH23hXb": {
    "title": "SE(3)-Stochastic Flow Matching for Protein Backbone Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILtA2ebLYR": {
    "title": "Efficient Interactive Preference Learning in Evolutionary Algorithms: Active Dueling Bandits and Active Learning Integration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlZIXgfWPH": {
    "title": "On the Hyperparameter Loss Landscapes of Machine Learning Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LGXlMmDarK": {
    "title": "On the Stochasticity in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x9cVJnlX9n": {
    "title": "Risk-Controlling Model Selection via Guided Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4pnhzuRtJ2": {
    "title": "Optimized Tradeoffs for Private Majority Ensembling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4iPw1klFWa": {
    "title": "Scalable Neural Network Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D7KJmfEDQP": {
    "title": "Model Merging by Uncertainty-Based Gradient Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GRlKzhHl9Z": {
    "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cfi68cGzIt": {
    "title": "Conservative Reinforcement Learning by Q-function Disagreement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fHZ04oyEed": {
    "title": "Representation Learning from Interventional Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzcuXQq0aR": {
    "title": "PlugVFL: Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=12Acp6ZcRa": {
    "title": "Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3uITarEQ7p": {
    "title": "Differentially Private Model Compression via Selective Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ifz3IgsEPX": {
    "title": "DP-OPT: Make Large Language Model Your Differentially-Private Prompt Engineer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymR2bz0cEs": {
    "title": "Interaction-centric Hypersphere Reasoning for Multi-person Video HOI Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ptXo0epLQo": {
    "title": "$\\alpha$TC-VAE: On the relationship between Disentanglement and Diversity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pv2U1BeC5Z": {
    "title": "Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7d1qXEn1e": {
    "title": "A Restoration Network as an Implicit Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rhp5PDNOgf": {
    "title": "Spaced Scheduling Enhances Instruction-Prompted Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PudduufFLa": {
    "title": "Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3KDbIWT26J": {
    "title": "The Reasonableness Behind Unreasonable Translation Capability of Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZIOBA2oDU": {
    "title": "Fast Value Tracking for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HrRKc9ei7h": {
    "title": "Oracle Efficient Algorithms for Groupwise Regret",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NlBuWEJCug": {
    "title": "PcLast: Discovering Plannable Continuous Latent States",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WSzRdcOkEx": {
    "title": "GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=26XphugOcS": {
    "title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d8w0pmvXbZ": {
    "title": "Small-scale proxies for large-scale Transformer training instabilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dfEuojp0rX": {
    "title": "Variational Quantum Linear Solver enhanced Quantum Support Vector machine",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfrpYGKGPL": {
    "title": "The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qsAckNdySL": {
    "title": "Causality is Invariance Across Heterogeneous Units",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHfIe4chR5": {
    "title": "Long-distance Targeted Poisoning Attacks on Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXoq9EqR9e": {
    "title": "FairVLM: Mitigating Bias In Pre-Trained Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoismgeX7z": {
    "title": "Generalized Schrödinger Bridge Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QMQBza9BCx": {
    "title": "Persistent homology for high-dimensional data based on spectral methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N1TyUhkvjW": {
    "title": "Time Series Anomaly Detection using Reconstruction and RBF Similarity Scores",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dQVtTdsvZH": {
    "title": "Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjU0jtXcYn": {
    "title": "A General Framework for User-Guided Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1RE0H6mU7M": {
    "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXrUarMM20": {
    "title": "Efficient and Quantization-Friendly Ternary Fourier Convolution Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZJehvRKGD": {
    "title": "Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mzb7XD0O1Q": {
    "title": "CRAFT: Cross-Representation modeling on Audio waveForms and specTrograms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EaB7Ue1X9p": {
    "title": "High-Dimensional Safe Exploration via Optimistic Local Latent Safe Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kiwyQsZIGP": {
    "title": "Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5AjtSen6L": {
    "title": "ELEGANT: Certified Defense on the Fairness of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7YEXo5qUmN": {
    "title": "Organ-DETR: 3D Organ Detection Transfomer with Multiscale Attention and Dense Query Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ztuCObOc2i": {
    "title": "Neural Sinkhorn Gradient Flow",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNrTYz1rXH": {
    "title": "Fairness-Aware Domain Generalization under Covariate and Dependence Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=88MalncLgU": {
    "title": "GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Networks Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LH2JNpfwdH": {
    "title": "Towards 4D Human Video Stylization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AAxIs3D2ZZ": {
    "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LdiVep1jpj": {
    "title": "RASP Quadratures: Efficient Numerical Integration for High-Dimensional Mean-Field Variational Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzElnMrgSD": {
    "title": "How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9BERij4Gbv": {
    "title": "Guided Evolution with Binary Discriminators for ML Program Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PyERBFX0wJ": {
    "title": "Reflected Schr\\\"odinger Bridge for Constrained Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BC4AUywMow": {
    "title": "Zero-Level-Set Encoder for Neural Distance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izdFGwDgvW": {
    "title": "ICE: Image-Caption Encoding for Improved Out-Of-Distribution Generalization In Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3eFMnZ3N4J": {
    "title": "Efficient-3Dim: Learning a Generalizable Single-image Novel-view Synthesizer in One Day",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ffJo4vtTY": {
    "title": "Robust multimodal models have outlier features and encode more concepts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hNhwSmtXRh": {
    "title": "Lemur: Harmonizing Natural Language and Code for Language Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vfzRRjumpX": {
    "title": "CODE REPRESENTATION LEARNING AT SCALE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ezBaMwOqY": {
    "title": "Trading-off Multiple Properties for Molecular Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=de1218PoEl": {
    "title": "Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B9klVS7Ddk": {
    "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gl4AsqInti": {
    "title": "How Hessian structure explains mysteries in sharpness regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vM1xZ9kDUj": {
    "title": "Capture Concept through Comparison: Vision-and-Language Representation Learning with Intrinsic Information Mining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hiHZVUIYik": {
    "title": "A path-norm toolkit for modern networks: consequences, promises and challenges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S4YVoQ70b2": {
    "title": "Characterising Partial Identifiability in Inverse Reinforcement Learning For Agents With Non-Exponential Discounting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4SiDyYQbo": {
    "title": "An Investigation of Representation and Allocation Harms in Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efeBC1sQj9": {
    "title": "SEPT: Towards Efficient Scene Representation Learning for Motion Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XkTz7gdpc": {
    "title": "Efficient and Scalable Graph Generation by Spectrum Preserving Local Expansion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J4V3lW9hq6": {
    "title": "A Multi-Grained Group Symmetric Framework for Learning Protein-Ligand Binding Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4i4fgCOBDE": {
    "title": "Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eSO9quCgmz": {
    "title": "Rethinking pseudo-labeling: Data-centric insights improve semi-supervised learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H98CVcX1eh": {
    "title": "Discovering modular solutions that generalize compositionally",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOHVDPqoUJ": {
    "title": "Less is More: Selective Layer Finetuning with SubTuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CUfSCwcgqm": {
    "title": "Long-range Neural Atom Learning for Molecular Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ArpwmicoYW": {
    "title": "FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2RJAzSphy9": {
    "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j56A1HUTQS": {
    "title": "Bridging Indexing Structure and Graph Learning: Expressive and Scalable Graph Neural Network via Core-Fringe",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Glcsog6zOe": {
    "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUUjIWntkU": {
    "title": "Explainable medical image clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K1bv86Uvbp": {
    "title": "LARGE LANGUAGE MODELS FOR BIOMEDICAL KNOWLEDGE GRAPH CONSTRUCTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QqjFHyQwtF": {
    "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C5u71ph75Q": {
    "title": "Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9FXRHoQdc": {
    "title": "Best Response Shaping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDQF6A1s6M": {
    "title": "LOQA: Learning with Opponent Q-Learning Awareness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kuh5qgCGCp": {
    "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8J7Pw7hpj": {
    "title": "Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WceuzWff5": {
    "title": "Understanding the Transfer of High-Level Reinforcement Learning Skills Across Diverse Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BoLqnXEdSE": {
    "title": "Measuring Fairness Using Probable Segmentation for Continuous Sensitive Attributes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eRAXvtP0gA": {
    "title": "Unsupervised Cognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iJBQAAhqvY": {
    "title": "RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0A5o6dCKeK": {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkRD6GsswM": {
    "title": "CLA-RA: COLLABORATIVE ACTIVE LEARNING AMIDST RELABELING AMBIGUITY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dA4EWchlbn": {
    "title": "Advancing the Adversarial Robustness of Neural Networks from the Data Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jolYuxpVn1": {
    "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qYb0CANLGC": {
    "title": "Auto-Regressive Next-Token Predictors are Universal Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IryGDUHxDE": {
    "title": "Unsupervised open-vocabulary action recognition with an autoregressive model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BGkqypmGvm": {
    "title": "A 2-Dimensional State Space Layer for Spatial Inductive Bias",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4rBEgZCubP": {
    "title": "Learning 3D Particle-based Simulators from RGB-D Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jxpsAj7ltE": {
    "title": "From Sparse to Soft Mixtures of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6kt50zAiG": {
    "title": "CAMBranch: Contrastive Learning with Augmented MILPs for Branching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NaxbdRi8Rv": {
    "title": "StyleAdapter: A Unified Stylized Image Generation Model without Test-Time Fine-Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xdnoULh5Sv": {
    "title": "CARSO: Blending Adversarial Training and Purification Improves Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxVBKhyfSo": {
    "title": "Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KKZaj2QS3G": {
    "title": "Enriching Time Series Representation: Integrating a Noise-Resilient Sampling Strategy with an Efficient Encoder Architecture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3TAhlGaMKD": {
    "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b1Hivmb86F": {
    "title": "MetaTST: Essential Transformer Components for Time Series Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cijOBlCxMa": {
    "title": "CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNOewRJLgQ": {
    "title": "Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OfXqQ5TRwp": {
    "title": "ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DqD59dQP37": {
    "title": "Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ALGFFPXWSi": {
    "title": "One Forward is Enough for Neural Network Training via Likelihood Ratio Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=etm456yoiq": {
    "title": "B$^{3}$CT: Three-branch Coordinated Training for Domain Adaptive Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBo7544jZx": {
    "title": "Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JU8TwFXGC": {
    "title": "LLM Performance Predictors are good initializers for Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NRIcs2TR7k": {
    "title": "Extending Multi-modal Contrastive Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZXlEFO3tZ": {
    "title": "Counterfactual Density Estimation using Kernel Stein Discrepancies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dnO3LLiJ1": {
    "title": "Vision Transformers Need Registers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8J0b9gNfG": {
    "title": "Multilingual Visual Speech Recognition with a Single Model using Visual Speech Unit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JG9PoF8o07": {
    "title": "Beyond Laplace and Gaussian: Exploring the Generalized Gaussian Mechanism for Private Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tsNLIBlG4p": {
    "title": "Analysis of a class of stochastic component-wise soft-clipping schemes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Feiz5HtCD0": {
    "title": "Does Writing with Language Models Reduce Content Diversity?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rt7ekFkSJZ": {
    "title": "Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCpayOuqBx": {
    "title": "DOS: Dreaming Outlier Semantics for Out-of-distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=homn1jOKI5": {
    "title": "Conformal Inductive Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BifeBRhikU": {
    "title": "PB-LLM: Partially Binarized Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6O3Q6AFUTu": {
    "title": "Beyond Linear Spherical Interpolation: Noise Correction for Image Interpolation with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0m0DdCCQ2": {
    "title": "Liteformer: Lightweight Evoformer for Protein Structure Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rBH7x87VfJ": {
    "title": "Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L7gQWBcFxK": {
    "title": "Efficient Gradient Estimation via Adaptive and Importance Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A18gWgc5mi": {
    "title": "Course Correcting Koopman Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AcRfzLS6se": {
    "title": "Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWHPW0sXFK": {
    "title": "PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=di52zR8xgf": {
    "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nq45xeghcL": {
    "title": "Intelligent Switching for Reset-Free RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EJPIzl7mgc": {
    "title": "Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvDeiLv7qc": {
    "title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHROe7Mfcb": {
    "title": "Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrE2fqAInO": {
    "title": "Fixed-Budget Differentially Private Best Arm Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UXALv0lJZS": {
    "title": "Separate and Diffuse: Using a Pretrained Diffusion Model for Better Source Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zavLQJ1XjB": {
    "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4jBL79L5QS": {
    "title": "Beyond Shortest-Paths: A Benchmark for Reinforcement Learning on Traffic Engineering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oo5spZRpH6": {
    "title": "HAct: Out-of-Distribution Detection with Neural Net Activation Histograms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mu1UUl14cw": {
    "title": "Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNQJtoPZmz": {
    "title": "Simplicity Bias in Overparameterized Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irBktGvHmC": {
    "title": "Characterizing Exceptional Distributions with Neural Rule Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NRRHkJE03w": {
    "title": "Beyond Dynamics: Learning to Discover Conservation Principles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Ed7b52z53": {
    "title": "On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eNoiRal5xi": {
    "title": "Unknown Domain Inconsistency Minimization for Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5MquO1g7R": {
    "title": "Change Point Detection via Variational Time-Varying Hidden Markov Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDxeSZ1wdI": {
    "title": "Entity-Centric Reinforcement Learning for Object Manipulation from Pixels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLWqIWPMDH": {
    "title": "Memoization-Aware Bayesian Optimization for AI Pipelines with Unknown Costs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6rEcB9m9AI": {
    "title": "Promoting Exploration in Memory-Augmented Adam using Critical Momenta",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjeQKFxFpZ": {
    "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bu3JGyfz23": {
    "title": "A multi-view latent space learning framework via adaptive graph embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c9xsaASm9L": {
    "title": "Enhancing Neural Training via a Correlated Dynamics Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mXJ9o2DNx": {
    "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4A5D1nsdtj": {
    "title": "An Effective Universal Polynomial Basis for Spectral Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8YwPdoSlr": {
    "title": "Capturing Static, Short-Term, and Long-Term Dynamics Through Self-Supervised Time Series Learning: CHRONOS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iVqd2aXzvd": {
    "title": "Automata Learning for Neural Event ODEs: An Interpretable Model of Piecewise Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxnbPPVvOG": {
    "title": "Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZuYvrjh2od": {
    "title": "ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UgTrngiN16": {
    "title": "LangProp: A code optimization framework using Language Models applied to driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tGOOP7DGxs": {
    "title": "Graph Transformers for Large Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nO344avRib": {
    "title": "A Simple and Scalable Representation for Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sBSC0OXEQG": {
    "title": "Correlated dense associative memories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hILVmJ4Uvu": {
    "title": "True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pijvVzhRdZ": {
    "title": "Rethinking the Starting Point: Enhancing Performance and Fairness of Federated Learning via Collaborative Pre-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcqWJ8JgMR": {
    "title": "AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ldJXXxPE0L": {
    "title": "The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yfdtkYQesu": {
    "title": "Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxurbfWwRr": {
    "title": "Learning Time-Varying Convexifications of Multiple Fairness Measures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d1zLRzhalF": {
    "title": "Knowledge Graph Reasoning with Reinforcement Learning Agent guided by Multi-relational Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=01ep65umEr": {
    "title": "TeLLMe what you see: Using LLMs to Explain Neurons in Vision Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nTNgkEIfeb": {
    "title": "FedInverse: Evaluating Privacy Leakage in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GpGJg1gsjl": {
    "title": "Uncertainty for Active Learning on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K3tHTPjFBM": {
    "title": "Equivariant Protein Multi-task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3WzT2mrhB": {
    "title": "From Sparse to Dense: Learning to Construct 3D Human Meshes from WiFi",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJ5N8qrEPl": {
    "title": "Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5NJzNAXAmx": {
    "title": "Informed POMDP: Leveraging Additional Information in Model-Based RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iKd99CYwPX": {
    "title": "Deterministic Diffusion for Sequential Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sgvb61ZM2x": {
    "title": "Effective Learning by Node Perturbation in Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xriGRsoAza": {
    "title": "Inherently Interpretable Time Series Classification via Multiple Instance Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W3T9rql5eo": {
    "title": "Uniform as Glass: Gliding over the Pareto Front with Neural Adaptive Preferences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ZUYJpvIys": {
    "title": "TOSS: High-quality Text-guided Novel View Synthesis from a Single Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6KkyweyYh": {
    "title": "Biological Sequence Analysis Using B ́ezier Curve",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PPjf4wife": {
    "title": "Leveraging Large Language Models for Optimised Coordination in Textual Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbKThNNFxr": {
    "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hvoVD7x7f8": {
    "title": "Uncertainty-Aware Decision Transformer for Stochastic Driving Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=upVI6V81Qn": {
    "title": "Structural Adversarial Objectives For Self-Supervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JrmPG9ufKg": {
    "title": "A Mutual Information Perspective on Federated Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZtt0pRnOl": {
    "title": "Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mt5NPvTp5a": {
    "title": "Improved Operator Learning by Orthogonal Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGdEM131Ht": {
    "title": "GENERATIVE TIME SERIES LEARNING WITH TIME-FREQUENCY FUSED ENERGY-BASED MODEL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ROGsTX3IR": {
    "title": "Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ux2cgxw6O": {
    "title": "LOVECon: Text-driven Training-free Long Video Editing with ControlNet",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4zAFFyoXK": {
    "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFYbBOEOtv": {
    "title": "V-JEPA: Latent Video Prediction for Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ry1SZkcYbX": {
    "title": "Edge-Sampler: Efficient Importance Sampling for Neural Implicit Surfaces Reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3b8CgMO5ix": {
    "title": "Model guidance via explanations turns image classifiers into segmentation models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5Xkw9fpty": {
    "title": "Smooth Min-Max Monotonic Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xEJMoj1SpX": {
    "title": "Elucidating the Exposure Bias in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKnbIZefER": {
    "title": "Availability Attacks Need to Create Shortcuts for Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0tsJ7Nv5hk": {
    "title": "Harnessing Orthogonality to Train Low-Rank Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIqjp9yTDq": {
    "title": "Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XbLffB0T2z": {
    "title": "Transferable Availability Poisoning Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pjz3jkCAir": {
    "title": "CONFIDE: CONtextual FInite DifferencE modelling of PDEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dPHLbUqGbr": {
    "title": "Fast, Expressive $\\mathrm{SE}(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwbQ2YwLhD": {
    "title": "Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tusy7IlWWw": {
    "title": "SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mE52zURNGc": {
    "title": "An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kv5xE1p3jz": {
    "title": "JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LkB80Rw6Ap": {
    "title": "Curvature MPNNs : Improving Message Passing with Local Structural Properties",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7jhe7gr7C": {
    "title": "Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GZ6AcZwA8r": {
    "title": "MMD Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iqHh5Iuytv": {
    "title": "RNNS with gracefully degrading continuous attractors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kvcbV8KQsi": {
    "title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P15CHILQlg": {
    "title": "Learning Energy Decompositions for Partial Inference of GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bshfchPM9H": {
    "title": "RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kgy2swARws": {
    "title": "S\\(^{2}\\)-DMs: Skip-Step Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y6TIwYucUC": {
    "title": "SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tYTCyYI27": {
    "title": "Calibration Bottleneck: What Makes Neural Networks less Calibratable?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HDbKLu0bkn": {
    "title": "Heterogeneity of Regularization between adjacent periods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7t7AOseAa": {
    "title": "ZEST: ZEROSHOT SPARSE FINE-TUNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=huGECz8dPp": {
    "title": "Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SU8vFBJXJt": {
    "title": "Why not both? Combining Bellman losses in deep reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTf7mXhTVt": {
    "title": "Query Efficient Black-Box Adversarial Attack with Automatic Region Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u0INlprg3U": {
    "title": "LIFT: Efficient Layer-wise Fine-tuning for Large Model Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cc8h3I3V4E": {
    "title": "Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6jBNQ8nSxA": {
    "title": "Just-in-Time Security Patch Detection - LLM At the Rescue for Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aLiinaY3ua": {
    "title": "Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WX9cd9iII4": {
    "title": "Fair Off-Policy Learning from Observational Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gvHvwL6Ks4": {
    "title": "Privacy-Preserving Data Quality Evaluation in Federated Learning Using Influence Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gtMbnrsyLA": {
    "title": "SMAAT: Scalable Manifold-Aware Adversarial Training for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XgklTOdV4J": {
    "title": "DualAug: Exploiting Additional Heavy Augmentation with OOD Data Rejection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tWNHQq7gZX": {
    "title": "Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bfRDhzG3vn": {
    "title": "Continual Contrastive Spoken Language Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jiDsk12qcz": {
    "title": "Knowledge Fusion of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K98byXpOpU": {
    "title": "Double Momentum Method for Lower-Level Constrained Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9St5HsXMOr": {
    "title": "Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89AOrk05uy": {
    "title": "Understanding and addressing spurious correlation via Neural Tangent Kernels: A spectral bias perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgOJlxzB16": {
    "title": "SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gAnRV4UaUv": {
    "title": "ISCUTE: Instance Segmentation of Cables Using Text Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7zxGHwe7Vw": {
    "title": "FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mxaOpDHpCW": {
    "title": "Breadth First Exploration in Grid-based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQfCboYwDK": {
    "title": "Adiabatic replay for continual learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DRu8PMHgCh": {
    "title": "FedTrans: Client-Transparent Utility Estimation for Robust Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjfOL9z5Xr": {
    "title": "DyVal: Graph-informed Dynamic Evaluation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTzdVhbTEt": {
    "title": "Designing Long-term Group Fair Policies in Dynamical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rleZtn5OqJ": {
    "title": "Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n2nPeZ9VJ0": {
    "title": "Optimized Large Language Models Accurately Identify Recurrence of VT After Ablation from Complex Medical Notes: Will Chart Review Become Obsolete?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ff2g30cZxj": {
    "title": "From Posterior Sampling to Meaningful Diversity in Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikX6D1oM1c": {
    "title": "A Neural Framework for Generalized Causal Sensitivity Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yLgr02IsXY": {
    "title": "AMPipe: Accelerating MoE Model Training with Intra-Block Pipelining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=msuaCcTMQ2": {
    "title": "Active Automated Machine Learning with Self-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NvQ4kzcRSL": {
    "title": "Graph Clustering with Masked AutoEncoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mngdhgi711": {
    "title": "OKR-Agent: An Object and Key Results Driven Agent System with Hierarchical Self-Collaboration and Self-Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qiduMcw3CU": {
    "title": "Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8DW3aSOnou": {
    "title": "Video Deblurring with Adaptive High-frequency Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ap1ByuwQrX": {
    "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQUXBoGiDU": {
    "title": "Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hvtSLkKeZ": {
    "title": "Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvwnYpesoD": {
    "title": "A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIaS66XkNA": {
    "title": "Idempotent Generative Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dnc3paMqDE": {
    "title": "DeepSPF: Spherical SO(3)-Equivariant Patches for Scan-to-CAD Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxL35zAxvT": {
    "title": "Test Time Adaptation with Auxiliary Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1AXvGjfF0V": {
    "title": "Evaluating Hallucinations in Chinese Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RofU5v2BvZ": {
    "title": "Efficient Human-AI Coordination via Preparatory Language-based Convention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x6CjUdI25z": {
    "title": "Suppressing Overestimation in Q-Learning through Adversarial Behaviors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgDrVM9Rpx": {
    "title": "P-MapNet: Far-seeing Map Constructer Enhanced by both SDMap and HDMap Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g0fHn95m3D": {
    "title": "Text-To-Energy: Accelerating Quantum Chemistry Calculations through Enhanced Text-to-Vector Encoding and Orbital-Aware Multilayer Perceptron",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HHWlwxDeRn": {
    "title": "SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LXnTFMvn8A": {
    "title": "A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0elDO9v31": {
    "title": "Intrinsic Mesh CNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JtKGkz9fAe": {
    "title": "Improving Natural Language Understanding with Computation-Efficient Retrieval Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=59nCKifDtm": {
    "title": "Improve Temporal Consistency In Diffusion Models through Noise Correlations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O0dW800ukz": {
    "title": "Multimodal Distillation of Protein Sequence, Structure, and Function",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wDE3clrYWR": {
    "title": "Combinatorial Optimization via Memory Metropolis: Template Networks for Proposal Distributions in Simulated Annealing applied to Nanophotonic Inverse Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nbPGqeH3lt": {
    "title": "FedCDA: Federated Learning with Cross-rounds Divergence-aware Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7C04OET3V": {
    "title": "Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=drovOv7IKB": {
    "title": "Divide-and-Conquer Time Series Forecasting with Auto-Frequency-Correlation via Cross-Channel Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fTiU8HhdBD": {
    "title": "A Unified Framework for Reinforcement Learning under Policy and Dynamic Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F5dhGCdyYh": {
    "title": "Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k0nlUXYKhX": {
    "title": "A Fault Forecasting Approach Using Two-Dimensional Optimization (TDO)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6crLU7MIE": {
    "title": "Who to imitate: Imitating desired behavior from divserse multi-agent datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8OOlBjhkU": {
    "title": "Optimization over Sparse Restricted Convex Sets via Two Steps Projection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89ZekEEsSJ": {
    "title": "Stealthy Targeted Backdoor Attack Against Image Captioning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nJsfYo3HDy": {
    "title": "Why are Modern GANs Poor Density Models?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GOt2kP383R": {
    "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7VW3KBm34": {
    "title": "Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWzxhtl1HP": {
    "title": "Exploring Diffusion Time-steps for Unsupervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ff5srKUefm": {
    "title": "Entropy Voting Between Capsules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=extpNXo6hB": {
    "title": "SweetDreamer: Aligning Geometric Priors in 2D diffusion for Consistent Text-to-3D",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9SwObx9Jdn": {
    "title": "Generation of Geodesics with Actor-Critic Reinforcement Learning to Predict Midpoints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V6JRkfj9dU": {
    "title": "How many samples are needed to train a deep-ReLU neural network?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLbRvr840Q": {
    "title": "Hypergraph Dynamic System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OvlcyABNQT": {
    "title": "Augmented Bayesian Policy Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qZB7KDN4L1": {
    "title": "Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XkLMGx60aZ": {
    "title": "Climate-sensitive Urban Planning through Optimization of Tree Placements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AxYTFpdlvj": {
    "title": "Graph Decoding via Generalized Random Dot Product Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mL8Q9OOamV": {
    "title": "Emu: Generative Pretraining in Multimodality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ghyeMoj1gK": {
    "title": "Client-centric Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A4mJuFRMN8": {
    "title": "Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jhiByZpuIS": {
    "title": "MSfusion: Enabling Collaborative Training of Large Models over Resource-Constraint Participants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EhmEwfavOW": {
    "title": "HoloNets: Spectral Convolutions do extend to Directed Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cNi2EJ8OCh": {
    "title": "Functional Classification Under Local Differential Privacy with Model Reversal and Model Average",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HilIIP4yzw": {
    "title": "Improving Learning Conditions for Computer Science Students by Using the Flipped Classroom",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ue93J8VV3W": {
    "title": "TabGraphs: new benchmark and insights for learning on graphs with tabular features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WesY0H9ghM": {
    "title": "Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wx97sznZwB": {
    "title": "CLIP-Guided Reinforcement Learning for Open-Vocabulary Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vgD20RxsC0": {
    "title": "Time Series Prediction With Events Disturbance Based Causal Representation Learnin",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8UfDs4J46": {
    "title": "Addressing Signal Delay in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OROKjdAfjs": {
    "title": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qTlcbLSm4p": {
    "title": "Relay Diffusion: Unifying diffusion process across resolutions for image synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7jUQHmz4Tq": {
    "title": "D3AD: DYNAMIC DENOISING DIFFUSION PROBABILISTIC MODEL FOR ANOMALY DETECTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h922Qhkmx1": {
    "title": "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Lqyut1y7M": {
    "title": "On the Optimality of Activations in Implicit Neural Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2gMwe9Duc4": {
    "title": "Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WcSofkUVge": {
    "title": "Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c72vop46KY": {
    "title": "CogVLM: Visual Expert for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KiespDPaRH": {
    "title": "Improving the Convergence of Dynamic NeRFs via Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZBpVcc2Xc": {
    "title": "HiddenKey: Parameter-Efficient FineTuning Meets Dropout under a Unified Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CX2RgsS29V": {
    "title": "Fast Updating of Truncated SVD for Representation Learning in Sparse Matrix",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0I2RtD8je": {
    "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lKK50q2MtV": {
    "title": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1bAUywYJTU": {
    "title": "DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9XdLlbxZCC": {
    "title": "MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9FXGX00iMF": {
    "title": "BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtmB8WrPSp": {
    "title": "Sparse-PGD: An Effective and Efficient Attack for $l_0$ Bounded Adversarial Perturbation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0AYosSFETw": {
    "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qfqb8ueIdy": {
    "title": "A Unified Framework for Consistency Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fTEPeQ00VM": {
    "title": "TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dszD2gZIif": {
    "title": "Long-term Time Series Forecasting with Vision Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJksrOhurE": {
    "title": "CARD: Channel Aligned Robust Blend Transformer for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4w4PDIT3h4": {
    "title": "Focus on Primary: Differential Diverse Data Augmentation for Generalization in Visual Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PJVUWpPnZC": {
    "title": "Reinforcement Symbolic Regression Machine",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFYksmdqgY": {
    "title": "Beyond Language: Empowering Unsupervised Machine Translation with Cross-modal Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u48tHG5f66": {
    "title": "ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hGKda1uVEn": {
    "title": "Support Vector-based Shapley Value Estimation for Feature Selection and Explanation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqhAA26vXE": {
    "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dkn9cEOQkU": {
    "title": "Addressing Real-Time Fragmentary Interaction Control Problems via Muti-step Representation Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o6eUNPBAEc": {
    "title": "Language Models Struggle to Explain Themselves",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BlCnycxgJQ": {
    "title": "An Inexact Regularized Adaptive Algorithm with Manifold Identification for Training Structured Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhsZwzBYaU": {
    "title": "Tailoring Mixup to Data using Kernel Warping functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6cMmSnOpCs": {
    "title": "ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cObFETcoeW": {
    "title": "Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRWrvzRU4w": {
    "title": "OneSpike: Ultra-low latency spiking neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkvZ7v4OmP": {
    "title": "Space Group Constrained Crystal Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHedM0Zem9": {
    "title": "BEEF: Building a BridgE from Event to Frame",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YXnggA4iiD": {
    "title": "Distribution Aware Active Learning via Gaussian Mixtures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1SSQ6Dekc": {
    "title": "TransLLaMa: LLM-based Simultaneous Translation System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ztL7Trdnx": {
    "title": "TAFS: Task-aware Activation Function Search for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vy6sjPt2Vr": {
    "title": "A Spitting Image: Superpixel Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qox9rO0kN0": {
    "title": "Learning Multi-Agent Communication from Graph Modeling Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1rgMkDWfYV": {
    "title": "Cleaning label noise with vision-language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfh8ZbSlyf": {
    "title": "SITReg: Multi-resolution architecture for symmetric, inverse consistent, and topology preserving image registration using deformation inversion layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=52igC7K5Mf": {
    "title": "GC-Mixer: A Novel Architecture for Time-varying Granger Causality Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHdXvu5ehy": {
    "title": "An Efficient Subgraph GNN with Provable Substructure Counting Power",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MSe8YFbhUE": {
    "title": "DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbkIwjmua3": {
    "title": "Vulnerable Region Discovery through Diverse Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WRxCuhTMB2": {
    "title": "Experimental methodology to evaluate the effectiveness of uncertainty disentanglement on regression models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CpnKq3UJwp": {
    "title": "Efficient Multi-agent Reinforcement Learning by Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yuYMJQIhEU": {
    "title": "Communication-efficient Random-Walk Optimizer for Decentralized Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlfD5cE1ep": {
    "title": "Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iT1ttQXwOg": {
    "title": "Equivariant Deep Weight Space Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7LCsDMcZ4": {
    "title": "EventRPG: Event Data Augmentation with Relevance Propagation Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qFIkOhq24": {
    "title": "Fundamental Limitation of Alignment in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WdhtdjoaVw": {
    "title": "Functional Wasserstein Bridge Inference for Bayesian Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xGvPKAiOhq": {
    "title": "How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOJriBlOFd": {
    "title": "NeRM: Learning Neural Representations for High-Framerate Human Motion Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qjFnENGhDE": {
    "title": "Regularization is Enough for Last-Iterate Convergence in Zero-Sum Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n9CqhWGK4o": {
    "title": "Cellular Interplay in COVID-19: Insights from Graph Neural Networks with Multidimensional Edge Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3edHHvu5GX": {
    "title": "Adaptive Visual Scene Understanding: Incremental Scene Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gS0XOu0JKs": {
    "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=toD3yzfuaf": {
    "title": "Meta-Learning with Personalized Learning Rates for Rapid Task Mastery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EX7AxKgc46": {
    "title": "Improved Generalization of cGAN using Vicinal Estimation and Early Stopping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xh3XUaB8M9": {
    "title": "Visual Evidence Prompting Mitigates Hallucinations in Multimodal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbLuklVaX7": {
    "title": "Node Classification in the Heterophilic Regime via Diffusion-Jump GNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezBH9WE9s2": {
    "title": "AnyText: Multilingual Visual Text Generation and Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QGk2UhrC0Z": {
    "title": "IGTO: Individual Global Transform Optimization for Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgndgAbBcR": {
    "title": "CNNGEN: A GENERATOR AND BENCHMARK FOR SUSTAINABLE CONVOLUTIONAL NEURAL NETWORK SEARCH",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EvBx5whpzJ": {
    "title": "Con4m: Unleashing the Power of Consistency and Context in Classification for Blurred-Segmented Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lqA5EuieJ": {
    "title": "Prediction Tasks in Graphs: a Framework to Control the Interpretability-Performance Trade-off",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3klVRLhK7w": {
    "title": "Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlvtjAB0gl": {
    "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=06mzMua9Rw": {
    "title": "A Trust Region Approach for Few-Shot Sim-to-Real Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEwKAZZmSw": {
    "title": "Efficient Backpropagation with Variance Controlled Adaptive Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSHEYpGF6P": {
    "title": "Meta-Learning with Task-Environment Interaction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhXisLeIqR": {
    "title": "WinNet:time series forecasting with a window-enhanced period extracting and interacting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l8je4qJR4K": {
    "title": "Domain Generalization via Content Factors Isolation: A Two-level Latent Variable Modeling Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OPpqmSp0wK": {
    "title": "Multi-label Cluster Discrimination for Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbiU3WZpTp": {
    "title": "On the Robustness of Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80wh3jjCZf": {
    "title": "Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3f5PALef5B": {
    "title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6mLjDwYte5": {
    "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OqTMUPuLuC": {
    "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdjKRbtrth": {
    "title": "Generative Retrieval with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wMWZ78ulsK": {
    "title": "An Information Theoretic Approach to Interaction Grounded Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cf4FJGmHRQ": {
    "title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0vdDSt9XM": {
    "title": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S46Knicu56": {
    "title": "A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K8Mbkn9c4Q": {
    "title": "TABLEYE: SEEING SMALL TABLES THROUGH THE LENS OF IMAGES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1k4yZbbDqX": {
    "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Ebi1euQZQ": {
    "title": "HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision-Language Models for Detailed Caption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VpCqrMMGVm": {
    "title": "Interpreting the Inner Mechanisms of Large Language Models in Mathematical Addition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nXgWT12tb": {
    "title": "Correlated Attention in Transformers for Multivariate Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wi8wMFuO0H": {
    "title": "Cross-domain Recommendation from Implicit Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=krIOxfqsOh": {
    "title": "Masked Pretraining for Multi-Agent Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpcVXiMlcv": {
    "title": "Object-Aware Inversion and Reassembly for Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Qyxw0cCuu": {
    "title": "CONTROL: A Contrastive Learning Framework for Open World Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ehSQZa4vuk": {
    "title": "Bad Habits: Policy Confounding and Out-of-Trajectory Generalization in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wWI1RYngAA": {
    "title": "Adaptive Offline Data Replay in Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7essnmWOK5": {
    "title": "Graph Neural Networks for Multivariate Time-Series Forecasting via Learning Hierarchical Spatiotemporal Dependencies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lK4QHgjUU8": {
    "title": "SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eiF7TU1E8E": {
    "title": "SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUz6Qefe5z": {
    "title": "How Neural Networks With Derivative Labels Work: A Neural Tangent Kernel Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dKl6lMwbCy": {
    "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7qhUMylLU": {
    "title": "Sample-Efficient Multi-Agent RL: An Optimization Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sRop0N5NYV": {
    "title": "Tactics of Robust Deep Reinforcement Learning with Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpDw5U04SU": {
    "title": "Minimum width for universal approximation using ReLU networks on compact domain",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Ok7ccvtf3": {
    "title": "UNLEARNING THE UNWANTED DATA FROM A PERSONALIZED RECOMMENDATION MODEL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KIPJKST4gw": {
    "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3K3aWRpRNq": {
    "title": "Reducing Atomic Clashes in Geometric Diffusion Models for 3D Structure-Based Drug Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UiLqimGm5": {
    "title": "Coordinate-Aware Modulation for Neural Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RXU6qde675": {
    "title": "Adversarial enhanced representation for link prediction in multi-layer networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKfMFtlz5D": {
    "title": "MG-NeRF: Multimodal Representation Learning for Generalizable NeRF",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLGliHckR8": {
    "title": "Drug Discovery with Dynamic Goal-aware Fragments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h57gkDO2Yg": {
    "title": "Self-Supervised Dataset Distillation for Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Go8hf9wKJx": {
    "title": "DOG: Diffusion-based Outlier Generation for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SL7djdVpde": {
    "title": "Rethinking the symmetry-preserving circuits for constrained variational quantum algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wilJbPvRYv": {
    "title": "Are We in (A)Sync?: Guidance for Efficient Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYu5Flqm9D": {
    "title": "Towards Codable Text Watermarking for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=opZTBFnX2G": {
    "title": "Bayesian Offline-to-Online Reinforcement Learning : A Realist Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHaokdQkoi": {
    "title": "GNRK: Graph Neural Runge-Kutta method for solving partial differential equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G7UtIGQmjm": {
    "title": "Hypothesis Search: Inductive Reasoning with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oNlPtI7QfQ": {
    "title": "Embed-Search-Align: DNA Sequence Alignment using Transformer models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i4ULDEeBss": {
    "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=488A64eOf6": {
    "title": "Language Model Decoding as Direct Metrics Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uMAujpVi9m": {
    "title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fQxLgR9gx7": {
    "title": "Factual and Personalized Recommendation Language Modeling with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUPrQXSKOv": {
    "title": "Decentralized Decoupled Training for Federated Long-Tailed Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8giiPtg6rw": {
    "title": "DataFreeShield: Defending Adversarial Attacks without Training Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=peZbJlOVAN": {
    "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5IOKw3AQe4": {
    "title": "On the Theoretical Analysis of Dense Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FoqZKsH9sE": {
    "title": "LSP: Low-Power Semi-structured Pruning for Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NZtF0um8S7": {
    "title": "Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GszBQ3ZTzk": {
    "title": "PDED: Revitalize physics laws submerged in data information for Traffic State Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Vh0XqOTGi": {
    "title": "GAN-based Vertical Federated Learning for Label Protection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JRsAj3ymy": {
    "title": "Time-Sensitive Replay for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bO1UP57GAw": {
    "title": "Dataset Distillation via Adversarial Prediction Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftdtqEiTXZ": {
    "title": "Pay attention to cycle for spatio-temporal graph neural network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i6EtCiIK4a": {
    "title": "Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jz35igczhm": {
    "title": "Brain-inspired $L_p$-Convolution benefits large kernels and aligns better with visual cortex",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHZm9vNm5H": {
    "title": "Efficient ConvBN Blocks for Transfer Learning and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7rex8lEZH2": {
    "title": "Prompt Tuning with Diffusion for Few-Shot Pre-trained Policy Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TrKq4Wlwcz": {
    "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2cRzmWXK9N": {
    "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z8TW0ttBPp": {
    "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2DDwxbjP9g": {
    "title": "In Defence Of Wasserstein",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c8McWs4Av0": {
    "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CeJEfNKstt": {
    "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwDuW7Ok5f": {
    "title": "Dual Associated Encoder for Face Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lYy9zPOxXS": {
    "title": "Topology-Informed Graph Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8FhwHJGUPZ": {
    "title": "Dual-Balancing for Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tz6HnhBzLl": {
    "title": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RUvzlotXY0": {
    "title": "HiCBridge: Resolution Enhancement of Hi-C Data Using Direct Diffusion Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ISrxxvXJQO": {
    "title": "On the Hidden Waves of Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkgULK8u4d": {
    "title": "MGTST: Multi-scale and Cross-channel Gated Transformer for Multivariate long-term time-series forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8dkp41et6U": {
    "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J88EKENxyF": {
    "title": "CAT-LLM: Context-Aware Training enhanced Large Language Models for multi-modal contextual image retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sp666x6Gh3": {
    "title": "Density-Softmax: Efficient Test-time Model for Uncertainty Estimation and Robustness under Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4XCfu7fTgw": {
    "title": "Spectral Contrastive Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VqEE9i6jhE": {
    "title": "Tensor methods to learn the Green's function to solve high-dimensional PDE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUiYbVqcuQ": {
    "title": "A2FC: A FEDERATED ADVANTAGE ACTOR-CRITIC LEARNING APPROACH FOR HETEROGENEOUS ACTION SPACES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5webNFDgQ": {
    "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N8UGyR3HTI": {
    "title": "FragSel: Fragmented Selection for Noisy Label Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gXfKPj4s7C": {
    "title": "Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DDX1u29Gqr": {
    "title": "DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuVlUn4T2G": {
    "title": "Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g6rZtxaXRm": {
    "title": "Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VfPWJM5FMr": {
    "title": "ColA: Collaborative Adaptation with Gradient Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=53kW6e1uNN": {
    "title": "AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hCaoJh01io": {
    "title": "InfoGround: Ground Manipulation Concepts with Maximal Information Boost",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aFWUY3E7ws": {
    "title": "Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYmF38ysDa": {
    "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsn1lPgDvU": {
    "title": "STABLE ESTIMATION OF SURVIVAL CAUSAL EFFECTS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itrOA1adPn": {
    "title": "A computational approach to visual ecology with deep reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60e1hl06Ec": {
    "title": "Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWyZeqE928": {
    "title": "Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KpoQSgxbKH": {
    "title": "Generative Pre-training for Speech with Flow Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vbqxaiHGmL": {
    "title": "Generative and Explainable Data Augmentation for Single-Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bn8iWvRSmq": {
    "title": "Successor Features for Efficient Multi-Subject Controlled Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qg2boc2AwU": {
    "title": "Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iad1yyyGme": {
    "title": "CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UQVhOVhUi4": {
    "title": "Graph Generation with Destination-Predicting Diffusion Mixture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mDIXfHvoqH": {
    "title": "ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FGoq622oqY": {
    "title": "BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZLSdwjDevK": {
    "title": "Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AXbN2qMNiW": {
    "title": "Protein-ligand binding representation learning from fine-grained interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEuTWBfVoe": {
    "title": "Model Based Inference of Synaptic Plasticity Rules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7FeIRqCedv": {
    "title": "SLiMe: Segment Like Me",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3KNrmW6Ql": {
    "title": "Adversarial Attacks on Fairness of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rp0EdI8X4e": {
    "title": "Faithful Vision-Language Interpretation via Concept Bottleneck Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HMe5CJv9dQ": {
    "title": "Efficiently Computing Similarities to Private Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQZicPtADC": {
    "title": "The Role of Representation Transfer in Multitask Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=liKkG1zcWq": {
    "title": "Sliced Denoising: A Physics-Informed Molecular Pre-Training Method",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bCNYFOaWsy": {
    "title": "Class-Imbalanced Graph Learning without Class Rebalancing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNzpTSuuVJ": {
    "title": "Every Mistake Counts: Spatial and Temporal Beliefs for Mistake Detection in Assembly Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RauUgiw7VX": {
    "title": "Fine-grained Text-to-Image Synthesis with Semantic Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UIBysXjVq": {
    "title": "Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrMnLl9RCl": {
    "title": "Improving Generalization for Missing Data Imputation via Dual Corruption Denoising Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hD3sGVqPsr": {
    "title": "P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FMLyPgqos1": {
    "title": "Graph-Relational Federated Learning: Enhanced Personalization and Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXk9gcKhqp": {
    "title": "Rethinking the Polynomial Filter of GNNs via Graph Information Activation Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xmQuUqSynb": {
    "title": "Rethinking Adversarial Robustness in the Context of the Right to be Forgotten",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5VElAKt2s": {
    "title": "LoRA ensembles for large language model fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lajn1iROCu": {
    "title": "SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOfDKxfwt0": {
    "title": "RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOveLu4O51": {
    "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BE5aK0ETbp": {
    "title": "A Unified and General Framework for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CZiY6OLktd": {
    "title": "MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7pWRLDBAtc": {
    "title": "Heterogeneous Personalized Federated Learning by Local-Global Updates Mixing via Convergence Rate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYhDcQudVI": {
    "title": "Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEHGSN8Hy8": {
    "title": "SetCSE: Set Operations using Contrastive Learning of Sentence Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNzBTVuMgq": {
    "title": "Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hmv1LpNfXa": {
    "title": "Polynormer: Polynomial-Expressive Graph Transformer in Linear Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pWSL8My6B": {
    "title": "Where We Have Arrived in Proving the Emergence of Sparse Interaction Primitives in AI Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vwlryNhWp7": {
    "title": "Improving Discriminative Multi-Modal Learning with Large-Scale Pre-Trained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=joMMM9eadc": {
    "title": "Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N3DEoB9fIQ": {
    "title": "Debiased Machine Learning and Network Cohesion for Doubly-Robust Differential Reward Models in Contextual Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xA25Ib7H8U": {
    "title": "Understanding Continuous-depth Networks through the Lens of Homogeneous Ricci Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7oU4nfKEA": {
    "title": "When Is Multilinguality a Curse? Language Modeling for 252 High- and Low-Resource Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VuCRpguZOr": {
    "title": "Gaussian Mutual Information Maximization for Graph Self-supervised Learning: Bridging Contrastive-based to Decorrelation-based",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zrr6kH1cSh": {
    "title": "AdaSR: Adaptive Super Resolution for Cross Platform and Dynamic Runtime Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1pNNQSzZv": {
    "title": "Rational Decision-Making Agent with Internalized Utility Judgment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yZJapMWdHZ": {
    "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ROC3UASRV7": {
    "title": "A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1U6sEgYkb": {
    "title": "DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jNR6s6OSBT": {
    "title": "ASID: Active Exploration for System Identification and Reconstruction in Robotic Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DIuSX4HqDZ": {
    "title": "Abductive Logical Reasoning on Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kuTZMZdCPZ": {
    "title": "Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ur4LqAOXIF": {
    "title": "SODA: Stream Out-of-Distribution Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cf8HBieRzL": {
    "title": "UniContact:A Basic Model for Robotic Manipulation of Contact Synthesis on Rigid and Articulated Rigid Bodies with Arbitrary Manipulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z7OWaSze0V": {
    "title": "Unifying User Preferences and Critic Opinions: A Multi-View Cross-Domain Item-sharing Recommender System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g5TIh84amg": {
    "title": "A Curriculum View of Robust Loss Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UmMa3UNDAz": {
    "title": "EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YcJCzJzQT5": {
    "title": "DipDNN: Decomposed Invertible Pathway Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=St7aZgQJBf": {
    "title": "Curriculum metric learning for robust image retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zKgrmMOQjg": {
    "title": "TCD: TEXT IMAGE CHANGE DETECTION FOR MULTILINGUAL DOCUMENT COMPARISON",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhCdJ93Wmi": {
    "title": "Graph Inference Acceleration by Bridging GNNs and MLPs with Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8fQlGQkj0S": {
    "title": "A Theoretical Analysis of In-context Task Retrieval and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RNGUbTYSjk": {
    "title": "Weaker MVI Condition: Extragradient Methods with Multi-Step Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3TO3TtnOFl": {
    "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AVBw2Ul4X9": {
    "title": "Towards Precise Prediction Uncertainty in GNNs: Refining GNNs with Topology-grouping Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qoYogklIPz": {
    "title": "Demystifying Embedding Spaces using Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=likXVjmh3E": {
    "title": "The Expressive Power of Low-Rank Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QlqdXrzzD1": {
    "title": "Towards Category Unification of 3D Single Object Tracking on Point Clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0lW9cDUtf8": {
    "title": "FairReweighing: density estimation-based reweighing framework for improving separation in fair regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7WsivwyHrS": {
    "title": "You Only Query Once: An Efficient Label-Only Membership Inference Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QtywskEyY": {
    "title": "Teaching wiser, Learning smarter: Multi-stage Decoupled Relational Knowledge Distillation with Adaptive Stage Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y5e9fvvBUz": {
    "title": "PRISM: Privacy-Preserving Improved Stochastic Masking For Federated Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7W4boWjb3Q": {
    "title": "Partitioned-Learned Count-Min Sketch",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0FI3Q66K5": {
    "title": "Frozen Transformers in Language Models Are Effective Visual Encoder Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HOpQt44EzC": {
    "title": "Differentially Private Vision-Language Foundation Models via Image Captioning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=udO3k28bEw": {
    "title": "Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqxBTcWRnj": {
    "title": "Bridging Neural and Symbolic Representations with Transitional Dictionary Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JW3jTjaaAB": {
    "title": "AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mxJEX6w5uN": {
    "title": "Scaff-PD: Communication Efficient Fair and Robust Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dk1ybhMrJv": {
    "title": "Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9z0HzWJJp": {
    "title": "SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gU6OqJfO0G": {
    "title": "ON LEARNABILITY AND EXPERIENCE REPLAY METHODS FOR GRAPH INCREMENTAL LEARNING ON EVOLVING GRAPHS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Km3Kprwyua": {
    "title": "Online Speculative Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpyFAhH9CK": {
    "title": "Morphological Maze: Control Reconfigurable Soft Robots with Fine-grained Morphology Change",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjXor87Xfy": {
    "title": "PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cgCKm5DOnu": {
    "title": "ROSA: Random Orthogonal Subspace Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5RUf9nEdyC": {
    "title": "TEDDY: Trimming Edges with Degree-based Graph Diffusion Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NtQqIcSbqv": {
    "title": "Learning to Jointly Understand Visual and Tactile Signals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60lNoatp7u": {
    "title": "NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eC4WlSZc4H": {
    "title": "Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XW0gD13oQp": {
    "title": "Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=92btneN9Wm": {
    "title": "SPDER: Semiperiodic Damping-Enabled Object Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7OO8tTOgh4": {
    "title": "Non-targeted Adversarial Attacks on Vision-Language Models via Maximizing Information Entropy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U9p10hgOpU": {
    "title": "Unsupervised Lifelong Learning with Sustained Representation Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKzGrt3m2g": {
    "title": "Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e4FG5PJ9uC": {
    "title": "The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJCIv4aUQu": {
    "title": "ADOPT: Modified Adam Can Converge with the Optimal Rate with Any Hyperparameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fxQiecl9HB": {
    "title": "Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mgq6kxl115": {
    "title": "Fast Ensembling with Diffusion Schr\\\"odinger Bridge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hP4iZU8I3Y": {
    "title": "Understanding Inter-Session Intentions via Complex Logical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UaMgmoKEBj": {
    "title": "Decoupling regularization from the action space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9V7ugVuUz": {
    "title": "Robust Similarity Learning with Difference Alignment Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51cjeYcXjs": {
    "title": "Search and Retrieval in Semantic-Structural Representations of Novel Malware",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PKICZXVY9M": {
    "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ix7rLVHXyY": {
    "title": "Learning Performance-Improving Code Edits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIuevDSK5V": {
    "title": "ConR: Contrastive Regularizer for Deep Imbalanced Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wN9HBrNPSX": {
    "title": "Enhancing Temporal Knowledge Graph Completion with Global Similarity and Weighted Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Wy6pLNQcG": {
    "title": "RegionSpot: Unleashing the Power of Frozen Foundation Models for Open-World Region Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ORHuMEwaC8": {
    "title": "The Role of Counterfactual Explanations in Model Extraction Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wd47f7HEXg": {
    "title": "Quasi-Monte Carlo for 3D Sliced Wasserstein",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LnLySuf1vp": {
    "title": "A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAhgJ1fQ4V": {
    "title": "A Multi-resolution Dataset of Self-consistent Cloth Drapes for Physics-based Upsampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l3qtSNsPvC": {
    "title": "A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XJ9vjEAqbx": {
    "title": "Adversarial Training Should Be Cast as a Non-Zero-Sum Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hQVCCxQrYN": {
    "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3fRbP8g2LT": {
    "title": "Efficient Redundancy-Free Graph Networks: Higher Expressiveness and Less Over-Squashing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uqPnesiGGi": {
    "title": "Motif-aware Attribute Masking for Molecular Graph Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QNW42cjkym": {
    "title": "A Data-Driven Solution for the Cold Start Problem in Biomedical Image Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kmnQYA8snK": {
    "title": "Scaling up Trustless DNN Inference with Zero-Knowledge Proofs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KjOAHlKMF5": {
    "title": "Cascading Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgyWXX8HCk": {
    "title": "Trustless Audits without Revealing Data or Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S5aUhpuyap": {
    "title": "Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4h1apFjO99": {
    "title": "Diffusion-TS: Interpretable Diffusion for General Time Series Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pp2j9BvpgC": {
    "title": "Attribute Recognition with Image-Conditioned Prefix Language Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWpwDdVbaM": {
    "title": "ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qbw861vueP": {
    "title": "BiDST: Dynamic Sparse Training is a Bi-Level Optimization Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ARPrtuzAnQ": {
    "title": "On the hardness of learning under symmetries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aVtQChA6WH": {
    "title": "Distributional off-policy evaluation with Bellman residual minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8j9hz8DVi8": {
    "title": "Combining Axes Preconditioners through Kronecker Approximation for Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sf2A2PUXO3": {
    "title": "Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irorVob9Eq": {
    "title": "Towards the Characterization of Representations Learned via Capsule-based Network Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=exei8zvY13": {
    "title": "Improving High-Frequency Details in Cerebellum for Brain MRI Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojIJZDNIBj": {
    "title": "Copula Conformal prediction for multi-step time series prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nc5GgFAvtk": {
    "title": "An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j3bkYuxITP": {
    "title": "Refined Partitioning Boosts MGDA: Introducing RP-MGDA for Multi-Objective Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4IT2pgc9v6": {
    "title": "One For All: Towards Training One Graph Model For All Classification Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sHEJJmzBIN": {
    "title": "Branch-GAN: Improving Text Generation with (not so) Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L6CgvBarc4": {
    "title": "Bag of Tricks to Boost Adversarial Transferability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GIUjLsDP4Z": {
    "title": "Effective Structural Encodings via Local Curvature Profiles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BydD1vNMCV": {
    "title": "Statistical Inference for Deep Learning via Stochastic Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GXtmuiVrOM": {
    "title": "Domain Randomization via Entropy Maximization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EJvFFedM2I": {
    "title": "TRAM: Benchmarking Temporal Reasoning for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgM3MzT99c": {
    "title": "OMNI: Open-endedness via Models of human Notions of Interestingness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmAzKz9DJs": {
    "title": "Centroid- and Orientation-aware Feature Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9hjVoPWPnh": {
    "title": "Machine Unlearning for Image-to-Image Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8cBsT9mcg": {
    "title": "Classification with Conceptual Safeguards",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0uI5415ry7": {
    "title": "Linear attention is (maybe) all you need (to understand Transformer optimization)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l0pPTGMqZt": {
    "title": "Domain Generalization for Domain-Linked Classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6yXAKleluj": {
    "title": "Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FUgrjq2pbB": {
    "title": "MVDream: Multi-view Diffusion for 3D Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QqdloE1QH2": {
    "title": "Multilingual Mathematical Autoformalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GaLCLvJaoF": {
    "title": "Robust Model Based Reinforcement Learning Using $\\mathcal{L}_1$ Adaptive Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f9RvYpXhFI": {
    "title": "Estimating Fréchet bounds for validating programmatic weak supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONPECq0Rk7": {
    "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZQVV6IY0OE": {
    "title": "The Implicit Bias of Stochastic AdaGrad-Norm on Separable Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wg8NPfeMF9": {
    "title": "$\\texttt{NAISR}$: A 3D Neural Additive Model for Interpretable Shape Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HhfcNgQn6p": {
    "title": "Towards a statistical theory of data selection under weak supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CvxcWCDX0h": {
    "title": "Learning Multi-Modal Representation Alignments from Noisy Data-Pairs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5rrYpa2vts": {
    "title": "EA2N: Evidence-based AMR Attention Network for Fake News Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UkLSvLqiO7": {
    "title": "The Emergence of Reproducibility and Consistency in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O9PArxKLe1": {
    "title": "Leveraging Optimization for Adaptive Attacks on Image Watermarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gx2BT0a9MQ": {
    "title": "ZeRO++: Extremely Efficient Collective Communication for Large Model Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IkmD3fKBPQ": {
    "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Psl75UCoZM": {
    "title": "Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EgASiEujt6": {
    "title": "Towards Controllable Diffusion Models via Training-Phase Guided Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUIANwOLBN": {
    "title": "Behind the Myth of Exploration in Policy Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UIGAtKp8nW": {
    "title": "MUBen: Benchmarking the Uncertainty of Molecular Representation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3QkzYBSWqL": {
    "title": "Universal Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfhG5znxzR": {
    "title": "Codebook Features: Sparse and Discrete Interpretability for Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yqIJoALgdD": {
    "title": "Towards Zero Memory Footprint Spiking Neural Network Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqiAfRT1Lq": {
    "title": "Eliciting Human Preferences with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NMPLBbjYFq": {
    "title": "Large Language Models as Rational Players in Competitive Economics Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u3dHl287oB": {
    "title": "The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dGH4kHFKFj": {
    "title": "GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgSfV6sGIn": {
    "title": "STExplainer: Global Explainability of GNNs via Frequent SubTree Mining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4Ero36Zr4": {
    "title": "Rethinking Teacher-Student Curriculum Learning under the Cooperative Mechanics of Experience",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jp3gWrMuIZ": {
    "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ny8NiVfi95": {
    "title": "Masked Audio Generative Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9wDX850jR": {
    "title": "Feature emergence via margin maximization: case studies in algebraic tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gT5hALch9z": {
    "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PsDFgTosqb": {
    "title": "Learning to Solve Bilevel Programs with Binary Tender",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4SmhpF1nO4": {
    "title": "Tabular Deep-SMOTE: A supervised autoencoder-based minority-oversampling technique for class-imbalanced tabular classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XSwxy3bojg": {
    "title": "Generating Molecular Conformer Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BZtEthuXRF": {
    "title": "Manifold Diffusion Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sAOtKKHh1i": {
    "title": "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGRDoPU0Lq": {
    "title": "Fast Explanation of RBF-Kernel SVM Models Using Activation Patterns",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G4D6jClNFl": {
    "title": "Deepfake Detection with Contrastive Learning in Curved Spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fH9eqpCcR3": {
    "title": "Multiple Physics Pretraining for Physical Surrogate Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vDArHJ68h": {
    "title": "Mastering Memory Tasks with World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T5Xb0iGCCv": {
    "title": "Neur2RO: Neural Two-Stage Robust Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PoB6QGAM38": {
    "title": "Neural Networks Decoded: Targeted and Robust Analysis of Neural Network Decisions via Causal Explanations and Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vY4iBYm9TU": {
    "title": "A Study of the Effects of Transfer Learning on Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZzQz8ikwg": {
    "title": "Efficient local linearity regularization to overcome catastrophic overfitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JORAfH2xFd": {
    "title": "On the Stability of Iterative Retraining of Generative Models on their own Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m8KWOgE0Cn": {
    "title": "FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jFJPd9kIiF": {
    "title": "Compressing Latent Space via Least Volume",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XvSLWB0kfN": {
    "title": "Cultural and Linguistic Diversity Improves Visual Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NuDmRQJ26K": {
    "title": "LUMEN-PRO: Automating Multi-Task Learning on Optical Neural Networks with Weight Sharing and Physical Rotation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HEcbGXzIHK": {
    "title": "Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jj8AAlNobk": {
    "title": "A Differentiable Sequence Model Perspective on Policy Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73dhbcXxtV": {
    "title": "LOLAMEME: LOGIC, LANGUAGE, MEMORY, MECHANISTIC FRAMEWORK",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wKB3XcQHcX": {
    "title": "Speed Limits for Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kALZASidYe": {
    "title": "Towards Enhanced Controllability of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iynRvVVAmH": {
    "title": "Parameter-Efficient Multi-Task Model Fusion with Partial Linearizeation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4znwzG92CE": {
    "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJYIyO4G1t": {
    "title": "OLGA: One-cLass Graph Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SksPFxRRiJ": {
    "title": "Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8L0pN6EOi": {
    "title": "Let's Verify Step by Step",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrslLZmkye": {
    "title": "SEE-OoD: Supervised Exploration for Enhanced Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VvAiCXwPvD": {
    "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PvyOYleymy": {
    "title": "Masked Completion via Structured Diffusion with White-Box Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p09XyFxZkc": {
    "title": "LaVie: High-Quality Video Generation with Cascaded Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlEUIydMMh": {
    "title": "Neuro-Causal Factor Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qz3mcn99cu": {
    "title": "Effectively Leveraging Capacity for Improved Deterministic Robustness Certification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jfTrsqRrpb": {
    "title": "Open-world Instance Segmentation: Top-down Learning with Bottom-up Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fYerSwf1Tb": {
    "title": "HawkesVAE: Sequential Patient Event Synthesis for Clinical Trials",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riQmzq5FaQ": {
    "title": "Reinforcement Learning with Elastic Time Steps",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ptmeLzcNyB": {
    "title": "Generalising Multi-Agent Cooperation through Task-Agnostic Communication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nHESwXvxWK": {
    "title": "Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1oijHJBRsT": {
    "title": "Self-Alignment with Instruction Backtranslation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y5kBPtU0o": {
    "title": "MEND: Meta Demonstration Distillation for Efficient and Effective In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jHdz0CIS2y": {
    "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DYIIRgwg2i": {
    "title": "The LLM Surgeon",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JEAlXPYSjC": {
    "title": "Your CLIP Model Might Be Undertrained",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6NEJ0ReNzr": {
    "title": "Learning to Plan and Generate Text with Citations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TXvaWOBuAC": {
    "title": "Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z7K2faBrDG": {
    "title": "Perceptual Measurements, Distances and Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kr7KpDm8MO": {
    "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2NwHLAffZZ": {
    "title": "Weak Correlations as the Underlying Principle for Linearization of Gradient-Based Learning Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckzglrAMsh": {
    "title": "Prompt-tuning Latent Diffusion Models for Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hP9JcXpNk": {
    "title": "Going beyond familiar features for deep anomaly detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t9dWHpGkPj": {
    "title": "Language Model Inversion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DsEhqQtfAG": {
    "title": "Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c7DND1iIgb": {
    "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmg0qMKYRQ": {
    "title": "Intriguing Properties of Generative Classifiers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HW2lIdrvPb": {
    "title": "Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qp33jnRKda": {
    "title": "Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FdVXgSJhvz": {
    "title": "Alpagasus: Training a Better Alpaca Model with Fewer Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMId7iPDOH": {
    "title": "Stylist: Style-Driven Feature Ranking for Robust Novelty Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzG4BqztV8": {
    "title": "EntProp: High Entropy Propagation via Auxiliary Batch Normalization Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=viftsX50Rt": {
    "title": "Universal Graph Random Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMbWsXPUVL": {
    "title": "LLM-Codebook for Extreme Compression of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8Zo7jACq7": {
    "title": "Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eh0Od2BJIM": {
    "title": "HyperAttention: Long-context Attention in Near-Linear Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cMQeDPwSrB": {
    "title": "Memorization Through the Lens of Curvature of Loss Function Around Samples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=31IOmrnoP4": {
    "title": "Repelling Random Walks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qnWtw3l0jb": {
    "title": "Fast Imitation via Behavior Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bozbTTWcaw": {
    "title": "Stabilizing Backpropagation Through Time to Learn Complex Physics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWwvC7As4S": {
    "title": "Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKE7YpUubu": {
    "title": "RACH-Space: Reconstructing Adaptive Convex Hull Space with applications in weak supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFyTZEqmUY": {
    "title": "Learning Interactive Real-World Simulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mJgymwRsWw": {
    "title": "Active Probabilistic Drug Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IzkgLgHt5Z": {
    "title": "Optimization and Generalizability: Fair Benchmarking for Stochastic Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vEfmVS5ywF": {
    "title": "Learning in reverse causal strategic environments with ramifications on two sided markets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C3msSjudA7": {
    "title": "ViFu: Visible Part Fusion for Multiple Scene Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPNwsJgnZJ": {
    "title": "Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VP20ZB6DHL": {
    "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJBGSVSTT2": {
    "title": "Backdoor Federated Learning by Poisoning Backdoor-Critical Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHIKtKzTj7": {
    "title": "PAPM: A Physics-aware Proxy Model for Process Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uIv5SaxXLv": {
    "title": "NeuralQP: A General Hypergraph-based Optimization Framework for Large-scale Quadratically Constrained Quadratic Programs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zSxpnKh1yS": {
    "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VkWbxFrCC8": {
    "title": "RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSBP7HzA5Z": {
    "title": "Inductive Transformers: How Large Language Models Form Concepts, And How to Make Them Even Better At It",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fk5IzauJ7F": {
    "title": "Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhwDw31DGI": {
    "title": "MIPGen: Learning to Generate Scalable MIP Instances",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G536mmC2HL": {
    "title": "TorSeq: Torsion Sequential Modeling for Molecular 3D Conformation Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=96nX9xIIx2": {
    "title": "Visual Prompting Upgrades Neural Network Sparsification: A Data-Model Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2oWRumm67L": {
    "title": "Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Small-scale Optimizer and Small Training Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDYcMtLHEr": {
    "title": "Emergent Robust Communication for Multi-Round Interactions in Noisy Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q1u25ahSuy": {
    "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iip7rt9UL3": {
    "title": "Lightweight, Pre-trained Transformers for Remote Sensing Timeseries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tvhaxkMKAn": {
    "title": "Towards Understanding Sycophancy in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GztevK7jDh": {
    "title": "Constructing Informative Subtask Representations for Multi-Agent Coordination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nfqABOIwQ": {
    "title": "RIME: Robust Preference-based Reinforcement Learning with Noisy Human Preferences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ufp0DVjRs0": {
    "title": "Feature Accentuation: Explaining 'what' features respond to in natural images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1RrOtCmuKr": {
    "title": "Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W44kiwovtC": {
    "title": "FastDCFlow: Fast and Diverse Counterfactual Explanations Using Normalizing Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jWxrIeWgir": {
    "title": "HOSC: Hyperbolic Oscillating Periodic Activations for Sharp Feature Preservation in Implicit Neural Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktdETU9JBg": {
    "title": "Towards image compression with perfect realism at ultra-low bitrates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JO7k0SJ5V6": {
    "title": "Scaling Laws of RoPE-based Extrapolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLdVl0q68X": {
    "title": "NuwaDynamics: Discovering and Updating in Causal Spatio-Temporal Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=94FKDbtTqO": {
    "title": "Rethinking the bert-like pretraining for dna sequences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6aGz0Zyvn": {
    "title": "Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=epZV1nykll": {
    "title": "Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqeRtP2T3R": {
    "title": "Embracing Diversity: Zero-shot Classification Beyond a Single Vector per Class",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vTgpSLVtyj": {
    "title": "On the Verification Complexity of Deterministic Nonsmooth Nonconvex Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDkpxG94sF": {
    "title": "V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBdE9r5XZV": {
    "title": "Factored-NeuS: Reconstructing Surfaces, Illumination, and Materials of Possibly Glossy Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riNuqYiD66": {
    "title": "A Branching Decoder for Set Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IlQxeKrWDt": {
    "title": "Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ueqTjOcuLc": {
    "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KAk6ngZ09F": {
    "title": "Data Filtering Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vDJ4tzczlG": {
    "title": "Fair Text-to-Image Diffusion via Fair Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TwBY17Hgiy": {
    "title": "Multi-task Learning with 3D-Aware Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R3CDj2DLln": {
    "title": "Disentangled Acoustic Fields For Multimodal Physical Scene Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NG7sS51zVF": {
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1M0qIxVKf6": {
    "title": "Uncovering hidden geometry in Transformers via disentangling position and context",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W7kxHxjeVm": {
    "title": "ImAD: An End-to-End Method for Unsupervised Anomaly Detection in the Presence of Missing Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZOQ9RKYJu": {
    "title": "OWL: A Large Language Model for IT Operations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPvK2e8o8M": {
    "title": "Teach Large Language Models the Concept of Meta-cognition to Reduce Hallucination Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J0cCuE3JRC": {
    "title": "Bag of Features: New Baselines for GNNs for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfFpK0JAsQ": {
    "title": "Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGk0ur4Tfr": {
    "title": "Retrieval-Based Video Language Model for Efficient Long Video Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5u7V2zD7K": {
    "title": "Temporal Spiking Generative Adversarial Networks for Heading Direction Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ARfhjGfdF": {
    "title": "Towards Control-Centric Representations in Reinforcement Learning from Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nNZzt54ZmU": {
    "title": "Rethink Depth Separation with Intra-layer Links",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZULjcYLWKe": {
    "title": "DMBP: Diffusion model based predictor for robust offline reinforcement learning against state observation perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdkGRV1vcf": {
    "title": "Generative Sliced MMD Flows with Riesz Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7J0NsFXnFd": {
    "title": "Optimal Action Abstraction for Imperfect Information Extensive-Form Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shgx0eqdw6": {
    "title": "Alignment as Reward-Guided Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiiAzqi6Ol": {
    "title": "Compositional Preference Models for Aligning LMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hESD2NJFg8": {
    "title": "Label-free Node Classification on Graphs with Large Language Models (LLMs)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IRtIHp7vsM": {
    "title": "AutoM3L: Automated Multimodal Machine Learning with Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ylhiMfpqkm": {
    "title": "Pre-Training and Fine-Tuning Generative Flow Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=StYc4hQAEi": {
    "title": "Sliced Wasserstein Estimation with Control Variates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=42lcaojZug": {
    "title": "Neural Rate Control for Learned Video Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kHTHf1XrFt": {
    "title": "MultiReAct: Multimodal Tools Augmented Reasoning-Acting Traces for Embodied Agent Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ub7YxVUQhQ": {
    "title": "Vision-based Discovery of Nonlinear Dynamics for 3D Moving Target",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHdf9F1tY4": {
    "title": "DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=am9IxubLKV": {
    "title": "Convolutions Through the Lens of Tensor Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aZH1dM3GOX": {
    "title": "Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=saj54kqrBj": {
    "title": "Self-Tuning Self-Supervised Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Th9VeE7flR": {
    "title": "Enhanced Label Propagation through Affinity Matrix Fusion for Source-Free Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kklwv4c4dI": {
    "title": "Local Composite Saddle Point Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YLJs4mKJCF": {
    "title": "Towards Poisoning Fair Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZO5cn4IfaN": {
    "title": "Efficient Distributed Training with Full Communication-Computation Overlap",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4JtwtT4nYC": {
    "title": "Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3yJ54gv3H": {
    "title": "Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFMS6wF2xq": {
    "title": "Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1op5YGZu8X": {
    "title": "Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNPcOaqC5r": {
    "title": "What's in a Prior? Learned Proximal Networks for Inverse Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkmJotfL42": {
    "title": "Fantastic Generalization Measures are Nowhere to be Found",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28gMnEAgl9": {
    "title": "Large Language Models Are Not Strong Abstract Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMoNrajk2X": {
    "title": "CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W6xD7K1ajR": {
    "title": "Efficient Precision and Recall Metrics for Assessing Generative Models using Hubness-aware Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FOlhvs5Wh4": {
    "title": "Slightly Harmonizing Certified Robust Radius and Accuracy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfTsvy05MX": {
    "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QKqWnNkwPL": {
    "title": "Self-distillation for diffusion models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOoKI3ouv1": {
    "title": "Robust agents learn causal world models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cfL8zApofK": {
    "title": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K2c04ulKXn": {
    "title": "Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CjPt1AC6w0": {
    "title": "IS SYNTHETIC DATA USEFUL FOR TRANSFER LEARNING? AN INVESTIGATION INTO DATA GENERATION, VOLUME, AND UTILIZATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJ5CF1aOOX": {
    "title": "A Self-Supervised Pre-Training Model for Time Series Classification based on Data Pre-Processing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1armpjgh8L": {
    "title": "Adaptive Hierarchical Certification for Semantic Segmentation using Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BeuTCoe3bf": {
    "title": "Subgraph-To-Node Translation for Efficient Representation Learning of Subgraphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RC2h1WQvPo": {
    "title": "Interpretable Latent Distributions Using Space-Filling Curves",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tFpqGk5hR5": {
    "title": "A Simple Open-Loop Baseline for Reinforcement Learning Locomotion Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUGhVYPVRM": {
    "title": "Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EMCXCTsmSx": {
    "title": "IRGen: Generative Modeling for Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rUf9G9k2im": {
    "title": "Image Inpainting via Iteratively Decoupled Probabilistic Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmFtICfcd8": {
    "title": "Regularized KL-Divergence for well-defined function space variational inference in BNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dhLIno8FmH": {
    "title": "Decoding Natural Images from EEG for Object Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5BXAXOpaWu": {
    "title": "Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NIouO0C0ex": {
    "title": "Open-Source Can Be Dangerous: On the Vulnerability of Value Alignment in Open-Source LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iNtEAeVQE0": {
    "title": "DISK: Domain Inference for Discovering Spurious Correlation with KL-Divergence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PXEY7ofFX": {
    "title": "Bespoke Solvers for Generative Flow Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSAgvYhZzg": {
    "title": "You Only Look at Screens: Multimodal Chain-of-Action Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wdteczB4mQ": {
    "title": "Learning to Compute Gröbner Bases",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rlCyHDzOjj": {
    "title": "A New Tensor Network: Tubal Tensor Train Network and its Applications",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49z97Y9lMq": {
    "title": "LCOT: Linear Circular Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tZ3JmSDbJM": {
    "title": "GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NZ5KXXDv1T": {
    "title": "Reinforcement Learning based Image Generation via Visual Consensus Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNktD3ib16": {
    "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7BwUyXz1f": {
    "title": "Catastrophic Negative Transfer: An Overlooked Problem in Continual Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIxhINXtQk": {
    "title": "InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=coIaBY8EVF": {
    "title": "Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nxy1bQWTG": {
    "title": "DiffEnc: Variational Diffusion with a Learned Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YCPDFfmkFr": {
    "title": "Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmzFZ9lJrD": {
    "title": "Boolformer: Symbolic Regression of Logic Functions with Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tf6nR1B8Nt": {
    "title": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzoHLiGVMo": {
    "title": "ODEFormer: Symbolic Regression of Dynamical Systems with Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8g26Yv1EOu": {
    "title": "Amortized Network Intervention to Steer the Excitatory Point Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAXmvOLtjA": {
    "title": "Diffusion World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zlkXLb3wpF": {
    "title": "Fast and unified path gradient estimators for normalizing flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SerYSFntLh": {
    "title": "Multimodal Variational Disentangled Knowledge Alignment for Cross-domain Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8rhHI6C8iC": {
    "title": "All for One and One for All: A Collaborative FL Framework for Generic Federated Learning with Personalized Plug-ins",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESt7ECoWpn": {
    "title": "Differentially Pivate Per-Instance Additive Noise Mechanism: A Game Theoretic Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H3UayAQWoE": {
    "title": "On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lzt60v45V4": {
    "title": "Variational Federated Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jRpD8VfGRf": {
    "title": "Multi-interest Disentangled Representation Learning for Multimodal Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhJDD85QHD": {
    "title": "CEIR: Concept-based Explainable Image Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gC6JTEU3jl": {
    "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=twSnZwiOIm": {
    "title": "Learning invariant representations of time-homogeneous stochastic dynamical systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nh4vQ1tGCt": {
    "title": "Forgedit: Text Guided Image Editing via Learning and Forgetting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=468KWV14ll": {
    "title": "Exploration and Anti-Exploration with Distributional Random Network Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oNkYPgnfHt": {
    "title": "Learning to Intervene on Concept Bottlenecks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ToHMTetlGr": {
    "title": "Noises are Transferable - An Empirical Study on Heterogeneous Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKinRUoBN9": {
    "title": "Investigating the Impact of Data Distribution Shifts on Cross-Modal Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUhCObWGw5": {
    "title": "PATHS: Parameter-wise Adaptive Two-Stage Training Harnessing Scene Transition Mask Adapters for Video Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4Bim1dDzb": {
    "title": "Fast Inverse Rendering by Unified Voxelization of Scene Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8utTlmhw8v": {
    "title": "Learning Nash equilibria in Rank-1 games: Going beyond the Minty Property",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k2a2aPOA4b": {
    "title": "Towards Realistic Unsupervised Fine-tuning with Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lqo5Jwfnq": {
    "title": "Class-Incremental Learning with Parameter-Efficient Cross-Task Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHtNW6xqCd": {
    "title": "Incorporating Implicit Regularization to Enhance the Transition Matrix Method for Effective Handling of Diverse Label Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EjIKerYk1O": {
    "title": "Enhancing Airside Monitoring: Multi-view Approach for Accurate Aircraft Distance-To-Touchdown Estimation in Digital Towers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WnEnU2K3Rb": {
    "title": "Beyond the Benchmark: Detecting Diverse Anomalies in Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=voVjW1PT2c": {
    "title": "Guaranteed Out-Of-Distribution Detection with Diverse Auxiliary Set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZEgj0clDp": {
    "title": "Revisiting Knowledge Tracing: A Simple and Powerful Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBZd6mCWXd": {
    "title": "WI3D: Weakly Incremental 3D Detection via Visual Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7XPZnIUHh": {
    "title": "Adversarial Learning of Decomposed Representations for Treatment Effect Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GxmltrqVNn": {
    "title": "Gated Attention Bins for Depth Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gSlLbfSOq1": {
    "title": "Temporally Equivariant Contrastive Learning for Disease Progression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lsvlvWB9vz": {
    "title": "EControl: Fast Distributed Optimization with Compression and Error Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLXpXa7iiz": {
    "title": "Convergence of Bayesian Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQ6RgKYiQq": {
    "title": "MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cG2BAbFnA4": {
    "title": "Learning with Complementary Labels Revisited: A Consistent Approach via Negative-Unlabeled Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cWdAYDLmPa": {
    "title": "State Representation Learning Using an Unbalanced Atlas",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dl0u4ODCuW": {
    "title": "Retro-fallback: retrosynthetic planning in an uncertain world",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NJyCoAIPln": {
    "title": "Branch-level Network Re-parameterization with Neural Substitution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=POFrdKvpea": {
    "title": "ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0d1gQI114C": {
    "title": "LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v5BcZzkAXg": {
    "title": "Multi-label Learning with Random Circular Vectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yrgQdA5NkI": {
    "title": "Equivariant Matrix Function Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aE6HazMgRz": {
    "title": "Spatio-temporal Twins with A Cache for Modeling Long-term System Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w327zcRpYn": {
    "title": "SUBER: An RL Environment with Simulated Human Behavior for Recommender Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nZP10evtkV": {
    "title": "Optimal transport based adversarial patch to leverage large scale attack transferability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6CfJp9NG6Q": {
    "title": "STUDY: Socially Aware Temporally Causal Decoder Recommender Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bj3jYirM37": {
    "title": "Can Agent Learn Robust Locomotion Skills without Modeling Environmental Observation Noise?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K804zYw6Wc": {
    "title": "NIR-Assisted Image Denoising: A Selective Fusion Approach and A Real-World Benchmark Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkEsEFFUyo": {
    "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GW4j4n2cjH": {
    "title": "Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICSvW69W5K": {
    "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IaKxCsJSOO": {
    "title": "Expressive Modeling is Insufficient for Offline RL: A Tractable Inference Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xxI4nAj7zi": {
    "title": "Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JfKCZQPxJ": {
    "title": "STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IhWtRwIbos": {
    "title": "Discovering Environments with XRM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kXNJ48Hvw1": {
    "title": "Accelerated Sampling with Stacked Restricted Boltzmann Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtlcdjE1K3": {
    "title": "DECOUPLE QUANTIZATION STEP AND OUTLIER-MIGRATED RECONSTRUCTION FOR PTQ",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IZVCzCWwoY": {
    "title": "Completion Consistency for Point Cloud Completion Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STxmh1ZLOI": {
    "title": "RTMPose: Real-Time Models for Multi-Person Pose Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSIVHTbZBR": {
    "title": "Image Inpainting via Tractable Steering of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=56jIlazr6a": {
    "title": "Unified Uncertainty Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WvFoJccpo8": {
    "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jzvWwv4gMx": {
    "title": "On the Paradox of Generalizable Logical Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OBITU0NAKl": {
    "title": "How Graph Neural Networks Learn: Lessons from Training Dynamics in Function Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ba84RDHFnz": {
    "title": "R-MAE: Regions Meet Masked Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qae04YACHs": {
    "title": "Transformer-Modulated Diffusion Models for Probabilistic Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOJyfhWYHf": {
    "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tQYsKBTTaV": {
    "title": "LATEC — A benchmark for large-scale attribution & attention evaluation in computer vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPIA7bgd5y": {
    "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qiOqgphnVL": {
    "title": "Democratized Diffusion Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z80CwkWXmq": {
    "title": "GETMusic: Generating Music Tracks with a Unified Representation and Diffusion Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JkMAlN3YcI": {
    "title": "How Temporal Unrolling Supports Neural Physics Simulators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oimPWHTg65": {
    "title": "TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VjAjZWJ7Fr": {
    "title": "A Graph-Theoretic Framework for Joint OOD Generalization and Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dTpbEdN9kr": {
    "title": "Human Motion Diffusion as a Generative Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjf3YenThE": {
    "title": "New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruQkcBfzpm": {
    "title": "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oj4KUNoKnf": {
    "title": "Knowledge Distillation for Closed-Source Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUuKFW7DIF": {
    "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q7XxKp2rHs": {
    "title": "SMAFace: Sample Mining Guided Adaptive Loss for Face Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PYdk0V880P": {
    "title": "Fast Neural Architecture Search with Random Neural Tangent Kernel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nZP6NgD3QY": {
    "title": "AdaMerging: Adaptive Model Merging for Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jjiOHEcS2c": {
    "title": "Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0c2qtalgG": {
    "title": "MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zsfiqpft6K": {
    "title": "Diffusion Model for Dense Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yb5KvPkKQg": {
    "title": "Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UjYeectI4p": {
    "title": "Evaluating graph generative models with graph kernels: what structural characteristics are captured?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A5nLEfjhJW": {
    "title": "SHARCS: SHARed Concept Space for\\\\Explainable Multimodal Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cMIUwcEEVw": {
    "title": "RAVL: Reach-Aware Value Learning for the Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPdZLlNXSm": {
    "title": "Mean Field Theory in Deep Metric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KUiMKRebi": {
    "title": "Implicit Neural Representation Inference for Low-Dimensional Bayesian Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4bSQ3lsfEV": {
    "title": "Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U5BZcr0H7r": {
    "title": "Multi-Armed Bandits with Abstention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bzO7cusxBl": {
    "title": "Cross-domain Few-shot Classification via Maximization Optimized Kernel Dependence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AXC9KydyZq": {
    "title": "M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3LOFL4jGp": {
    "title": "MacDC: Masking-augmented Collaborative Domain Congregation for Multi-target Domain Adaptation in Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y33lDRBgWI": {
    "title": "AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nLWiR5P3wr": {
    "title": "Input-gradient space particle inference for neural network ensembles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d4UiXAHN2W": {
    "title": "LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UbxWjq0UO2": {
    "title": "Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2pvECsmld3": {
    "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xqxG5WogN6": {
    "title": "Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yIKjkRZBrX": {
    "title": "Learning variable-length skills through Novelty-based Decision Point Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBj2Qdhgew": {
    "title": "Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qH9nrMNTIW": {
    "title": "Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VAwgL8kPvr": {
    "title": "Structural Pruning of Large Language Models via Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNkOx3IVWI": {
    "title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cNwugejbW6": {
    "title": "SoftHash: High-dimensional Hashing with A Soft Winner-Take-All Mechanism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n3z5oALWci": {
    "title": "ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlpPflNCKV": {
    "title": "Video2StyleGAN: Disentangling Local and Global Variations in a Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gp5dPMBzMH": {
    "title": "BELT-2: Bootstrapping EEG-to-Language representation alignment for multi-task brain decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cP2W2PJtBj": {
    "title": "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnVO1PvbTv": {
    "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K7KQkiHanD": {
    "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XqGsMD6KH5": {
    "title": "MultiHot Embedding: A Multiple Activation Embedding Model for Numerical Features in Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FJOKjooIj": {
    "title": "Self-supervised Heterogeneous Graph Learning: a Homogeneity and Heterogeneity Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ISRqgtjPc": {
    "title": "CoBIT: A Contrastive Bi-directional Image-Text Generation Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDfxOMJqgL": {
    "title": "CAST: Cluster-Aware Self-Training for Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=otHZ8JAIgh": {
    "title": "Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhUinwpVSQ": {
    "title": "Policy Disentangled Variational Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OHpvivXrQr": {
    "title": "Protein Multimer Structure Prediction via PPI-guided Prompt Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yen1lGns2o": {
    "title": "Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rPyHyjfwP": {
    "title": "Domain-Agnostic Molecular Generation with Self-feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8xliOUg9EW": {
    "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=exKHibougU": {
    "title": "LLM-grounded Video Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByAhXwV4bH": {
    "title": "Adversarial Data Robustness via Implicit Neural Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gusHSc09zj": {
    "title": "Discovering Mixtures of Structural Causal Models from Time Series Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dp27P5HBBt": {
    "title": "Periodicity Decoupling Framework for Long-term Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPJTQYOpNI": {
    "title": "Imitation Learning from Observation with Automatic Discount Scheduling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjzwioGLux": {
    "title": "ROBUST SPARSE AND DENSE MATCHING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1p3uQ8pzl": {
    "title": "Explore Outworld Knowledge in Large Language Models: A Case Study in Pokemon Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBNTeQ7dPP": {
    "title": "Reinforcement Learning for Control with Stability Guarantee",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a2ljjXeDcE": {
    "title": "iGraphMix: Input Graph Mixup Method for Node Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T7YV5UZKBc": {
    "title": "Neural Fine-Tuning Search for Few-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pxOUk9OHYP": {
    "title": "CutSharp: A Simple Data Augmentation Method for Learned Image Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mhgm0IXtHw": {
    "title": "Noise Map Guidance: Inversion with Spatial Context for Real Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cH3oufN8Pl": {
    "title": "Output-Domain Focused Inductive Bias on Latent Feature Clusters in Visual Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ii8idH4tH": {
    "title": "Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xsd2llWYSA": {
    "title": "GLD: Generative Latent Dynamics for Structured Motion Representation and Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqjEhvUC6H": {
    "title": "Data De-Duplication and Semantic Enhancement for Contrastive Language-Image Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0gTW5JUFTW": {
    "title": "TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tw9wemV6cb": {
    "title": "Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4sGoA7Eih8": {
    "title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Gzkhoc6YS": {
    "title": "Personalize Segment Anything Model with One Shot",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PR6RMsxuW7": {
    "title": "Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QeemQCJAdQ": {
    "title": "Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QWgUAx7nIi": {
    "title": "Contrastive Graph Autoencoder for Geometric Polygon Retrieval from Building Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmOXAa5H5Y": {
    "title": "An Empirical Study of Simplicial Representation Learning with Wasserstein Distance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZxU0q2S11": {
    "title": "Data geometry and topology dependent bounds on network widths in deep ReLU networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Z1gxuAQrA": {
    "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dteBEZiCxB": {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UrmnIDCzLA": {
    "title": "Overcoming bias towards base sessions in few-shot class-incremental learning (FSCIL)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ziDFH8TPPK": {
    "title": "Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ADSxCpCu9s": {
    "title": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O072Rc8uUy": {
    "title": "Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFR0GrzERG": {
    "title": "On Task Description of In-context Learning: A Study from Information Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bUGagbBGaY": {
    "title": "Momentum-accelerated Diffusion Process for Faster Training and Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wxJ0eXwwda": {
    "title": "Urial: Aligning Untuned LLMs with Just the 'Write' Amount of In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AfiM6F2YPY": {
    "title": "Applying language models to algebraic topology: generating simplicial cycles using multi-labeling in Wu's formula",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhZXk5Ctz2": {
    "title": "Rethinking RGB Color Representation for Image Restoration Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DNvzCsQG1D": {
    "title": "InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DgRdeJF0k7": {
    "title": "Masked Dual-Temporal Autoencoders for Semi-Supervised Time-Series Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hf17y6u9BC": {
    "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itKMOWSP6K": {
    "title": "FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal Consistent Transformer for 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2UlfvGU6rL": {
    "title": "Equivariant Graph Neural Operator for Modeling 3D Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLregLuXpn": {
    "title": "On the Analysis of GAN-based Image-to-Image Translation with Gaussian Noise Injection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1Gd2d1WXY": {
    "title": "Adaptive Resolution Residual Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwHAzbkk5z": {
    "title": "Swift Sampler: Efficient Learning of Sampler by 10 parameters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kl9CqKf7h6": {
    "title": "FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fg772k6x6U": {
    "title": "Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3a505tMjGE": {
    "title": "AVOID: Alleviating VAE's Overestimation in Unsupervised OOD Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wa6Ebn4AYL": {
    "title": "Big Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPb5AhT2Vf": {
    "title": "FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nLxH6a6Afe": {
    "title": "CITING: Large Language Models Create Curriculum for Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUGjLB0N4l": {
    "title": "Big Learning Variational Auto-Encoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9mX0AZVEet": {
    "title": "Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KksPo0zXId": {
    "title": "A Fast Framework for Post-training Structured Pruning Without Retraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3bq3jsvcQ1": {
    "title": "Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Di7xKawV7x": {
    "title": "End-to-End Neural Network Compression via $\\frac{\\ell_1}{\\ell_2}$ Regularized Latency Surrogates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OuV9ZrkQlc": {
    "title": "ImagenHub: Standardizing the evaluation of conditional image generation models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKOjkFrhSs": {
    "title": "Prompt-Guided Dynamic Network for Image Super Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCocsAF7MY": {
    "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=014CgNPAGy": {
    "title": "On the Role of Momentum in the Implicit Bias of Gradient Descent for Diagonal Linear Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H7z1gHsaZ0": {
    "title": "Staleness-based subgraph sampling for large-scale GNNs training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GEZACBPDn7": {
    "title": "KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bLKcCe7hYh": {
    "title": "UC-NERF: Neural Radiance Field for under-calibrated multi-view cameras",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y886UXPEZ0": {
    "title": "Adapting Large Language Models via Reading Comprehension",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kCnLHHtk1y": {
    "title": "Building a Special Representation for the Chinese Ancient Buildings in Diffusion models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQXX3ZV2HE": {
    "title": "Adversarial Instance Attacks for Interactions between Human and Object",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=91DFSjAva8": {
    "title": "SERA: Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f8S3aLm0Vp": {
    "title": "DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdvI91pInB": {
    "title": "Discovering Logic-Informed Intrinsic Rewards to Explain Human Policies",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbu1lIYQ19": {
    "title": "Hybrid Kernel Stein Variational Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8iTpB4RNvP": {
    "title": "Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Vw7DQqq7U": {
    "title": "LEMON: Lossless model expansion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dumkzmqTmS": {
    "title": "FUND-RELATED GRAPH REPRESENTATION FOR MARGINAL EFFECTIVENESS IN MULTI-FACTORS QUANTITATIVE STRATEGY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkDon7WpX1": {
    "title": "A Consistent Lebesgue Measure for Multi-label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ft0hSoZTVe": {
    "title": "YOLOR-Based Multi-Task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qud5pDnpzo": {
    "title": "ViP: A Differentially Private Foundation Model for Computer Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hlj6HiGJeB": {
    "title": "NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix Operations for Efficient Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Js5PJPHDyY": {
    "title": "A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5h0qf7IBZZ": {
    "title": "MiniLLM: Knowledge Distillation of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yhrWPZs6On": {
    "title": "HYBRID GRANULARITY DISTRIBUTION ESTIMATION FOR FEW-SHOT LEARNING: STATISTICS TRANSFER FROM CATEGORIES AND INSTANCES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUkcfqa6GX": {
    "title": "Spatio-Temporal Graph Learning with Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqzknNJShg": {
    "title": "FourierAugment: Frequency-Based Image Encoding for Resource-Constrained Vision Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vja3ecieXY": {
    "title": "Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlTDBZFXWp": {
    "title": "The importance of feature preprocessing for differentially private linear optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pCbCcXLzSz": {
    "title": "Maximizing Benefits under Harm Constraints: A Generalized Linear Contextual Bandit Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c1QBcYLd7f": {
    "title": "Deep graph kernel point processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vCnDyQkjg": {
    "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G32oY4Vnm8": {
    "title": "PTaRL: Prototype-based Tabular Representation Learning via Space Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lJkOCMP2aW": {
    "title": "Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eddd0YTCiq": {
    "title": "Graph-level Representation Learning with Joint-Embedding Predictive Architectures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sdev4EomdC": {
    "title": "Bridging the gap between offline and online continual learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gYa9R2Pmp8": {
    "title": "Jailbreaking Language Models at Scale via Persona Modulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IZbLMHbOQg": {
    "title": "Diffusion Models for Imperceptible and Transferable Adversarial Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bTMMNT7IdW": {
    "title": "Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vw24wtSddM": {
    "title": "Tree Cross Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPAhClBy8F": {
    "title": "Overcoming both Domain Shift and Label Shift for Referring Video Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28kAFnQZ5V": {
    "title": "TENSORIZED ATTENTION MODEL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gLARhFLE0F": {
    "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pVaMBfI2eR": {
    "title": "Dual Prompt Tuning for Domain-Aware Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J8FGCqT314": {
    "title": "D2T2: Decision Transformer with Temporal Difference via Steering Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XmkuQfWZAB": {
    "title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kIZ3S3tel6": {
    "title": "Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Po6lYYsrB4": {
    "title": "ALP: Action-Aware Embodied Learning for Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbQH9AOVd5": {
    "title": "Stable Anisotropic Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bTjokqYl5B": {
    "title": "On the Onset of Robust Overfitting in Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VIgNuQ1pY": {
    "title": "Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vE5MyzpP92": {
    "title": "Threshold-Consistent Margin Loss for Open-World Deep Metric Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5jcav5RcKw": {
    "title": "Jointly Training Large Autoregressive Multimodal Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=srsXnKPx5T": {
    "title": "Hexa: Self-Improving for Knowledge Augmented Dialogue System",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pDCublKPmG": {
    "title": "Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2V1Z0Jdmss": {
    "title": "On the Over-Memorization During Natural, Robust and Catastrophic Overfitting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJTPyCZmbj": {
    "title": "PagFormer: Polar Accumulator Grid Integrated into Transformers for Medical Image Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CF8H8MS5P8": {
    "title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A2mRcRyGdl": {
    "title": "Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ur2xmuw7w": {
    "title": "Revisiting Link Prediction: a data perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fq8tKtjACC": {
    "title": "Textbooks Are All You Need",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OWUWWr50PF": {
    "title": "Deterministic Error Bounds for Euclidean Clustering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tFYcEUlUTt": {
    "title": "Learning from the Future: Improve Long-term Mesh-based Simulation with Foresight",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2HJRwwbV3G": {
    "title": "What does the Knowledge Neuron Thesis Have to do with Knowledge?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ac7f7xL4bU": {
    "title": "Universal Clustering Bounds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JKZZxJAZ3": {
    "title": "Nonnegative Matrix Factorization through Canonical Edges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6sfRRcynDy": {
    "title": "Out-of-Distribution Detection with Hyperspherical Energy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UbOzNf6hGq": {
    "title": "FiLM: Fill-in Language Models for Any-Order Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4L0xnS4GQM": {
    "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86w3LbTNI1": {
    "title": "Preventing Reward Hacking with Occupancy Measure Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HipfLjyLUW": {
    "title": "Hierarchical GFlownet for Crystal Structure Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvj1mn8q8D": {
    "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nk8HrBad2O": {
    "title": "Task-Guided Biased Diffusion Models for Point Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FKksTayvGo": {
    "title": "Denoising Diffusion Bridge Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tSLtvkHCh": {
    "title": "Learning Temporal Causal Representation under Non-Invertible Generation Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6BIRu23ow": {
    "title": "TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SdeAPV1irk": {
    "title": "Incremental Randomized Smoothing Certification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vzvCaYFTLq": {
    "title": "Sapling: $\\underline{S}$uccessive $\\underline{A}$daptation and Com$\\underline{p}$ression with $\\underline{L}$ayer Dropp$\\underline{ing}$ for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89A5c6enfc": {
    "title": "Local Graph Clustering with Noisy Labels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICDJDL5lmQ": {
    "title": "Wasserstein Distortion: Unifying fidelity and realism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SO93f7sVf": {
    "title": "Training Neural Networks from Scratch with Parallel Low-Rank Adapters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ANJxbH4eQQ": {
    "title": "Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6J3ehSUrMU": {
    "title": "Principled Federated Domain Adaptation: Gradient Projection and Auto-Weighting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZqic2sPTY": {
    "title": "Graphpulse: Topological representations for temporal graph property prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mFTPRV5hYw": {
    "title": "Where have you been? A Study of Privacy Risk for Point-of-Interest Recommendation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbniI5RyWH": {
    "title": "SEESAW: Do Graph Neural Networks Improve Node Representation Learning for All?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xZDWO0oejD": {
    "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ay0Vjj3oyL": {
    "title": "SCOT: Improved Temporal Counterfactual Estimation with Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1uHTIjXjkk": {
    "title": "Potential Based Diffusion Motion Planning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6BXvqWWmq": {
    "title": "MOTSC: Model-based Offline Traffic Signal Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mdk7YP52V3": {
    "title": "Understanding Pathologies of Deep Heteroskedastic Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jb37oaGZXy": {
    "title": "Musketeer: Joint Training/Inference for Multi-task Vision-Language Model with Task Explanation Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gCZyD7WD0w": {
    "title": "Guided Decoupled Exploration for Offline Reinforcement Learning Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aSbJhaVDi": {
    "title": "Exploiting Open-World Data for Adaptive Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqHvfEUxGu": {
    "title": "A Collaborative Perspective on Exploration in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DqziS8DG4M": {
    "title": "Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vA5Rs9mu97": {
    "title": "Compressed Online Sinkhorn",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ceNnsnA5gu": {
    "title": "WL-Tree: a New Tool for Analyzing Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QrEHs9w5UF": {
    "title": "PRIME: Prioritizing Interpretability in Failure Mode Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=92KV9xAMhF": {
    "title": "On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u2as4lHoyl": {
    "title": "ReFACT: Updating Text-to-Image Models by Editing the Text Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jKTUlxo5zy": {
    "title": "Less is More: Fewer Interpretable Region via Submodular Subset Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EanCFCwAjM": {
    "title": "Cameras as Rays: Sparse-view Pose Estimation via Ray Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PuKRVPXXpR": {
    "title": "ResTran: A GNN Alternative To Learn Graph With Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qB0IV2DpeS": {
    "title": "Byzantine Robustness and Partial Participation Can Be Achieved Simultaneously: Just Clip Gradient Differences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mNFsbvo2P": {
    "title": "Domain constraints improve risk prediction when outcome data is missing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZZ4hhniJU": {
    "title": "Learning Multi-Agent Communication with Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dc4rXq3HIA": {
    "title": "Improving Out-of-Domain Generalization with Domain Relations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=50P9TDPEsh": {
    "title": "Critique Ability of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OatZMyMuIo": {
    "title": "Causal Representation Learning and Inference for Generalizable Cross-Domain Predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7m5jhNXklB": {
    "title": "VTruST : Controllable value function based subset selection for Data-Centric Trustworthy AI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qg5JENs0N4": {
    "title": "Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ju0VmvMCW": {
    "title": "Sample Relationship from Learning Dynamics Matters for Generalisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xvfz8NHmCj": {
    "title": "Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0fpLLsAynh": {
    "title": "Sporadicity in Decentralized Federated Learning: Theory and Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rcFXg2aqEj": {
    "title": "LMDX: Language Model-based Document Information Extraction and Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f37TVPH62h": {
    "title": "Compound Returns Reduce Variance in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fZmdNb5ThL": {
    "title": "ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8TAGx549Ns": {
    "title": "REX: Rapid Exploration and eXploitation for AI agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XlkN11Xj6J": {
    "title": "Adding 3D Geometry Control to Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pUKps5dL4s": {
    "title": "Momentum Particle Maximum Likelihood",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3NMYMLL92j": {
    "title": "Brain encoding models based on binding multiple modalities across audio, language, and vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U3ROVRTKTa": {
    "title": "Prompting-based Efficient Temporal Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sa0t0vGPDv": {
    "title": "FARS: FSM-Augmentation to Make LLMs Hallucinate the Right APIs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U96nHn0dmK": {
    "title": "Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Subnetworks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nfMyERXNru": {
    "title": "Video Decomposition Prior: Editing Videos Layer by Layer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lqwk6tNVEi": {
    "title": "Neighborhood-Informed Diffusion Model for Source-Free Domain Adaptation: Retrieving Source Ground Truth from Target Query's Neighbors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CF6gfZSCVg": {
    "title": "Anarchic Federated Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mMaQvkMzDi": {
    "title": "Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CgPs04l9TO": {
    "title": "Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uznKlCpWjV": {
    "title": "On Stationary Point Convergence of PPO-Clip",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTsHStJKcm": {
    "title": "Make a Donut: Language-Guided Hierarchical EMD-Space Planning for Zero-shot Deformable Object Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BV1PHbTJzd": {
    "title": "Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8cNMMrWRbZ": {
    "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ucMRo9IIC1": {
    "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHjigVnI04": {
    "title": "High-dimensional SGD aligns with emerging outlier eigenspaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3LLkES6nNs": {
    "title": "Infinitely Deep Residual Networks: Unveiling Wide Neural ODEs as Gaussian Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HwQ8NVvdmm": {
    "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fnBYPL5Ged": {
    "title": "CPLLM: Clinical Prediction with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UQNSe4Da29": {
    "title": "One-Hot Encoding Strikes Back: Fully Orthogonal Coordinate-Aligned Class Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcNwnuWMe0": {
    "title": "Exploiting River Network Topology for Flood Forecasting with Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwJgCIKXJS": {
    "title": "Linear Indexed Minimum Empirical Divergence Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zsfrzYWoOP": {
    "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yXklOV0ZIv": {
    "title": "Learnable Counterfactual Attention for Singer Identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcl6WeMARK": {
    "title": "Improved Regret Bounds in Stochastic Contextual Bandits with Graph Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3JoQqW35GQ": {
    "title": "Training-free Linear Image Inversion via Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLf589bx1f": {
    "title": "$\\mathcal{B}$-Coder: On Value-Based Deep Reinforcement Learning for Program Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vR5h3cAfXS": {
    "title": "On robust overfitting: adversarial training induced distribution matters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uikf2Ue0XQ": {
    "title": "Visual Grounding with attention-driven constraint balancing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fnYvczj0OU": {
    "title": "Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzT61ziSCu": {
    "title": "Automatic Functional Differentiation in JAX",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=msXxrttLOi": {
    "title": "FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wT8G45QGdV": {
    "title": "Consistent123: Improve Consistency for One Image to 3D Object Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xuKVVYxU5D": {
    "title": "Single-Trajectory Distributionally Robust Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auKAUJZMO6": {
    "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qIRkFyLZnR": {
    "title": "Robustify the Latent Space: Offline Distributionally Robust Reinforcement Learning with Linear Function Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x1ptaXpOYa": {
    "title": "ADoPD: A Large-Scale Document Page Decomposition Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BgzE4zwkFW": {
    "title": "Curriculum Reinforcement Learning via Morphology-Environment Co-Evolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29pGC6IYaL": {
    "title": "Maximizing LLMs Potential: Enhancing Mongolian Chinese Machine Translation with RL Agents and Adversarial Multi Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9x6yrFAPnx": {
    "title": "Provably Efficient CVaR RL in Low-rank MDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GqI4fTVUXC": {
    "title": "On the Disconnect Between Theory and Practice of Overparametrized Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8XgCH9y1Bs": {
    "title": "3D Object Representation Learning for Robust Classification and Pose estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MEbNz44926": {
    "title": "Flexible Residual Binarization for Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNkUeTUbSQ": {
    "title": "Understanding and Controlling a Maze-Solving Policy Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jnFcKjtUPN": {
    "title": "COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mQ72XRfYRZ": {
    "title": "A Hierarchical Bayesian Model for Few-Shot Meta Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z2avrOUajn": {
    "title": "SubDiff: Subgraph Latent Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgZUcwFhjr": {
    "title": "Can Transformers Capture Spatial Relations between Objects?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hbbus5IOYt": {
    "title": "How Does RLHF Shift Behavior Distributions? Distinguishability and Steerability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=66arKkGiFy": {
    "title": "Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1qSHSFOew": {
    "title": "CSI: Enhancing the Robustness of 3D Point Cloud Recognition against Corruption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4CLLlIaaH": {
    "title": "Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wfzXa8e783": {
    "title": "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ishA3LxN8": {
    "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H49g8rRIiF": {
    "title": "From Language to 3D Worlds: Adapting Language Models for Point Cloud Perception",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nnicaG5xiH": {
    "title": "Interpretable Meta-Learning of Physical Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDKTMjoFbC": {
    "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=84n3UwkH7b": {
    "title": "Detecting, Explaining, and Mitigating Memorization in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GH2LYb9XV0": {
    "title": "Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOWdQQgMJY": {
    "title": "Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XbydvPq92M": {
    "title": "Information-Ordered Bottlenecks for Adaptive Dimensionality Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GTk0AdOYLq": {
    "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lATusnzNRT": {
    "title": "Enhancing Graph Injection Attacks Through Over-Smoothing Amplification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaGA40pfFY": {
    "title": "Rationality of Thought Improves Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbjSwwrQOe": {
    "title": "Statistical Rejection Sampling Improves Preference Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zyBJodMrn5": {
    "title": "On the generalization capacity of neural networks during generic multimodal reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4JbrdrHxYy": {
    "title": "The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTRekADULK": {
    "title": "Sparse Training of Discrete Diffusion Models for Graph Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ijmMNaSJk": {
    "title": "Towards Understanding Masked Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RacYdzHxcz": {
    "title": "Human-Producible Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RXVYOCGO7g": {
    "title": "Mitigating Backdoor Attacks in Federated Learning through Noise-Guided Aggregation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqHgSxRwiK": {
    "title": "Test Relative Fairness in Human Decisions With Machine Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JDd46WodYf": {
    "title": "Active Procedure Planning with Uncertainty-awareness in Instructional Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UndmcWatBN": {
    "title": "Dissecting Zero-Shot Visual Reasoning Capabilities in Vision and Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=umUIYdLtvh": {
    "title": "EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zzqn5G9fjn": {
    "title": "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXLOOoR2ft": {
    "title": "DoraemonGPT: Toward Solving Real-world Tasks with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A8et2yjbly": {
    "title": "Cross-Modality Masked Pre-training for Visible-Infrared Person Re-identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QtCpQHsO1Q": {
    "title": "Alleviating Label Shift Through Self-trained Intermediate Distribution: Theory and Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAvcKbUXwb": {
    "title": "Interpreting Equivariant Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yKksu38BpM": {
    "title": "Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E2ePtpKJpy": {
    "title": "Improving Compositional Text-to-image Generation with Large Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruGY8v10mK": {
    "title": "A Data-Driven Measure of Relative Uncertainty for Misclassification Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9W6KaAcYlr": {
    "title": "Maximally discriminative stimuli for functional cell type identification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O9nZCwdGcG": {
    "title": "Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nudMydhZZW": {
    "title": "A primal-dual perspective for distributed TD-learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y9wQf6ZWS2": {
    "title": "RegQ: Convergent Q-Learning with Linear Function Approximation using Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dN1IV0Ov8a": {
    "title": "Long BERT for bankruptcy prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2CxkRDMIG4": {
    "title": "Precision and Recall Reject Curves for Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIbH5ekQpr": {
    "title": "IMP: Benchmarking Image Polysemy in Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyW37euNXb": {
    "title": "Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzG7kSpjJk": {
    "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7cYRi7NxT": {
    "title": "Hierarchical Side-Tuning for Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3BbxvAQS9": {
    "title": "DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8h2uUX22d": {
    "title": "Understanding MLP-Mixer as a wide and sparse MLP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpuQonyeXN": {
    "title": "Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sdn7ocpvuX": {
    "title": "Advective Diffusion Transformers for Topological Generalization in Graph Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UMfcdRIotC": {
    "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=30N3bNAiw3": {
    "title": "Separating common from salient patterns with Contrastive Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBCuRzjqK7": {
    "title": "Self-Supervised Contrastive Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=532tcx7IHF": {
    "title": "RLLTE: Long-Term Evolution Project of Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UHdd6EUAy7": {
    "title": "Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eUAr4HwU0X": {
    "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PjS5RnxeK": {
    "title": "On progressive sharpening, flat minima and generalisation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Q1mBvUgmt": {
    "title": "VIPER: Vibrant Period Representation for Robust and Efficient Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4PzxLPEGRn": {
    "title": "OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AqXzHRU2cs": {
    "title": "Generative Pretrained Embedding and Hierarchical Representation to Unlock Human Rhythm in Activities of Daily Living",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyMXxpBfct": {
    "title": "Forward Explanation : Why Catastrophic Forgetting Occurs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6p8lpe4MNf": {
    "title": "A Semantic Invariant Robust Watermark for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qgWJkDiI5p": {
    "title": "Fast Equilibrium of SGD in Generic Situations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qm46g9Ri15": {
    "title": "AlignCLIP: Enhancing Stable Representations in Vision-Language Pretraining Models through Attention and Prediction Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QncaLObzi": {
    "title": "Binary Hyperbolic Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izrOLJov5y": {
    "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dnn7SoD224": {
    "title": "Robust Stereo Matching by Risk Minimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PP1rudnxiW": {
    "title": "Transport meets Variational Inference: Controlled Monte Carlo Diffusions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRdEBlwUW6": {
    "title": "DAFA: Distance-Aware Fair Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fvTaoyH96Z": {
    "title": "Non-Parameterized Randomization for Environmental Generalization in Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=of2rhALq8l": {
    "title": "AffineQuant: Affine Transformation Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBw8JGBJWj": {
    "title": "Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LQDJO7txyN": {
    "title": "Prototypical Influence Function for Fully Test-time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nEmi3YIz4": {
    "title": "ProtoNMF: Turning a Black Box into a Prototype Based Interpretable Model via Non-negative Matrix Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnuSBQjgqK": {
    "title": "CIM: Constrained Intrinsic Motivation for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iqAbdT35hE": {
    "title": "Out-Of-Distribution Detection With Smooth Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WdWGe88RdX": {
    "title": "Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=33XGfHLtZg": {
    "title": "Conformal Risk Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUCgHbmO11": {
    "title": "SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dHmhoWweE": {
    "title": "Lookbehind Optimizer: k steps back, 1 step forward",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=770DetV8He": {
    "title": "RetroBridge: Modeling Retrosynthesis with Markov Bridges",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C1sQBG6Sqp": {
    "title": "Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uIKZSStON3": {
    "title": "In-context Exploration-Exploitation for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oKglS1cFdb": {
    "title": "Feature Accompaniment: Is It Feasible to Learn Out-of-Distribution Generalizable Representations with In-Distribution Data?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nanyAujl6e": {
    "title": "Out-of-Distribution Detection with Negative Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o5Bqa4o5Mi": {
    "title": "$\\pi$2vec: Policy Representation with Successor Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TVg6hlfsKa": {
    "title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gM8X6RbXkV": {
    "title": "Hierarchical Concept Discovery Models: A Concept Pyramid Scheme",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJAlw561AH": {
    "title": "The Uncertainty-Perception Tradeoff",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSOTacnSNf": {
    "title": "Multimodal Meta-learning of Implicit Neural Representations with Iterative Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9TSv6ZVhvN": {
    "title": "Improving Accelerated Federated Learning with Compression and Importance Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Svy1XoOLXj": {
    "title": "BiLoRA: A Bi-level Optimization Framework for Low-rank Adapters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGsR3MJvwS": {
    "title": "Generalizable Deep RL-Based TSP Solver via Approximate Invariance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vjHCyOWc7h": {
    "title": "Mixture Stochastic Block Model for Multi-Group Community Detection in Multiplex Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zYXFMeHRtO": {
    "title": "FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SKulT2VX9p": {
    "title": "Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qrGjFJVl3m": {
    "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1yvEwYfK9": {
    "title": "Learning Label Shift Correction for Test-Agnostic Long-Tailed Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c2R7ajodcI": {
    "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pw2ssoOTpo": {
    "title": "CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJKlmCpOQ7": {
    "title": "Removing Multiple Shortcuts through the Lens of Multi-task Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43WKxTuJxu": {
    "title": "Orthogonal Function Representations for Continuous Armed Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJvXNpvOdM": {
    "title": "TASK PLANNING FOR VISUAL ROOM REARRANGEMENT UNDER PARTIAL OBSERVABILITY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v9Sfo2hMJl": {
    "title": "Rethinking the Temporal Modeling for Time Series Forecasting with Hybrid Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eSr9iK1z8n": {
    "title": "Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=83w0LPowHz": {
    "title": "On Reconstructability of Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E34AlVLN0v": {
    "title": "Parallelizing non-linear sequential models over the sequence length",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQ6rnDriHj": {
    "title": "Unifying Diverse Decision-Making Scenarios with Learned Discrete Actions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NW2s5XXwXU": {
    "title": "Long-tailed Diffusion Models with Oriented Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=82A2EfMu3e": {
    "title": "Efficient Discrete Physics-informed Neural Networks for Solving Evolutionary Partial Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktiikNTgK5": {
    "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LtuRgL03pI": {
    "title": "InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJeYtRuguR": {
    "title": "A Simple Romance Between Multi-Exit Vision Transformer and Token Reduction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6FvBXs8t8K": {
    "title": "Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RUKTunGWJe": {
    "title": "INRSTEG: FLEXIBLE CROSS-MODAL LARGE CAPACITY STEGANOGRAPHY VIA IMPLICIT REPRESENTATIONS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DrhZneqz4n": {
    "title": "Single Motion Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jOm5p3q7c7": {
    "title": "Optimal Sample Complexity for Average Reward Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XrunSYwoLr": {
    "title": "Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=II0OENWgi8": {
    "title": "Skill-Conditioned Policy Optimization with Successor Features Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Dwqu5urzs": {
    "title": "Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrjDNgAfp4": {
    "title": "SEMANTIC RHEOLOGY: THE FLOW OF IDEAS IN LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlcWvyf5rm": {
    "title": "UniBoost: Boost Zero-shot Vision-Language Tasks via Multitask Fine-tuning with Unsupervised Unimodal Pre-training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJFBMqCE4X": {
    "title": "SimVAE: Narrowing the gap between Discriminative & Generative Self-Supervised Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GstK7tITrE": {
    "title": "AniHead: Efficient and Animatable 3D Head Avatars Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sxGugrYhP9": {
    "title": "BatteryML:An Open-source platform for Machine Learning on Battery Degradation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YWOieLv40v": {
    "title": "Representation Bottleneck of Graph Neural Networks for Scientific Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1hLFLNu4uy": {
    "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JVeM7uwDwK": {
    "title": "Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4YESQqIys7": {
    "title": "NfgTransformer: Equivariant Representation Learning for Normal-form Games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNm7TNIL6O": {
    "title": "UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pszewhybU9": {
    "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JewzobRhay": {
    "title": "When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hzNVNSz8O": {
    "title": "No learning rates needed: Introducing SaLSa - Stable Armijo Line Search Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTbAGlu4jR": {
    "title": "Learning Identifiable Balanced Prognostic Score for Treatment Effect Estimation Under Limited Overlap",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fACNPcPcrs": {
    "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bGGYcvw8mp": {
    "title": "Understanding In-Context Learning from Repetitions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QdHg1SdDY2": {
    "title": "LEA: Learning Latent Embedding Alignment Model for fMRI Decoding and Encoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ndCJeysCPe": {
    "title": "Analysis of Learning a Flow-based Generative Model from Limited Sample Complexity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qpuxHL9X3d": {
    "title": "Efficient Diversified Attack: Multiple Diversification Strategies Lead to the Efficient Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWsJkH1tNi": {
    "title": "Federated Learning, Lessons from Generalization Study: Communicate Less, Learn More",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m9zWBn1Y2j": {
    "title": "Ligand Conformation Generation: from singleton to pairwise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE2e8664Sl": {
    "title": "Few-shot Hybrid Domain Adaptation of Image Generator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auguNUCto5": {
    "title": "Boosting Temporal Graph Learning From Global and Local Perspectives",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GWSIo2MzuH": {
    "title": "Rethinking Information-theoretic Generalization: Loss Entropy Induced PAC Bounds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gs8jWk0F01": {
    "title": "Deep Reinforcement Learning for Dynamic Capacitated Vehicle Routing Problem",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8RgPl5OQX": {
    "title": "Imagination Mechanism: Mesh Information Propagation for Enhancing Data Efficiency in Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2qZhSTtzU": {
    "title": "IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vePdNU3u6n": {
    "title": "Towards Robust and Efficient Cloud-Edge Model Adaptation via Selective Entropy Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFG1YmQTqi": {
    "title": "TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzFLBOFMZ2": {
    "title": "Causal Structure Learning Supervised by Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3kDP3IytM": {
    "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CanomFZssu": {
    "title": "Boosting Graph Anomaly Detection with Adaptive Message Passing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hf54sNeeBM": {
    "title": "Knowledge Accumulating Contrastive Prompt for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNyDvFQNEm": {
    "title": "Unsupervised Learning via Network-Aware Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=85gNpcUhmx": {
    "title": "Context-Aware Unsupervised Domain Adaptive Lane Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0kWd8SJq8d": {
    "title": "MINDE: Mutual Information Neural Diffusion Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DE2RMJVjgI": {
    "title": "Fine-grained Separation of Action-Background for Point-Level Temporal Action Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GtnNhtuVrc": {
    "title": "Semi-Supervised Semantic Segmentation via Marginal Contextual Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LyO8WTVTU": {
    "title": "A Teacher-Guided Framework for Graph Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MM30SJ4wAf": {
    "title": "Point Neighborhood Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BllUWdpIOA": {
    "title": "Continual Momentum Filtering on Parameter Space for Online Test-time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P895PSh41Z": {
    "title": "Relaxed State-Adversarial Offline Reinforcement Learning: A Leap Towards Robust Model-Free Policies from Historical Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=381QSrWdF2": {
    "title": "Law of Balance and Stationary Distribution of Stochastic Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oiUhQ4fDLE": {
    "title": "Mixup Your Own Pairs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MsfQ2H0lP": {
    "title": "Generative Adversarial Policy Network for Modelling Protein Complexes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnyD0Rjx2B": {
    "title": "fairret: a Framework for Differentiable Fairness Regularization Terms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gq1Zjhovjr": {
    "title": "Consistency Regularization for Domain Generalization with Logit Attribution Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=huwR9N2ea0": {
    "title": "Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J1SzMZn5lH": {
    "title": "Multi-Agent Bayesian Optimization with Coupled Black-box and Affine Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YnaGcMJQ0M": {
    "title": "Detecting Out-of-Distribution Samples via Conditional Distribution Entropy with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7vKWg2Vdrs": {
    "title": "LeBD: A Run-time Defense Against Backdoor Attack in YOLO",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIZEFyVGC9": {
    "title": "Debiasing Algorithm through Model Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7KDuQPrAF3": {
    "title": "A Foundation Model for Error Correction Codes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HL5P4H8eO2": {
    "title": "Differentiable Trajectory Optimization as a Policy Class for Reinforcement and Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qT1I15Zodx": {
    "title": "The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqh1zdXIra": {
    "title": "Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T97kxctihq": {
    "title": "Revisiting Long-term Time Series Forecasting: An Investigation on Affine Mapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n39ilTxSDY": {
    "title": "Ditto: Quantization-Aware Secure Inference of Transformers upon MPC",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uAp7YdKrlx": {
    "title": "Time Series Missing Imputation with Multivariate Radial Based Function Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qHGgNyQk31": {
    "title": "Seer: Language Instructed Video Prediction with Latent Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=30aSE3FB3L": {
    "title": "Matrix Manifold Neural Networks++",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4bLXfRd0CX": {
    "title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzxaRFVsgC": {
    "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hUs8YHAUEr": {
    "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tyIPw2m3Um": {
    "title": "Probability-dependent gradient decay in large margin softmax",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=frRDT6EOhg": {
    "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrOefpTvev": {
    "title": "Rethinking Texture Patterns in Transformer Neural NetWork for Medical Image Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dCyt9k4U6N": {
    "title": "FLNERF: 3D FACIAL LANDMARKS ESTIMATION IN NEURAL RADIANCE FIELDS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=maRYffiUpI": {
    "title": "Improving Code Style for Accurate Code Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9NYnsC4Mq": {
    "title": "Learning without Forgetting for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxD2ZCExRG": {
    "title": "HumanTOMATO: Text-aligned Whole-body Motion Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ul6EYKM1Kv": {
    "title": "Cognition-Supervised Learning: Contrasting EEG Signals and Visual Stimuli For Saliency Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6MRm3G4NiU": {
    "title": "SaProt: Protein Language Modeling with Structure-aware Vocabulary",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eAKmQPe3m1": {
    "title": "PixArt-$\\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3ch3kJL7q": {
    "title": "Sentence-level Prompts Benefit Composed Image Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VXak3CZZGC": {
    "title": "Provable Out-of-Distribution Generalization in Hypersphere",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kqHxpHKMSz": {
    "title": "Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jODehvtTDx": {
    "title": "Analyzing and Improving OT-based Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kbQIWi4ZiL": {
    "title": "Unsupervised combinatorial optimization under complex conditions: Principled objectives and incremental greedy derandomization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZNMZdEQQga": {
    "title": "Transplant of Perceptrons",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b27FJxtFeY": {
    "title": "Quantum AdaBoost with Supervised Learning Guarantee",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNyOI3C7YB": {
    "title": "SEABO: A Simple Search-Based Method for Offline Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MxuFXJtVTt": {
    "title": "Hot PATE: Private Aggregation of Distributions for Diverse Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGjvMcKrrl": {
    "title": "From generalization analysis to optimization designs for state space models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGmx41FTTy": {
    "title": "Two Time-Slices Help Topological Ordering for Learning Directed Acyclic Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7F4ioiKQFT": {
    "title": "ColCLIP: Enhancing Fine-Grained Image Retrieval with Pre-trained Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ndDmZdT4g": {
    "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RtDok9eS3s": {
    "title": "Simplifying Transformer Blocks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xzRnzHUVE9": {
    "title": "Enhancing Sample Efficiency in Black-box Combinatorial Optimization via Symmetric Replay Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wtJS8YDQBc": {
    "title": "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmX0CqFSd7": {
    "title": "Compositional Generative Inverse Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0NruoU6s5Z": {
    "title": "CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DslxExr5Kn": {
    "title": "APC: Predict Global Representation From Local Observation In Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8vKknbgXxf": {
    "title": "What does automatic differentiation compute for neural networks?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrwIZVSc7b": {
    "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cINwAhrgLf": {
    "title": "Free Lunches in Auxiliary Learning: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juE0rWGCJW": {
    "title": "ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jLnygpRFYm": {
    "title": "Predicting masked tokens in stochastic locations improves masked image modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tm8s3696Ox": {
    "title": "Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aOnUe8ah7j": {
    "title": "Symbol as Points: Panoptic Symbol Spotting via Point-based Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wE1I9IGqeH": {
    "title": "Continual Learning in Open-vocabulary Classification with Complementary Memory Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZUzOKE6og": {
    "title": "HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43flsheS4s": {
    "title": "Improving Robustness and Accuracy with Retrospective Online Adversarial Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fCeUoDr9Tq": {
    "title": "Zero-Shot Robustification of Zero-Shot Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZQW7eutCv": {
    "title": "pEBR: A Probabilistic Approach to Embedding Based Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PmJoRfdaK": {
    "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNY3HiaF0J": {
    "title": "MoLE: Human-centric Text-to-image Diffusion with Mixture of Low-rank Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UEdS2lIgfY": {
    "title": "In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBoRhRCzM3": {
    "title": "THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DY6uhcv4Xm": {
    "title": "FedSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JfcLYCqOkQ": {
    "title": "Conditional MAE: An Empirical Study of Multiple Masking in Masked Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=82Mc5ilInM": {
    "title": "FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YhT1ZemZow": {
    "title": "Sobolev acceleration for neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GruDNzQ4ux": {
    "title": "DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Wuvhh0LYW": {
    "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qCyhvr0GG8": {
    "title": "VONET: ADVANCING UNSUPERVISED VIDEO OBJECT LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WXXuORQwbQ": {
    "title": "Sparse Mask Representation for Human-Scene Interaction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yONJt6nFc3": {
    "title": "Node Duplication Improves Cold-start Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UX9lljSZqX": {
    "title": "Unified Static and Dynamic: Temporal Filtering Network for Efficient Video Grounding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rt6btdXS2b": {
    "title": "Continuous Indeterminate Probability Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HoY24hOeVP": {
    "title": "Efficient Personalized Text-to-image Generation by Leveraging Textual Subspace",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEkFq4RUCX": {
    "title": "Directional Distance Field for Modeling the Difference between 3D Point Clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9KVT1e1qf7": {
    "title": "LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1d2cLKeNgY": {
    "title": "ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTDyN8yajn": {
    "title": "Octavius: Mitigating Task Interference in MLLMs via MoE",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mWT3Ftkc3e": {
    "title": "Sampling is as easy as keeping the consistency: convergence guarantee for Consistency Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNTexdca08": {
    "title": "P2P: Transforming from Point Supervision to Explicit Visual Prompt for Object Detection and Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WIAO4vbnNV": {
    "title": "Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kKmi2UTlBN": {
    "title": "Cosine Similarity Knowledge Distillation for Individual Class Information Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xz13DtbOVW": {
    "title": "Balancing Act: Sparse Models with Constrained Disparate Impact",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2OwSqvxjP2": {
    "title": "Boosting Semi-Supervised Learning via Variational Confidence Calibration and Unlabeled Sample Elimination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQ5eVDIMu4": {
    "title": "Distribution Shift Resilient GNN via Mixture of Aligned Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykhRO1mAg3": {
    "title": "FPTQ: FINE-GRAINED POST-TRAINING QUANTIZATION FOR LARGE LANGUAGE MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ROuKblmi7": {
    "title": "NECO: NEural Collapse Based Out-of-distribution detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHasEfGsXL": {
    "title": "LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HC0msxE3sf": {
    "title": "Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMTa1tcn7V": {
    "title": "3D Point Cloud Sequences as 2D Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3WB5hT27zf": {
    "title": "Partial Optimal Transport for Open-set Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9grjdFDiAj": {
    "title": "Probabilistic Stability of Stochastic Gradient Descent",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4wB3HA3dJ": {
    "title": "Domain-Inspired Sharpness Aware Minimization Under Domain Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A1z0JnxnGp": {
    "title": "Power Characterization of Noisy Quantum Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Nui91LBQS": {
    "title": "Planting a SEED of Vision in Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1FLTvgyAh": {
    "title": "Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KC58bVmxyN": {
    "title": "A Cognitive Model for Learning Abstract Relational Structures from Memory-based Decision-Making Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ARFRZh6pzI": {
    "title": "Tuning-Free Accountable Intervention for LLM Deployment - A Metacognitive Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=koYsgfEwCQ": {
    "title": "DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phWkgFXvdG": {
    "title": "Task Regularized Hybrid Knowledge Distillation For Incremental Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6u6GjS0vKZ": {
    "title": "Coloring Deep CNN Layers with Activation Hue Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tuh4nZVb0g": {
    "title": "TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0IOMStUQ8": {
    "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2msbbX3ydD": {
    "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t4HiVGGzML": {
    "title": "Search-Adaptor: Text Embedding Customization for Information Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QMkYEau02q": {
    "title": "Physics-Guided Learning of Meteorological Dynamics for Weather Forecasting and Downscaling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYo9r0CwUf": {
    "title": "Continuously Volumetric Rendering with Neural Density-Distance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mtlt3RQTXJ": {
    "title": "Bi-level Contrastive Learning for Knowledge Enhanced Molecule Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1V1QQYARmd": {
    "title": "Nearest neighbor-based out-of-distribution detection via label smoothing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vstaHBy5N4": {
    "title": "Communication Efficient Federated Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZA9XUTseA9": {
    "title": "On the Implicit Bias of Adam",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jE6VXUhxq9": {
    "title": "On Causal Discovery in the Presence of Deterministic Relations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmw3Jy8MVF": {
    "title": "Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Mo7v69otj": {
    "title": "Pooling Image Datasets with Multiple Covariate Shift and Imbalance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gn0mIhQGNM": {
    "title": "SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zKtaqxLhW": {
    "title": "Generalized Knowledge Distillation for Auto-regressive Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apXtolxDaJ": {
    "title": "Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXjXPQU3yJ": {
    "title": "Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZL6yd6N1S2": {
    "title": "A Stochastic Centering Framework for Improving Calibration in Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rpP1eWWgOs": {
    "title": "Surface Representation in LiDAR Scenes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wTRpjTO3F7": {
    "title": "Quantifying Zero-shot Coordination Capability with Behavior Preferring Partners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thFwKIRqmG": {
    "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Buvbx3xRdu": {
    "title": "VideoClusterNet: Self-Supervised and Adaptive Face Clustering for Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a7eIuzEh2R": {
    "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhnGhO4VfF": {
    "title": "Towards Understanding the Effect of Pretraining Label Granularity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MIEnYtlGyv": {
    "title": "Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JLulsRraDc": {
    "title": "Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUH5liW3c1": {
    "title": "When Hard Negative Sampling Meets Supervised Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZo6pDtDZr": {
    "title": "Near-optimal algorithms for private estimation and sequential testing of collision probability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89XNDtqhpL": {
    "title": "MatFormer: Nested Transformer for Elastic Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZ3S17EI0o": {
    "title": "Set Learning for Accurate and Calibrated Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5iENGLEJKG": {
    "title": "Interpreting and Controlling Vision Foundation Models via Text Explanations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KOZu91CzbK": {
    "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z59Rb5bPPP": {
    "title": "Trajeglish: Learning the Language of Driving Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l4k0MJuO9D": {
    "title": "Network calibration under domain shift based on estimating the target domain accuracy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UyGWafcopT": {
    "title": "Meaning Representations from Trajectories in Autoregressive Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHPjs3ivdV": {
    "title": "Is margin all you need? An extensive empirical study of deep active learning on tabular data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JZC8cEmMWY": {
    "title": "How Does Message Passing Improve Collaborative Filtering?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8w6FzR68DS": {
    "title": "PriViT: Vision Transformers for Fast Private Inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s1ByDEbpI8": {
    "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xsts7MRLey": {
    "title": "DEEP UNSUPERVISED DOMAIN ADAPTATION FOR TIME SERIES CLASSIFICATION: A BENCHMARK",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LJWizuuBUy": {
    "title": "Robust Network Pruning With Sparse Entropic Wasserstein Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ag3o2T51Ht": {
    "title": "Circumventing Concept Erasure Methods For Text-To-Image Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5t44vPlv9x": {
    "title": "Pose Modulated Avatars from Video",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XyB4VvF01X": {
    "title": "Graph2Tac: Learning hierarchical representations of math concepts in theorem proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ixP76Y33y1": {
    "title": "The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FBDtqWXfuq": {
    "title": "Exploring Modality Collaboration with Modality-Agnostic Transformers in Multi-Modal Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZ3syNYe7D": {
    "title": "PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Phicg0WAg": {
    "title": "FlexCap: Generating Rich, Localized, and Flexible Captions in Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e4wgZqF9uG": {
    "title": "On the Viability of Monocular Depth Pre-training for Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rQRDt8F2Yh": {
    "title": "A Discrete and Variational Approach to Speech Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Of2RhzJ8UJ": {
    "title": "Trust Regions for Explanations via Black-Box Probabilistic Certification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8iojQVLLWb": {
    "title": "Bayesian Knowledge Distillation for Online Action Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BnQY9XiRAS": {
    "title": "Complete and Efficient Graph Transformers for Crystal Material Property Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NL6bspkWft": {
    "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgSRPRz8cI": {
    "title": "Patched Denoising Diffusion Models For High-Resolution Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCRZq5nNZu": {
    "title": "Chunking: Forgetting Matters in Continual Learning even without Changing Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W478nWXfwO": {
    "title": "What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgeVptDYAt": {
    "title": "Towards Causal Foundation Model: on Duality between Causal Inference and Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AiPfnM73h": {
    "title": "Projected Off-Policy Q-Learning (POP-QL) for Stabilizing Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3TQxxuquZ": {
    "title": "One-stage Prompt-based Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5E1HnzEBSf": {
    "title": "Local Superior Soups: A Catalyst for Reducing Communication Rounds in Federated Learning with Pre-trained Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjfXcDgvzk": {
    "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQPAAXRswY": {
    "title": "Stabilizing Policy Gradients for Stochastic Differential Equations by enforcing Consistency with Perturbation Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YD0GQBOFFZ": {
    "title": "Structured Evaluation of Synthetic Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a8VETFwcVR": {
    "title": "Unveiling Options with Neural Network Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBYDP46s5N": {
    "title": "Policy Gradient without Boostrapping via Truncated Value Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IZMPWmcS3H": {
    "title": "HIFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JgqftqZQZ7": {
    "title": "FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k0RQHNulm7": {
    "title": "Generalizable Cross-Modality Distillation with Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k9SVcrmXL8": {
    "title": "BECLR: Batch Enhanced Contrastive Unsupervised Few-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5tfWyeQI2": {
    "title": "Symbolic equation solving via reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jlEjB8MVGa": {
    "title": "How Does Wild Data Provably Help OOD Detection?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbVZt3aN0b": {
    "title": "Modeling Annotation Delay In Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DiG14qg4ok": {
    "title": "Low-coherence Subspace Projection: Enhance the Learning Capacity of Orthogonal Projection Methods on Long Task Sequences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LemSSn8htt": {
    "title": "Delta-AI: Local objectives for amortized inference in sparse graphical models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZxsKRuP0o8": {
    "title": "Meta-Tasks: Improving Robustness in Few-Shot Classification with Unsupervised and Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQ2i6jazVK": {
    "title": "Learning Implicit Representation for Reconstructing Articulated Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cPmLjxedbD": {
    "title": "A path toward primitive machine intelligence: LMM not LLM is what you need",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mDBsBB1enO": {
    "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rxlF2Zv8x0": {
    "title": "Improving protein optimization with smoothed fitness landscapes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J7ioefqDPw": {
    "title": "Rethinking Label Poisoning for GNNs: Pitfalls and Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOa3ZCtMjJ": {
    "title": "generative adversarial network with hierarchical semantic prompt constrainting clip for high-quality text-to-image synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQFDJLyJNB": {
    "title": "PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXOB15k1br": {
    "title": "Time-Series AutoAugment: Data Augmentation Policy Search for Long-Term Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nHkMm0ywWm": {
    "title": "Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v0zNCwwkaV": {
    "title": "How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0b328CMwn1": {
    "title": "Visual Prompting Reimagined: The Power of Activation Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BUDxvMRkc4": {
    "title": "BLG: BALANCED LANGUAGE DISTRIBUTION AS GUIDANCE FOR ROBUST LONG-TAILED VISION CLASSIFICATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y01KGvd9Bw": {
    "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bJ3gFiwRgi": {
    "title": "Meta Inverse Constrained Reinforcement Learning: Convergence Guarantee and Generalization Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mhb5fpA1T0": {
    "title": "Learning to Act from Actionless Videos through Dense Correspondences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Diq6urt3lS": {
    "title": "Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUCAA0XnPC": {
    "title": "Ensembler: Combating model inversion attacks using model ensemble during collaborative inference",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ouj6p4ca60": {
    "title": "Amortizing intractable inference in large language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hv8l922Ad7": {
    "title": "Correcting Flaws in Common Disentanglement Metrics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eo9dHwtTFt": {
    "title": "Combining Spatial and Temporal Abstraction in Planning for Better Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aIok3ZD9to": {
    "title": "LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJiUyzOF1m": {
    "title": "Bayesian low-rank adaptation for large language models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9rXBGpLMxV": {
    "title": "xMLP: Revolutionizing Private Inference with Exclusive Square Activation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CvYBvgEUK9": {
    "title": "On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dhxxIKhqz": {
    "title": "Function-space Parameterization of Neural Networks for Sequential Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3RRWWFaVe": {
    "title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjNGlPh8Wh": {
    "title": "The Expressive Power of Transformers with Chain of Thought",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2TFfLiTGBS": {
    "title": "DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HNOqhcua6b": {
    "title": "TransFusion: Contrastive Learning with Attention Layers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F5UgXkPgSn": {
    "title": "Fusion over the Grassmannian for High-Rank Matrix Completion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9K2ZWkYIP": {
    "title": "Scaling Laws for Sparsely-Connected Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=anG2Y15mwc": {
    "title": "Diff-Privacy: Diffusion-based Face Privacy Protection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vpV7fOFQy4": {
    "title": "Decision Transformer is a Robust Contender for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UU9Icwbhin": {
    "title": "Retentive Network: A Successor to Transformer for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FQepisCUWu": {
    "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ms0VgzSGF2": {
    "title": "Bridging State and History Representations: Understanding Self-Predictive RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r5njV3BsuD": {
    "title": "Linear Convergence Bounds for Diffusion Models via Stochastic Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OeH6Fdhv7q": {
    "title": "TapMo: Shape-aware Motion Generation of Skeleton-free Characters",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hss35aoQ1Y": {
    "title": "InstructDET: Diversifying Referring Object Detection with Generalized Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nfu3bUkmdH": {
    "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OEL4FJMg1b": {
    "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dgc5RWZwTR": {
    "title": "Efficient Training of Multi-task Combinarotial Neural Solver with Multi-armed Bandits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G5Fo7H6dqE": {
    "title": "Iterated Deep $Q$-Network: Efficient Learning of Bellman Iterations for Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RcANissyP4": {
    "title": "SelfEval: Leveraging the discriminative nature of generative models for evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pETSfWMUzy": {
    "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pPh9p8anUi": {
    "title": "PBADet: A One-Stage Anchor-Free Approach for Part-Body Association",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FWsGuAFn3n": {
    "title": "Prompt-based 3D Molecular Diffusion Models for Structure-based Drug Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILqA09Oeq2": {
    "title": "Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wcaE4Dfgt8": {
    "title": "Uni3D: Exploring Unified 3D Representation at Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWjPRRyiqm": {
    "title": "EZ-CLIP: EFFICIENT ZERO-SHOT VIDEO ACTION RECOGNITION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jsWCmrsHHs": {
    "title": "Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VgPmCLQke7": {
    "title": "Training-time Neuron Alignment for Improving Linear Mode Connectivity and Model Fusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0aEUd9UtiA": {
    "title": "DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hORCalGn3Z": {
    "title": "Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qVBTlo0T4s": {
    "title": "AutoNeRF: Training Implicit Scene Representations with Autonomous Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOSYMHfENq": {
    "title": "Batch normalization is sufficient for universal function approximation in CNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YN4uWzcbtt": {
    "title": "On the Positive Definiteness of the Neural Tangent Kernel",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lDbjooxLkD": {
    "title": "Unlock Predictable Scaling from Emergent Abilities",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vuK8MhVtuu": {
    "title": "GRAPH-CONSTRAINED DIFFUSION FOR END-TO-END PATH PLANNING",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=10fsmnw6aD": {
    "title": "How Out-of-Distribution important is",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pc8AU1aF5e": {
    "title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7D9X2cFnt1": {
    "title": "Elastic Feature Consolidation For Cold Start Exemplar-Free Incremental Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1YIpFkPSf": {
    "title": "Hyperbolic Visual-Semantic Alignment for Structural Visual Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HANfmG6tQK": {
    "title": "REVISITING LARS FOR LARGE BATCH TRAINING GENERALIZATION OF NEURAL NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YcW8i9VCf5": {
    "title": "Adversarial Causal Bayesian Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LhNZqkuVte": {
    "title": "HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxt2Auexc3": {
    "title": "Editing Personality for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktG8Tun1Cy": {
    "title": "Text-to-3D with Classifier Score Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ShQrnAsbPI": {
    "title": "Accurate Forgetting for Heterogeneous Federated Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3GVrWRKuB": {
    "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QYovwMLF7p": {
    "title": "ProFITi: Probabilistic Forecasting of Irregular Time Series via Conditional Flows",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ujX2l7mNX6": {
    "title": "MindGPT: Interpreting What You See with Non-invasive Brain Recordings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTN8dV2pL8": {
    "title": "GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CJPzLnQvIr": {
    "title": "QuickDrop: Efficient Federated Unlearning by Integrated Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eBeECjacpw": {
    "title": "PORF: POSE RESIDUAL FIELD FOR ACCURATE NEURAL SURFACE RECONSTRUCTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O2jyuo89CK": {
    "title": "Modelling complex vector drawings with stroke-clouds",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6H4RBi7RH": {
    "title": "Spurious Feature Diversification Improves Out-of-distribution Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28L2FCtMWq": {
    "title": "Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RB0RQ3XkYB": {
    "title": "Harmonized Learning with Concurrent Arbitration: A Brain-inspired Motion Planning Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sw0O2ESxbf": {
    "title": "Collapsing the Learning: Crafting Broadly Transferable Unlearnable Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UBVNwD3hPN": {
    "title": "CivRealm: A Learning and Reasoning Odyssey for Decision-Making Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mz8owj4DXu": {
    "title": "Scalable Language Model with Generalized Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vRyp2dhEQp": {
    "title": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7CLvyZ6Xn7": {
    "title": "Cross-domain Adaptation for Few-shot 3D Shape Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKgRwNhI9R": {
    "title": "Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZneJ55mRO": {
    "title": "G$^2$N$^2$ : Weisfeiler and Lehman go grammatical",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2Ji0Bzd4g": {
    "title": "Lightweight Image Super-Resolution via Flexible Meta Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aNuQyV30Yw": {
    "title": "An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concepts Prompts Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=glwwbaeKm2": {
    "title": "VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JauBLBEjOy": {
    "title": "AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUzHegCq6f": {
    "title": "Polyak Parameter Ensemble: Exponential Parameter Growth Leads to Better Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JbOsMrwjZ3": {
    "title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OdGyza5FO1": {
    "title": "Motion PointNet: Solving Dynamic Capture in Point Cloud Video Human Action",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efFmBWioSc": {
    "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JQtrumvg8": {
    "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkrqCY0GhW": {
    "title": "Language Model Agents Suffer from Compositional Decision Making",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V4oQAR8uoE": {
    "title": "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=svIdLLZpsA": {
    "title": "Real-Fake: Effective Training Data Synthesis Through Distribution Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tUVG9nGzgE": {
    "title": "Learning Conditional Invariances through Non-Commutativity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e2rBzbWwGC": {
    "title": "Mitigating Label Noise on Graphs via Topological Curriculum Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cijO0f8u35": {
    "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BZkKMQ25Z7": {
    "title": "fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y4bvKRvUz5": {
    "title": "KernelWarehouse: Rethinking the Design of Dynamic Convolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXt2EP0PW1": {
    "title": "Distribution-Free Fair Federated Learning with Small Samples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cElJ9KOat3": {
    "title": "Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zHxXaYEgw": {
    "title": "LEO: Generative Latent Image Animator for Human Video Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pu3qMB9aKD": {
    "title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EXitynZhYn": {
    "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qgyF6JVmar": {
    "title": "OTMatch: Improving Semi-Supervised Learning with Optimal Transport",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOelVq8fwL": {
    "title": "Adapting LLM Agents Through Communication",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZn1Ex72Lv": {
    "title": "Block-operations: Creating an Inductive Bias to Route Data and Reuse Subnetworks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MbfAK4s61A": {
    "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WLRlL3zR7f": {
    "title": "Vibroacoustic Frequency Response Prediction with Query-based Operator Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HadkNCPhfU": {
    "title": "SEAL: Simultaneous Label Hierarchy Exploration And Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZiF1bJ9K6B": {
    "title": "Learning Coverage Paths in Unknown Environments with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFJr7okOBi": {
    "title": "NL2ProGPT: Taming Large Language Model for Conversational Protein Design",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zh2iqiOtMt": {
    "title": "Towards the Fundamental Limits of Knowledge Transfer over Finite Domains",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YIIYhSqf1L": {
    "title": "Activation Function Matters in Graph Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ale56Ya59q": {
    "title": "Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WfjJOEfAf7": {
    "title": "Information Flow in Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8FP6eJsVCv": {
    "title": "Explanation Shift: How Did the Distribution Shift Impact the Model?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cc0qk6r4Nd": {
    "title": "Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WReszdNNdP": {
    "title": "BOWLL: A DECEPTIVELY SIMPLE OPEN WORLD LIFELONG LEARNER",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVIq7jYIda": {
    "title": "Manifold Kernel Rank Reduced Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kC5i5X9xrn": {
    "title": "LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLZQTFGToA": {
    "title": "Contrastive Learning is Spectral Clustering on Similarity Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kILAd8RdzA": {
    "title": "On the Generalization and Approximation Capacities of Neural Controlled Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lnB7rTsT9Y": {
    "title": "Knowledge Transfer through Value Function for Compositional Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmShn57DRD": {
    "title": "Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NYN1b8GRGS": {
    "title": "GIM: Learning Generalizable Image Matcher From Internet Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrOmLMFYHi": {
    "title": "Voila-A: Aligning Vision-Language Models with User's Gaze Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iayEcORsGd": {
    "title": "Ultra-sparse network advantage in deep learning via Cannistraci-Hebb brain-inspired training with hyperbolic meta-deep community-layered epitopology",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5pKLogzjQP": {
    "title": "Purify Perturbative Availability Poisons via Rate-Constrained Variational Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eXrUdcxfCw": {
    "title": "Continual Test-Time Adaptation by Leveraging Source Prototypes and Exponential Moving Average Target Prototypes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JXm3QYlNPn": {
    "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tr0lPx9woF": {
    "title": "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GxCGsxiAaK": {
    "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9NqC72m31m": {
    "title": "Neural Field Classifiers via Target Encoding and Classification Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3VD4PNEt5q": {
    "title": "Fusion is Not Enough: Single Modal Attack on Fusion Models for 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZCPSC5OgD": {
    "title": "LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LIBZ7Mp0OJ": {
    "title": "Fairness Metric Impossibility: Investigating and Addressing Conflicts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXj1qjFEpQ": {
    "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yKC6Jd0CsP": {
    "title": "Vision ELECTRA: Adversarial Masked Image Modeling with Hierarchical Discriminator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IPhm01y9a9": {
    "title": "Window Attention is Bugged: How not to Interpolate Position Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uXjfOmTiDt": {
    "title": "Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SbkubNdbW": {
    "title": "Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8GmPLkO0oR": {
    "title": "NeRFuser: Diffusion Guided Multi-Task 3D Policy Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70IgE3tRbu": {
    "title": "Continuous Invariance Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnstiBOfnv": {
    "title": "Style Over Substance: Evaluation Biases for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LEYUkvdUhq": {
    "title": "ZipIt! Merging Models from Different Tasks without Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jUWktnsplU": {
    "title": "Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MN3yH2ovHb": {
    "title": "SyncDreamer: Generating Multiview-consistent Images from a Single-view Image",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKhpp9YAkO": {
    "title": "Associative Transformer is a Sparse Representation Learner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qixVmrOOmT": {
    "title": "UMMAN: UNSUPERVISED MULTI-GRAPH MERGE ADVERSARIAL NETWORK FOR DISEASE PREDICTION BASED ON INTESTINAL FLORA",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bJx4iOIOxn": {
    "title": "Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D5mJSNtUtv": {
    "title": "Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CbmAtAmQla": {
    "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLmqxkfSIw": {
    "title": "Grounding Multimodal Large Language Models to the World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Pu0H7y3gg": {
    "title": "Understanding the Initial Condensation of Convolutional Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sqRgz88TM3": {
    "title": "VFLAIR: A Research Library and Benchmark for Vertical Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATFPZbSZia": {
    "title": "Grouplane: End-to-End 3D Lane Detection with Channel-Wise Grouping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jFa5KESW65": {
    "title": "IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rnHNDihrIT": {
    "title": "Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHng2O0Jjr": {
    "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1hsVvgW0rU": {
    "title": "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PcxQgtHGj2": {
    "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ukT8oODY0": {
    "title": "Careful at Estimation and Bold at Exploration for Deterministic Policy Gradient Algorithm",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QaKdsh15T": {
    "title": "An Embodied Generalist Agent in 3D World",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vtMrbs8Zwd": {
    "title": "The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHg5GDnyq1": {
    "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=keA1Ea7v6p": {
    "title": "Federated Learning Empowered by Generative Content",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aQij7UmwTF": {
    "title": "Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FKg1N2fAFG": {
    "title": "Towards Mitigating Architecture Overfitting in Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U6Mb3CRuj8": {
    "title": "TADA: Timestep-Aware Data Augmentation for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ypXhS83Lh": {
    "title": "Robust Reinforcement Learning with Structured Adversarial Ensemble",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6RR3wU4mSZ": {
    "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OP4crhgkD": {
    "title": "Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btpgDo4u4j": {
    "title": "Efficient Planning with Latent Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i0MsmV7hYZ": {
    "title": "Advancing Counterfactual Inference through Quantile Regression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkonJu7ScD": {
    "title": "MIND: Masked and Inverse Dynamics Modeling for Data-Efficient Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2BfZMh9td4": {
    "title": "Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4YgfwJBJeQ": {
    "title": "StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rulxyXjf46": {
    "title": "Conformal Prediction via Regression-as-Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y29rdPpPu4": {
    "title": "The Logarithm Trick: achieve better long term forecast via Mean Logarithm Square Loss",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQWXWtJSda": {
    "title": "Unlocking the Potential of Knowledge Distillation: The Role of Teacher Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezscMer8L0": {
    "title": "Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ez68a5UqI": {
    "title": "Reinforcement Learning for Node Selection in Branch-and-Bound",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r92RVhnzKy": {
    "title": "Diving Deep into Regions: Exploiting Regional information Transformer for Single Image Deraining",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EPfGHb9Y68": {
    "title": "Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kCcIYc98ho": {
    "title": "Mixing Corrupted Preferences for Robust and Feedback-Efficient Preference-Based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x6u2BQ7xcq": {
    "title": "Tag2Text: Guiding Vision-Language Model via Image Tagging",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uELjxVbrqG": {
    "title": "Enhanced Face Recognition using Intra-class Incoherence Constraint",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8QfK9Dq4q0": {
    "title": "Class Incremental Learning via Likelihood Ratio Based Task Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3jXCF5dNpC": {
    "title": "Re-Reading Improves Reasoning in Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XfcZjjd0UW": {
    "title": "Rethinking the Number of Shots in Robust Model-Agnostic Meta-Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9w3iw8wDuE": {
    "title": "Entropy is not Enough for Test-time Adaptation: From the Perspective of Disentangled Factors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Z0yB8rmQ2": {
    "title": "Lyra: Orchestrating Dual Correction in Automated Theorem Proving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQbFUMKLFp": {
    "title": "Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lr69PmF2Ov": {
    "title": "Discriminatively Matched Part Tokens for Pointly Supervised Instance Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pA8Q5WiEMg": {
    "title": "Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TdhkAcXkRi": {
    "title": "Momentum Benefits Non-iid Federated Learning Simply and Provably",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHJiJ4zYOU": {
    "title": "Vector-valued Representation is the Key: A Study on Disentanglement and Compositional Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=riP5PPTnSM": {
    "title": "LLM-Oriented Retrieval Tuner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3fEKavFsnv": {
    "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGlczSBBSj": {
    "title": "SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLhS1TrjK3": {
    "title": "Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p8ujRTjEf3": {
    "title": "Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QIrYb3Vlze": {
    "title": "Isometric Representation Learning for Disentangled Latent Space of Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Gt68fnttu": {
    "title": "Dynamic Electroencephalography Representation Learning for Improved Epileptic Seizure Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QO3yH7X8JJ": {
    "title": "Dissecting Arbitrary-scale Super-resolution Capability from Pre-trained Diffusion Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=66wQM45W28": {
    "title": "CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AyzkDpuqcl": {
    "title": "Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjyf5FyQ0a": {
    "title": "Valley: Video Assistant with Large Language model Enhanced abilitY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RAA0vCLMhp": {
    "title": "Semantic Memory Guided Diffusion Networks for Image-to-Long Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q7jXHlWVLC": {
    "title": "Re-imagine the Negative Prompt Algorithm for 2D/3D Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YlleMywQzX": {
    "title": "Anytime Neural Architecture Search on Tabular Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQhh1sbfzp": {
    "title": "Differential Model Scaling using Differential Topk",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MvOaI1mmMY": {
    "title": "Learning Label Refinement and Thresholds for Imbalanced Semi-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FoMZ4ljhVw": {
    "title": "Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gMGUa8C0tL": {
    "title": "TaCA: Hot-Plugging Upgrades for Foundation Model with Task-agnostic Compatible Adapter",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNkW0cOwiz": {
    "title": "Lipschitz Singularities in Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yLClGs770I": {
    "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GzNhzX9kVa": {
    "title": "A Benchmark Study on Calibration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TWC4gLoAxY": {
    "title": "Enhancing Human-AI Collaboration Through Logic-Guided Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNkKaD3MCs": {
    "title": "Learning with Mixture of Prototypes for Out-of-Distribution Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DO2WFXU1Be": {
    "title": "PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWmuPfEYhH": {
    "title": "Attention-Guided Contrastive Role Representations for Multi-agent Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xHbRLymyZ": {
    "title": "DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xq7h9nfdY2": {
    "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S62iZf0cba": {
    "title": "Multi-Objective Molecular Design through Learning Latent Pareto Set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lf7gguJgpq": {
    "title": "UniINR: Unifying Spatial-Temporal INR for RS Video Correction, Deblur, and Interpolation with an Event Camera",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJ5EFFGuFU": {
    "title": "SAIR: LEARNING SEMANTIC-AWARE IMPLICIT REPRESENTATION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjRPZsfeBO": {
    "title": "A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HU1pesCJF4": {
    "title": "Pixel Reweighted Adversarial Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8GCcSXlkZN": {
    "title": "Dense Representation Learning for a Joint-Embedding Predictive Architecture",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfFVATffPd": {
    "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgaFMvZHSJ": {
    "title": "Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLoepRnoue": {
    "title": "Decodable and Sample Invariance Continuous Object Encoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djM3WzpOmK": {
    "title": "Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWHMe2O5VW": {
    "title": "Graph ODE with Factorized Prototypes for Modeling Complicated Interacting Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8VPWfqtQMX": {
    "title": "Context is Environment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLSWbk9kPw": {
    "title": "Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gG38EBe2S8": {
    "title": "IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YG01CZDpCq": {
    "title": "Don't Paint Everyone with the Same Brush: Adaptive Prompt Prototype Learning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Zz5UELkIt": {
    "title": "Adaptive Instrument Design for Indirect Experiments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzWLecXH6I": {
    "title": "Towards Personalized AI: Early-stopping Low-Rank Adaptation of Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2MpOjashKU": {
    "title": "Divided Attention: Unsupervised Multiple-object Discovery and Segmentation with Interpretable Contextually Separated Slots",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=etGEIggjWS": {
    "title": "Sub-token ViT Embedding via Stochastic Resonance Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z0B4GJuRjo": {
    "title": "Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FcBmz8nLnq": {
    "title": "Finding Adversarially Robust Graph Lottery Tickets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HiYMiZYwkw": {
    "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Uc7Fgwrsm": {
    "title": "OmniMixup: Generalize Mixup with Mixing-Pair Sampling Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oPZZcLZXT1": {
    "title": "Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e9YuyOaJbc": {
    "title": "Optimal and Generalizable Multimodal Representation Learning Framework through Adaptive Graph Construction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vkzPuZJ80a": {
    "title": "Accelerating Retrieval-augmented Language Model Serving with Speculation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N6o0ZtPzTg": {
    "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Rwq6c3tvr": {
    "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tfiS7oz64k": {
    "title": "Compact Text-to-SDF via Latent Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20L7txbIa8": {
    "title": "UniPredict: Large Language Models are Universal Tabular Predictors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJCeMiwHdH": {
    "title": "BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0XXA9OG13": {
    "title": "MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cSSHiLnjsJ": {
    "title": "Traveling Words: A Geometric Interpretation of Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7yyAoyfVEC": {
    "title": "Hypothesis- and Structure-based prompting for medical and business diagnosis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XM7INBbvwT": {
    "title": "Does Calibration Affect Human Actions?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Ca9sSzuDp": {
    "title": "Interpreting CLIP's Image Representation via Text-Based Decomposition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JpzVlO9X7r": {
    "title": "Does GPT-4 have good intuition about functions?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXZK1Z8tHa": {
    "title": "ShareFormer: Share Attention for Efficient Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jbh2e0b2K": {
    "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KdVvOA00Or": {
    "title": "ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtx8B94JMS": {
    "title": "Variational Inference for SDEs Driven by Fractional Noise",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RCKoQGpPEN": {
    "title": "MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkQSwtx0p3": {
    "title": "Leveraging Task Structures for Improved Identifiability in Neural Network Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5MlPrLO52d": {
    "title": "Neural Tangent Kernels for Axis-Aligned Tree Ensembles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjFJrdK0nO": {
    "title": "Integrating View Conditions for Image Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgehGq2bDv": {
    "title": "GPAvatar: Generalizable and Precise Head Avatar from Image(s)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1xyar0Ko3E": {
    "title": "Efficient Quantization-aware Training with Adaptive Coreset Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zpOUsOvLM": {
    "title": "Aligning Persistent Homology with Graph Pooling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AbXGwqb5Ht": {
    "title": "Implicit regularization of deep residual networks towards neural ODEs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KY8ZNcljVU": {
    "title": "NetInfoF Framework: Measuring and Exploiting Network Usable Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TW0MVSflg5": {
    "title": "Self-Evolving Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbXASfz8MD": {
    "title": "Latent Space Symmetry Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cjdmIUYj03": {
    "title": "On the Generalization of Temporal Graph Learning with Theoretical Insights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQYHbr36Fo": {
    "title": "Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGNdBvymod": {
    "title": "Entropy-MCMC: Sampling from Flat Basins with Ease",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tNAucRS0QQ": {
    "title": "General-purpose Pre-trained Model Towards Cross-domain Molecule Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nQsimt9atc": {
    "title": "IPR-NeRF: Ownership Verification Meets Neural Radiance Field",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vXrIQLzIKY": {
    "title": "Xformer: Hybrid X-Shaped Transformer for Image Denoising",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZJHdiYDD5k": {
    "title": "LatentWarp: Consistent Diffusion Latents for Zero-Shot Video-to-Video Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dUCWpEUrWo": {
    "title": "Asynchronous Graph Generators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2aebB2mf0q": {
    "title": "SemiAugIR: Semi-supervised Infrared Small Target Detection via Thermodynamics-Inspired Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nz2UApmv2e": {
    "title": "Spiking Hybrid Attentive Mechanism with Decoupled Layer Normalization for Joint Sound Localization and Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WS7GuBDFa2": {
    "title": "Learning to Embed Time Series Patches Independently",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y8dHnNEcJu": {
    "title": "SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71oyMJiUm2": {
    "title": "TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EcetCr4trp": {
    "title": "Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHLDHRkmEu": {
    "title": "BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UDbEpJojik": {
    "title": "Unleashing the power of Neural Collapse for Transferability Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6cFcw1Rxww": {
    "title": "Local Search GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=my0RqY48xz": {
    "title": "Awakening Collective Wisdom: Elevating Super-Resolution Network Generalization through Cooperative Game Theory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QXCjvHnDmu": {
    "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pz2E1Q9Wni": {
    "title": "Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mYhH0CDFFa": {
    "title": "Rethinking CNN's Generalization to Backdoor Attack from Frequency Domain",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cu5wJa5LGO": {
    "title": "LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATaE46G1eJ": {
    "title": "CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqExiDNsa7": {
    "title": "Which pre-trained model is effective for speech separation ?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NsCXDyv2Bn": {
    "title": "VoiceGen: Describing and Generating Voices with Text Prompt",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=74YdSRFORA": {
    "title": "Out of Sight: A Framework for Egocentric Active Speaker Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rnHqwPH4TZ": {
    "title": "T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mhyQXJ6JsK": {
    "title": "Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lygdvIKDxi": {
    "title": "SEEKER: Semi-Supervised Knowledge Transfer for Query-Efficient Model Extraction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oz6ABL8o8C": {
    "title": "Unified Interpretation of Smoothing Methods for Negative Sampling Loss Functions in Knowledge Graph Embedding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zihqr7qqpg": {
    "title": "A SYSTEMATIC STUDY ON EARLY STOPPING CRITERIA IN HPO AND THE IMPLICATIONS OF UNCERTAINTY",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmRA7Q0lwQ": {
    "title": "Stay on Topic with Classifier-Free Guidance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oV72wHuRNy": {
    "title": "VC dimensions for deep neural networks with bounded-rank weight matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEuDO2EiDr": {
    "title": "RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P6gYcTj6YC": {
    "title": "Incorporating Domain Knowledge in VAE Learning via Exponential Dissimilarity-Dispersion Family",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zfbwpGhd8": {
    "title": "Vision-Language Instruction-enhanced Tuning via Parameter-efficient Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bgKGwLYmAy": {
    "title": "DGTAT: DECOUPLED GRAPH TRIPLE ATTENTION NETWORKS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YxzEPTH4Ny": {
    "title": "Arithmetic with Language Models: from Memorization to Computation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTH3HnJeRC": {
    "title": "DER-Solomon: A Large Number of CVRPTW Instances Generated Based on the Solomon Benchmark Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IoKRezZMxF": {
    "title": "Consistent Video-to-Video Transfer Using Synthetic Dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6KS9D1dxt": {
    "title": "Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PfqBfC7bO9": {
    "title": "Causal Unsupervised Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5xadJmgwix": {
    "title": "Scale-Adaptive Diffusion Model for Complex Sketch Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dCDX1fjrXr": {
    "title": "Sparse Labels Node Classification: Unsupervised Learning for Mentoring Supervised Learning in Sparse Label Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NHb6mbD99v": {
    "title": "Uncertainty-aware Distributional Offline Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GOvTGntFNj": {
    "title": "Query-Efficient Offline Preference-Based Reinforcement Learning via In-Dataset Exploration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bpcgcr8E8Z": {
    "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OCqyFVFNeF": {
    "title": "Defining and extracting generalizable interaction primitives from DNNs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l60EM8md3t": {
    "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yzRXdhk2he": {
    "title": "Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cy5v64DqEF": {
    "title": "Idempotence and Perceptual Image Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ncbDXOdURn": {
    "title": "Characterizing Robust Overfitting in Adversarial Training via Cross-Class Features",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89bUur0Q4J": {
    "title": "Vision-Language Subspace Prompting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a9xZqOqzEW": {
    "title": "A Logical Framework for Verification of AI Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=95ObXevgHx": {
    "title": "The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zMR2dTNdft": {
    "title": "Symmetry Leads to Structured Constraint of Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qCUWVT0Ayy": {
    "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bCvm9h0FmQ": {
    "title": "Causality-Based Black-Box Backdoor Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZDJKTlOUe": {
    "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MeB86edZ1P": {
    "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HD5Y7M8Xdk": {
    "title": "Forward $\\chi^2$ Divergence Based Variational Importane Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Th6NyL07na": {
    "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QgSwyVsOzK": {
    "title": "Modeling Knowledge as Functionals for Knowledge Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZNzDXDFZ0B": {
    "title": "PanoDiffusion: 360-degree Panorama Outpainting via Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEmm0hYA2u": {
    "title": "ZeroP: Zero-Shot Quantization via Proxy Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJ88Wg5Bp5": {
    "title": "ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Spp2i1hKwV": {
    "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dj940KfZl3": {
    "title": "PIE: Simulating Disease Progression via Progressive Image Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ttXg3SKAg5": {
    "title": "Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bQfJLRlfYO": {
    "title": "Keqing: Knowledge-based Question Answering is A Nature Chain-of-Thought mentor of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jyiD0q2wp2": {
    "title": "Human Pose Estimation via Parse Graph of Body Structure",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JopVmAPyx6": {
    "title": "How does representation impact in-context learning: An exploration on a synthetic task",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0KVkTDB6KZ": {
    "title": "EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fQHb1uZzl7": {
    "title": "Unifying Feature and Cost Aggregation with Transformers for Dense Correspondence",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHQMZ8FOL5": {
    "title": "Dual-level Adaptive Self-Labeling for Novel Class Discovery in Point Cloud Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Ol6foUi1G": {
    "title": "Data-independent Module-aware Pruning for Hierarchical Vision Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tlqmkftgpw": {
    "title": "DBRNet: Advancing Individual-Level Continuous Treatment Estimation through Disentangled and Balanced Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rtw2xd4ZdK": {
    "title": "LeCO-NeRF: Learning Compact Occupancy for Large-scale Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FVhmnvqnsI": {
    "title": "Multisize Dataset Condensation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KncRpAnprQ": {
    "title": "A Novel Approach For Adversarial Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBs0IfPj5e": {
    "title": "Backdiff: a diffusion model for generalized transferable protein backmapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UyNXMqnN3c": {
    "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jXOVnxvLic": {
    "title": "To Simulate Neural Organoid: A Framework and A Benchmark based on AI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nubKjBbazd": {
    "title": "APD: Boosting Adversarial Transferability via Perturbation Dropout",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDSTjtnqCg": {
    "title": "Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JB3lbDtsFS": {
    "title": "It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N9wD4RFWY0": {
    "title": "Benchmarking Large Language Models as AI Research Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxoFut3dWW": {
    "title": "A Simple and Effective Pruning Approach for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TqL2xBwXP3": {
    "title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=67t4ikhlvP": {
    "title": "Agent-Centric State Discovery for Finite-Memory POMDPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GTUoTJXPBf": {
    "title": "Noisy Interpolation Learning with Shallow Univariate ReLU Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2lDQLiH1W4": {
    "title": "Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M36m3buBVD": {
    "title": "LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VCscggkg2t": {
    "title": "Goal2FlowNet: Learning Diverse Policy Covers using GFlowNets for Goal-Conditioned RL",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=py4ZV2qYQI": {
    "title": "Effective and Efficient Federated Tree Learning on Hybrid Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x13bw5VQkf": {
    "title": "A Coefficient Makes SVRG Effective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dyrGMhicMw": {
    "title": "Weight Selection for Model Initialization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJ3K7uDGGl": {
    "title": "Knowledge Distillation Based on Transformed Teacher Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yozwqhIHXj": {
    "title": "Image Translation as Diffusion Visual Programmers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eS5zjXvxf8": {
    "title": "MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOE6N8dp4w": {
    "title": "Harnessing large-language models to generate private synthetic text",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5aHmaMFJns": {
    "title": "Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u4CQHLTfg5": {
    "title": "Individual Fairness as an Extension of Group Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4yQefeXhp": {
    "title": "DMV3D: Denoising Multi-view Diffusion Using 3D Large Reconstruction Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBUUNj0Fnz": {
    "title": "Active Learning for Image Segmentation with Binary User Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmGRoNDQgJ": {
    "title": "Influencer Backdoor Attack on Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=noe76eRcPC": {
    "title": "PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sllU8vvsFF": {
    "title": "LRM: Large Reconstruction Model for Single Image to 3D",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExpwgjvwmC": {
    "title": "OmniInput: A Model-centric Evaluation Framework through Output Distribution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=na7AgFyp1r": {
    "title": "Empowering Active Learning for 3D Molecular Graphs with Geometric Graph Isomorphism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbznWbXq2b": {
    "title": "GPT-FL: Generative Pre-trained Model-Assisted Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fpYIlzOpIA": {
    "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ywD00GsxgD": {
    "title": "Synthetic Data as Validation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhizIPytk4": {
    "title": "How Well Do Supervised Models Transfer to 3D Image Segmentation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tc1jaWpi7M": {
    "title": "Completing Visual Objects via Bridging Generation and Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ukmwyfjqoN": {
    "title": "ReBotNet: Fast Real-time Video Enhancement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxfPefbu1s": {
    "title": "Procedural Fairness Through Decoupling Objectionable Data Generating Components",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bQWE2UqXmf": {
    "title": "Detecting Generated Text via Rewriting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAgwFiIY4p": {
    "title": "Graph as Point Set",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3bqesUzZPH": {
    "title": "FTA: Stealthy and Adaptive Backdoor Attack with Flexible Triggers on Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KjegfPGRde": {
    "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZ6NY4JfFX": {
    "title": "Revisiting the Role of Language Priors in Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=32camXjW25": {
    "title": "Covariance-corrected Whitening Alleviates Network Degeneration on Imbalanced Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GGPyzKsHZ1": {
    "title": "Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imZcqOrbig": {
    "title": "Multi-View Representation is What You Need for Point-Cloud Pre-Training",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=You77eOFDv": {
    "title": "RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F9Qy7mH34l": {
    "title": "Key-Graph Transformer for Image Restoration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C6a0Obrp3o": {
    "title": "SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Un0rgm9f04": {
    "title": "VDT: General-purpose Video Diffusion Transformers via Mask Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UM6QLuOVNi": {
    "title": "EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EA8dTp96GY": {
    "title": "RelationVLM: Making Large Vision-Language Models Understand Visual Relations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e9bEoxNiTJ": {
    "title": "TransCues: Boundary and Reflection-empowered Pyramid Vision Transformer for Semantic Transparent Object Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T6pC0E2ziE": {
    "title": "Learning Identifiable Causal Structures with Pairwise Representation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DchC116F4H": {
    "title": "Non-negative Probabilistic Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KAseclJyj5": {
    "title": "Diverse Offline Imitation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YB7z2AOqm3": {
    "title": "Stress Testing Byzantine Robustness in Distributed Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K7l94Z81bH": {
    "title": "Sparsity-Aware Grouped Reinforcement Learning for Designated Driver Dispatch",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lFYj0oibGR": {
    "title": "Vision-Language Foundation Models as Effective Robot Imitators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q9NMLp2lft": {
    "title": "A Generalized Convolutional Neural Network for Small Dataset Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aHmNpLlUlb": {
    "title": "InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u859gX7ADC": {
    "title": "Augmenting transformers with recursively composed multi-grained representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRvBXm7WK3": {
    "title": "MOESR: MULTI-OBJECTIVE EVOLUTIONARY ALGORITHM FOR IMAGE SUPER-RESOLUTION",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3nPFco1EKt": {
    "title": "Evolving Neural Network's Weights at Imagenet Scale",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3LFy3dUS86": {
    "title": "P2RBOX:A SINGLE POINT IS ALL YOU NEED TRAINING ORIENTED OBJECT DETECTOR",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4vzu2aokv": {
    "title": "P2Seg: Pointly-supervised Segmentation via Mutual Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CqzO3z9kVK": {
    "title": "Knowledge Fusion by Evolving Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFT9jWI7vT": {
    "title": "Towards Architecture-Insensitive Untrained Network Priors for Accelerated MRI",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0kTH3HVLz": {
    "title": "A Light-robust Reconstruction Method for Spike Camera",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7oLshfEIC2": {
    "title": "TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7hxoYxKDTV": {
    "title": "Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bvw9H80jyW": {
    "title": "ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCUG6P9RkZ": {
    "title": "Better Imitation Learning in Discounted Linear MDP",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DorP300Q3b": {
    "title": "Learning Pseudo 3D Representation for Ego-centric 2D Multiple Object Tracking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAO4VS9QRV": {
    "title": "Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0IaTFNJner": {
    "title": "On the Embedding Collapse When Scaling up Recommendation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=naEeJTlRsr": {
    "title": "Revisiting High-Resolution ODEs for Faster Convergence Rates",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XD0PHQ5ry4": {
    "title": "SELF: Language-Driven Self-Evolution for Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYBdkHcXXK": {
    "title": "When Semantic Segmentation Meets Frequency Aliasing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Od39h4XQ3Y": {
    "title": "Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QBlegfNZNE": {
    "title": "Language as Kernels",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fBlHaSGKNg": {
    "title": "Unleashing the Power of Annotation: Enhancing Semi-Supervised Learning through Unsupervised Sample Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CulHdELJ1S": {
    "title": "HUB: Enhancing Learned Optimizers via Hybrid Update-based Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=olOheQ0ZcK": {
    "title": "Distance Estimation for High-Dimensional Distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vsW5vJqBuv": {
    "title": "Toward Open-ended Embodied Tasks Solving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hJCinlknXn": {
    "title": "UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cVea4KQ4xm": {
    "title": "Beyond Demographic Parity: Redefining Equal Treatment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=brOAVSPPjw": {
    "title": "Wide Neural Network Training Dynamics for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wXWfvSpYHh": {
    "title": "MVSFormer++: Revealing the Devil in the Transformer's Details for Multi-View Stereo",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9g8h5HwZMy": {
    "title": "Subgraph Diffusion for 3D Molecular Representation Learning: Combining Continuous and Discrete",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mw1PWNSWZP": {
    "title": "OctoPack: Instruction Tuning Code Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZ4VwxpzCK": {
    "title": "PROSPECT: Learn MLPs Robust against Graph Adversarial Structure Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0V5TVt9bk0": {
    "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JePfAI8fah": {
    "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QyFm3D3Tzi": {
    "title": "A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k06CbKrdIk": {
    "title": "A Effective Variance Change Detection Method under constantly Changing Mean",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4HohsyNEk": {
    "title": "NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dq7iJqKKM7": {
    "title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9UIGyJJpay": {
    "title": "De novo Protein Design Using Geometric Vector Field Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6PVgHZUepm": {
    "title": "Rep-Adapter: Parameter-free Automatic Adaptation of Pre-trained ConvNets via Re-parameterization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IJBsKYXaH4": {
    "title": "Molecular Conformation Generation via Shifting Scores",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8HG2QrtXXB": {
    "title": "HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ww9rWUAcdo": {
    "title": "Theoretical Understanding of Learning from Adversarial Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nmBjBZoySX": {
    "title": "Graph Lottery Ticket Automated",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EH2O3h7sBI": {
    "title": "Prompt Gradient Projection for Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RN7RzMxwjC": {
    "title": "Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XasWgF5WsZ": {
    "title": "Elucidating the Solution Space of Extended Reverse-Time SDE for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ijoqFqSC7p": {
    "title": "FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0QAzIMq32X": {
    "title": "Inner Classifier-Free Guidance and Its Taylor Expansion for Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AnPX5Jual9": {
    "title": "Rotative Factorization Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mmCIov21zD": {
    "title": "RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cMPm8YFXZe": {
    "title": "ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pZhdz4oyzo": {
    "title": "SqueezeLLM: Dense and Sparse Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Si3YFA641c": {
    "title": "R-EDL: Relaxing Nonessential Settings of Evidential Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V0CUOBWUHa": {
    "title": "Scaling Sentence Embeddings with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0isTh3rml": {
    "title": "Graph Learning with Distributional Edge Layouts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nsNyN0vox": {
    "title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJRrG43BYC": {
    "title": "State-drive Implicit Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEF7qExD9F": {
    "title": "LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJgVY0zOB0": {
    "title": "Weakly-supervised Camera Localization by Ground-to-satellite Image Registration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pwW807WJ9G": {
    "title": "InterpGNN: Understand and Improve Generalization Ability of Transdutive GNNs through the Lens of Interplay between Train and Test Nodes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bRm0rul3SZ": {
    "title": "Unpaired Panoramic Image-to-Image Translation Leveraging Pinhole Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6iwg437CZs": {
    "title": "STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IfqXxs1lCJ": {
    "title": "On the Evaluation of Generative Models in Distributed Learning Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNGXbZtK6Q": {
    "title": "Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYPx4NukeB": {
    "title": "SSL Framework for Causal Inconsistency between Structures and Representations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=syoLhUJmth": {
    "title": "From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHrvRNs2Y0": {
    "title": "ResFields: Residual Neural Fields for Spatiotemporal Signals",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xelrLobW0n": {
    "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BteuUysuXX": {
    "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGFrRMBbOq": {
    "title": "Progressive Fourier Neural Representation for Sequential Video Compilation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OgTMbLDQZu": {
    "title": "LAMDA: Unified Language-Driven Multi-Task Domain Adaption",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J9Vwp7TiE5": {
    "title": "SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aarj9MrG8Y": {
    "title": "Towards the Universal Learning Principle for Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bPG48f3ppz": {
    "title": "SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oxh5CstDJU": {
    "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4pW8NL1UwH": {
    "title": "LIRE: Listwise Reward Enhancement for Preference Alignment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpnlc2ONu0": {
    "title": "Adaptive deep spiking neural network with global-local learning via balanced excitatory and inhibitory mechanism",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ehXWDitaKt": {
    "title": "Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lgA84TbHxm": {
    "title": "DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YbSUcEd7oR": {
    "title": "Realistic Human Motion Generation with Cross-Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OcaKeyGb0K": {
    "title": "A unified theory of scene representation learning and object representation learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tplXNcHZs1": {
    "title": "Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jj5ZjZsWJe": {
    "title": "Stochastic Controlled Averaging for Federated Learning with Communication Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PcBJ4pA6bF": {
    "title": "Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=slSmYGc8ee": {
    "title": "How connectivity structure shapes rich and lazy learning in neural circuits",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fGXyvmWpw6": {
    "title": "Federated Virtual Learning on Heterogeneous Data with Local-global Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VVgGbB9TNV": {
    "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=09xFexjhqE": {
    "title": "AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AhcxMGfqQn": {
    "title": "Collaborative World Models: An Online-Offline Transfer RL Approach",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fx2SbBgcte": {
    "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c43FGk8Pcg": {
    "title": "Denoising Diffusion Step-aware Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nh6pXEkZkK": {
    "title": "Learning Rate Re-scheduling for AdaGrad in training Deep Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QmYNBVukex": {
    "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bVzLZr0S8s": {
    "title": "Action Shapley: A training data selection metric for high performance and cost efficient reinforcement learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e1IMBXiDhW": {
    "title": "Matrix Information Theory for Self-Supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lu5gGqhFTw": {
    "title": "RelationMatch: Matching In-batch Relationships for Semi-supervised Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wn82BUF7jH": {
    "title": "On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lifLHzadgr": {
    "title": "Cumulative Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QV6uB196cR": {
    "title": "A/B testing under Identity Fragmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIrpuifCJW": {
    "title": "Exploring the Impact of Information Entropy Change in Learning Systems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=USWkUOfxOO": {
    "title": "Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Domain Adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nu7dDaVF5a": {
    "title": "3D Reconstruction with Generalizable Neural Fields using Scene Priors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MukGKGtgnr": {
    "title": "Causal Structure Recovery with Latent Variables under Milder Distributional and Graphical Assumptions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNxlJJIEVj": {
    "title": "Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wR9qVlPh0P": {
    "title": "AutoVP: An Automated Visual Prompting Framework and Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJ3H9K7Mcb": {
    "title": "Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9CXfU2mD5": {
    "title": "Score Propagation as a Catalyst for Graph Out-of-distribution Detection: A Theoretical and Empirical Study",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BfMQIJ0nLc": {
    "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gvKZyTlUgQ": {
    "title": "Warped Convolutional Neural Networks For Large Homography Transformation with $\\mathfrak{sl}(3)$ Algebra",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NqQjoncEDR": {
    "title": "Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zEBK3E9bX": {
    "title": "SPOT: Scalable 3D Pre-training via Occupancy Prediction for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5dlfiJIXoh": {
    "title": "Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fihkcXeG7N": {
    "title": "Gate-guided and subgraph-aware Bilateral Fusion for Molecular Property Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uxYye6i2Xi": {
    "title": "Composing Recurrent Spiking Neural Networks using Locally-Recurrent Motifs and Risk-Mitigating Architectural Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gFR4QwK53h": {
    "title": "Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnUwSIgK5W": {
    "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7gqOBG8ow": {
    "title": "Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1RKWSyZ2Y": {
    "title": "Guiding Instruction-based Image Editing via Multimodal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=va9hbzIggi": {
    "title": "Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VXDPXuq4oG": {
    "title": "Order-Preserving GFlowNets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YXn76HMetm": {
    "title": "Hyperbolic Active Learning for Semantic Segmentation under Domain Shift",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h6Tz85BqRI": {
    "title": "VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNe1T0Ahby": {
    "title": "Efficacy of Dual-Encoders for Extreme Multi-label Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YR3ETaElNK": {
    "title": "Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0Z7sBavgm": {
    "title": "Class-Context-Aware Phantom Uncertainty Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kB4yBiNmXX": {
    "title": "FasterViT: Fast Vision Transformers with Hierarchical Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jTdqzBGMsq": {
    "title": "Aligner: One Global Token is Worth Millions of Parameters When Aligning LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=esqRHCwTJ2": {
    "title": "Long-Term Impacts of Model Retraining with Strategic Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OrOd8PxOO2": {
    "title": "Universal Humanoid Motion Representations for Physics-Based Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=COYDmKkQH4": {
    "title": "AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyC5qvRPz7": {
    "title": "Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gctmyMiPHH": {
    "title": "Feature Collapse",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e3JsfnAi9f": {
    "title": "Interleaving Multi-Task Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=adQ2YC2IV7": {
    "title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g90ysX1sVs": {
    "title": "Adaptive Rational Activations to Boost Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTq2XSBNsa": {
    "title": "MOESART: An Effective Sampling-based Router for Sparse Mixture of Experts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wISvONp3Kq": {
    "title": "Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dh0RmiwkWY": {
    "title": "Large-Scale Public Data Improves Differentially Private Image Generation Quality",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vyQmKicyVw": {
    "title": "Revealing Hidden Causal Variables and Latent Factors from Multiple Distributions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AwyxtyMwaG": {
    "title": "LLMs Represent Contextual Tasks as Compact Function Vectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8sKcAWOf2D": {
    "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnYaYVODyV": {
    "title": "Perceptual Group Tokenizer: Building Perception with Iterative Grouping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2y8XnaIiB8": {
    "title": "Vision-Language Dataset Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vxZW1aROZA": {
    "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VTYg5ykEGS": {
    "title": "ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fsW7wJGLBd": {
    "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EpYnZpDpsQ": {
    "title": "Self-supervised Representation Learning from Random Data Projectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aKJEHWmBEf": {
    "title": "Approximately Piecewise E(3) Equivariant Point Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ihr4X2qK62": {
    "title": "Choosing Public Datasets for Private Machine Learning via Gradient Subspace Distance",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNjrhD67LP": {
    "title": "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybavRGEmpw": {
    "title": "Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CE7lUzrp1o": {
    "title": "CODA: Temporal Domain Generalization via Concept Drift Simulator",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ImwrWH6U0Y": {
    "title": "A Comprehensive Study of Privacy Risks in Curriculum Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HL9YzviPCy": {
    "title": "PrACTiS: Perceiver-Attentional Copulas for Time Series",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4NhMhElWqP": {
    "title": "DAM: A Foundation Model for Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C5sxQsqv7X": {
    "title": "SELECTFORMER: PRIVATE AND PRACTICAL DATA SELECTION FOR TRANSFORMERS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6INCxtPVXd": {
    "title": "Mode-Aware Continual Learning for Conditional Generative Adversarial Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IhD1rBHhDy": {
    "title": "Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AKJLnDgzkm": {
    "title": "Welfare Diplomacy: Benchmarking Language Model Cooperation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=enT2rGC7h2": {
    "title": "Impact of Agent Behavior in Distributed SGD and Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4N97bz1sP6": {
    "title": "Weakly-supervised Audio Separation via Bi-modal Semantic Similarity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyfOZKXpcN": {
    "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uH0FGECSEI": {
    "title": "Expected flow networks in stochastic environments and two-player zero-sum games",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dTlKCQuuxP": {
    "title": "Neural Polynomial Gabor Fields for Macro Motion Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JbsdayvhO": {
    "title": "Denoising Diffusion via Image-Based Rendering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q6WXlm2Kxo": {
    "title": "Masked Diffusion as Self-supervised Representation Learner",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v7ZPwoHU1j": {
    "title": "Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F8l0llkMk0": {
    "title": "The Map Equation goes Neural",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TmcH09s6pT": {
    "title": "Generalized Neural Collapse for a Large Number of Classes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WTFN4gxLQN": {
    "title": "Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KPmajBxEaF": {
    "title": "LEAP: Liberate Sparse-View 3D Modeling from Camera Poses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1E0EahS5w": {
    "title": "Limits to Reservoir Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cmcD05NPKa": {
    "title": "Learning the greatest common divisor: explaining transformer predictions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iEFMwP5wng": {
    "title": "Reliable Test-Time Adaptation via Agreement-on-the-Line",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikqcUzUogm": {
    "title": "Programmatic Evaluation of Rule-Following Behavior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BdWLzmPKst": {
    "title": "Sequential Data Generation with Groupwise Diffusion Process",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vmiV4Z99lK": {
    "title": "SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f9Djqso1p1": {
    "title": "CResT: Cross-Query Residual Transformer for Object Goal Navigation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1FeTLOwzr": {
    "title": "Dynamic Adapter Merging for Continual Video Question-Answering Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VUA9LSmC2r": {
    "title": "Learning Embodied Vision-Language Programming From Instruction, Exploration, and Environmental Feedback",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6qtDu7hVPF": {
    "title": "Generative Reinforcement Learning with Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jznbgiynus": {
    "title": "Language Modeling Is Compression",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9yKzVMxlkw": {
    "title": "TiG-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cvGdPXaydP": {
    "title": "Planning with an Ensemble of World Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SgjAojPKb3": {
    "title": "OpenNerf: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8HCARN2hhw": {
    "title": "Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ttMwEuEPeB": {
    "title": "3D-GPT: Procedural 3D Modeling with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jBt8qp1iYK": {
    "title": "SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4rODxXsmM": {
    "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Agyicd577r": {
    "title": "BatchPrompt: Accomplish more with less",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bb4VGOWELI": {
    "title": "Large Language Models as Optimizers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j0ZvKSNZiP": {
    "title": "ContextRef: Evaluating Referenceless Metrics for Image Description Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h7nOCxFsPg": {
    "title": "Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VOrfPVYvbN": {
    "title": "Domain Bridge: Generative Model-based Domain Forensic for Black-box Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duyA42HlCK": {
    "title": "HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FRCHDhbxZF": {
    "title": "ZeroFlow: Scalable Scene Flow via Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2nD1SvxTZc": {
    "title": "One-Versus-Others Attention: Scalable Multimodal Integration",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Q4uVOJ5bX": {
    "title": "R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4yaFQ7181M": {
    "title": "Space and time continuous physics simulation from partial observations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ye3NrNrYOY": {
    "title": "Temporal Causal Mechanism Transfer for Few-shot Action Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1quoTXZzc": {
    "title": "Energy-Based Concept Bottleneck Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPEY9gvwoj": {
    "title": "Amicable Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AF9Q8Vip84": {
    "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hftNRiSQQq": {
    "title": "Boosting Efficiency in Task-Agnostic Exploration Through Causal Knowledge",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ukidfml68f": {
    "title": "Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uleDLeiaT3": {
    "title": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ChHx5ORqF0": {
    "title": "Translating Labels to Solve Annotation Mismatches Across Object Detection Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fCQe7ei2f5": {
    "title": "Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5EniAcsO7f": {
    "title": "Tailoring Retrieval Representations to Long-term Visual Localization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ALuy19mPa": {
    "title": "DreamClean: Restoring Clean Image Using Deep Diffusion Prior",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuFHei1vuE": {
    "title": "Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=guRNebwZBb": {
    "title": "CausalLM is not optimal for in-context learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cphhnHjCvC": {
    "title": "End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hywpSoHwgX": {
    "title": "Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsRXwlwx4w": {
    "title": "Consistency-guided Prompt Learning for Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YjSKB1sfOE": {
    "title": "From Trojan Horses To Castle Walls: Revealing Bilateral Backdoor Effects In Diffision Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WhgB5sispV": {
    "title": "Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juuyW8B8ig": {
    "title": "Language-Informed Visual Concept Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFPfYccHJ1": {
    "title": "Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7M0EzjugaN": {
    "title": "Online Continual Learning for Interactive Instruction Following Agents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F5ERvanO6m": {
    "title": "Deep Stochastic Mechanics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7HfliVAtCG": {
    "title": "Detect Every Thing with Few Examples",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lUWf41nR4v": {
    "title": "Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qmw9ne6SOQ": {
    "title": "Localizing and Editing Knowledge In Text-to-Image Generative Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=za9tj3izLn": {
    "title": "A Unified View on Neural Message Passing with Opinion Dynamics for Social Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7NzgkEdGyr": {
    "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qtE9K23ISq": {
    "title": "Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HjIa9SoYEZ": {
    "title": "CTRL: Graph condensation via crafting rational trajectory matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LsURkIPYR5": {
    "title": "LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AoRIT2Uzfg": {
    "title": "DRMGuard: Defending Deep Regression Models against Backdoor Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SIBN5Xyw7": {
    "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mQYHXUUTkU": {
    "title": "BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAfGlmRBTU": {
    "title": "Representing part-whole hierarchy with coordinated synchrony in neural networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bh4BW69ILq": {
    "title": "Solving (partial) unbalanced optimal transport via transform coefficients and beyond",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ioBIT7gLBm": {
    "title": "Hard View Selection for Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FvK2noilxT": {
    "title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zpVPhvVKXk": {
    "title": "Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NTNLlEmx8Y": {
    "title": "Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jr03SfWsBS": {
    "title": "Unprocessing Seven Years of Algorithmic Fairness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZitfSXpdT": {
    "title": "Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CEkIyshNbC": {
    "title": "Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4fbFKO4a2W": {
    "title": "Guided Sketch-Based Program Induction by Search Gradients",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1gkePTsAWf": {
    "title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N1cjy5iznY": {
    "title": "Modeling Time Series as Text Sequence A Frequency-vectorization Transformer for Time Series Forecasting",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XqLcFMMwNb": {
    "title": "MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BwG8hwohU4": {
    "title": "StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RthOl4jHw5": {
    "title": "Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FhbZ1PQCaG": {
    "title": "Think Before You Act: Decision Transformers with Internal Memory",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0lGp17AjO": {
    "title": "A Wasserstein-2 Distance for Efficient Reconstruction of Stochastic Differential Equations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h1sFUGlI09": {
    "title": "DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L2kbdthX5M": {
    "title": "SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQsrRW9mq9": {
    "title": "DeCUR: decoupling common & unique representations for multimodal self-supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OqLrv5oH6r": {
    "title": "Encoding Expert Knowledge into Federated Learning using Weak Supervision",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ep0TtjVoap": {
    "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZlEtXIxl3q": {
    "title": "Contrastive losses as generalized models of global epistasis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wPq7fkzL2j": {
    "title": "Self-Paced Augmentations (SPAug) for Improving Model Robustness",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZYm1Ql6udy": {
    "title": "Bayesian Bi-clustering of Neural Spiking Activity with Latent Structures",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XEFWBxi075": {
    "title": "GRANDE: Gradient-Based Decision Tree Ensembles",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uJVHygNeSZ": {
    "title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cOLzQHklmn": {
    "title": "Independently-prepared Query-efficient Model Selection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uDNP1q5aZq": {
    "title": "Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ygxTuVz9eU": {
    "title": "VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C61sk5LsK6": {
    "title": "InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPdagk1mV7": {
    "title": "Implicit Semi-auto-regressive Image-to-Video Diffusion",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJPUmX4LXD": {
    "title": "Brain2Music: Reconstructing Music from Human Brain Activity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rTBL8OhdhH": {
    "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oyFCgkkLUK": {
    "title": "αMax-B-CUBED: A Supervised Metric for Addressing Completeness and Uncertainty in Cluster Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1JPfHljXL4": {
    "title": "When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x8ElSuQWQp": {
    "title": "IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLJcd43U7a": {
    "title": "SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsvgVuaWXK": {
    "title": "Provably Efficient Learning in Partially Observable Contextual Bandit",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZvyQTNt2qp": {
    "title": "Provable Convergence of Clipped Normalized-gradient Heavy-Ball Momentum for Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xbUlKe1iE8": {
    "title": "Doubly Robust Structure Identification from Temporal Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JbcwfmYrob": {
    "title": "SEA: Sparse Linear Attention with Estimated Attention Mask",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCe9ut1s7i": {
    "title": "On the Importance of Backbone to the Adversarial Robustness of Object Detectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJoMzslBIa": {
    "title": "ReweightOOD: Loss Reweighting for Distance-based OOD Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RZBy8oHTz4": {
    "title": "Zero-Mean Regularized Spectral Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q4pC5Gn8HJ": {
    "title": "Contraction and Alienation: Towards Theoretical Understanding of Non-Contrastive Learning with Neighbor-Averaging Dynamics",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yeeVBMDAwy": {
    "title": "Variance-enlarged Poisson Learning for Graph-based Semi-Supervised Learning with Extremely Sparse Labeled Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SEiuSzlD1d": {
    "title": "Mask-based modeling for Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1hhja8ZxcP": {
    "title": "Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shr9PXz7T0": {
    "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2yFdUSJ4I": {
    "title": "Linear diffusion models meet contextual bandits with large action spaces",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kx2XZlmgB1": {
    "title": "Enhancing Contrastive Learning for Ordinal Regression via Ordinal Content Preserved Data Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PDL5A6facN": {
    "title": "Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0b0vryZRX": {
    "title": "Self-Distilled Disentanglement for Counterfactual Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5BoXZXTJvL": {
    "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pTHfApDakA": {
    "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4Uheo6nR1": {
    "title": "Robust prediction under missingness shifts",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFOoOdaiue": {
    "title": "Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kpEz4Bxs6e": {
    "title": "Dataset Distillation in Large Data Era",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUwfjPzI8g": {
    "title": "Continual Learning via Winning Subnetworks That Arise Through Stochastic Local Competition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gd0lAEtWso": {
    "title": "OmniControl: Control Any Joint at Any Time for Human Motion Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BXY6fe7q31": {
    "title": "Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IAZVktzmG5": {
    "title": "Learning Epipolar Feature Fields for Multi-Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FVAqmAY2C9": {
    "title": "Towards Faster and Stronger Deep Earth Mover's Distance for Few-Shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QJGj07PD9C": {
    "title": "Guaranteed Approximation Bounds for Mixed-Precision Neural Operators",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J6QKWjq05Z": {
    "title": "TreeDQN: Learning to minimize Branch-and-Bound tree",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KcRbiPwuNS": {
    "title": "LINK PREDICTION USING NEUMANN EIGENVALUES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w7BwaDHppp": {
    "title": "Geometry-Aware Projective Mapping for Unbounded Neural Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wfoiy6dIEk": {
    "title": "Multi-Instance Learning Based Anomaly Detection Method for Sequence Data with Application to the Credit Card Delinquency Risk Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gf15GsnfTy": {
    "title": "REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ClqyY6Bvb7": {
    "title": "ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fKfvyJeAlY": {
    "title": "LeRaC: Learning Rate Curriculum",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CNZmaInj9n": {
    "title": "Exploring Unified Perspective For Fast Shapley Value Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzYgsZgwXa": {
    "title": "Path Choice Matters for Clear Attributions in Path Methods",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8EyRkd3Qj2": {
    "title": "CLAP: Collaborative Adaptation for Checkerboard Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEOnlJaRKp": {
    "title": "Collaboration! Towards Robust Neural Methods for Vehicle Routing Problems",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xmQMz9OPF5": {
    "title": "Exploring Target Representations for Masked Autoencoders",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JN7TcCm9LF": {
    "title": "Koopman-based generalization bound: New aspect for full-rank weights",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJlIwGqPdL": {
    "title": "The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eoSeaK4QJo": {
    "title": "Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIj1CVbkpr": {
    "title": "Online Stabilization of Spiking Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GT57SN8xt9": {
    "title": "Parameter-Efficient Long-Tailed Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Mq096hr9Y": {
    "title": "OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aA33A70IO6": {
    "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sBQwvucduK": {
    "title": "MagicDrive: Street View Generation with Diverse 3D Geometry Control",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XhYWgjqCrV": {
    "title": "MogaNet: Multi-order Gated Aggregation Network",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RzV7QRowUl": {
    "title": "Test like you Train in Implicit Deep Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNLr8WXDEh": {
    "title": "What Does Stable Diffusion Know about the 3D Scene?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xBfQZWeDRH": {
    "title": "GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QGR5IeMNDF": {
    "title": "Pure Message Passing Can Estimate Common Neighbor for Link Prediction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xyxU99Nutg": {
    "title": "Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=scozYAatUd": {
    "title": "MULTISCALE ATTENTION VIA WAVELET NEURAL OPERATORS FOR VISION TRANSFORMER",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFaFvyKKbX": {
    "title": "Understanding deep neural networks through the lens of their non-linearity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KWO8LSUC5W": {
    "title": "Constraint-Free Structure Learning with Smooth Acyclic Orientations",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfDUzzQa3g": {
    "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b66P1u0k15": {
    "title": "Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mkjKqeBXkt": {
    "title": "KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LndMyiBl3n": {
    "title": "SheAttack: A Silhouette Score Motivated Restricted Black-Box Attack on Graphs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80faVLl6ji": {
    "title": "BRIDGING THE GAP BETWEEN HUMAN MOTION AND ACTION SEMANTICS VIA KINEMATIC PHRASES",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q1vkAhdI6j": {
    "title": "MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oZ8FmnLpCA": {
    "title": "Knowledge Distillation via Flow Matching",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KiH8QXn2pk": {
    "title": "PPTSER: A Plug-and-Play Tag-guided Method for Few-shot Semantic Entity Recognition on Visually-rich Documents",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hfAEEsIQ6D": {
    "title": "Perceptual Metrics for Video Game Playstyle Similarity and Diversity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rmpixOjPS": {
    "title": "Boosting Vanilla Lightweight Vision Transformers via Re-parameterization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAgrdEhcr4": {
    "title": "Learning Deep Improvement Representation to Accelerate Evolutionary Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5sjxMwWmk8": {
    "title": "Robust Angular Synchronization via Directed Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oSuVEv4X7w": {
    "title": "Clover: Closed-Loop Verifiable Code Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lt6xKGGWov": {
    "title": "Feature selection with neural estimation of mutual information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HBFzStNrS9": {
    "title": "Unitention: Attend a sample to the dataset",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zb6qOouUJO": {
    "title": "Efficient Fully Single-Loop Variance Reduced Methods for Stochastic Bilevel Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fV54cBCGEV": {
    "title": "Explicit Personalization and Local Training: Double Communication Acceleration in Federated Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lAhWGOkpSR": {
    "title": "Multi-Scale Representations by Varing Window Attention for Semantic Segmentation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBU33YNVL3": {
    "title": "Bounded Loss Robustness: Enhancing the MAE Loss for Large-Scale Noisy Data Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hbHwZYqk9T": {
    "title": "FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8FZXjRWX1": {
    "title": "ReBaR: Reference-Based Reasoning for Robust Human Pose and Shape Estimation from Monocular Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w8eCnnq57m": {
    "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLKgDBUXTR": {
    "title": "How many views does your deep neural network use for prediction?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhJUd3mbhP": {
    "title": "AutoAgents: A Framework for Automatic Agent Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YSTaRLVP2G": {
    "title": "The Power of Linear Combinations: Learning with Random Convolutions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0OwtVEejJ": {
    "title": "Weight-Entanglement Meets Gradient-Based Neural Architecture Search",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64kSvC4iPg": {
    "title": "Compressed Context Memory for Online Language Model Interaction",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dN4vpVTvWX": {
    "title": "TUVF: Learning Generalizable Texture UV Radiance Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=enUArz7TeR": {
    "title": "Decoupled Prioritized Resampling: Advancing Offline RL with Improved Behavior Policy",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3M0GXoUEzP": {
    "title": "CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zRkM6UcA22": {
    "title": "Neural Processing of Tri-Plane Hybrid Neural Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dw6y6bEtXm": {
    "title": "PICL: Incorporating Coarse-Grained Data and Physics Information for Superior Physical Systems Modeling",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZHr0JajZfH": {
    "title": "A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q57JLSE2j5": {
    "title": "Large-Vocabulary 3D Diffusion Model with Transformer",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZfi5to2Xl": {
    "title": "SAS: Structured Activation Sparsification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7pFgsSPd7": {
    "title": "Sample-aware RandAugment",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S77skzM12O": {
    "title": "PROTO: Iterative Policy Regularizied Offline-to-Online Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g52tgL8jy6": {
    "title": "A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mvMI3N4AvD": {
    "title": "Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LixtB4TYY2": {
    "title": "REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning Datasets",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ERQPyR2eb": {
    "title": "Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FOSBQuXgAq": {
    "title": "A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=waeGeAdZUx": {
    "title": "AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqWobzlAGb": {
    "title": "Modelling brain connectomes networks: Solv is a worthy competitor to hyperbolic geometry!",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xv8iGxENyI": {
    "title": "Threaten Spiking Neural Networks through Combining Rate and Temporal Information",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1P1nxem1jU": {
    "title": "Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FIplmUWdm3": {
    "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U6hEOZlDf5": {
    "title": "3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=38E4yUbrgr": {
    "title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKfESGFdas": {
    "title": "GeONet: a neural operator for learning the Wasserstein geodesic",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=feZ7RpTLRy": {
    "title": "Bridging ML and algorithms: comparison of hyperbolic embeddings",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSyytcewxe": {
    "title": "Divide and not forget: Ensemble of selectively trained experts in Continual Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQwULZhiSF": {
    "title": "Unsupervised Discovery of Object-Centric Neural Fields",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I5lcjmFmlc": {
    "title": "Robust Classification via a Single Diffusion Model",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7duh4Ml5rc": {
    "title": "Based on What We Can Control Artificial Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2SwHngthig": {
    "title": "Towards Offline Opponent Modeling with In-context Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Cu8MRmhq2": {
    "title": "Multi-granularity Correspondence Learning from Noisy Instructional Videos",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tItq3cwzYc": {
    "title": "Lightweight Image Classification Network Based on Feature Extraction Network SimpleResUNet and Attention",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DMJNaBUv3D": {
    "title": "Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AtLW9HU3bo": {
    "title": "Discovering the question-critical moments: Towards building event-aware multi-modal large language models for complex video question answering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CMzF2aOfqp": {
    "title": "Early Stopping Against Label Noise Without Validation Data",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owziuM1nsR": {
    "title": "Recursive Generalization Transformer for Image Super-Resolution",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AcJrSoArlh": {
    "title": "Rethinking Model Ensemble in Transfer-based Adversarial Attacks",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8vT0f6x1BY": {
    "title": "Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZN8BaYVFkx": {
    "title": "Training Adversarially Robust SNNs with Gradient Sparsity Regularization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hOxgrGM63n": {
    "title": "Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHgc5SMdtd": {
    "title": "MuSc : Zero-Shot Anomaly Classification and Segmentation by Mutual Scoring of the Unlabeled Images",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p6UwN2Rxhx": {
    "title": "Unveiling Temporal Telltales: Are Unconditional Video Generation Models Implicitly Encoding Temporal Information?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6EbeJR20o": {
    "title": "A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m2NVG4Htxs": {
    "title": "To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1bbPQShCT2": {
    "title": "I-PHYRE: Interactive Physical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ny150AblPu": {
    "title": "EXPOSING TEXT-IMAGE INCONSISTENCY USING DIFFUSION MODELS",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J2kRjUAOLh": {
    "title": "Contrastive Predict-and-Search for Mixed Integer Linear Programs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0P622bfUN": {
    "title": "Federated Generative Learning with Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}