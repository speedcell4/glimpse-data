{
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Image_Retrieval_with_Well-Separated_Semantic_Hash_Centers_ACCV_2022_paper.html": {
    "title": "Image Retrieval with Well-Separated Semantic Hash Centers",
    "abstract": "Recently, some point-wise hash learning methods such as CSQ [1] and DPN [2] adapted \"hash centers\" as the global similarity label for each category and force the hash codes of the images with the same category to get closed to their corresponding hash centers. Although they outperformed other pairwise/triplet hashing methods, they assign hash centers to each class randomly and result in a sub-optimal performance because of ignoring the semantic relationship between categories, which means that they ignore the fact that the Hamming distance between the hash centers corresponding to two semantically similar classes should be smaller than the Hamming distance between the hash centers corresponding to two semantically dissimilar classes. To solve the above problem and generate well-separated and semantic hash centers, in this paper, we propose an optimization approach which aims at generating hash centers not only with semantic category information but also distinguished from each other. Specifically, we adopt the weight of last fully-connected layer in ResNet-50 model as category features to help inject semantic information into the generation of hash centers and try to maximize the expectation of the Hamming distance between each two hash centers. With the hash centers corresponding to each image category, we propose two effective loss functions to learn deep hashing function. Importantly, extensive experiments show that our proposed hash centers and training method outperform the state-of-the-art hash models on three image retrieval datasets",
    "volume": "main",
    "checked": false,
    "id": "b0bc3af8369ad22ed7a22af247d6c9b86e888170",
    "citation_count": 109
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Rosa_A_Differentiable_Distance_Approximation_for_Fairer_Image_Classification_ACCV_2022_paper.html": {
    "title": "A Differentiable Distance Approximation for Fairer Image Classification",
    "abstract": "Naively trained AI models can be heavily biased. This can\nbe particularly problematic when the biases involve legally or morally\nprotected attributes such as ethnic background, age or gender. Existing\nsolutions to this problem come at the cost of extra computation, unstable\nadversarial optimisation or have losses on the feature space structure that\nare disconnected from fairness measures and only loosely generalise to\nfairness. In this work we propose a differentiable approximation of the\nvariance of demographics, a metric that can be used to measure the\nbias, or unfairness, in an AI model. Our approximation can be optimised\nalongside the regular training objective which eliminates the need for\nany extra models during training and directly improves the fairness of\nthe regularised models. We demonstrate that our approach improves the\nfairness of AI models in varied task and dataset scenarios, whilst still\nmaintaining a high level of classification accuracy. Code is available at\nhttps://bitbucket.org/nelliottrosa/base_fairness",
    "volume": "main",
    "checked": true,
    "id": "06b847dfc9a8d2ec34f8ee3c137dd83b126c513f",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Han_3D_Shape_Temporal_Aggregation_for_Video-Based_Clothing-Change_Person_Re-identification_ACCV_2022_paper.html": {
    "title": "3D Shape Temporal Aggregation for Video-Based Clothing-Change Person Re-identification",
    "abstract": "3D shape of human body can be both discriminative and clothing-independent information in video-based clothing-change person re-identification (Re-ID). However, existing Re-ID methods usually generate 3D body shapes without considering identity modelling, which severely weakens the discriminability of 3D human shapes. In addition, different video frames provide highly similar 3D shapes, but existing methods cannot capture the differences among 3D shapes over time. They are thus insensitive to the unique and discriminative 3D shape information of each frame and ineffectively aggregate many redundant framewise shapes in a videowise representation for Re-ID. To address these problems, we propose a 3D Shape Temporal Aggregation (3STA) model for video-based clothing-change Re-ID. To generate the discriminative 3D shape for each frame, we first introduce an identity-aware 3D shape generation module. It embeds the identity information into the generation of 3D shapes by the joint learning of shape estimation and identity recognition. Second, a difference-aware shape aggregation module is designed to measure inter-frame 3D human shape differences and automatically select the unique 3D shape information of each frame. This helps minimise redundancy and maximise complementarity in temporal shape aggregation. We further construct a Video-based Clothing-Change Re-ID (VCCR) dataset to address the lack of publicly available datasets for video-based clothing-change Re-ID. Extensive experiments on the VCCR dataset demonstrate the effectiveness of the proposed 3STA model. The dataset is available at https://vhank.github.io/vccr.github.io",
    "volume": "main",
    "checked": false,
    "id": "50efe497ea21bb2e2f0a6c56f436f37d4d308543",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Tayyub_Explaining_Deep_Neural_Networks_for_Point_Clouds_using_Gradient-based_Visualisations_ACCV_2022_paper.html": {
    "title": "Explaining Deep Neural Networks for Point Clouds using Gradient-based Visualisations",
    "abstract": "Explaining decisions made by deep neural networks is a rapidly advancing research topic. In recent years, several approaches have attempted to provide visual explanations of decisions made by neural networks designed for structured 2D image input data. In this paper, we propose a novel approach to generate coarse visual explanations of networks designed to classify unstructured 3D data, namely point clouds. Our method uses gradients flowing back to the final feature map layers and maps these values as contributions of the corresponding points in the input point cloud. Due to dimensionality disagreement and lack of spatial consistency between input points and final feature maps, our approach combines gradients with points dropping to compute explanations of different parts of the point cloud iteratively. The generality of our approach is tested on various point cloud classification networks, including 'single object' networks PointNet, PointNet++, DGCNN, and a 'scene' network VoteNet. Our method generates symmetric explanation maps that highlight important regions and provide insight into the decision-making process of network architectures. We perform an exhaustive evaluation of trust and interpretability of our explanation method against comparative approaches using quantitative, quantitative and human studies. All our code is implemented in PyTorch and will be made publicly available",
    "volume": "main",
    "checked": true,
    "id": "efc2212545388ca4191ba83694cc983ea93ab582",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lou_TeCM-CLIP_Text-based_Controllable_Multi-attribute_Face_Image_Manipulation_ACCV_2022_paper.html": {
    "title": "TeCM-CLIP: Text-based Controllable Multi-attribute Face Image Manipulation",
    "abstract": "In recent years, various studies have demonstrated that utilizing the prior information of StyleGAN can effectively manipulate and generate realistic images. However, the latent code of StyleGAN is designed to control global styles, and it is arduous to precisely manipulate the property to achieve fine-grained control over synthesized images. In this work, we leverage a recently proposed Contrastive Language Image Pretraining (CLIP) model to manipulate latent code with text to control image generation. We encode image and text prompts in shared embedding space, leveraging powerful image-text representation capabilities pretrained on contrastive language images to manipulate partial style codes in the latent code. For multiple fine-grained attribute manipulations, we propose multiple attribute manipulation frameworks. Compared with previous CLIP-driven methods, our method can perform high-quality attribute editing much faster with less coupling between attributes. Extensive experimental illustrate the effectiveness of our approach",
    "volume": "main",
    "checked": false,
    "id": "5aeade1f8bd33c6b7c93c034648297704450553a",
    "citation_count": 5
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Hanel_Enhancing_Fairness_of_Visual_Attribute_Predictors_ACCV_2022_paper.html": {
    "title": "Enhancing Fairness of Visual Attribute Predictors",
    "abstract": "The performance of deep neural networks for image recognition tasks such as predicting a smiling face is known to degrade with under-represented classes of sensitive attributes. We address this problem by introducing fairness-aware regularization losses based on batch estimates of Demographic Parity, Equalized Odds, and a novel Intersection-over-Union measure. The experiments performed on facial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma classification challenge show the effectiveness of our proposed fairness losses for bias mitigation as they improve model fairness while maintaining high classification performance. To the best of our knowledge, our work is the first attempt to incorporate these types of losses in an end-to-end training scheme for mitigating biases of visual attribute predictors",
    "volume": "main",
    "checked": true,
    "id": "7f67c26c25dd266ff9158b6524667b5ca42437c8",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Blind_Image_Super-Resolution_with_Degradation-Aware_Adaptation_ACCV_2022_paper.html": {
    "title": "Blind Image Super-Resolution with Degradation-Aware Adaptation",
    "abstract": "Most existing super-resolution (SR) methods are designed to restore high resolution (HR) images from certain low resolution (LR) images with a simple degradation, e.g. bicubic downsampling. Their generalization capability to real-world degradation is limited because it often couples several degradation factors such as noise and blur. To solve this problem, existing blind SR methods rely on either explicit degradation estimation or translation to bicubicly downsampled LR images, where inaccurate estimation or translation would severely deteriorate the SR performance. In this paper, we propose a plug-and-play module, which could be applied to any existing image super-resolution model for feature-level adaptation to improve the generalization ability to real-world degraded images. Specifically, the degradation encoder computes an implicit degradation representation which is supervised by a ranking loss based on the degradation level. The degradation representation then works as a kind of condition and is applied to the existing image super-resolution model pretrained on bicubicly downsampled LR images through the proposed region-aware modulation. With the proposed method, the base super-resolution model could be fine-tuned to adapt to the condition of degradation representation for further improvement. Experimental results on both synthetic and real-world datasets show that the proposed image SR method with compact model size performs favorably against state-of-the-art methods",
    "volume": "main",
    "checked": false,
    "id": "fef70be5ee0fd80d53c5f91dd760a2cdac010a77",
    "citation_count": 47
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_Causal-SETR_A_SEgmentation_TRansformer_Variant_Based_on_Causal_Intervention_ACCV_2022_paper.html": {
    "title": "Causal-SETR: A SEgmentation TRansformer Variant Based on Causal Intervention",
    "abstract": "We present a novel SEgmentaion TRansformer variant based on causal intervention. It serves as an improved vision encoder for semantic segmentation. Many studies have proved that vision transformers (ViT) can achieve a competitive benchmark on these downstream tasks, which shows that they can learn feature representations well. In other words, it is good at observing the instance from the image. However, in the human visual system, to recognize the objects in the scene, it is necessary to observe the objects themselves and introduce some prior knowledge for producing higher confidence results. Inspired by this, we introduced a structural causal model \n(SCM) to model images, category labels, and context. Beyond observing, we propose a causal intervention method by removing the confounding bias of global context and plugging it in the ViT encoder. Unlike other sequence-to-sequence prediction tasks, we use causal intervention instead of likelihood. Besides, the proxy training objective of the framework is to predict the contextual objects of a region. Finally, we combine this encoder with the segmentation decoder. Experiments show that our proposed method is flexible and effective",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Three-Stage_Bidirectional_Interaction_Network_for_Efficient_RGB-D_Salient_Object_Detection_ACCV_2022_paper.html": {
    "title": "Three-Stage Bidirectional Interaction Network for Efficient RGB-D Salient Object Detection",
    "abstract": "The addition of depth maps improves the performance of salient object detection (SOD). However, most existing RGB-D SOD methods are inefficient. We observe that existing models take into account the respective advantages of the two modalities but do not fully explore the roles of cross-modality features of various levels. To this end, we remodel the relationship between RGB features and depth features from a new perspective of the feature encoding stage and propose a three-stage bidirectional interaction network (TBINet). Specifically, to obtain robust feature representations, we propose three interaction strategies: bidirectional attention guidance (BAG), bidirectional feature supplement (BFS), and shared network, and use them for the three stages of feature encoder, respectively. In addition, we propose a cross-modality feature aggregation (CFA) module for feature aggregation and refinement. Our model is lightweight (3.7 M parameters) and fast (329 ms on CPU). Experiments on six benchmark datasets show that TBINet outperforms other SOTA methods. Our model achieves the best performance and efficiency trade-off",
    "volume": "main",
    "checked": false,
    "id": "db277051812fca945f91587248065012cf455195",
    "citation_count": 51
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Jiang_APAUNet_Axis_Projection_Attention_UNet_for_Small_Target_in_3D_ACCV_2022_paper.html": {
    "title": "APAUNet: Axis Projection Attention UNet for Small Target in 3D Medical Segmentation",
    "abstract": "In 3D medical image segmentation, small targets segmentation is crucial for diagnosis but still faces challenges. In this paper, we propose the Axis Projection Attention UNet, named APAUNet, for 3D medical image segmentation, especially for small targets. Considering the large proportion of the background in the 3D feature space, we introduce a projection strategy to project the 3D features into three orthogonal 2D planes to capture the contextual attention from different views. In this way, we can filter out the redundant feature information and mitigate the loss of critical information for small lesions in 3D scans. Then we utilize a dimension hybridization strategy to fuse the 3D features with attention from different axes and merge them by a weighted summation to adaptively learn the importance of different perspectives. Finally, in the APA Decoder, we concatenate both high and low resolution features in the 2D projection process, thereby obtaining more precise multi-scale information, which is vital for small lesion segmentation. Quantitative and qualitative experimental results on two public datasets (BTCV and MSD) demonstrate that our proposed APAUNet outperforms the other methods. Concretely, our APAUNet achieves an average dice score of 87.84 on BTCV, 84.48 on MSD-Liver and 69.13 on MSD-Pancreas, and significantly surpass the previous SOTA methods on small targets",
    "volume": "main",
    "checked": true,
    "id": "ab7894a854a6ce64ae9033bd89aaf45974151cf4",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhou_Modular_Degradation_Simulation_and_Restoration_for_Under-Display_Camera_ACCV_2022_paper.html": {
    "title": "Modular Degradation Simulation and Restoration for Under-Display Camera",
    "abstract": "Under-display camera (UDC) provides an elegant solution for full-screen smartphones. However, UDC captured images suffer from severe degradation since sensors lie under the display. Although this issue can be tackled by image restoration networks, these networks require large-scale image pairs for training. To this end, we propose a modular network dubbed MPGNet trained using the generative adversarial network (GAN) framework for simulating UDC imaging. Specifically, we note that the UDC imaging degradation process contains brightness attenuation, blurring, and noise corruption. Thus we model each degradation with a characteristic-related modular network, and all modular networks are cascaded to form the generator. Together with a pixel-wise discriminator and supervised loss, we can train the generator to simulate the UDC imaging degradation process. Furthermore, we present a Transformer-style network named DWFormer for UDC image restoration. For practical purposes, we use depth-wise convolution instead of the multi-head self-attention to aggregate local spatial information. Moreover, we propose a novel channel attention module to aggregate global information, which is critical for brightness recovery. We conduct evaluations on the UDC benchmark, and our method surpasses the previous state-of-the-art models by 1.23 dB on the P-OLED track and 0.71 dB on the T-OLED track, respectively",
    "volume": "main",
    "checked": true,
    "id": "038135cf31ca521477aafd9f25d95f41dd4d11b8",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chung_Domain_Generalized_RPPG_Network_Disentangled_Feature_Learning_with_Domain_Permutation_ACCV_2022_paper.html": {
    "title": "Domain Generalized RPPG Network: Disentangled Feature Learning with Domain Permutation and Domain Augmentation",
    "abstract": "Remote photoplethysmography (rPPG) offers a contactless method for monitoring physiological signals from facial videos. Existing learning-based methods, although work effectively on intra-dataset scenarios, degrade severely on cross-dataset testing. In this paper, we address the cross-dataset testing as a domain generalization problem and propose a novel DG-rPPGNet to learn a domain generalized rPPG estimator. To this end, we develop a feature disentangled learning framework to disentangle rPPG, identity, and domain features from input facial videos. Next, we propose a domain permutation strategy to further constrain the disentangled rPPG features to be invariant to different domains. Finally, we design a novel adversarial domain augmentation strategy to enlarge the domain sphere of DG-rPPGNet. Our experimental results show that DG-rPPGNet outperforms other rPPG estimation methods in many cross-domain settings on UBFC-rPPG, PURE, COHFACE, and VIPL-HR datasets",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Ding_Adaptive_Range_guided_Multi-view_Depth_Estimation_with_Normal_Ranking_Loss_ACCV_2022_paper.html": {
    "title": "Adaptive Range guided Multi-view Depth Estimation with Normal Ranking Loss",
    "abstract": "Deep learning algorithms for Multi-view Stereo (MVS) have surpassed traditional MVS methods in recent years, due to enhanced reconstruction quality and runtime. Learning-based methods, on the other side, continue to generate overly smoothed depths, resulting in poor reconstruction. In this paper, we aim to boost depth estimation (BDE) for MVS and present an approach for reconstructing high-quality point clouds with precise depth prediction. This method is termed as BDE-MVSNet. We present a non-linear technique that derives an adaptive depth range (ADR) from the estimated probability, motivated by distinctive differences in estimated probability between foreground and background pixels. ADR offers accurate estimation while processing same-resolution depth maps in only two stages since the depth range is well-adapted for each pixel. ADR also tends to decrease fuzzy boundaries via upsampling low-resolution depth maps between stages. Additionally, we provide a novel structure-guided normal ranking (SGNR) loss that imposes geometrical consistency in boundary areas by using the surface normal vector. Extensive experiments on DTU dataset, Tanks and Temples benchmark, and BlendedMVS dataset demonstrate that our method outperforms known methods and achieves state-of-the-art performance",
    "volume": "main",
    "checked": false,
    "id": "4ed7a5b25fe8677b565757f02dd27b94e8d15e95",
    "citation_count": 1
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lai_Gated_cross_word-visual_attention-driven_generative_adversarial_networks_for_text-to-image_synthesis_ACCV_2022_paper.html": {
    "title": "Gated cross word-visual attention-driven generative adversarial networks for text-to-image synthesis",
    "abstract": "The main objective of text-to-image (Txt2Img) synthesis is to generate realistic images from text descriptions. We propose to insert a gated cross word-visual attention unit (GCAU) into the conventional multiple-stage generative adversarial network Txt2Img framework. Our GCAU consists of two key components. First, a cross word-visual attention mechanism is proposed to draw fine-grained details at different subregions of the image by focusing on the relevant words (via the visual-to-word attention), and select important words by paying attention to the relevant synthesized subregions of the image (via the word-to-visual attention). Second, a gated refinement mechanism is proposed to dynamically select important word information for refining the generated image. Extensive experiments are conducted to demonstrate the superior image generation performance of the proposed approach on CUB and MS-COCO benchmark datasets",
    "volume": "main",
    "checked": false,
    "id": "aff93cf7b4a70edc4565ebe1688931b0b38fda83",
    "citation_count": 4
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Cai_PBCStereo_A_Compressed_Stereo_Network_with_Pure_Binary_Convolutional_Operations_ACCV_2022_paper.html": {
    "title": "PBCStereo: A Compressed Stereo Network with Pure Binary Convolutional Operations",
    "abstract": "Although end-to-end stereo matching networks achieve great performance for disparity estimation, most of them require far too many floating-point operations to deploying on resource-constrained devices. To solve this problem, we propose PBCStereo, the first lightweight stereo network using pure binarized convolutional operations. The degradation of feature diversity, which is aggravated by binary deconvolution, is alleviated via our novel upsampling module (IBC). Furthermore, we propose an effective coding method, named BIL, for the insufficient binarization of the input layer. Based on IBC modules and BIL coding, all convolutional operations become binary in our stereo matching pipeline. PBCStereo gets 39x saving in OPs while achieving comparable accuracy on SceneFlow and KITTI datasets",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_HiCo_Hierarchical_Contrastive__Learning_for_Ultrasound_Video_Model_Pretraining_ACCV_2022_paper.html": {
    "title": "HiCo: Hierarchical Contrastive Learning for Ultrasound Video Model Pretraining",
    "abstract": "The self-supervised ultrasound (US) video model pretraining can use a small amount of labeled data to achieve one of the most promising results on US diagnosis. However, it does not take full advantage of multi-level knowledge for learning deep neural networks (DNNs), and thus is difficult to learn transferable feature representations. This work proposes a hierarchical contrastive learning (HiCo) method to improve the transferability for the US video model pretraining. HiCo introduces both peer-level semantic alignment and cross-level semantic alignment to facilitate the interaction between different semantic levels, which can effectively accelerate the convergence speed, leading to better generalization and adaptation of the learned model. Additionally, a softened objective function is implemented by smoothing the hard labels, which can alleviate the negative effect caused by local similarities of images between different classes. Experiments with HiCo on five datasets demonstrate its favorable results over state-of-the-art approaches. The source code of this work is publicly available at https://github.com/983632847/HiCo",
    "volume": "main",
    "checked": true,
    "id": "d04a5b5b49f957fc03f9e9c183af5dd73ad91fa6",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_SymmNeRF_Learning_to_Explore_Symmetry_Prior_for_Single-View_View_Synthesis_ACCV_2022_paper.html": {
    "title": "SymmNeRF: Learning to Explore Symmetry Prior for Single-View View Synthesis",
    "abstract": "We study the problem of novel view synthesis of objects from a single image. Existing methods have demonstrated the potential in single-view view synthesis. However, they still fail to recover the fine appearance details, especially in self-occluded areas. This is because a single view only provides limited information. We observe that manmade objects usually exhibit symmetric appearances, which introduce additional prior knowledge. Motivated by this, we investigate the potential performance gains of explicitly embedding symmetry into the scene representation. In this paper, we propose SymmNeRF, a neural radiance field (NeRF) based framework that combines local and global conditioning under the introduction of symmetry priors. In particular, SymmNeRF takes the pixel-aligned image features and the corresponding symmetric features as extra inputs to the NeRF, whose parameters are generated by a hypernetwork. As the parameters are conditioned on the image-encoded latent codes, SymmNeRF is thus scene-independent and can generalize to new scenes. Experiments on synthetic and real-world datasets show that SymmNeRF synthesizes novel views with more details regardless of the pose transformation, and demonstrates good generalization when applied to unseen objects. Code is available at: https://github.com/xingyi-li/SymmNeRF",
    "volume": "main",
    "checked": true,
    "id": "9d251a51cad03c0716d2c136f073b33ab3066c5c",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Maximov_Decoupling_identity_and_visual_quality_for_image_and_video_anonymization_ACCV_2022_paper.html": {
    "title": "Decoupling identity and visual quality for image and video anonymization",
    "abstract": "The widespread usage of computer vision applications in the public domain has opened the delicate question of image data privacy. In recent years, computer vision researchers have proposed technological solutions to anonymize image and video data so that computer vision systems can still be used without compromising data privacy. While promising, these methods come with a range of limitations, including low diversity of outputs, low-resolution generation quality, the appearance of artifacts when handling extreme poses, and non-smooth temporal consistency. In this work, we propose a novel network based on generative adversarial networks (GANs) for face anonymization in images and videos. The key insight of our approach is to decouple the problems of image generation and image blending. This allows us to reach significant improvements in image quality, diversity, and temporal consistency while making possible to train the network in different tasks and datasets. Furthermore, we show that our framework is able to anonymize faces containing extreme poses, a long-standing problem in the field",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Yuan_Slice-mask_based_3D_Cardiac_Shape_Reconstruction_from_CT_volume_ACCV_2022_paper.html": {
    "title": "Slice-mask based 3D Cardiac Shape Reconstruction from CT volume",
    "abstract": "An accurate 3D ventricular model is essential for diagnosing and analyzing cardiovascular disease. It is challenging to obtain accurate patient-specific models on scarce data via widely accepted deep-learning methods. To fully use the characteristics of medical volume-based images, we present a slice-mask representation to better regress the parameters of the 3D model. A data synthesis strategy is proposed to alleviate the lack of training data by sampling in the constructed statistical shape model space and obtaining the corresponding slice-masks. We train the end-to-end structure by combining the segmentation and parametric regression modules. Furthermore, we establish a larger left ventricular CT dataset than before, which fills the gap in relevant data of the healthy population. Our method is evaluated on both synthetic data and real cardiac scans. Experiments demonstrate that our method can achieve advanced results in shape reconstruction and segmentation tasks. Code is publicly available at https://github.com/yuan-xiaohan/Slice-mask-based-3D-Cardiac-Shape-Reconstruction",
    "volume": "main",
    "checked": false,
    "id": "fcd4d482d815caa7b85835151c77bfb83a26e359",
    "citation_count": 1
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Dolatabadi_COLLIDER_A_Robust_Training_Framework_for_Backdoor_Data_ACCV_2022_paper.html": {
    "title": "COLLIDER: A Robust Training Framework for Backdoor Data",
    "abstract": "Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An adversary poisons some of the training data in such attacks by installing a trigger. The goal is to make the trained DNN output the attacker's desired class whenever the trigger is activated while performing as usual for clean data. Various approaches have recently been proposed to detect malicious backdoored DNNs. However, a robust, end-to-end training approach, like adversarial training, is yet to be discovered for backdoor poisoned data. In this paper, we take the first step toward such methods by developing a robust training framework, COLLIDER, that selects the most prominent samples by exploiting the underlying geometric structures of the data. Specifically, we effectively filter out candidate poisoned data at each training epoch by solving a geometrical coreset selection objective. We first argue how clean data samples exhibit (1) gradients similar to the clean majority of data and (2) low local intrinsic dimensionality (LID). Based on these criteria, we define a novel coreset selection objective to find such samples, which are used for training a DNN. We show the effectiveness of the proposed method for robust training of DNNs on various poisoned datasets, reducing the backdoor success rate significantly",
    "volume": "main",
    "checked": true,
    "id": "1db54228a831d893c1580e21c9f99fb7f4144381",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Tian_A_General_Divergence_Modeling_Strategy_for_Salient_Object_Detection_ACCV_2022_paper.html": {
    "title": "A General Divergence Modeling Strategy for Salient Object Detection",
    "abstract": "Salient object detection is subjective in nature, which implies that multiple estimations should be related to the same input image. Most existing salient object detection models are deterministic following a point to point estimation learning pipeline, making them incapable of estimating the predictive distribution. Although latent variable model based stochastic prediction networks exist to model the prediction variants, the latent space based on the single clean saliency annotation is less reliable in exploring the subjective nature of saliency, leading to less effective saliency \"divergence modeling\". Given multiple saliency annotations, we introduce a general divergence modeling strategy via random sampling, and apply our strategy to an ensemble based framework and three latent variable model based solutions to explore the \"subjective nature\" of saliency. Experimental results prove the superior performance of our general divergence modeling strategy",
    "volume": "main",
    "checked": true,
    "id": "54a2ec107d43a9134b1593c2b30f086957078d75",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Kalb_Causes_of_Catastrophic_Forgetting_in_Class-Incremental_Semantic_Segmentation_ACCV_2022_paper.html": {
    "title": "Causes of Catastrophic Forgetting in Class-Incremental Semantic Segmentation",
    "abstract": "Class-incremental learning for semantic segmentation (CiSS) is presently a highly researched field which aims at updating a semantic segmentation model by sequentially learning new semantic classes. A major challenge in CiSS is overcoming the effects of catastrophic forgetting, which describes the sudden drop of accuracy on previously learned classes after the model is trained on a new set of classes. \nDespite latest advances in mitigating catastrophic forgetting, the underlying causes of forgetting specifically in CiSS are not well understood.\nTherefore, in a set of experiments and representational analyses, we demonstrate that the semantic shift of the background class and a bias towards new classes are the major causes of forgetting in CiSS. Furthermore, we show that both causes mostly manifest themselves in deeper classification layers of the network, while the early layers of the model are not affected. Finally, we demonstrate how both causes are effectively mitigated utilizing the information contained in the background, with the help of knowledge distillation and an unbiased cross-entropy loss",
    "volume": "main",
    "checked": true,
    "id": "cd7f48385cb62dda23cded6345d4d2c9129103b8",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Kim_Cross-View_Self-Fusion_for_Self-Supervised_3D_Human_Pose_Estimation_in_the_ACCV_2022_paper.html": {
    "title": "Cross-View Self-Fusion for Self-Supervised 3D Human Pose Estimation in the Wild",
    "abstract": "Human pose estimation methods have recently shown remarkable results with supervised learning that requires large amounts of labeled training data. However, such training data for various human activities does not exist since 3D annotations are acquired with traditional motion capture systems that usually require a controlled indoor environment. To address this issue, we propose a self-supervised approach that learns a monocular 3D human pose estimator from unlabeled multi-view images by using multi-view consistency constraints. Furthermore, we refine inaccurate 2D poses, which adversely affect 3D pose predictions, using the property of canonical space without relying on camera calibration. Since we do not require camera calibrations to leverage the multi-view information, we can train a network from in-the-wild environments. The key idea is to fuse the 2D observations across views and combine predictions from the observations to satisfy the multi-view consistency during training. We outperform state-of-the-art methods in self-supervised learning on the two benchmark datasets Human3.6M and MPI-INF-3DHP as well as on the in-the-wild dataset SkiPose",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wiles_Compressed_Vision_for_Efficient_Video_Understanding_ACCV_2022_paper.html": {
    "title": "Compressed Vision for Efficient Video Understanding",
    "abstract": "Experience and reasoning occur across multiple temporal scales: milliseconds, seconds, hours or days. The vast majority of computer vision research, however, still focuses on individual images or short videos lasting only a few seconds. This is because handling longer videos require more scalable approaches even to process them. In this work, we propose a framework enabling research on hour-long videos with the same hardware that can now process second-long videos. We replace standard video compression, e.g. JPEG, with neural compression and show that we can directly feed  compressed videos as inputs to regular video networks. Operating on compressed videos improves efficiency at all pipeline levels -- data transfer, speed and memory -- making it possible to train models faster and on much longer videos. Processing compressed signals has, however, the downside of precluding standard augmentation techniques if done naively. We address that by introducing a small network that can apply transformations to latent codes corresponding to commonly used augmentations in the original video space. We demonstrate that with our compressed vision pipeline, we can train video models more efficiently on popular benchmarks such as Kinetics600 and COIN. We also perform proof-of-concept experiments with new tasks defined over hour-long videos at standard frame rates. Processing such long videos is impossible without using compressed representation",
    "volume": "main",
    "checked": true,
    "id": "1dc68762b4e2fb146349bdfb401f2ffc2e25158d",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Kaneda_Flare_Transformer_Solar_Flare_Prediction_using_Magnetograms_and_Sunspot_Physical_ACCV_2022_paper.html": {
    "title": "Flare Transformer: Solar Flare Prediction using Magnetograms and Sunspot Physical Features",
    "abstract": "The prediction of solar flares is essential for reducing the potential damage to social infrastructures that are vital to society. However, predicting solar flares accurately is a very challenging task. Existing methods predict flares using either physical features or images, but the main bottleneck is that they sometimes incorrectly predict a class that is smaller than the actual solar flare. In this paper, we propose the Flare Transformer, a solar flare prediction model that handles both images and physical features through the Magnetogram Module and the Sunspot Feature Module. The transformer attention mechanism is introduced to model the temporal relationships between input features. We also introduce a new differentiable loss function to balance the two major metrics of the Gandin--Murphy--Gerrity score and Brier skill score. We validate our model on a publicly available dataset. The results show that the Flare Transformer outperformed the baseline methods in terms of the Gandin--Murphy--Gerrity score and true skill statistic, and achieved better performance than those given by human experts",
    "volume": "main",
    "checked": false,
    "id": "8cecb553dc72eb8df289d06aa86e901345f5333c",
    "citation_count": 4
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Semi-Supervised_Semantic_Segmentation_with_Uncertainty-guided_Self_Cross_Supervision_ACCV_2022_paper.html": {
    "title": "Semi-Supervised Semantic Segmentation with Uncertainty-guided Self Cross Supervision",
    "abstract": "As a powerful way of realizing semi-supervised segmentation, the cross supervision method learns cross consistency based on independent ensemble models using abundant unlabeled images. \nIn this work, we propose a novel cross supervision method, namely uncertainty-guided self cross supervision (USCS). \nTo avoid multiplying the cost of computation resources caused by ensemble models, we first design a multi-input multi-output (MIMO) segmentation model which can generate multiple outputs with the shared model. The self cross supervision is imposed over the results from one MIMO model, heavily saving the cost of parameters and calculations. On the other hand, to further alleviate the large noise in pseudo labels caused by insufficient representation ability of the MIMO model, we employ uncertainty as guided information to encourage the model to focus on the high confident regions of pseudo labels and mitigate the effects of wrong pseudo labeling in self cross supervision, improving the performance of the segmentation model.\nExtensive experiments show that our method achieves state-of-the-art performance while saving 40.5% and 49.1% cost on parameters and calculations",
    "volume": "main",
    "checked": false,
    "id": "a3dbcea7747bd2f6cd5ce6cad5d2180e405e4a33",
    "citation_count": 1
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chen_GB-CosFace_Rethinking_Softmax-based_Face_Recognition_from_the_Perspective_of_Open_ACCV_2022_paper.html": {
    "title": "GB-CosFace: Rethinking Softmax-based Face Recognition from the Perspective of Open Set Classification",
    "abstract": "State-of-the-art face recognition methods typically take the multi-classification pipeline and adopt the softmax-based loss for optimization. Although these methods have achieved great success, the softmax-based loss has its limitation from the perspective of open set classification: the multi-classification objective in the training phase does not strictly match the objective of open set classification testing. In this paper, we derive a new loss named global boundary CosFace (GB-CosFace). Our GB-CosFace introduces an adaptive global boundary to determine whether two face samples belong to the same identity so that the optimization objective is aligned with the testing process from the perspective of open set classification. Meanwhile, since the loss formulation is derived from the softmax-based loss, our GB-CosFace retains the excellent properties of the softmax-based loss, and CosFace is proved to be a special case of the proposed loss. We analyze and explain the proposed GB-CosFace geometrically. Comprehensive experiments on multiple face recognition benchmarks indicate that the proposed GB-CosFace outperforms current state-of-the-art face recognition losses in mainstream face recognition tasks. Compared to CosFace, our GB-CosFace improves 5.30%, 0.70%, and 0.36% at TAR@FAR=1e-6, 1e-5, 1e-4 on IJB-C benchmark",
    "volume": "main",
    "checked": true,
    "id": "54e92a5b08347217195fbd9edd0745550eaf48e7",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Cui_Dynamic_Feature_Aggregation_for_Efficient_Video_Object_Detection_ACCV_2022_paper.html": {
    "title": "Dynamic Feature Aggregation for Efficient Video Object Detection",
    "abstract": "Video object detection is a fundamental yet challenging task in computer vision. One practical solution is to take advantage of temporal information from the video and apply feature aggregation to enhance the object features in each frame. Though effective, those existing methods always suffer from low inference speeds because they use a fixed number of frames for feature aggregation regardless of the input frame. Therefore, this paper aims to improve the inference speed of the current feature aggregation-based video object detectors while maintaining their performance. To achieve this goal, we propose a vanilla dynamic aggregation module that adaptively selects the frames for feature enhancement. Then, we extend the vanilla dynamic aggregation module to a more effective and reconfigurable deformable version. Finally, we introduce inplace distillation loss to improve the representations of objects aggregated with fewer frames. Extensive experimental results validate the effectiveness and efficiency of our proposed methods: On the ImageNet VID benchmark, integrated with our proposed methods, FGFA and SELSA can improve the inference speed by 31% and 76% respectively while getting comparable performance on accuracy",
    "volume": "main",
    "checked": false,
    "id": "cdd98aae1979d70e14c2cc71bee60266e7f3605f",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chung_Shape_Prior_is_Not_All_You_Need_Discovering_Balance_between_ACCV_2022_paper.html": {
    "title": "Shape Prior is Not All You Need: Discovering Balance between Texture and Shape bias in CNN",
    "abstract": "As Convolutional Neural Network (CNN) trained under ImageNet is known to be biased in image texture rather than object shapes, recent works proposed that elevating shape awareness of the CNNs makes them similar to human visual recognition. However, beyond the ImageNet-trained CNN, how can we make CNNs similar to human vision in the wild? In this paper, we present a series of analyses to answer this question. First, we propose AdaBA, a novel method of quantitatively illustrating CNN's shape and texture bias by resolving several limits of the prior method. With the proposed AdaBA, we focused on fine-tuned CNN's bias landscape which previous studies have not dealt with. We discover that fine-tuned CNNs are also biased to texture, but their bias strengths differ along with the downstream dataset; thus, we presume a data distribution is a root cause of texture bias exists. To tackle this root cause, we propose a granular labeling scheme, a simple but effective solution that redesigns the label space to pursue a balance between texture and shape biases. We empirically examine that the proposed scheme escalates CNN's classification and OOD detection performance. We expect key findings and proposed methods in the study to elevate understanding of the CNN and yield an effective solution to mitigate this texture bias",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Liu_FunnyNet_Audiovisual_Learning_of_Funny_Moments_in_Videos_ACCV_2022_paper.html": {
    "title": "FunnyNet: Audiovisual Learning of Funny Moments in Videos",
    "abstract": "Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as facial expression, body language, dialogues and culture. In this paper, we propose FunnyNet, a model that relies on cross- and self-attention for both visual and audio data to predict funny moments in videos. Unlike most methods that focus on text with or without visual data to identify funny moments, in this work in addition to visual cues, we exploit audio. Audio comes naturally with videos, and moreover it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive experiments and analysis show that FunnyNet successfully exploits visual and auditory cues to identify funny moments, while our findings corroborate our claim that audio is more suitable than text for funny moment prediction. FunnyNet sets the new state of the art for laughter detection with audiovisual or multimodal cues on all datasets",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Huang_A_Joint_Framework_Towards_Class-aware_and_Class-agnostic_Alignment_for_Few-shot_ACCV_2022_paper.html": {
    "title": "A Joint Framework Towards Class-aware and Class-agnostic Alignment for Few-shot Segmentation",
    "abstract": "Few-shot segmentation (FSS) aims to segment objects of unseen classes given only a few annotated support images. Most existing methods simply stitch query features with independent support prototypes and segment the query image by feeding the mixed features to a decoder. Although significant improvements have been achieved, existing methods are still face class biases due to class variants and background confusion. In this paper, we propose a joint framework that combines more valuable class-aware and class-agnostic alignment guidance to facilitate the segmentation. Specifically, we design a hybrid alignment module which establishes multi-scale query-support correspondences to mine the most relevant class-aware information for each query image from the corresponding support features. In addition, we explore utilizing base-classes knowledge to generate class-agnostic prior mask which makes a distinction between real background and foreground by highlighting all object regions, especially those of unseen classes. By jointly aggregating class-aware and class-agnostic alignment guidance, better segmentation performances are obtained on query images. Extensive experiments on PASCAL-5i and COCO-20i datasets demonstrate that our proposed joint framework performs better, especially on the 1-shot setting",
    "volume": "main",
    "checked": true,
    "id": "d4010a982aa1a0092a449c6d9d354eb465fe4bfd",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lv_Continuous_Self-Study_Scene_Graph_Generation_with_Self-Knowledge_Distillation_and_Spatial_ACCV_2022_paper.html": {
    "title": "Continuous Self-Study: Scene Graph Generation with Self-Knowledge Distillation and Spatial Augmentation",
    "abstract": "As an extension of visual detection tasks, scene graph generation (SGG) has drawn increasing attention with the achievement of complex image understanding. However, it still faces two challenges: one is the distinguishing of objects with high visual similarity, the other is the discriminating of relationships with long-tailed bias. In this paper, we propose a Continuous Self-Study model (CSS) with self-knowledge distillation and spatial augmentation to refine the detection of hard samples. We design a long-term memory structure for CSS to learn its own behavior with the context feature, which can perceive the hard sample of itself and focus more on similar targets in different scenes. Meanwhile, a fine-grained relative position encoding method is adopted to augment spatial features and supplement relationship information. On the Visual Genome benchmark, experiments show that the proposed CSS achieves obvious improvements over the previous state-of-the-art methods. Our code is available at https://github.com/LINYE1998/Continuous_Self_Study",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lan_Temporal-aware_Siamese_Tracker_Integrate_Temporal_Context_for_3D_Object_Tracking_ACCV_2022_paper.html": {
    "title": "Temporal-aware Siamese Tracker: Integrate Temporal Context for 3D Object Tracking",
    "abstract": "Learning discriminative target-specific feature representation for object localization is the core of the 3D Siamese object tracking algorithms. \nCurrent Siamese trackers focus on aggregating the target information from the latest template into the search area for target-specific feature construction, which presents the limited performance in the case of object occlusion or object missing. \nTo this end, in this paper, we propose a novel temporal-aware Siamese tracking framework, where the rich target clue lying in a set of historical templates is integrated into the search area for reliable target-specific feature aggregation. \nSpecifically, our method consists of three modules, including a template set sampling module, a temporal feature enhancement module and a temporal-aware feature aggregation module. \nIn the template set sampling module, an effective scoring network is proposed to evaluate the tracking quality of the template so that the high-quality templates are collected to form the historical template set. \nThen, with the initial feature embeddings of the historical templates, the temporal feature enhancement module concatenates all template embeddings as a whole and then feeds them into a linear attention module for cross-template feature enhancement. \nFurthermore, the temporal-aware feature aggregation module aggregates the target clue lying in each template into the search area to construct multiple historical target-specific search-area features. \nParticularly, we follow the collection orders of the templates to fuse all generated target-specific features via an RNN-based module so that the fusion weight of the previous template information can be discounted to better fit the current tracking state. \nFinally, we feed the temporal fused target-specific feature into a modified CenterPoint detection head for target position regression.\nExtensive experiments on KITTI, NuScenes and waymo open datasets show the effectiveness of our proposed method",
    "volume": "main",
    "checked": false,
    "id": "659c65a028653ae90bea58e4414cc21330de1d2b",
    "citation_count": 6
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Koren_Consistent_Semantic_Attacks_on_Optical_Flow_ACCV_2022_paper.html": {
    "title": "Consistent Semantic Attacks on Optical Flow",
    "abstract": "We present a novel approach for semantically targeted adversarial\nattacks on Optical Flow. In such attacks the goal is to corrupt\nthe flow predictions of a specific object category or instance. Usually,\nan attacker seeks to hide the adversarial perturbations in the input.\nHowever, a quick scan of the output reveals the attack. In contrast, our\nmethod helps to hide the attacker's intent in the output flow as well. We\nachieve this thanks to a regularization term that encourages off-target\nconsistency. We perform extensive tests on leading optical flow models to\ndemonstrate the benefits of our approach in both white-box and blackbox\nsettings. Also, we demonstrate the effectiveness of our attack on\nsubsequent tasks that depend on the optical flow",
    "volume": "main",
    "checked": true,
    "id": "014447fa3fa40d046d3bbe752125ae74092d0fb6",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_Neural_Plenoptic_Sampling_Learning_Light-field_from_Thousands_of_Imaginary_Eyes_ACCV_2022_paper.html": {
    "title": "Neural Plenoptic Sampling: Learning Light-field from Thousands of Imaginary Eyes",
    "abstract": "The Plenoptic function describes light rays observed from any given position in every viewing direction. It is often parameterized as a 5-D function L(x, y, z, \\theta, \\phi) for a static scene. Capturing all the plenoptic functions in the space of interest is paramount for Image-Based Rendering (IBR) and Novel View Synthesis (NVS). It encodes a complete light-field (i.e., lumigraph) therefore allows one to freely roam in the space and view the scene from any location in any direction. However, achieving this goal by conventional light-field capture technique is expensive, requiring densely sampling the ray space using arrays of cameras or lenses. This paper proposes a much simpler solution to address this challenge by using only a small number of sparsely configured camera views as input. Specifically, we adopt a simple Multi-Layer Perceptron (MLP) network as a universal function approximator to learn the plenoptic function at every position in the space of interest. By placing virtual viewpoints (dubbed `imaginary eyes') at thousands of randomly sampled locations and leveraging multi-view geometric relationship, we train the MLP to regress the plenoptic function for the space. Our network is trained on a per-scene basis, and the training time is relatively short (in the order of tens of minutes). When the model is converged, we can freely render novel images. Extensive experiments demonstrate that our method well approximates the complete plenoptic function and generates high-quality results",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Causal_Property_based_Anti-Conflict_Modeling_with_Hybrid_Data_Augmentation_for_ACCV_2022_paper.html": {
    "title": "Causal Property based Anti-Conflict Modeling with Hybrid Data Augmentation for Unbiased Scene Graph Generation",
    "abstract": "Scene Graph Generation(SGG) aims to detect visual triplets\nof pairwise objects based on object detection. There are three key factors\nbeing explored to determine a scene graph: visual information, local\nand global context, and prior knowledge. However, conventional methods\nbalancing losses among these factors lead to conflict, causing ambiguity,\ninaccuracy, and inconsistency. In this work, to apply evidence theory to\nscene graph generation, a novel plug-and-play Causal Property based\nAnti-conflict Modeling (CPAM) module is proposed, which models key\nfactors by Dempster-Shafer evidence theory, and integrates quantitative\ninformation effectively. Compared with the existing methods, the proposed\nCPAM makes the training process interpretable, and also manages\nto cover more fine-grained relationships after inconsistencies reduction.\nFurthermore, we propose a Hybrid Data Augmentation (HDA) method,\nwhich facilitates data transfer as well as conventional debiasing methods\nto enhance the dataset. By combining CPAM with HDA, significant improvement\nhas been achieved over the previous state-of-the-art methods.\nAnd extensive ablation studies have also been conducted to demonstrate\nthe effectiveness of our method",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Cha_SAC-GAN__Face_Image_Inpainting_with_Spatial-aware_Attribute_Controllable_GAN_ACCV_2022_paper.html": {
    "title": "SAC-GAN : Face Image Inpainting with Spatial-aware Attribute Controllable GAN",
    "abstract": "The objective of image inpainting is refilling the masked area with semantically appropriate pixels and producing visually realistic images as an output. After the introduction of generative adversarial networks (GAN), many inpainting approaches are showing promising development. Several attempts have been recently made to control reconstructed output with the desired attribute on face images using exemplar images and style vectors. Nevertheless, conventional style vector has the limitation that to project style attribute representation onto linear vector without preserving dimensional information. We introduce spatial-aware attribute controllable GAN (SAC-GAN) for face image inpainting, which is effective for reconstructing masked images with desired controllable facial attributes with advantage of utilizing style tensors as spatial forms. Various experiments to control over facial characteristics demonstrate the superiority of our method compared with previous image inpainting methods",
    "volume": "main",
    "checked": false,
    "id": "91f6ad36f6fb8fcb731a7645b087467ecfab37fd",
    "citation_count": 1
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Revisiting_Unsupervised_Domain_Adaptation_Models_a_Smoothness_Perspective_ACCV_2022_paper.html": {
    "title": "Revisiting Unsupervised Domain Adaptation Models: a Smoothness Perspective",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to leverage the labeled source data and unlabeled target data to generalize better in the target domain. UDA methods utilize better domain alignment or carefully-designed regularizations to increase the discriminability of target features. However, most methods focus on directly increasing the distance between cluster centers of target features, i.e., enlarging inter-class variance, which intuitively increases the discriminability of target features and is easy to implement. However, due to intra-class variance optimization being under-explored, there are still some samples of the same class are prone to be classified into several classes. To handle this problem, we aim to equip UDA methods with the high smoothness constraint. We first define the model's smoothness as the predictions similarity within each class, and propose a simple yet effective technique LeCo (impLicit smoothness Constraint) to promote the smoothness. We construct the weak and strong \"views\" of each target sample and enforce the model predictions of these two views to be consistent. Besides, a new uncertainty measure named Instance Class Confusion conditions the consistency is proposed to guarantee the transferability. LeCo implicitly reduces the model sensitivity to perturbations for target samples and guarantees smaller intra-class variance. Extensive experiments show that the proposed technique improves various baseline approaches by a large margin, and helps yield comparable results to the state-of-the-arts on four public datasets. Our codes are publicly available at https://github.com/Wang-Xiaodong1899/LeCo_UDA",
    "volume": "main",
    "checked": false,
    "id": "d631987e1ba66a49ad9fad91ca98ac9b5e85b877",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Yi_Not_End-to-End_Explore_Multi-Stage_Architecture_for_Online_Surgical_Phase_Recognition_ACCV_2022_paper.html": {
    "title": "Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition",
    "abstract": "Surgical phase recognition is of particular interest to computer assisted surgery systems, in which the goal is to predict what phase is occurring at each frame for a surgery video. Networks with multi-stage architecture have been widely applied in many computer vision tasks with rich patterns, where a predictor stage first outputs initial predictions and an additional refinement stage operates on the initial predictions to perform further refinement. Existing works show that surgical video contents are well ordered and contain rich temporal patterns, making the multi-stage architecture well suited for the surgical phase recognition task. However, we observe that when simply applying the multi-stage architecture to the surgical phase recognition task, the end-to-end training manner will make the refinement ability fall short of its wishes. To address the problem, we propose a new non end-to-end training strategy and explore different designs of multi-stage architecture for surgical phase recognition task. For the non end-to-end training strategy, the refinement stage is trained separately with proposed two types of disturbed sequences. Meanwhile, we evaluate three different choices of refinement models to show that our analysis and solution are robust to the choices of specific multi-stage models. We conduct experiments on two public benchmarks, the M2CAI16 Workflow Challenge and the Cholec80 dataset. The SOTA comparable results show that the multi-stage architecture holds the great potential to boost the performance of existing single-stage models. Code is available at https://github.com/ChinaYi/NETE",
    "volume": "main",
    "checked": true,
    "id": "31cc50eacb6628da0acbadfc348846d28931bec3",
    "citation_count": 4
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Dai_Cluster_Contrast_for_Unsupervised_Person_Re-Identification_ACCV_2022_paper.html": {
    "title": "Cluster Contrast for Unsupervised Person Re-Identification",
    "abstract": "Thanks to the recent research development in contrastive learning, the gap of visual representation learning between supervised and unsupervised approaches has been gradually closed in the tasks of computer vision.\nIn this paper, we focus on the downstream task of unsupervised person re-identification (re-ID).\nState-of-the-art unsupervised re-ID methods train the neural networks using a dictionary-based non-parametric softmax loss. They store the pre-computed instance feature vectors inside the dictionary, assign pseudo labels to them using clustering algorithm, and compare the query instances to the cluster using a form of contrastive loss.\nTo enforce a consistent dictionary, that is the features in the dictionary are computed by a similar or the same encoder network,  we present Cluster Contrast which stores feature vectors and computes contrastive loss at the cluster level.\nMoreover, the momentum update is introduced to reinforce the cluster-level feature consistency in the sequential space.\nDespite the straightforward design, experiments on four representative re-ID benchmarks demonstrate the effective performance of our method",
    "volume": "main",
    "checked": true,
    "id": "a6a0775aaabca6f6ca18f9d281c284c4af9ecd40",
    "citation_count": 41
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Gandikota_A_Simple_Strategy_to_Provable_Invariance_via_Orbit_Mapping_ACCV_2022_paper.html": {
    "title": "A Simple Strategy to Provable Invariance via Orbit Mapping",
    "abstract": "Many applications require robustness, or ideally invariance, of\na neural network to certain transformations of input data. Most commonly,\nthis requirement is addressed by either augmenting the training data, using\nadversarial training, or defining network architectures that include the\ndesired invariance by design. In this work, we propose a method to make\nnetwork architectures provably invariant with respect to group actions by\nchoosing one element from a (possibly continuous) orbit based on a fixed\ncriterion. In a nutshell, we intend to 'undo' any possible transformation\nbefore feeding the data into the actual network. We analyze properties of\nsuch approaches, and demonstrate their advantages in terms of robustness\nand computational efficiency in several numerical examples. In particular,\nwe investigate the robustness with respect to rotations of images (which\ncan hold up to discretization artifacts) as well as the provable orientation\nand scaling invariance of 3D point cloud classification",
    "volume": "main",
    "checked": true,
    "id": "60ceb188fbd74fe8c9d0087fa7e9ff4122a9f10a",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhao_From_Sparse_to_Dense_Semantic_Graph_Evolutionary_Hashing_for_Unsupervised_ACCV_2022_paper.html": {
    "title": "From Sparse to Dense: Semantic Graph Evolutionary Hashing for Unsupervised Cross-Modal Retrieval",
    "abstract": "In recent years, cross-modal hashing has attracted an increasing attention due to its fast retrieval speed and low storage requirements. However, labeled datasets are limited in real application, and existing unsupervised cross-modal hashing algorithms usually employ heuristic geometric prior as semantics, which introduces serious deviations as the similarity score from original features cannot reasonably represent the relationships among instances. In this paper, we study the unsupervised deep cross-modal hash retrieval method and propose a novel Semantic Graph Evolutionary Hashing (SGEH) to solve the above problem. The key novelty of SGEH is its evolutionary affinity graph construction method. To be concrete, we explore the sparse similarity graph with clustering results, which evolve from fusing the affinity information from code-driven graph on intrinsic data and subsequently extends to dense hybrid semantic graph which restricts the process of hash code learning to learn more discriminative results. Moreover, the batch-inputs are chosen from edge set rather than vertexes for better exploring the original spatial information in the sparse graph. Experiments on four benchmark datasets demonstrate the superiority of our framework over the state-of-the-art unsupervised cross-modal retrieval methods",
    "volume": "main",
    "checked": false,
    "id": "9d05118a02dfbfdc00e15abee10862691b8d3d90",
    "citation_count": 4
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Guo_LHDR_HDR_Reconstruction_for_Legacy_Content_using_a_Lightweight_DNN_ACCV_2022_paper.html": {
    "title": "LHDR: HDR Reconstruction for Legacy Content using a Lightweight DNN",
    "abstract": "High dynamic range (HDR) image is widely-used in graphics and photography due to the rich information it contains. Recently the community has started using deep neural network (DNN) to reconstruct standard dynamic range (SDR) images into HDR. Albeit the superiority of current DNN-based methods, their application scenario is still limited: (1) heavy model impedes real-time processing, and (2) inapplicable to legacy SDR content with more degradation types. Therefore, we propose a lightweight DNN-based method trained to tackle legacy SDR. For better design, we reform the problem modeling and emphasize degradation model. Experiments show that our method reached appealing performance with minimal computational cost compared with others",
    "volume": "main",
    "checked": true,
    "id": "a7be56d8d34fabe1bb5f2977b98a6c5ad9dd2109",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Xia_Augmenting_Softmax_Information_for_Selective_Classification_with_Out-of-Distribution_Data_ACCV_2022_paper.html": {
    "title": "Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data",
    "abstract": "Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Combination (SIRC), that augments softmax-based confidence scores with feature-agnostic information such that their ability to identify OOD samples is improved without sacrificing separation between correct and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale datasets and convolutional neural network architectures show that SIRC is able to consistently match or outperform the baseline for SCOD, whilst existing OOD detection methods fail to do so.\nCode is available at https://github.com/Guoxoug/SIRC",
    "volume": "main",
    "checked": true,
    "id": "f1a46235b858dbcb365150c1ecd03807f33a74c8",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Image_Denoising_using_Convolutional_Sparse_Coding_Network_with_Dry_Friction_ACCV_2022_paper.html": {
    "title": "Image Denoising using Convolutional Sparse Coding Network with Dry Friction",
    "abstract": "Convolutional sparse coding model has been successfully used in some tasks such as signal or image processing and classification. The recently proposed  supervised convolutional sparse coding network (CSCNet) model based on the Minimum Mean Square Error (MMSE) approximation shows the similar PSNR value for image denoising problem with state of the art methods while using much fewer parameters. The CSCNet uses the learning convolutional iterative shrinkage-thresholding algorithms (LISTA) based on the convolutional dictionary setting. However, LISTA methods are known to converge to local minima. In this paper we proposed one novel algorithm based on LISTA with dry friction, named LISTDFA. The dry friction enters the LISTDFA algorithm through proximal mapping. Due to the nature of dry friction, the LISTDFA algorithm is proven to converge in a finite time. The corresponding iterative neural network preserves the computational simplicity of the original CSCNet, and can reach a better local minima practically",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_GaitStrip_Gait_Recognition_via_Effective_Strip-based_Feature_Representations_and_Multi-Level_ACCV_2022_paper.html": {
    "title": "GaitStrip: Gait Recognition via Effective Strip-based Feature Representations and Multi-Level Framework",
    "abstract": "Many gait recognition methods first partition the human gait into N-parts and then combine them to establish part-based feature representations.\nTheir gait recognition performance is often affected by partitioning strategies, which are empirically chosen in different datasets. However, we observe that strips as the basic component of parts are agnostic against different partitioning strategies. Motivated by this observation, we present a strip-based multi-level gait recognition network, named GaitStrip, to extract comprehensive gait information at different levels.\nTo be specific, our high-level branch explores the context of gait sequences and our low-level one focuses on detailed posture changes.\nWe introduce a novel StriP-Based feature extractor (SPB) to learn the strip-based feature representations by directly taking each strip of the human body as the basic unit. \nMoreover, we propose a novel multi-branch structure, called Enhanced Convolution Module (ECM), to extract different representations of gaits. ECM consists of the Spatial-Temporal feature extractor (ST), the Frame-Level feature extractor (FL) and SPB, and has two obvious advantages: \nFirst, each branch focuses on a specific representation, which can be used to improve the robustness of the network. Specifically, ST aims to extract spatial-temporal features of gait sequences, while FL is used to generate the feature representation of each frame.\nSecond, the parameters of the ECM can be reduced in test by introducing a structural re-parameterization technique.\nExtensive experimental results demonstrate that our GaitStrip achieves state-of-the-art performance in both normal walking and complex conditions. The source code is published at https://github.com/M-Candy77/GaitStrip",
    "volume": "main",
    "checked": true,
    "id": "4e49bb246d0e3edce4a79c2d7efaa49143e229d6",
    "citation_count": 2
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Active_Domain_Adaptation_with_Multi-level_Contrastive_Units_for_Semantic_Segmentation_ACCV_2022_paper.html": {
    "title": "Active Domain Adaptation with Multi-level Contrastive Units for Semantic Segmentation",
    "abstract": "To further reduce the cost of semi-supervised domain adaptation (SSDA) labeling, a more effective way is to use active learning (AL) to annotate a selected subset with specific properties. However, DA tasks are always addressed in two interactive aspects: domain transfer and the enhancement of discrimination, which requires the selected data to be both uncertain under the model and diverse in feature space. Contrary to AL in classification tasks, it is usually challenging to select pixels that contain both the above properties in segmentation tasks, leading to the complex design of pixel selection strategy. To address such an issue, we propose a novel Active Domain Adaptation scheme with Multi-level Contrastive Units (ADA-MCU) for semantic segmentation. A simple pixel selection strategy followed with the construction of multi-level contrastive units is introduced to optimize the model for both domain adaptation and active supervised learning. In practice, MCUs are constructed from intra-image, cross-image, and cross-domain levels by using both labeled and unlabeled pixels. At each level, we define contrastive losses from center-to-center and pixel-to-pixel manners, with the aim of jointly aligning the category centers and reducing outliers near the decision boundaries. In addition, we also introduce a categories correlation matrix to implicitly describe the relationship between categories, which are used to adjust the weights of the losses for MCUs. Extensive experimental results show that the proposed method achieves competitive performance against state-of-the-art SSDA methods with 50% fewer labeled pixels and significantly outperforms state-of-the-art with a large margin by using the same level of annotation cost",
    "volume": "main",
    "checked": true,
    "id": "5658f45bfb99193b23d4c3a281d267c527d91ad4",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhao_Class_Concentration_with_Twin_Variational_Autoencoders_for_Unsupervised_Cross-modal_Hashing_ACCV_2022_paper.html": {
    "title": "Class Concentration with Twin Variational Autoencoders for Unsupervised Cross-modal Hashing",
    "abstract": "Multi-modal deep hash learning is arguably one of the most commonly used unsupervised methods in cross-modal retrieval tasks. Most existing deep hashing methods focus on maintaining similarity information in the hash code learning step. Although accurate and compact binary representations are learned, these methods fail to encourage discriminative learning of features. In this paper, we propose a new method called Class Concentrated Variational auto-encoder (CCTV) to learn discriminative hash codes. The novelty of CCTV lies in two aspects. First, the proposed method focuses on the concentration of the mean vector of latent features. Based on the assumption that the features in the shared latent space produce multivariate Gaussian, CCTV updates the mean vectors and the cluster centroids of the latent features at the same time by minimizing the class concentration loss, so as to narrow the distance between the cluster centroids and the mean vectors, and further make the concentration More compact. Secondly, under the constraints of raw similarity information, CCTV is different from previous works, it uses the mean vector of latent features as the representation of the images to reduce the influence of variance, and then embeds them in the Hamming space. Our experimental evaluation of four multimedia benchmarks shows a significant improvement over the state-of-the-art methods",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Shibata_Robustizing_Object_Detection_Networks_Using_Augmented_Feature_Pooling_ACCV_2022_paper.html": {
    "title": "Robustizing Object Detection Networks Using Augmented Feature Pooling",
    "abstract": "This paper presents a framework to robustize object detection networks against large geometric transformation. Deep neural networks rapidly and dramatically have improved object detection performance. Nevertheless, modern detection algorithms are still sensitive to large geometric transformation. Aiming at improving the robustness of the modern detection algorithms against the large geometric transformation, we propose a new feature extraction called augmented feature pooling. The key is to integrate the augmented feature maps obtained from the transformed images before feeding it to the detection head without changing the original network architecture. In this paper, we focus on rotation as a simple-yet-influential case of geometric transformation, while our framework is applicable to any geometric transformations. It is noteworthy that, with only adding a few lines of code from the original implementation of the modern object detection algorithms and applying simple fine-tuning, we can improve the rotation robustness of these original detection algorithms while inheriting modern network architectures' strengths. Our framework overwhelmingly outperforms typical geometric data augmentation and its variants used to improve robustness against appearance changes due to rotation. We construct a dataset based on MS COCO to evaluate the robustness of the rotation, called COCO-Rot. Extensive experiments on three datasets, including our COCO-Rot, demonstrate that our method can improve the rotation robustness of state-of-the-art algorithms",
    "volume": "main",
    "checked": false,
    "id": "0d547e3208a5978372cc8740d159075bb534ff7a",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Tang_Learning_Inter-Superpoint_Affinity_for_Weakly_Supervised_3D_Instance_Segmentation_ACCV_2022_paper.html": {
    "title": "Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation",
    "abstract": "Weakly supervised 3D instance segmentation on point clouds has been rarely studied in recent years. Due to the few annotated labels of 3D point clouds, how to learn discriminative features of point clouds to segment object instances is a challenging problem. In this paper, we propose a simple but effective instance segmentation framework that can achieve striking performance by annotating only one point for each instance. Specifically, to tackle extremely few labels, we first oversegment the point cloud into superpoints in an unsupervised manner and extend the point-level annotations to the superpoint level. Then, based on the superpoint graph, we propose an inter-superpoint affinity mining module that considers the semantic and spatial relations to adaptively learns inter-superpoint affinity to generate high-quality pseudo labels via random walk. Finally, we propose a volume-aware instance refinement module to segment high-quality instances by applying volume constraints of objects in clustering on the superpoint graph. Extensive experiments on the ScanNet-v2 and S3DIS datasets demonstrate that our method achieves state-of-the-art performance in the weakly supervised point cloud instance segmentation task, and even outperforms some fully supervised methods",
    "volume": "main",
    "checked": true,
    "id": "22c51c6d273f9d17abab100e82934d8300922140",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Qin_PathTR_Context-Aware_Memory_Transformer_for_Tumor_Localization_in_Gigapixel_Pathology_ACCV_2022_paper.html": {
    "title": "PathTR: Context-Aware Memory Transformer for Tumor Localization in Gigapixel Pathology Images",
    "abstract": "With the development of deep learning and computation pathology, whole-slide images (WSIs) are wildly used in clinical diagnosis. The WSI, which refers to the scanning of conventional glass slides in order to produce digital slides, usually has gigapixels. Most existing methods in computer vision process WSIs as many patches. The model infers patch by patch to get the results on WSI, which loses the global context of WSI. In this paper, we developed PATHology TRansformer (PathTR), which fully uses the global information of WSI. In PathTR, the local context is aggregated by the self-attention mechanism. We further design a recursive mechanism to encode the global context in extra states. In tumor detection of metastases of lymph node sections for breast cancer,  we got the FROC score of 87.68% which outperforms the baseline and NCRF method with +8.99% and +7.08%, respectively. We highlight that we also achieve a significant 94.25% sensitivity at 8 false positives per image",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Oh_Semi-supervised_Breast_Lesion__Segmentation_using_Local_Cross_Triplet_Loss_ACCV_2022_paper.html": {
    "title": "Semi-supervised Breast Lesion Segmentation using Local Cross Triplet Loss for Ultrafast Dynamic Contrast-Enhanced MRI",
    "abstract": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) and its fast variant, ultrafast DCE-MRI, are useful for the management of breast cancer. Segmentation of breast lesions is necessary for automatic clinical decision support. Despite the advantage of acquisition time, existing segmentation studies on ultrafast DCE-MRI are scarce, and they are mostly fully supervised studies with high annotation costs. Herein, we propose a semi-supervised segmentation approach that can be trained with small amounts of annotations for ultrafast DCE-MRI. A time difference map is proposed to incorporate the distinct time-varying enhancement pattern of the lesion. Furthermore, we present a novel loss function that efficiently distinguishes breast lesions from non-lesions based on triple loss. This loss reduces the potential false positives induced by the time difference map. Our approach is compared to that of five competing methods using the dice similarity coefficient and two boundary-based metrics. Compared to other models, our approach achieves better segmentation results using small amounts of annotations, especially for boundary-based metrics relevant to spatially continuous breast lesions. An ablation study demonstrates the incremental effects of our study. Our code is available on GitHub (https://github.com/yt- oh96/SSL-CTL)",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Hong_QS-Craft_Learning_to_Quantize_Scrabble_and_Craft_for__Conditional_ACCV_2022_paper.html": {
    "title": "QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human Motion Animation",
    "abstract": "This paper studies the task of conditional Human Motion Animation (cHMA). Given a source image and a driving video, the model should animate the new frame sequence, in which the person in the source image should perform a similar motion as the pose sequence from the driving video. Despite the success of Generative Adversarial Network (GANs) methods in image and video synthesis,  it is still very challenging to conduct cHMA due to the difficulty in efficiently utilizing the conditional guided information such as images or poses, and generating images of good visual quality. To this end, this paper proposes a novel  model of learning to Quantize, Scrabble, and Craft (QS-Craft) for conditional human motion animation. The key novelties come from the newly introduced three key steps:  quantize, scrabble and craft. Particularly, our QS-Craft employs transformer in its structure to utilize the  attention architectures. The guided  information is represented as a pose coordinate sequence extracted from the driving videos. Extensive experiments on human motion datasets validate the efficacy of our model",
    "volume": "main",
    "checked": true,
    "id": "9913cebc958ddcb13d82300ab89dabff10ca3b64",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Alhawwary_PatchFlow_A_Two-Stage_Patch-Based_Approach_for_Lightweight_Optical_Flow_Estimation_ACCV_2022_paper.html": {
    "title": "PatchFlow: A Two-Stage Patch-Based Approach for Lightweight Optical Flow Estimation",
    "abstract": "The deep learning-based optical flow methods have shown noticeable advancements in flow estimation. The dense optical flow map offers high flexibility and quality for aligning neighbouring video frames. However, they are computationally expensive, and the memory requirements for processing high-resolution images such as 2K, 4K and 8K on resources-limited devices such as mobile phones can be prohibitive.\nWe propose a patch-based approach for optical flow estimation. We redistribute the regular CNN-based optical flow regression into a two-stage pipeline, where the first stage estimates an optical flow for a low-resolution image version. The pre-flow is input to the second stage, where the high-resolution image is partitioned into small patches for optical flow refinement. With such a strategy, it becomes possible to process high-resolution images when the memory requirements are not sufficient. On the other hand, this solution also offers the ability to parallelize the optical flow estimation when possible. Furthermore, we show that such a pipeline can additionally allow for utilizing a lighter and shallower model in the two stages. It can perform on par with FastFlowNet (FFN) while being 1.7x faster computationally and with almost a half of the parameters. Against the state-of-the-art optical flow methods, the proposed solution can show a reasonable accuracy trade-off for running time and memory requirements",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhuge_Multi-granularity_Transformer_for_Image_Super-resolution_ACCV_2022_paper.html": {
    "title": "Multi-granularity Transformer for Image Super-resolution",
    "abstract": "Recently, transformers have made great success in computer vision. Thus far, most of those works focus on high-level tasks, e.g., image classification and object detection, and fewer attempts were made to solve low-level problems. In this work, we tackle image super-resolution. Specifically, transformer architectures with multi-granularity transformer groups are explored for complementary information interaction, to improve the accuracy of super-resolution. We exploit three transformer patterns, i.e., the window transformers, dilated transformers and global transformers. We further investigate the combination of them and propose a Multi-granularity Transformer (MugFormer). Specifically, the window transformer layer is aggregated with other transformer layers to compose three transformer\ngroups, namely, Local Transformer Group, Dilated Transformer Group and Global Transformer Group, which efficiently aggregate both local and global information for accurate reconstruction. Extensive experiments on five benchmark datasets demonstrate that our MugFormer performs favorably against state-of-the-art methods in terms of both quantitative and qualitative against state-of-the-art methods in terms of both quantitative and qualitative results",
    "volume": "main",
    "checked": false,
    "id": "f310e0b5c7f72acfa1552ead01864404233ab80e",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Hu_Multi-scale_Residual_Interaction_for_RGB-D_Salient_Object_Detection_ACCV_2022_paper.html": {
    "title": "Multi-scale Residual Interaction for RGB-D Salient Object Detection",
    "abstract": "RGB-D salient object detection (SOD) is used to detect the most attractive object in the scene. There is a problem in front of the existing RGB-D SOD task: how to integrate the different context information between the RGB and depth map effectively. In this work, we propose the Siamese Residual Interactive Refinement Network (SiamRIR) equipped with the encoder and decoder to handle the above problem. Concretely, we adopt the Siamese Network shared parameters to encode two modalities and fuse them during decoding phase. Then, we design the Multi-scale Residual Interavtive Refinement Block (RIRB) which contains Residual Interactive Module (RIM) and Residual Refinement Module (RRM). This block utilizes the multi-type cues to fuse and refine features, where RIM takes interaction between modalities to integrate the complementary regions with residual manner, and RRM refines features during fusion phase by incorporating spatial detail context with multi-scale manner. Extensive experiments on five benchmarks demonstrate that our method outperforms the state-of-the-art RGB-D SOD methods both quantitatively and qualitatively",
    "volume": "main",
    "checked": false,
    "id": "fe9b777ed7d9c9c8d6c772eeac0fe5bf176995a8",
    "citation_count": 204
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Fine-Grained_Image_Style_Transfer_with_Visual_Transformers_ACCV_2022_paper.html": {
    "title": "Fine-Grained Image Style Transfer with Visual Transformers",
    "abstract": "With the development of the convolutional neural network,\nimage style transfer has drawn increasing attention. However, most existing approaches adopt a global feature transformation to transfer style\npatterns into content images (e.g., AdaIN and WCT). Such a design\nusually destroys the spatial information of the input images and fails to\ntransfer fine-grained style patterns into style transfer results. To solve\nthis problem, we propose a novel STyle TRansformer (STTR) network\nwhich breaks both content and style images into visual tokens to achieve\na fine-grained style transformation. Specifically, two attention mechanisms are adopted in our STTR. We first propose to use self-attention to\nencode content and style tokens such that similar tokens can be grouped\nand learned together. We then adopt cross-attention between content\nand style tokens that encourages fine-grained style transformations. To\ncompare STTR with existing approaches, we conduct user studies on\nAmazon Mechanical Turk (AMT), which are carried out with 50 human\nsubjects with 1,000 votes in total. Extensive evaluations demonstrate the\neffectiveness and efficiency of the proposed STTR in generating visually\npleasing style transfer results",
    "volume": "main",
    "checked": true,
    "id": "3437dd9bbec507e26e13e5d0b260add535b306ad",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Cai_RGB_Road_Scene_Material_Segmentation_ACCV_2022_paper.html": {
    "title": "RGB Road Scene Material Segmentation",
    "abstract": "We address RGB road scene material segmentation, i.e., per-pixel segmentation of materials in real-world driving views with pure RGB images, by building a new tailored benchmark dataset and model for it. Our new dataset, KITTI-Materials, based on the well-established KITTI dataset, consists of 1000 frames covering 24 different road scenes of urban/suburban landscapes, annotated with one of 20 material categories for every pixel in high quality. It is the first dataset tailored to RGB material segmentation in realistic driving scenes which allows us to train and test any RGB material segmentation model. Based on an analysis on KITTI-Materials, we identify the extraction and fusion of texture and context as the key to robust road scene material appearance. We introduce Road scene Material Segmentation Network (RMSNet), a new Transformer-based framework which will serve as a baseline for this challenging task. RMSNet encodes multi-scale hierarchical features with self-attention. We construct the decoder of RMSNet based on a novel lightweight self-attention model, which we refer to as SAMixer. SAMixer achieves adaptive fusion of informative texture and context cues across multiple feature levels. It also significantly accelerates self-attention for feature fusion with a balanced query-key similarity measure. We also introduce a built-in bottleneck of local statistics to achieve further efficiency and accuracy. Extensive experiments on KITTI-Materials validate the effectiveness of our RMSNet. We believe our work lays a solid foundation for further studies on RGB road scene material segmentation",
    "volume": "main",
    "checked": false,
    "id": "b64a91e5903be4e4ff4268c8deedc3828e22d3f2",
    "citation_count": 45
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wei_UHD_Underwater_Image_Enhancement_via_Frequency-Spatial_Domain_Aware_Network_ACCV_2022_paper.html": {
    "title": "UHD Underwater Image Enhancement via Frequency-Spatial Domain Aware Network",
    "abstract": "Currently, carrying ultra high definition (UHD) imaging equipment to record rich environmental conditions in deep water has become a hot issue in underwater exploration. However, due to the poor light transmission in deep water spaces and the large number of impurity particles, UHD underwater imaging is often plagued by low contrast and blur. To overcome these challenges, we propose an efficient two-path model (UHD-SFNet) that recovers the color and the texture of an underwater blurred image in the frequency and the spatial domains. Specifically, the method consists of two branches: in the first branch, we use a bilateral enhancement pipeline that extracts the frequency domain information of a degraded image to reconstruct clear textures. In the pipeline, we embed 1D convolutional layers in the MLP-based framework to capture the local characteristics of the token sequence. In the second branch, we develop U-RSGNet to capture the color features of the image after Gaussian blurring to generate a feature map rich in color information. Finally, the extracted texture features are fused with the color features to produce a clear underwater image. In addition, to construct paired high-quality underwater image enhancement dataset, we propose UHD-CycleGAN with the help of domain adaptation to produce more realistic UHD synthetic images. Experimental results show that our algorithm outperforms existing methods significantly in underwater image enhancement on a single GPU with 24G RAM. Codes are available at https://github.com/wyw0112/UHD-SFNet",
    "volume": "main",
    "checked": false,
    "id": "5a9ce590ce47c4920d778f9115a397271f7edca9",
    "citation_count": 3
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Mun_BaSSL_Boundary-aware_Self-Supervised_Learning_for_Video_Scene_Segmentation_ACCV_2022_paper.html": {
    "title": "BaSSL: Boundary-aware Self-Supervised Learning for Video Scene Segmentation",
    "abstract": "Self-supervised learning has drawn attention through its effectiveness in learning in-domain representations with no ground-truth annotations; in particular, it is shown that properly designed pretext tasks bring significant performance gains for downstream tasks. Inspired from this, we tackle video scene segmentation, which is a task of temporally localizing scene boundaries in a long video, with a self-supervised learning framework where we mainly focus on designing effective pretext tasks. In our framework, given a long video, we adopt a sliding window scheme; from a sequence of shots in each window, we discover a moment with a maximum semantic transition and leverage it as pseudo-boundary to facilitate the pre-training. Specifically, we introduce three novel boundary-aware pretext tasks: 1) Shot-Scene Matching (SSM), 2) Contextual Group Matching (CGM) and 3) Pseudo-boundary Prediction (PP); SSM and CGM guide the model to maximize intra-scene similarity and inter-scene discrimination by capturing contextual relation between shots while PP encourages the model to identify transitional moments. We perform an extensive analysis to validate effectiveness of our method and achieve the new state-of-the-art on the MovieNet-SSeg benchmark. The code is available at https://github.com/kakaobrain/bassl",
    "volume": "main",
    "checked": false,
    "id": "bcd1dfaf6716476d19caec7fae58f821ed6daa4a",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Liu_Coil-Agnostic_Attention-Based_Network_for_Parallel_MRI_Reconstruction_ACCV_2022_paper.html": {
    "title": "Coil-Agnostic Attention-Based Network for Parallel MRI Reconstruction",
    "abstract": "Magnetic resonance imaging (MRI) is widely used in clinical diagnosis. However, as a slow imaging modality, the long scan time hinders its development in time-critical applications. The acquisition process can be accelerated by types of under-sampling strategies in k-space and reconstructing images from a few measurements. To reconstruct the image, many parallel imaging methods use the coil sensitivity maps to fold multiple coil images with model-based or deep learning-based estimation methods. However, they can potentially suffer from the inaccuracy of sensitivity estimation. In this work, we propose a novel coil-agnostic attention-based framework for multi-coil MRI reconstruction which completely avoids the sensitivity estimation and performs data consistency (DC) via a sensitivity-agnostic data aggregation consistency block (DACB). Experiments were performed on the FastMRI knee dataset and show that the proposed DACB and attention module-integrated framework outperforms other deep learning-based algorithms in terms of image quality and reconstruction accuracy. Ablation studies also indicate the superiority of DACB over conventional DC methods",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhu_Multi-View_Coupled_Self-Attention_Network_for_Pulmonary_Nodules_Classification_ACCV_2022_paper.html": {
    "title": "Multi-View Coupled Self-Attention Network for Pulmonary Nodules Classification",
    "abstract": "Evaluation of the malignant degree of pulmonary nodules plays an important role in early detecting lung cancer.  Deep learning-based methods have obtained promising results in this domain with their effectiveness in learning feature representation. Both local and global features are crucial for medical image classification tasks, particularly for 3D medical image data, however, the receptive field of convolution kernel limits the global feature learning. Although self-attention mechanism can success fully model long-range dependencies by directly flattening the input image to a sequence, which has high computational complexity. Additionally, which unable to model the image local context information across spatial and depth dimensions. To address the above challenges, in this paper,  we carefully design a Multi-View Coupled Self-Attention Module (MVCS). Specifically, a novel self-attention module is proposed to model spatial and dimensional correlations sequentially for learning global spatial contexts and further improving the identification accuracy. Compared with  vanilla self-attention, which have three-fold advances: 1) uses fewer memory consumption and computational complexity than the existing self-attention methods; 2) except for exploiting the correlations along the spatial and channel dimension, the dimension correlations are also exploited; 3) the proposed self-attention module can be easily integrated with other frameworks. By adding the proposed module into 3D ResNet50, we build a classification network for lung nodules' malignancy evaluation. The nodule classification network was validated on a public dataset from LIDC-IDRI. Extensive experimental results demonstrate that our proposed model has performance comparable to state-of-the-art approaches",
    "volume": "main",
    "checked": false,
    "id": "cefc85e8895d93c22698aa5129a73b6491f6fdcf",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Fang_Affinity-Aware_Relation_Network_for_Oriented_Object_Detection_in_Aerial_Images_ACCV_2022_paper.html": {
    "title": "Affinity-Aware Relation Network for Oriented Object Detection in Aerial Images",
    "abstract": "Object detection in aerial images is a challenging task due to the oriented and densely packed objects. However, densely packed objects constitute a significant characteristic of aerial images: objects are not randomly scattered around in images but in groups sharing similar orientations. Such a recurring pattern of object arrangement could enhance the rotated features and improve the detection performance. This paper proposes a novel and flexible Affinity-Aware Relation Network based on two-stage detectors. Specifically, an affinity-graph construction module is adopted to measure the affinity among objects and to select bounding boxes sharing high similarity with the reference box. Furthermore, we design a dynamic enhancement module, which uses the attention to learn neighbourhood message and dynamically determines weights for feature enhancement. Finally, we conduct experiments on several public benchmarks and achieve notable AP improvements as well as state-of-the-art performances on DOTA, HRSC2016 and UCAS-AOD datasets",
    "volume": "main",
    "checked": false,
    "id": "ac4aed27a7f8dcdb7162b2774de5459fe3b9a4ab",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Hang_Spatial-Temporal_Adaptive_Graph_Convolutional_Network_for_Skeleton-based_Action_Recognition_ACCV_2022_paper.html": {
    "title": "Spatial-Temporal Adaptive Graph Convolutional Network for Skeleton-based Action Recognition",
    "abstract": "Skeleton-based action recognition approaches usually construct the skeleton sequence as spatial-temporal graphs and perform graph convolution on these graphs to extract discriminative features. However, due to the fixed topology shared among different poses and the lack of direct long-range temporal dependencies, it is not trivial to learn the robust spatial-temporal feature. Therefore, we present a spatial-temporal adaptive graph convolutional network (STA-GCN) to learn adaptive spatial and temporal topologies and effectively aggregate features for skeletonbased action recognition. The proposed network is composed of spatial adaptive graph convolution (SA-GC) and temporal adaptive graph convolution (TA-GC) with an adaptive topology encoder. The SA-GC can extract the spatial feature for each pose with the spatial adaptive topology, while the TA-GC can learn the temporal feature by modeling adaptively the direct long-range temporal dependencies. On three large-scale skeleton action recognition datasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton, the STA-GCN outperforms the existing stateof-the-art methods",
    "volume": "main",
    "checked": false,
    "id": "73a5f265a72565faa42430a6e07e708628bd5e4a",
    "citation_count": 2
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Gao_Action_Representing_by_Constrained_Conditional_Mutual_Information_ACCV_2022_paper.html": {
    "title": "Action Representing by Constrained Conditional Mutual Information",
    "abstract": "Contrastive learning achieves a remarkable performance for representation learning by constructing the InfoNCE loss function. It enables learned representations to describe the invariance in data transformation without labels. Contrastive learning also been employed in self-supervised learning of action recognition. However, this kind of method fails to introduce assumptions according to human knowledge about the prior distribution of representations in the training process. \nFor solving this problem, this paper proposes a self-supervised learning framework, which can achieve different self-supervised learning methods by choosing different assumptions about the prior distribution of representations, while still learning the description of invariance in data transformation as contrastive learning. This framework minimizes the CCMI (Constrained Conditional Mutual Information) loss function, which represents the conditional mutual information between input augmented samples of the same sample and the output representations of the encoder while the prior distribution of representations is constrained. By theoretical analysis of the framework, it is proved that traditional contrastive learning by InfoNCE is a special case without human knowledge constraint of this framework. The Gaussian Mixture Model on Unit Hyper-sphere is chosen as the representation prior distribution to achieve the self-supervised method called CoMInG. Compared with the existing methods, the performance of the learned representation by this method in the downstream task of action recognition is significantly improved",
    "volume": "main",
    "checked": false,
    "id": "6cb2fe09b75fd7674da168084fdd1aef2225e735",
    "citation_count": 6
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Kim_ADEL_Adaptive_Distribution_Effective-matching_Method_for_Guiding_Generators_of_GANs_ACCV_2022_paper.html": {
    "title": "ADEL: Adaptive Distribution Effective-matching Method for Guiding Generators of GANs",
    "abstract": "Research on creating high-quality, realistic fake images has engendered immense improvement in GANs. However, GAN training is still subject to mode collapse or vanishing gradient problems. To address these issues, we propose an adaptive distribution effective-matching method (ADEL) that sustains the stability of training and enables high performance by ensuring that the training abilities of the generator and discriminator are maintained in balance without bias in either direction. ADEL can help the generator's training by matching the difference between the distribution of real and fake images. As training is ideal when the discriminator and generator are in a balanced state, ADEL works when it is out of a certain optimal range based on the loss value. Through this, ADEL plays an important role in guiding the generator to create images similar to real images in the early stage when training is difficult. As training progresses, it naturally decays and gives model more freedom to generate a variety of images. ADEL can be applied to a variety of loss functions such as Kullback-Liebler divergence loss, Wasserstein loss, and Least-squares loss. Through extensive experiments, we show that ADEL improves the performance of diverse models such as DCGAN, WGAN, WGAN-GP, LSGAN, and StyleGANv2 upon five datasets, including low-resolution (CIFAR-10 and STL-10) as well as high-resolution (LSUN-Bedroom, Church, and ImageNet) datasets. Our proposed method is very simple and has a low computational burden, so it is expandable and can be used for diverse models",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_3D-Yoga_A_3D_Yoga_Dataset_for_Visual-based_Hierarchical_Sports_Action_ACCV_2022_paper.html": {
    "title": "3D-Yoga: A 3D Yoga Dataset for Visual-based Hierarchical Sports Action Analysis",
    "abstract": "Visual-based human action analysis is an important research topic in the field of computer vision, and has great application prospect in sports performance analysis. Currently available 3D action analysis datasets have a number of limitations in sports application, including the lack of special sports actions, distinct class or score labels and variety of samples. Existing researches mainly use various special RGB videos for sports action analysis, but analysis with 2D features is less effective than 3D representation. In this paper, we introduce a new 3D yoga pose dataset (3D-Yoga) with more than 3,792 action samples and 16,668 RGB-D key frames, collected from 22 subjects performing 117 kinds of yoga poses with two RGB-D cameras. We have reconstructed 3D yoga poses with sparse multi-view data and carried out experiments with the proposed cascade two-stream adaptive graph convolutional neural network (Cascade 2S-AGCN) to recognize and assess these poses. Experimental results have shown the advantage of applying our 3D skeleton fusion and hierarchical analysis methods on 3D-Yoga, and the accuracy of Cascade 2S-AGCN outperforms the state-of-the-art methods. The introduction of 3D-Yoga will enable the community to apply, develop and adapt various methods for visual-based sports activity analysis",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_HaViT_Hybrid-attention_based_Vision_Transformer_for_Video_Classification_ACCV_2022_paper.html": {
    "title": "HaViT: Hybrid-attention based Vision Transformer for Video Classification",
    "abstract": "Video transformers have become a promising tool for video classification due to its great success in modeling long-range interactions through the self-attention operation. However, existing transformer models only exploit the patch dependencies within a video when doing self-attention, while ignoring the patch dependencies across different videos. This paper argues that external patch prior information is beneficial to the performance of video transformer models for video classification. Motivated by this assumption, this paper proposes a novel Hybrid-attention based Vision Transformer (HaViT) model for video classification, which explicitly exploits both internal patch dependencies within a video and external patch dependencies across videos. Different from existing self-attention, the hybrid-attention is computed based on internal patch tokens and an external patch token dictionary which encodes external patch prior information across different videos. Experiments on Kinetics-400, Kinetics-600 and Something-something-v2 show that our HaViT model achieves state-of-the-art performance in the video classification task against existing methods. Moreover, experiments show that our proposed hybrid-attention scheme can be integrated into existing video transformer models to improve the performance",
    "volume": "main",
    "checked": false,
    "id": "28bf079c53560897738790e72e0fb983e6212793",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Sun_Filter_Pruning_via_Automatic_Pruning_Rate_Search_ACCV_2022_paper.html": {
    "title": "Filter Pruning via Automatic Pruning Rate Search",
    "abstract": "Model pruning is important for deploying models on devices with limited resources. However, the searching of optimal pruned model is still a significant challenge due to the large space to be exploited. In this paper, we propose an Automatic Pruning Rate Search(APRS) method to achieve automatic pruning. We reveal the connection between the model performance and Wasserstein distance to automatic searching optimal pruning rate. To reduce the search space, we quantify the sensitivity of each filter layer by layer and reveal the connection between model performance and Wasserstein distance. We introduce an end-to-end optimization method called Pareto plane to automatically search for the pruning rate to fit the overall size of the model. APRS can obtain more compact and efficient pruning models. To verify the effectiveness of our method, we conduct extensive experiments on ResNet, VGG and DenseNet, and the results show that our method outperforms the state-of-the-art methods under different parameter settings",
    "volume": "main",
    "checked": false,
    "id": "2ea4b0b2e7df55554d331b9e95c61b1f331875aa",
    "citation_count": 104
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_Is_an_Object-Centric_Video_Representation_Beneficial_for_Transfer_ACCV_2022_paper.html": {
    "title": "Is an Object-Centric Video Representation Beneficial for Transfer?",
    "abstract": "The objective of this work is to learn an object-centric video representation, with \nthe aim of improving transferability to novel tasks, ie, tasks different\nfrom the pre-training task of action classification.To this end, we introduce a new object-centric video recognition model based on a transformer architecture. \nThe model learns a set of object-centric  summary vectors for the video, and\nuses these vectors to fuse the visual and spatio-temporal trajectory \n`modalities' of the video clip.  We also introduce a novel trajectory contrast\nloss to further enhance objectness in these summary vectors. \n\nWith experiments on four datasets -- SomethingSomething-V2, SomethingElse, Action Genome and EpicKitchens -- we show that the object-centric model outperforms prior video representations (both object-agnostic and object-aware), when: (1) classifying actions on unseen objects and unseen environments;  (2) low-shot learning to novel classes; (3) linear probe to other downstream tasks;  as well as (4) for standard action classification",
    "volume": "main",
    "checked": true,
    "id": "52fcd9178c489d3b5d056fd214de115871e69e06",
    "citation_count": 2
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_MatchFormer_Interleaving_Attention_in_Transformers_for_Feature_Matching_ACCV_2022_paper.html": {
    "title": "MatchFormer: Interleaving Attention in Transformers for Feature Matching",
    "abstract": "Local feature matching is a computationally intensive task at the subpixel level. While detector-based methods coupled with feature descriptors struggle in low-texture scenes, CNN-based methods with a sequential extract-to-match pipeline, fail to make use of the matching capacity of the encoder and tend to overburden the decoder for matching. In contrast, we propose a novel hierarchical extract-and-match transformer, termed as MatchFormer. Inside each stage of the hierarchical encoder, we interleave self-attention for feature extraction and cross-attention for feature matching, enabling a human-intuitive extract-and-match scheme. Such a match-aware encoder releases the overloaded decoder and makes the model highly efficient. Further, combining self- and cross-attention on multi-scale features in a hierarchical architecture improves matching robustness, particularly in low-texture indoor scenes or with less outdoor training data. Thanks to such a strategy, MatchFormer is a multi-win solution in efficiency, robustness, and precision. Compared to the previous best method in indoor pose estimation, our lite MatchFormer has only 45% GFLOPs, yet achieves a +1.3% precision gain and a 41% running speed boost. The large MatchFormer reaches state-of-the-art on four different benchmarks, including indoor pose estimation (ScanNet), outdoor pose estimation (MegaDepth), homography estimation and image matching (HPatch), and visual localization (InLoc)",
    "volume": "main",
    "checked": true,
    "id": "24ec4d74890f99b9db4e97660d16e76eb2eebe73",
    "citation_count": 9
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Guo_MGTR_End-to-End_Mutual_Gaze_Detection_with_Transformer_ACCV_2022_paper.html": {
    "title": "MGTR: End-to-End Mutual Gaze Detection with Transformer",
    "abstract": "People's looking at each other or mutual gaze is ubiquitous in our daily interactions, and detecting mutual gaze is of great significance for understanding human social scenes. Current mutual gaze detection methods focus on two-stage methods, whose inference speed is limited by the two-stage pipeline and the performance in the second stage is affected by the first one. In this paper, we propose a novel one-stage mutual gaze detection framework called Mutual Gaze TRansformer or MGTR to perform mutual gaze detection in an end-to-end manner. By designing mutual gaze instance triples, MGTR can detect each human head bounding box and simultaneously infer mutual gaze relationship based on global image information, which streamlines the whole process with simplicity. Experimental results on two mutual gaze datasets show that our method is able to accelerate mutual gaze detection process without losing performance. Ablation study shows that different components of MGTR can capture different levels of semantic information in images. Code is available at https://github.com/Gmbition/MGTR",
    "volume": "main",
    "checked": true,
    "id": "1ce315b46a8142602fa4aabbdc22998baf527bff",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wu_Learning_Video-independent_Eye_Contact_Segmentation_from_In-the-Wild_Videos_ACCV_2022_paper.html": {
    "title": "Learning Video-independent Eye Contact Segmentation from In-the-Wild Videos",
    "abstract": "Human eye contact is a form of non-verbal communication and can have a great influence on social behavior. Since the location and size of the eye contact targets vary across different videos, learning a generic video-independent eye contact detector is still a challenging task. In this work, we address the task of one-way eye contact detection for videos in the wild. Our goal is to build a unified model that can identify when a person is looking at his gaze targets in an arbitrary input video. Considering that this requires time-series relative eye movement information, we propose to formulate the task as a temporal segmentation. Due to the scarcity of labeled training data, we further propose a gaze target discovery method to generate pseudo-labels for unlabeled videos, which allows us to train a generic eye contact segmentation model in an unsupervised way using in-the-wild videos. To evaluate our proposed approach, we manually annotated a test dataset consisting of 52 videos of human conversations. Experimental results show that our eye contact segmentation model outperforms the previous video-dependent eye contact detector and can achieve 71.88% framewise accuracy on our annotated test set. Our code and evaluation dataset are available at https://github.com/ut-vision/Video-Independent-ECS",
    "volume": "main",
    "checked": true,
    "id": "8c7a4fbcbf598dc3f343b7077e2f83d635599a01",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Gao_AONet_Attentional_Occlusion-aware_Network_for_Occluded_Person_Re-identification_ACCV_2022_paper.html": {
    "title": "AONet: Attentional Occlusion-aware Network for Occluded Person Re-identification",
    "abstract": "Occluded person Re-identification (Occluded ReID) aims to verify the identity of a pedestrian with occlusion across non-overlapping cameras. Previous works for this task often rely on external tasks, e.g., pose estimation or semantic segmentation, to extract local features over fixed given regions. However, these external models may perform poorly on Occluded ReID, since they themselves are still open problems with no reliable performance guarantee and also not oriented towards ReID tasks to provide discriminative local features. In this paper, we propose an Attentional Occlusion-aware Network (AONet) for Occluded ReID that does not rely on any external tasks. AONet adaptively learns discriminative local features over latent landmark regions by the trainable pattern vectors, and softly weights the summation of landmark-wise similarities based on the occlusion awareness. Also, as there are no ground truth occlusion annotations, we measure the occlusion of landmarks by the awareness scores, when referring to a memorized dictionary storing average landmark features. These awareness scores are then used as a soft weight for training and inferring. Meanwhile, the memorized dictionary is momenta updated according to the landmark features and their awareness scores of each input image. The AONet achieves 53.1% mAP and 66.5% Rank1 on the Occluded-DukeMTMC dataset, significantly outperforming state-of-the-arts without any bells and whistles, and also shows obvious improvements on the holistic datasets Market-1501 and DukeMTMC-reID, as well as partial datasets Partial-REID and Partial-iLIDS. Code and pre-trained models will be released online soon",
    "volume": "main",
    "checked": false,
    "id": "269ebbd1543ea8dbb8f1a1b06eaef2c31ee6d0cb",
    "citation_count": 17
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Tian_TCVM_Temporal_Contrasting_Video_Montage_Framework_for_Self-supervised_Video_Representation_ACCV_2022_paper.html": {
    "title": "TCVM: Temporal Contrasting Video Montage Framework for Self-supervised Video Representation Learning",
    "abstract": "Extracting appropriate temporal differences and ignoring irrelevant backgrounds are two important perspectives on preserving sufficient motion information in video representation. In this paper, we propose a unified contrastive learning framework called Temporal Contrasting Video Montage (TCVM) to learn action-specific motion patterns, which can be implemented in a plug-and-play way. On the one hand, Temporal Contrasting (TC) module is designed to guarantee appropriate temporal difference between frames. It utilizes high-level feature space to capture raveled temporal information. On the other hand, Video Montage (VM) module is devised for alleviating the effect from video background. It demonstrates similar temporal motion variances in different positive samples by implicitly mixing up the backgrounds of different videos. Experimental results show that our TCVM reaches promising performances on both large action recognition dataset (i.e. Something-Somethingv2) and small datasets (i.e. UCF101 and HMDB51)",
    "volume": "main",
    "checked": false,
    "id": "2099f89a895097665cb8079696cec9f80bb9075d",
    "citation_count": 19
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zheng_Depth_Estimation_via_Sparse_Radar_Prior_and_Driving_Scene_Semantics_ACCV_2022_paper.html": {
    "title": "Depth Estimation via Sparse Radar Prior and Driving Scene Semantics",
    "abstract": "Depth estimation is an essential module for the perception system of autonomous driving. The state-of-the-art methods introduce LiDAR to improve the performance of monocular depth estimation, but it faces the challenges of weather durability and high hardware cost. Unlike existing LiDAR and image-based methods, a two-stage network is proposed to integrate highly sparse radar data in this paper, in which sparse pre-mapping module and feature fusion module are proposed for radar feature extraction and feature fusion respectively. Considering the highly structured driving scenario, we introduce semantic information of the scenario to further improve the loss function, thus making the network more focused on the target region. Finally, we propose a novel depth dataset construction strategy by integrating binary mask-based filtering and interpolation methods based on the nuScenes dataset. And the effectiveness of our proposed method has been demonstrated through extensive experiments, which outperform existing methods in all metrics",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhao_IoU-Enhanced_Attention_for_End-to-End_Task_Specific_Object_Detection_ACCV_2022_paper.html": {
    "title": "IoU-Enhanced Attention for End-to-End Task Specific Object Detection",
    "abstract": "Without densely tiled anchor boxes or grid points in the image, sparse R-CNN achieves promising results through a set of object queries and proposal boxes updated in the cascaded training manner. However, due to the sparse nature and the one-to-one relation between the query and its attending region, it heavily depends on the self attention,  which is usually inaccurate in the early training stage. Moreover, in a scene of dense objects, the object query interacts with many irrelevant ones, reducing its uniqueness and harming the performance. This paper proposes to use IoU between different boxes as a prior for the value routing in self attention. The original attention matrix multiplies the same size matrix computed from the IoU of proposal boxes, and they determine the routing scheme so that the irrelevant features can be suppressed. Furthermore, to accurately extract features for both classification and regression, we add two lightweight projection heads to provide the dynamic channel masks based on object query, and they multiply with the output from dynamic convs, making the results suitable for the two different tasks. We validate the proposed scheme on different datasets, including MS-COCO and CrowdHuman, showing that it significantly improves the performance and increases the model convergence speed",
    "volume": "main",
    "checked": true,
    "id": "1788fa6daa8c85d35fad424e8d51c0503d55e733",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_EPSANet_An_Efficient_Pyramid_Squeeze_Attention_Block_on_Convolutional_Neural_ACCV_2022_paper.html": {
    "title": "EPSANet: An Efficient Pyramid Squeeze Attention Block on Convolutional Neural Network",
    "abstract": "Recently, it has been demonstrated that the performance of a deep convolutional neural network can be effectively improved by embedding an attention module into it. In this work, a novel lightweight and effective attention method named Pyramid Squeeze Attention (PSA) module is proposed. By replacing the 3x3 convolution with the PSA module in the bottleneck blocks of the ResNet, a novel representational block named Efficient Pyramid Squeeze Attention (EPSA) is obtained. The EPSA block can be easily added as a plug-and-play component into a well-established backbone network, and significant improvements on performance can be achieved. Hence, a simple and efficient backbone architecture named EPSANet is developed in this work by stacking these ResNet-style EPSA blocks. Correspondingly, a stronger multiscale representation ability can be offered by the proposed EPSANet for various computer vision tasks including but not limited to, image classification, object detection, instance segmentation, etc. Without bells and whistles, the performance of the proposed EPSANet outperforms most of the state-of-the-art channel attention methods. As compared to the SENet-50, the Top-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of +2.7 box AP for object detection and an\nimprovement of +1.7 mask AP for instance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained",
    "volume": "main",
    "checked": true,
    "id": "f570cc346acbf44df30f8569e2db08b78ba7cd76",
    "citation_count": 20
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Guo_SWPT_Spherical_Window-based_Point_Cloud_Transformer_ACCV_2022_paper.html": {
    "title": "SWPT: Spherical Window-based Point Cloud Transformer",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks and has shown promising prospects in image analysis domains, applying it to the 3D point cloud directly is still a challenge due to the irregularity and lack of order. Most current approaches adopt the farthest point searching as a downsampling method and construct local areas with the k-nearest neighbor strategy to extract features hierarchically. However, this scheme inevitably consumes lots of time and memory, which impedes its application to near-real-time systems and large-scale point cloud. This research designs a novel transformer-based network called Spherical Window-based Point Transformer (SWPT) for point cloud learning, which consists of a Spherical Projection module, a Spherical Window Transformer module and a crossing self-attention module. Specifically, we project the points on a spherical surface, then a window-based local self-attention is adopted to calculate the relationship between the points within a window. To obtain connections between different windows, the crossing self-attention is introduced, which rotates all the windows as a whole along the spherical surface and then aggregates the crossing features. It is inherently permutation invariant because of using simple and symmetric functions, making it suitable for point cloud processing. Extensive experiments demonstrate that SWPT can achieve the state-of-the-art performance with about 3-8 times faster than previous transformer-based methods on shape classification tasks, and achieve competitive results on part segmentation and the more difficult real-world classification tasks",
    "volume": "main",
    "checked": false,
    "id": "1a4355f0df6ef7b6eb50fa2e0884987f46673ec6",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_MSF2DNMulti_Scale_Feature_Fusion_Dehazing__Network_with_Dense_connection_ACCV_2022_paper.html": {
    "title": "MSF$^2$DN:Multi Scale Feature Fusion Dehazing Network with Dense connection",
    "abstract": "Single image dehazing is a challenging problem in computer vision. Previous work has mostly focused on designing new encoder and decoder in common network architectures, while neglecting the connection between the two. In this paper, we propose a multi-scale feature fusion dehazing network based on dense connection, MSF^2DN. The design principle of this network is to make full use of dense connection to achieve efficient reuse of features. On the one hand, we use a dense connection inside the base module of the encoder-decoder to fuse the features of different convolutional layers several times, and on the other hand, we design a simple multi-stream feature fusion module which fuses the features of different stages after uniform scaling and feeds them into the base module of the decoder for enhancement. Numerous experiments have demonstrated that our network outperforms the existing state-of-the-art networks in real-world datasets",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Xie_Synchronous_Bi-Directional_Pedestrian_Trajectory_Prediction_with_Error_Compensation_ACCV_2022_paper.html": {
    "title": "Synchronous Bi-Directional Pedestrian Trajectory Prediction with Error Compensation",
    "abstract": "Pedestrian trajectory prediction as an essential part of reasoning human motion behaviors, has been deployed in a number of vision applications, such as autonomous navigation and video surveillance. \nMost existing methods adopt autoregressive frameworks to forecast the future trajectory, where the trajectory is iteratively generated based on the previous outputs. Such a process will suffer from large accumulated errors over the long-term forecast horizon. To address this issue, in this paper, we propose a Synchronous Bi-Directional framework (SBD) with error compensation for pedestrian trajectory prediction, which can greatly alleviate the error accumulation during prediction. Specifically, we first develop a bi-directional trajectory prediction mechanism, and force the predicting procedures for two opposite directions to be synchronous through a shared motion characteristic. Different from previous works, the mutual constraints inherent to our framework from the synchronous opposite-predictions can significantly prevent the error accumulation. In order to reduce the possible prediction error in each timestep, we further devise an error compensation network to model and compensate for the positional deviation between the ground-truth and the predicted trajectory, thus improving the prediction accuracy of our scheme. Experiments conducted on the Stanford Drone dataset and the ETH-UCY dataset show that our method achieves much better results than existing algorithms.\nParticularly, by resorting to our alleviation methodology for the error accumulation, our scheme exhibits superior performance in the long-term pedestrian trajectory prediction",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhao_EAI-Stereo_Error_Aware_Iterative_Network_for_Stereo_Matching_ACCV_2022_paper.html": {
    "title": "EAI-Stereo: Error Aware Iterative Network for Stereo Matching",
    "abstract": "Current state-of-the-art stereo algorithms use a 2D CNN to extract features and then form a cost volume, which is fed into the following cost aggregation and regularization module composed of 2D or 3D CNNs. However, a large amount of high-frequency information like texture, color variation, sharp edge etc. is not well exploited during this process, which leads to relatively blurry and lacking detailed disparity maps. In this paper, we aim at making full use of the high-frequency information from the original image. Towards this end, we propose an error-aware refinement module that incorporates high-frequency information from the original left image and allows the network to learn error correction capabilities that can produce excellent subtle details and sharp edges. In order to improve the data transfer efficiency between our iterations, we propose the Iterative Multiscale Wide-LSTM Network which could carry more semantic information across iterations. We demonstrate the efficiency and effectiveness of our method on KITTI 2015, Middlebury, and ETH3D. At the time of writing this paper, EAI-Stereo ranks 1st on the Middlebury leaderboard and 1st on the ETH3D Stereo benchmark for 50% quantile metric and second for 0.5px error rate among all published methods. Our model performs well in cross-domain scenarios and outperforms current methods specifically designed for generalization",
    "volume": "main",
    "checked": false,
    "id": "06e89127926108353b02fd64e60e80fc24412816",
    "citation_count": 9
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chen_Reading_Arbitrary-Shaped_Scene_Text_from_Images_Through_Spline_Regression_and_ACCV_2022_paper.html": {
    "title": "Reading Arbitrary-Shaped Scene Text from Images Through Spline Regression and Rectification",
    "abstract": "Scene text in natural images contains a wealth of valuable semantic information. To read scene text from the image, various text spotting techniques that jointly detect and recognize scene text have been proposed in recent years. In this paper, we present a novel end-to-end text spotting network SPRNet for arbitrary-shaped scene text. We propose a parametric B-spline centerline-based representation model to describe the distinctive global shape characteristics of the text, which helps to effectively deal with interferences such as local connection and tight spacing of text and other object, and a text is detected by regressing its shape parameters. Further, exploiting the text's shape cues, we employ adaptive projection transformations to rectify the feature representation of an irregular text, which improves the accuracy of the subsequent text recognition network. Our method achieves competitive text spotting performance on standard benchmarks through a simple architecture equipped with the proposed text representation and rectification mechanism, which demonstrates the effectiveness of the method in detecting and recognizing scene text with arbitrary shapes",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Iida_Visual_Explanation_Generation_Based_on_Lambda_Attention_Branch_Networks_ACCV_2022_paper.html": {
    "title": "Visual Explanation Generation Based on Lambda Attention Branch Networks",
    "abstract": "Explanation generation for transformers enhances accountability for their predictions.\nHowever, there have been few studies on generating visual explanations for the transformers that use multidimensional context, such as LambdaNetworks.\nIn this paper, we propose the Lambda Attention Branch Networks, which attend to important regions in detail and generate easily interpretable visual explanations.\nWe also propose the Patch Insertion-Deletion score, an extension of the Insertion-Deletion score, as an effective evaluation metric for images with sparse important regions. \nExperimental results on two public datasets indicate that the proposed method successfully generates visual explanations",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lin_PEDTrans_A_fine-grained_visual_classification_model_for_self-attention_patch_enhancement_ACCV_2022_paper.html": {
    "title": "PEDTrans: A fine-grained visual classification model for self-attention patch enhancement and dropout",
    "abstract": "Fine-grained visual classification (FGVC) is an essential and challenging classification task in computer visual classification, aiming to identify different cars and birds. Recently, most studies use a convolutional neural network combined with an attention mechanism to find discriminant regions to improve algorithm accuracy automatically. However, the discriminant regions selected by the convolutional neural network are extensive. Vision Transformer divides the image into patches and relies on self-attention to select more accurate discriminant regions. However, the Vision Transformer model ignores the response between local patches before patch embedding. In addition, patches usually have high similarity, and they are considered redundant. Therefore, we propose a PEDTrans model based on Vision Transformer. The model has a patch enhancement module based on attention mechanism and a random similar group patch discarding module based on similarity. These two modules can establish patch local feature relationships and select patches that are easier to distinguish between images. Combining these two modules with the Vision Transformer backbone network can improve the fine-grained visual classification accuracy. We employ commonly used fine-grained visual classification datasets CUB-200-2011, Stanford Cars, Stanford Dogs and NABirds to get advanced results",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Yan_Style_Image_Harmonization_via_Global-Local_Style_Mutual_Guided_ACCV_2022_paper.html": {
    "title": "Style Image Harmonization via Global-Local Style Mutual Guided",
    "abstract": "The process of style image harmonization is attaching an area of the source image to the target style image to form a harmonious new image. Existing methods generally have problems such as distorted foreground, missing content, and semantic inconsistencies caused by the excessive transfer of local style. In this paper, we present a framework for style image harmonization via global and local styles mutual guided to ameliorate these problems. Specifically, we learn to extract global and local information from the Vision Transformer and Convolutional Neural Networks, and adaptively fuse the two kinds of information under a multi-scale fusion structure to ameliorate disharmony between foreground and background styles. Then we train the blending network GradGAN to smooth the image gradient. Finally, we take both style and gradient into consideration to solve the sudden change in the blended boundary gradient. In addition, supervision is unnecessary in our training process. Our experimental results show that our algorithm can balance global and local styles in the foreground stylization, retaining the original information of the object while keeping the boundary gradient smooth, which is more advanced than other methods",
    "volume": "main",
    "checked": false,
    "id": "66d695c001e245ef943a41e159a2761b727b589a",
    "citation_count": 9
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Huang_A_Compressive_Prior_Guided_Mask_Predictive_Coding_Approach_for_Video_ACCV_2022_paper.html": {
    "title": "A Compressive Prior Guided Mask Predictive Coding Approach for Video Analysis",
    "abstract": "In real-world scenarios, video analysis algorithms are conducted for visual signals after compression and transmission. Generally speaking, most codecs introduce irreversible distortion due to coarse quantization during compression. The distortion may lead to significant perception degradation in terms of video analysis performance. To tackle this problem, we propose an efficient plug-and-play approach to preserve the essential semantic information in video sequences explicitly. The proposed approach could boost the video analysis performance with a little extra bit cost. Specifically, we employ the proposed approach on an emerging video analysis task, video object segmentation(VOS). Massive experimental results prove that the our work outperforms the existing coding approaches over multiple VOS datasets. Concretely, it could improve the analysis performance by up to 13% at similar bitrates. Additional experiments also verifies the flexibility of our scheme because there is no dependency on any specific VOS model or encoding method. Essentially, the proposed approach provides novel insights for the emerging Video Coding for Machine (VCM) standard",
    "volume": "main",
    "checked": false,
    "id": "5b7009191f60b05979fe1d36e403227c6817cdc2",
    "citation_count": 4
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chen_Complex_Handwriting_Trajectory_Recovery_Evaluation_Metrics_and_Algorithm_ACCV_2022_paper.html": {
    "title": "Complex Handwriting Trajectory Recovery: Evaluation Metrics and Algorithm",
    "abstract": "Many important tasks such as forensic signature verification, calligraphy synthesis, etc, rely on handwriting trajectory recovery of which, however, even an appropriate evaluation metric is still missing. Indeed, existing metrics only focus on the writing orders but overlook the fidelity of glyphs. Taking both facets into account, we come up with two new metrics, the adaptive intersection on union (AIoU) which eliminates the influence of various stroke widths, and the length-independent dynamic time warping (LDTW) which solves the trajectory-point alignment problem. After that, we then propose a novel handwriting trajectory recovery model named Parsing-and-tracing ENcoder-decoder Network (PEN-Net), in particular for characters with both complex glyph and long trajectory, which was believed very challenging. In the PEN-Net, a carefully designed double-stream parsing encoder parses the glyph structure, and a global tracing decoder overcomes the memory difficulty of long trajectory prediction. Our experiments demonstrate that the two new metrics AIoU and LDTW together can truly assess the quality of handwriting trajectory recovery and the proposed PEN-Net exhibits satisfactory performance in various complex-glyph languages including Chinese, Japanese and Indic. The source code is available at https://github.com/ChenZhounan/PEN-Net",
    "volume": "main",
    "checked": true,
    "id": "bfa5de6cf8b4e8f2bfc8a4834858fa7f58f2c26a",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Duan_Network_Pruning_via_Feature_Shift_Minimization_ACCV_2022_paper.html": {
    "title": "Network Pruning via Feature Shift Minimization",
    "abstract": "Channel pruning is widely used to reduce the complexity of deep network models. Recent pruning methods usually identify which parts of the network to discard by proposing a channel importance criterion. However, recent studies have shown that these criteria do not work well in all conditions. In this paper, we propose a novel Feature Shift Minimization (FSM) method to compress CNN models, which evaluates the feature shift by converging the information of both features and filters. Specifically, we first investigate the compression efficiency with some prevalent methods in different layer-depths and then propose the feature shift concept. Then, we introduce an approximation method to estimate the magnitude of the feature shift, since it is difficult to compute it directly. Besides, we present a distribution-optimization algorithm to compensate for the accuracy loss and improve the network compression efficiency. The proposed method yields state-of-the-art performance on various benchmark networks and datasets, verified by extensive experiments. Our codes are available at: https://github.com/lscgx/FSM",
    "volume": "main",
    "checked": true,
    "id": "199af0c1930842d1e31153674592446087fa2b94",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhao_Training-free_NAS_for_3D_Point_Cloud_Processing_ACCV_2022_paper.html": {
    "title": "Training-free NAS for 3D Point Cloud Processing",
    "abstract": "Deep neural networks for 3D point cloud processing have exhibited superior performance on many tasks. However, the structure and computational complexity of existing networks are relatively fixed, which makes it difficult for them to be flexibly applied to devices with different computational constraints. Instead of manually designing the network structure for each specific device, in this paper, we propose a novel training-free neural architecture search algorithm which can quickly sample network structures that satisfy the computational constraints of various devices. Specifically, we design a cell-based search space that contains a large number of latent network structures. The computational complexity of these structures varies within a wide range to meet the needs of different devices. We also propose a multi-objective evolutionary search algorithm. This algorithm scores the candidate network structures in the search space based on multiple training-free proxies, encourages high-scoring networks to evolve, and gradually eliminates low-scoring networks, so as to search for the optimal network structure. Because the calculation of training-free proxies is very efficient, the whole algorithm can be completed in a short time. Experiments on 3D point cloud classification and part segmentation demonstrate the effectiveness of our method",
    "volume": "main",
    "checked": false,
    "id": "805a29e9b9cd08cdcf6fb70ea37c55e6a40220ed",
    "citation_count": 1
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Bonfiglioli_The_Eyecandies_Dataset_for_Unsupervised_Multimodal_Anomaly_Detection_and_Localization_ACCV_2022_paper.html": {
    "title": "The Eyecandies Dataset for Unsupervised Multimodal Anomaly Detection and Localization",
    "abstract": "We present Eyecandies, a novel synthetic dataset for unsupervised anomaly detection and localization. Photo-realistic images of procedurally generated candies are rendered in a controlled environment under multiple lightning conditions, also providing depth and normal maps in an industrial conveyor scenario. We make available anomaly-free samples for model training and validation, while anomalous instances with precise ground-truth annotations are provided only in the test set. The dataset comprises ten classes of candies, each showing different challenges, such as complex textures, self-occlusions and specularities. Furthermore, we achieve large intra-class variation by randomly drawing key parameters of a procedural rendering pipeline, which enables the creation of an arbitrary number of instances with photo-realistic appearance. Likewise, anomalies are injected into the rendering graph and pixel-wise annotations are automatically generated, overcoming human-biases and possible inconsistencies.\n\nWe believe this dataset may encourage the exploration of original approaches to solve the anomaly detection task, e.g. by combining color, depth and normal maps, as they are not provided by most of the existing datasets. Indeed, in order to demonstrate how exploiting additional information may actually lead to higher detection performance, we show the results obtained by training a deep convolutional autoencoder to reconstruct different combinations of inputs",
    "volume": "main",
    "checked": true,
    "id": "1dc01c8381308f6d01f61196ceb7f3c4f6443d03",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Fan_MaxGNR_A_Dynamic_Weight_Strategy_via_Maximizing_Gradient-to-Noise_Ratio_for_ACCV_2022_paper.html": {
    "title": "MaxGNR: A Dynamic Weight Strategy via Maximizing Gradient-to-Noise Ratio for Multi-Task Learning",
    "abstract": "When modeling related tasks in computer vision, Multi-Task Learning (MTL) can outperform Single-Task Learning (STL) due to its ability to capture intrinsic relatedness among tasks. However, MTL may encounter the insufficient training problem, i.e., some tasks in MTL may encounter non-optimal situation compared with STL. A series of studies point out that too much gradient noise would lead to performance degradation in STL, however, in the MTL scenario, Inter-Task Gradient Noise (ITGN) is an additional source of gradient noise for each task, which can also affect the optimization process. In this paper, we point out ITGN as a key factor leading to the insufficient training problem. We define the Gradient-to-Noise Ratio (GNR) to measure the relative magnitude of gradient noise and design the MaxGNR algorithm to alleviate the ITGN interference of each task by maximizing the GNR of each task. We carefully evaluate our MaxGNR algorithm on two standard image MTL datasets: NYUv2 and Cityscapes. The results show that our algorithm outperforms the existing baselines under identical experimental conditions",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Hong_SG-Net_Semantic_Guided_Network_for_Image_Dehazing_ACCV_2022_paper.html": {
    "title": "SG-Net: Semantic Guided Network for Image Dehazing",
    "abstract": "From traditional handcrafted priors to learning-based neural networks, image dehazing technique has gone through great development. In this paper, we propose an end-to-end Semantic Guided Network (SG-Net) for directly restoring the haze-free images. Inspired by the high similarity (mapping relationship) between the transmission maps and the segmentation results of hazy images, we found that the semantic information of the scene provides a strong natural prior for image restoration. To guide the dehazing more effectively and systematically, we utilize the information of semantic segmentation with three easily portable modes: Semantic Fusion (SF), Semantic Attention (SA), and Semantic Loss (SL), which compose our Semantic Guided (SG) mechanisms. By embedding these SG mechanisms into existing dehazing networks, we construct the SG-Net series: SG-AOD, SG-GCA, SG-FFA, and SG-AECR. The outperformance on image dehazing of these SG networks is demonstrated by the experiments in terms of both quantity and quality. It is worth mentioning that SG-FFA achieves the state-of-the-art performance",
    "volume": "main",
    "checked": false,
    "id": "11fc332bdcc843aad7475bb4566e73a957dffda5",
    "citation_count": 115
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chen_Robust_Human_Matting_via_Semantic_Guidance_ACCV_2022_paper.html": {
    "title": "Robust Human Matting via Semantic Guidance",
    "abstract": "Automatic human matting is highly desired for many real applications. We investigate recent human matting methods and show that common bad cases happen when semantic human segmentation fails. This indicates that semantic understanding is crucial for robust human matting. From this, we develop a fast yet accurate human matting framework, named Semantic Guided Human Matting (SGHM). It builds on a semantic human segmentation network and introduces a light-weight matting module with only marginal computational cost. Unlike previous works, our framework is data efficient, which requires a small amount of matting ground-truth to learn to estimate high quality object mattes. Our experiments show that trained with merely 200 matting images, our method can generalize well to real-world datasets, and outperform recent methods on multiple benchmarks, while remaining efficient. Considering the unbearable labeling cost of matting data and widely available segmentation data, our method becomes a practical and effective solution for the task of human matting. Source code is available at https://github.com/cxgincsu/SemanticGuidedHumanMatting",
    "volume": "main",
    "checked": true,
    "id": "0188670f9c1335ad22a51e33d6fb1c33456a8149",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Cheng_KinStyle_A_Strong_Baseline_Photorealistic_Kinship_Face_Synthesis_with_An_ACCV_2022_paper.html": {
    "title": "KinStyle: A Strong Baseline Photorealistic Kinship Face Synthesis with An Optimized StyleGAN Encoder",
    "abstract": "High-fidelity kinship face synthesis is a challenging task due to the limited amount of kinship data available for training and low-quality images. In addition, it is also hard to trace the genetic traits between parents and children from those low-quality training images. To address these issues, we leverage the pre-trained state-of-the-art face synthesis model, StyleGAN2, for kinship face synthesis. To handle large age, gender and other attribute variations between the parents and their children, we conduct a thorough study of its rich latent spaces and different encoder architectures for an optimized encoder design to repurpose StyleGAN2 for kinship face synthesis. The obtained latent representation from our developed encoder pipeline with stage-wise training strikes a better balance of editability and synthesis fidelity for identity preserving and attribute manipulations than other compared approaches. With extensive subjective, quantitative, and qualitative evaluations, the proposed approach consistently achieves better performance in terms of facial attribute heredity and image generation fidelity than other compared state-of-the-art methods. This demonstrates the effectiveness of the proposed approach which can yield promising and satisfactory kinship face synthesis using only a single and straightforward encoder architecture",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Ming_Soft_Label_Mining_and_Average_Expression_Anchoring_for_Facial_Expression_ACCV_2022_paper.html": {
    "title": "Soft Label Mining and Average Expression Anchoring for Facial Expression Recognition",
    "abstract": "Facial expression recognition (FER) suffers from high interclass similarity and large intraclass variation, leading to ambiguity or uncertainty and further confusing annotators. They also hinder the network in learning the valuable features of facial expression. Recently, many studies have revealed that the uncertainty or ambiguity is one of the key challenges in FER.  In this paper, we propose a new method to address this issue from two aspects: a soft label mining module to convert the original hard labels to soft labels dynamically during training, and an average facial expression anchoring module to separate unique expression features from similarity expression features. The soft label mining module breaks the limits of the categorical model and mitigates the uncertainty or ambiguity.  And the average facial expression anchoring module suppresses the high interclass similarity of facial expressions. Our method can train any backbone network for facial expression recognition. The experiments on the popular datasets show that our method achieves state-of-the-art results by 92.82% on RAF-DB and 67.91% on SFEW, and achieves a comparable result of 62.26% on AffectNet. The code is available at https://github.com/HaipengMing/SLM-AEA",
    "volume": "main",
    "checked": false,
    "id": "c328b509644371a49bdf3444731f0707ac2e49c8",
    "citation_count": 7
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Hunt_Rove-Tree-11_The_not-so-Wild_Rover_A_hierarchically_structured_image_dataset_for_ACCV_2022_paper.html": {
    "title": "Rove-Tree-11: The not-so-Wild Rover, A hierarchically structured image dataset for deep metric learning research",
    "abstract": "We present a new dataset of images of pinned insects from museum collections along with a ground truth phylogeny (a graph representing the relative evolutionary distance between species). The images include segmentations, and can be used for clustering and deep hierarchical metric learning. As far as we know, this is the first dataset released specifically for generating phylogenetic trees. We provide several benchmarks for deep metric learning using a selection of state-of-the-art methods",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zheng_PromptLearner-CLIP_Contrastive_Multi-Modal_Action_Representation_Learning_with_Context_Optimization_ACCV_2022_paper.html": {
    "title": "PromptLearner-CLIP: Contrastive Multi-Modal Action Representation Learning with Context Optimization",
    "abstract": "An action contains rich multi-modal information, and current methods generally map the action class to a digital number as supervised information to train models. However, numerical labels cannot describe the semantic content contained in the action. This paper proposes PromptLearner-CLIP for action recognition, where the text pathway uses PromptLearner to automatically learn the text content of prompt as the input and calculates the semantic features of actions, and the vision pathway takes video data as the input to learn the visual features of actions. To strengthen the interaction between features of different modalities, this paper proposes a multi-modal information interaction module that utilizes Graph Neural Network(GNN) to process both the semantic features of text content and the visual features of a video. In addition, the single-modal video classification problem is transformed into a multi-modal video-text matching problem. Multi-modal contrastive learning is used to disclose the feature distance of the same but different modalities samples. The experimental results showed that PromptLearner-CLIP could utilize the textual semantic information to significantly improve the performance of various single-modal backbone networks on action recognition and achieved top-tier results on Kinetics400, UCF101, and HMDB51 datasets",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Class_Specialized_Knowledge_Distillation_ACCV_2022_paper.html": {
    "title": "Class Specialized Knowledge Distillation",
    "abstract": "Knowledge Distillation (KD) is a compression framework that transfers distilled knowledge from a teacher to a smaller student model. KD approaches conventionally address problem domains where the teacher and student network have equal numbers of classes for classification. We provide a knowledge distillation solution tailored for the class specialization setting, where the user requires a compact and performant network specializing in a subset of classes from the class set used to train the teacher model. To this end, we introduce a novel knowledge distillation framework, Class Specialized Knowledge Distillation (CSKD), that combines two loss functions: Renormalized Knowledge Distillation (RKD) and Intra-Class Variance (ICV) to render a computationally-efficient, specialized student network. We report results on several popular architectural benchmarks and tasks. In particular, CSKD consistently demonstrates significant performance improvements over teacher models for highly restrictive specialization tasks (e.g., instances where the number of subclasses or datasets is relatively small), in addition to outperforming other state-of-the-art knowledge distillation approaches for class specialization tasks",
    "volume": "main",
    "checked": false,
    "id": "47062861716adf45bb7cbe1fc5bf6b6d414adad7",
    "citation_count": 44
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lin_MVFI-Net_Motion-aware_Video_Frame_Interpolation_Network_ACCV_2022_paper.html": {
    "title": "MVFI-Net: Motion-aware Video Frame Interpolation Network",
    "abstract": "Video frame interpolation (VFI) is to synthesize the intermediate frame given successive frames. Most existing learning-based VFI methods generate each target pixel by using the warping operation with either one predicted kernel or flow, or both. However, their performances are often degraded due to the issues on the limited direction and scope of the reference regions, especially encountering complex motions. In this paper, we propose a novel motion-aware VFI network (MVFI-Net) to address these issues. One of the key novelties of our method lies in the newly developed warping operation,  i.e., motion-aware convolution (MAC). By predicting multiple extensible temporal motion vectors (MVs) and filter kernels for each target pixel, the direction and scope could be enlarged simultaneously. Besides, we first attempt to incorporate the pyramid structure into the kernel-based VFI, which can decompose large motions into smaller scales to improve the prediction efficiency. The quantitative and qualitative experimental results have demonstrated the proposed method delivers the state-of-the-art performance on the diverse benchmarks with various resolutions. Our codes are available at https://github.com/MediaLabVFI/MVFI-Net",
    "volume": "main",
    "checked": false,
    "id": "d833c48334e906537f21757b6f9fa44da66f6c76",
    "citation_count": 191
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/He_D3_Duplicate_Detection_Decontaminator_for_Multi-Athlete_Tracking_in_Sports_Videos_ACCV_2022_paper.html": {
    "title": "D^3: Duplicate Detection Decontaminator for Multi-Athlete Tracking in Sports Videos",
    "abstract": "Tracking multiple athletes in sports videos is a very challenging Multi-Object Tracking (MOT) task, since athletes often have the same appearance and are intimately covered with each other, making a common occlusion problem becomes an abhorrent duplicate detection. In this paper, the duplicate detection is newly and precisely defined as occlusion misreporting on the same athlete by multiple detection boxes in one frame. To address this problem, we meticulously design a novel transformer-based Duplicate Detection Decontaminator (D^3) for training, and a specific algorithm Rally-Hungarian (RH)  for matching. Once duplicate detection occurs, D^3 immediately modifies the procedure by generating enhanced boxes losses. RH, triggered by the team sports substitution rules, is exceedingly suitable for sports videos. Moreover, to complement the tracking dataset that without shot changes, we release a new dataset based on sports video named RallyTrack. Extensive experiments on RallyTrack show that combining D^3 and RH can dramatically improve the tracking performance with 9.2 in MOTA and 4.5 in HOTA. Meanwhile, experiments on MOT-series and DanceTrack discover that D^3 can accelerate convergence during training, especially save up to 80 percent of the original training time on MOT17. Finally, our model, which is trained only with volleyball videos, can be applied directly to basketball and soccer videos, which shows priority of our method",
    "volume": "main",
    "checked": false,
    "id": "8be1de8e534cf1cbffdfce27b1cf3d076a235a01",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Yoon_Lightweight_Alpha_Matting_Network_Using_Distillation-Based_Channel_Pruning_ACCV_2022_paper.html": {
    "title": "Lightweight Alpha Matting Network Using Distillation-Based Channel Pruning",
    "abstract": "Recently, alpha matting has received a lot of attention because of its usefulness in mobile applications such as selfies. Therefore, there has been a demand for a lightweight alpha matting model due to the limited computational resources of commercial portable devices. To this end, we suggest a distillation-based channel pruning method for the alpha matting networks. In the pruning step, we remove channels of a student network having fewer impacts on mimicking the knowledge of a teacher network. Then, the pruned lightweight student network is trained by the same distillation loss. A lightweight alpha matting model from the proposed method outperforms existing lightweight methods. To show superiority of our algorithm, we provide various quantitative and qualitative experiments with in-depth analyses. Furthermore, we demonstrate the versatility of the proposed distillation-based channel pruning method by applying it to semantic segmentation",
    "volume": "main",
    "checked": true,
    "id": "ef691c914a3a59c12fc58c671f5488883cdc8d21",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Ghosh_Labelling_the_Gaps_A_Weakly_Supervised_Automatic_Eye_Gaze_Estimation_ACCV_2022_paper.html": {
    "title": "`Labelling the Gaps': A Weakly Supervised Automatic Eye Gaze Estimation",
    "abstract": "Over the past few years, there has been an increasing interest to interpret gaze direction in an unconstrained environment with limited supervision. Owing to data curation and annotation issues, replicating gaze estimation method to other platforms, such as unconstrained outdoor or AR/VR, might lead to significant drop in performance due to insufficient availability of accurately annotated data for model training. In this paper, we explore an interesting yet challenging problem of gaze estimation method with a limited amount of labelled data. The proposed method utilize domain knowledge from the labelled subset with visual features; including identity-specific appearance, gaze trajectory consistency and motion features. Given a gaze trajectory, the method utilizes label information of only the start and the end frames of a gaze sequence. An extension of the proposed method further reduces the requirement of labelled frames to only the start frame with a minor drop in the generated label's quality. We evaluate the proposed method on four benchmark datasets (CAVE, TabletGaze, MPII and Gaze360) as well as web-crawled YouTube videos. Our proposed method reduces the annotation effort to as low as 2.67%, with minimal impact on performance; indicating the potential of our model enabling gaze estimation `in-the-wild' setup",
    "volume": "main",
    "checked": false,
    "id": "0cbb53b3d39a4e1e64c5112e63e4741e2e9bed7b",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Liu_Cross-Domain_Local_Characteristic_Enhanced_Deepfake_Video_Detection_ACCV_2022_paper.html": {
    "title": "Cross-Domain Local Characteristic Enhanced Deepfake Video Detection",
    "abstract": "As ultra-realistic face forgery techniques emerge, deepfake detection has attracted increasing attention due to security concerns. Many detectors cannot achieve accurate results when detecting unseen manipulations despite excellent performance on known forgeries. In this paper, we are motivated by the observation that the discrepancies between real and fake videos are extremely subtle and localized, and inconsistencies or irregularities can exist in some critical facial regions across various information domains. To this end, we propose a novel pipeline, Cross-Domain Local Forensics (XDLF), for more general deepfake video detection. In the proposed pipeline, a specialized framework is presented to simultaneously exploit local forgery patterns from space, frequency, and time domains, thus learning cross-domain features to detect forgeries. Moreover, the framework leverages four high-level forgery-sensitive local regions of a human face to guide the model to enhance subtle artifacts and localize potential anomalies. Extensive experiments on several benchmark datasets demonstrate the impressive performance of our method, and we achieve superiority over several state-of-the-art methods on cross-dataset generalization. We also examined the factors that contribute to its performance through ablations, which suggests that exploiting cross-domain local characteristics is a noteworthy direction for developing more general deepfake detectors",
    "volume": "main",
    "checked": true,
    "id": "a61612acef9bb7bde09874c8411095300bb3f7f8",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Nguyen_Multi-stream_Fusion_for_Class_Incremental_Learning_in_Pill_Image_Classification_ACCV_2022_paper.html": {
    "title": "Multi-stream Fusion for Class Incremental Learning in Pill Image Classification",
    "abstract": "Classifying pill categories from real-world images is crucial for various smart healthcare applications. Although existing approaches in image classification might achieve a good performance on fixed pill categories, they fail to handle novel instances of pill categories that are frequently presented to the learning algorithm. To this end, a trivial solution is to train the model with novel classes. However, this may result in a phenomenon known as catastrophic forgetting, in which the system forgets what it learned in previous classes. In this paper, we address this challenge by introducing the class incremental learning (CIL) ability to traditional pill image classification systems. Specifically, we propose a novel incremental multi-stream intermediate fusion framework enabling incorporation of an additional guidance information stream that best matches the domain of the problem into various state-of-the-art CIL methods. From this framework, we consider color-specific information of pill images as a guidance stream and devise an approach, namely \"Color Guidance with Multi-stream intermediate fusion\"(CG-IMIF) for solving CIL pill image classification task. We conduct comprehensive experiments on real-world incremental pill image classification dataset, namely VAIPE-PCIL, and find that the CG-IMIF consistently outperforms several state-of-the-art methods by a large margin in different task settings. Our code, data, and trained model are available at https://github.com/vinuni-vishc/CG-IMIF",
    "volume": "main",
    "checked": true,
    "id": "8b0d90a3a3aa79219f82cff9a7284c599d9e4451",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Ma_CrossUFormer_A_Cross_Attention_U-Shape_Transformer_for_Low_Light_Image_ACCV_2022_paper.html": {
    "title": "CrossUFormer: A Cross Attention U-Shape Transformer for Low Light Image Enhancement",
    "abstract": "Low light image enhancement is the focus of low level vision task, which restores low light image to normal light image. In recent years, low-light image enhancement methods based on convolution neural networks are dominant. However, the Vision Transformers, which have recently made breakthroughs in advanced visual tasks, do not bring a new dimension to low light image enhancement. Therefore, CrossUFormer algorithm is proposed in this paper. Both encoder and decoder parts are composed of several EnhanceFormer blocks. Each EnhanceFormer block is composed of long and short dual-branch structures to aggregate spatial-wise information. The long branch is composed of multi-head self-attention to extract low-frequency information, while the short branch is composed of parallel convolution layer to extract high-frequency information. In order to better model the global context information,  multi-scale feature cross attention (MSFCA) block is proposed to guide the fused multi-scale channel-wise information by cross attention to effectively connect to the decoder features. The adpative fusion of encoder and decoder features are guided by adaptive feature fusion (AFF) block. Our method outperforms state-of-the-art methods on multiple datasets",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chen_Teacher-Guided_Learning_for_Blind_Image_Quality_Assessment_ACCV_2022_paper.html": {
    "title": "Teacher-Guided Learning for Blind Image Quality Assessment",
    "abstract": "The performance of deep learning models for blind image quality assessment (BIQA) suffers from annotated data insufficiency. However, image restoration, as a closely-related task with BIQA, can easily acquire training data without annotation. Moreover, both image semantic and distortion information are vital knowledge for the two tasks to predict and improve image quality. Inspired by these, this paper proposes a novel BIQA framework, which builds an image restoration model as a teacher network (TN) to learn the two aspects of knowledge and then guides the student network (SN) for BIQA. In TN, multi-branch convolutions are leveraged for performing adaptive restoration from diversely distorted images to strengthen the knowledge learning. Then the knowledge is transferred to the SN and progressively aggregated by computing long-distance responses to improve BIQA on small annotated data. Experimental results show that our method outperforms many state-of-the-arts on both synthetic and authentic datasets. Besides, the generalization, robustness and effectiveness of our method are fully validated. The code is available in https://github.com/chencn2020/TeacherIQA",
    "volume": "main",
    "checked": false,
    "id": "e8012b78c4ec3f9f66669d339290509d4d2342a1",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Aggarwal_Layered-Garment_Net_Generating_Multiple_Implicit_Garment_Layers_from_a_Single_ACCV_2022_paper.html": {
    "title": "Layered-Garment Net: Generating Multiple Implicit Garment Layers from a Single Image",
    "abstract": "Recent research works have focused on generating human models and garments from their 2D images. However, state-of-the-art researches focus either on only a single layer of the garment on a human model or on generating multiple garment layers without any guarantee of the intersection-free geometric relationship between them. In reality, people wear multiple layers of garments in their daily life, where an inner layer of garment could be partially covered by an outer one. In this paper, we try to address this multi-layer modeling problem and propose the Layered-Garment Net (LGN) that is capable of generating intersection-free multiple layers of garments defined by implicit function fields over the body surface, given the person's near front-view image. With a special design of garment indication fields (GIF), we can enforce an implicit covering relationship between the signed distance fields (SDF) of different layers to avoid self-intersections among different garment surfaces and the human body. Experiments demonstrate the strength of our proposed LGN framework in generating multi-layer garments as compared to state-of-the-art methods. To the best of our knowledge, LGN is the first research work to generate intersection-free multiple layers of garments on the human body from a single image",
    "volume": "main",
    "checked": true,
    "id": "3540d25e21c5623a47553cc1ded31fc373e381c9",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhou_Pyramidal_Signed_Distance_Learning_for_Spatio-Temporal_Human_Shape_Completion_ACCV_2022_paper.html": {
    "title": "Pyramidal Signed Distance Learning for Spatio-Temporal Human Shape Completion",
    "abstract": "We address the problem of completing partial human shape observations as obtained with a depth camera. Existing methods that solve this problem can provide robustness, with for instance model-based strategies that rely on parametric human models, or precision, with learning approaches that can capture local geometric patterns using implicit neural representations. We investigate how to combine both properties with a novel pyramidal spatio-temporal learning model. This model exploits neural signed distance fields in a coarse-to-fine manner, this in order to benefit from the ability of implicit neural representations to preserve local geometry details while enforcing more global spatial consistency for the estimated shapes through features at coarser levels. In addition, our model also leverages temporal redundancy with spatio-temporal features that integrate information over neighboring frames. Experiments on standard datasets show that both the coarse-to-fine and temporal aggregation strategies contribute to outperform the state-of-the-art methods on human shape completion",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Giebenhain_Neural_Puppeteer_Keypoint-Based_Neural_Rendering_of_Dynamic_Shapes_ACCV_2022_paper.html": {
    "title": "Neural Puppeteer: Keypoint-Based Neural Rendering of Dynamic Shapes",
    "abstract": "We introduce Neural Puppeteer, an efficient neural rendering\npipeline for articulated shapes. By inverse rendering, we can predict 3D\nkeypoints from multi-view 2D silhouettes alone, without requiring texture information. Furthermore, we can easily predict 3D keypoints of the\nsame class of shapes with one and the same trained model and generalize\nmore easily from training with synthetic data which we demonstrate by\nsuccessfully applying zero-shot synthetic to real-world experiments. We\ndemonstrate the flexibility of our method by fitting models to synthetic\nvideos of different animals and a human, and achieve quantitative results\nwhich outperform our baselines. Our method uses 3D keypoints in conjunction with individual local feature vectors and a global latent code\nto allow for an efficient representation of time-varying and articulated\nshapes such as humans and animals. In contrast to previous work, we do\nnot perform reconstruction in the 3D domain, but project the 3D features into 2D cameras and perform reconstruction of 2D RGB-D images\nfrom these projected features, which is significantly faster than volumetric rendering. Our synthetic dataset will be publicly available, to further\ndevelop the evolving field of animal pose and shape reconstruction",
    "volume": "main",
    "checked": false,
    "id": "6b1103498a08298be4ea90afd800da6fcde74b53",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Ye_SCOAD_Single-frame_Click_Supervision_for_Online_Action_Detection_ACCV_2022_paper.html": {
    "title": "SCOAD: Single-frame Click Supervision for Online Action Detection",
    "abstract": "Online action detection based on supervised learning requires heavy manual annotation, which is difficult to obtain and may be impractical in real applications. Weakly supervised online action detection (WOAD) can effectively mitigate the problem of substantial labeling costs by using video-level labels. In this paper, we revisit WOAD and propose a weakly supervised online action detection using click-level labels for training, named Single-frame Click Supervision for Online Action Detection (SCOAD). Comparatively, click-level labels can effectively improve prediction accuracy by carrying a small amount of temporal information without massively increase the difficulty and cost of annotation. Specifically, SCOAD includes two joint training modules, i.e., Action Instance Miner (AIM) and Online Action Detector (OAD). To provide more guidance for training network as accuracy as possible, AIM mines pseudo-action instances under the supervision of click labels. Meanwhile, we generate video similarity instances offline by the similarity between video frames and use it to perform finer granularity filtering of error instances generated by AIM. OAD is trained jointly with AIM for online action detection by the pseudo frame-level labels converted from the filtered pseudo-action instances. We conduct extensive experiments on two benchmark datasets to demonstrate that SCOAD can effectively mine and utilize the small amount of temporal information in click-level labels. Code is available at https://github.com/zstarN70/SCOAD.git",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_OVPT_Optimal_Viewset_Pooling_Transformer_for_3D_Object_Recognition_ACCV_2022_paper.html": {
    "title": "OVPT: Optimal Viewset Pooling Transformer for 3D Object Recognition",
    "abstract": "The current methods for multi-view-based 3D object recognition have the problem of losing the correlation between views and rendering 3D objects with multi-view redundancy. This makes it difficult to improve recognition performance and unnecessarily increases the computational cost and running time of the network. Especially in the case of limited computing resources, the recognition performance is further affected. Our study developed an optimal viewset pooling transformer (OVPT) method for efficient and accurate 3D object recognition. The OVPT method constructs the optimal viewset based on information entropy to reduce the redundancy of the multi-view scheme. We used convolutional neural network (CNN) to extract the multi-view low-level local features of the optimal viewset. Embedding class token into the headers of multi-view low-level local features and splicing with position encoding generates local-view token sequences. This sequence was trained parallel with a pooling transformer to generate a local view information token sequence. At the same time, the global class token captured the global feature information of the local view token sequence. The two were aggregated next into a single compact 3D global feature descriptor. On two public benchmarks, ModelNet10 and ModelNet40, for each 3D object we only need a smaller number of optimal viewsets, achieving an overall recognition accuracy (OA) of 99.33% and 97.48%, respectively. Compared with other deep learning methods, our method still achieves state-of-the-art performance with limited computational resources. Our source code is available at https://github.com/shepherds001/OVPT",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_DIG_Draping_Implicit_Garment_over_the_Human_Body_ACCV_2022_paper.html": {
    "title": "DIG: Draping Implicit Garment over the Human Body",
    "abstract": "Existing data-driven methods for draping garments over human bodies, despite being effective, cannot handle garments of arbitrary topology and are typically not end-to-end differentiable. To address these limitations, we propose an end-to-end differentiable pipeline that represents garments using implicit surfaces and learns a skinning field conditioned on shape and pose parameters of an articulated body model. To limit body-garment interpenetrations and artifacts, we propose an interpenetration-aware pre-processing strategy of training data and a novel training loss that penalizes self-intersections while draping garments. We demonstrate that our method yields more accurate results for garment reconstruction and deformation with respect to state of the art methods. Furthermore, we show that our method, thanks to its end-to-end differentiability, allows to recover body and garments parameters jointly from image observations, something that previous work could not do. Our code is available at https://github.com/liren2515/DIG",
    "volume": "main",
    "checked": true,
    "id": "40049f18c464bb3250423d06890fb9a05416a283",
    "citation_count": 2
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Murata_Learning_and_Transforming_General_Representations_to_Break_Down_Stability-Plasticity_Dilemma_ACCV_2022_paper.html": {
    "title": "Learning and Transforming General Representations to Break Down Stability-Plasticity Dilemma",
    "abstract": "In the Class Incremental Learning (CIL) setup, a learning model must have the ability to incrementally update its knowledge to recognize newly appeared classes (plasticity) while maintaining the knowledge to recognize the classes it has already learned (stability). Such conflicting requirements are known as the stability-plasticity dilemma, and most existing studies attempt to achieve a good balance between them by stability improvements. Unlike those attempts, we focus on the generality of representations. The basic idea is that a model does not need to change if it has already learned such general representations that they contain enough information to recognize new classes. However, the general representations are not optimal for recognizing the classes a model has already learned because the representations must contain unrelated and noisy information for recognizing them. To acquire representations suitable for recognizing known classes while leveraging general representations, in this paper, we propose a new CIL framework that learns general representations and transforms them into suitable ones for the target classification tasks. In our framework, we achieve the acquisition of general representations and their transformation by self-supervised learning and attention techniques, respectively. In addition, we introduce a novel knowledge distillation loss to make the transformation mechanism stable. Using benchmark datasets, we empirically confirm that our framework can improve the average incremental accuracy of four types of CIL methods that employ knowledge distillation in the CIL setting",
    "volume": "main",
    "checked": false,
    "id": "442cdbbc5e16b4d89b87f66fbabce2df8977630f",
    "citation_count": 1
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_Foreground-Specialized_Model_Imitation_for_Instance_Segmentation_ACCV_2022_paper.html": {
    "title": "Foreground-Specialized Model Imitation for Instance Segmentation",
    "abstract": "Instance segmentation is formulated as a multi-task learning problem. However, knowledge distillation is not well-suited to all sub-tasks except the multi-class object classification. Based on such a competence, we introduce a lightweight foreground-specialized (FS) teacher model, which is trained with foreground-only images and highly optimized for object classification. Yet, this leads to discrepancy between inputs to the teacher and student models. Thus, we introduce a novel Foreground-Specialized model Imitation (FSI) method with two complementary components. First, a reciprocal anchor box selection method is introduced to distill from the most informative output of the FS teacher. Second, we embed the foreground-awareness into student's feature learning via either adding a co-learned foreground segmentation branch or applying a soft feature mask. We conducted an extensive evaluation against the others on COCO and Pascal VOC",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Li_PPR-Net_Patch-based_multi-scale_pyramid_registration_network_for_defect_detection_of_ACCV_2022_paper.html": {
    "title": "PPR-Net: Patch-based multi-scale pyramid registration network for defect detection of printed labels",
    "abstract": "Detecting defects in printed labels is essential to ensure product quality. Reference-based comparison is a potential method to challenge this task, which is widely used for defect detection. However, this method gets poor performance under large deformation, due to the lack of ability of registering the testing image with the reference image. Therefore, accurate image registration is an urgent case for defect detection of printed labels. In this paper, a patch-based multi-scale pyramid registration network (PPR-Net) is proposed. First, an image patch splitting and stitching strategy is proposed, which is scalable in image resolution. Second, a multi-scale pyramid registration module is designed to fuse multiple convolutional features to enhance the registration capability for large deformation, which gradually refines multi-scale deformation fields in a coarse-to-fine manner.  Third, a distortion loss function is introduced to improve text distortions of registered images. Finally, a synthetic database is generated based on real printed labels, to simulate defective printed labels with large deformation for performance comparison. Extensive experimental results show that our method dramatically outperforms other comparable approaches",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Chen_Exposing_Face_Forgery_Clues_via_Retinex-based_Image_Enhancement_ACCV_2022_paper.html": {
    "title": "Exposing Face Forgery Clues via Retinex-based Image Enhancement",
    "abstract": "Public concerns about deepfake face forgery are continually rising in recent years. Existing deepfake detection approaches typically use convolutional neural networks (CNNs) to mine subtle artifacts under high-quality forged faces. However, most CNN-based deepfake detectors tend to over-fit the content-specific color textures, and thus fail to generalize across different data sources, forgery methods, and/or postprocessing operations. It motivates us to develop a method to expose the subtle forgery clues in RGB space. Herein, we propose to utilize multiscale retinex-based enhancement of RGB space and compose a novel modality, named MSR, to complementary capture the forgery traces. To take full advantage of the MSR information, we propose a two-stream network combined with salience-guided attention and feature re-weighted interaction modules. The salience-guided attention module guides the RGB feature extractor to concentrate more on forgery traces from an MSR perspective. The feature re-weighted interaction module implicitly learns the correlation between the two complementary modalities to promote feature learning for each other. Comprehensive experiments on several benchmarks show that our method outperforms the state-of-the-art\nface forgery detection methods in detecting severely compressed deepfakes. Besides, our method also shows superior performances on crossdatasets evaluation",
    "volume": "main",
    "checked": false,
    "id": "64266a2bea58937799631bfecda250f85b44bed1",
    "citation_count": 10
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Occluded_Facial_Expression_Recognition_using_Self-supervised_Learning_ACCV_2022_paper.html": {
    "title": "Occluded Facial Expression Recognition using Self-supervised Learning",
    "abstract": "Recent studies on occluded facial expression recognition typically required fully expression-annotated facial images for training. However, it is time consuming and expensive to collect a large number of facial images with various occlusions and expression annotations. To address this problem, we propose an occluded facial expression recognition method through self-supervised learning, which leverages the profusion of available unlabeled facial images to explore robust facial representations. Specifically, we generate a variety of occluded facial images by randomly adding occlusions to unlabeled facial images. Then we define occlusion prediction as the pretext task for representation learning. We also adopt contrastive learning to make facial representation of a facial image and those of its variations with synthesized occlusions close. \nFinally, we train an expression classifier as the downstream task. The experimental results on several databases containing both synthesized and realistic occluded facial images demonstrate the superiority of the proposed method over state-of-the-art methods",
    "volume": "main",
    "checked": false,
    "id": "1631b05b828c30a0a10d0eee9640ee5ecad30b78",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Endo_Re-parameterization_Making_GC-Net-style_3DConvNets_More_Efficient_ACCV_2022_paper.html": {
    "title": "Re-parameterization Making GC-Net-style 3DConvNets More Efficient",
    "abstract": "For depth estimation using a stereo pair, deep learning methods using 3D convolution have been proposed. While the estimation accuracy is high, 3D convolutions on cost volumes are computationally expensive. Hence, we propose a method to reduce the computational cost of 3D convolution-based disparity networks. We apply kernel re-parameterization, which is used for constructing efficient backbones, to disparity estimation. We convert learned parameters, and these values are used for inference to reduce the computational cost of filtering cost volumes. Experimental results on the KITTI 2015 dataset show that our method can reduce the computational cost by 31-61% from those of trained models without any performance loss. Our method can be used for any disparity network that uses 3D convolution for cost volume filtering",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Xu_DHG-GAN_Diverse_Image_Outpainting_via_Decoupled_High_Frequency_Semantics_ACCV_2022_paper.html": {
    "title": "DHG-GAN: Diverse Image Outpainting via Decoupled High Frequency Semantics",
    "abstract": "Diverse image outpainting aims to restore large missing regions surrounding a known region while generating multiple plausible results. Although existing outpainting methods have demonstrated promising quality of image reconstruction, they are ineffective for providing both diverse and realistic content. This paper proposes a Decoupled High-frequency semantic Guidance-based GAN (DHG-GAN) for diverse image outpainting with the following contributions. 1) We propose a two-stage method, in which the first stage generates high-frequency semantic images for guidance of structural and textural information in the outpainting region and the second stage is a semantic completion network for completing the image outpainting based on this semantic guidance. 2) We design spatially varying stylemaps to enable targeted editing of high-frequency semantics in the outpainting region to generate diverse and realistic results. We evaluate the photorealism and quality of the diverse results generated by our model on CelebA-HQ, Place2 and Oxford Flower102 datasets. The experimental results demonstrate large improvement over state-of-the-art approaches",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Generalized_Person_Re-identification_by_Locating_and_Eliminating_Domain-Sensitive_Features_ACCV_2022_paper.html": {
    "title": "Generalized Person Re-identification by Locating and Eliminating Domain-Sensitive Features",
    "abstract": "In this paper, we study the problem of domain generalization for person re-identification (re-ID), which adopts training data from multiple domains to learn a re-ID model that can be directly deployed to unseen target domains without further fine-tuning. One promising idea is removing the subsets of features that are not beneficial to the generalization of models. This can be achieved by muting the subset features that correspond to high back-propagated gradients as these subsets are easy for the model to overfit. But this method ignores the interaction of multiple domains. Therefore, we propose a novel method to solve this problem by comparing the gradients from two different training schemes. One of the training schemes discriminates input data from their corresponding domain to obtain back-propagated temporary gradients in the intermediate features. At the same time, another scheme discriminates input data from all domains to obtain the temporary gradients. By comparing the temporary gradient between the two schemes, we can identify the domain-generalizable subset features from those domain-specific subset features. We thus mute them in the subsequent training process to enforce the model to learn domain-generalizable information and improve its generalization. Extensive experiments on four large-scale re-ID benchmarks have verified the effectiveness of our method. Code is available at https://github.com/Ssd111/LEDF.git",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Phan_Patch_Embedding_as_Local_Features_Unifying_Deep_Local_and_Global_ACCV_2022_paper.html": {
    "title": "Patch Embedding as Local Features: Unifying Deep Local and Global Features Via Vision Transformer for Image Retrieval",
    "abstract": "Image retrieval is the task of finding all images in the database\nthat are similar to a query image. Two types of image representations\nhave been studied to address this task: global and local image features.\nThose features can be extracted separately or jointly in a single model.\nState-of-the-art methods usually learn them with Convolutional Neural Networks (CNNs) and perform retrieval with multi-scale image representation. This paper's main contribution is to unify global and local features with Vision Transformers (ViTs) and multi-atrous convolutions for high-performing retrieval. We refer to the new model as ViTGaL, standing for Vision Transformer based Global and Local features (ViTGaL). Specifically, we add a multi-atrous convolution to the output of the transformer encoder layer of ViTs to simulate the image pyramid used in standard image retrieval algorithms. We use class attention to aggregate the token embeddings output from the multi-atrous layer to get both global and local features. The entire network can be learned end-to-end, requiring only image-level labels. Extensive experiments show the proposed method outperforms the state-of-the-art methods on the Revisited Oxford and Paris datasets",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Liu_Multi-Scale_Wavelet_Transformer_for_Face_Forgery_Detection_ACCV_2022_paper.html": {
    "title": "Multi-Scale Wavelet Transformer for Face Forgery Detection",
    "abstract": "Currently, many face forgery detection methods aggregate spatial and frequency features to enhance the generalization ability and gain promising performance under the cross-dataset scenario. However, these methods only leverage one level frequency information which limits their expressive ability. To overcome these limitations, we propose a multi-scale wavelet transformer framework for face forgery detection. Specifically, to take full advantage of the multi-scale and multi-frequency wavelet representation, we gradually aggregate the multi-scale wavelet representation at different stages of the backbone network. To better fuse the frequency feature with the spatial features, frequency-based spatial attention is designed to guide the spatial feature extractor to concentrate more on forgery traces. Meanwhile, cross-modality attention is proposed to fuse the frequency features with the spatial features. These two attention modules are calculated through a unified transformer block for efficiency. A wide variety of experiments demonstrate that the proposed method is efficient and effective for both within and cross datasets",
    "volume": "main",
    "checked": true,
    "id": "a832ff78c1fb755e2b71a7fa7f0547e0fccd7942",
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lin_Full-scale_Selective_Transformer_for_Semantic_Segmentation_ACCV_2022_paper.html": {
    "title": "Full-scale Selective Transformer for Semantic Segmentation",
    "abstract": "In this paper, we rethink the multi-scale feature fusion from two perspectives (scale-level and spatial-level) and propose a full-scale selective fusion strategy for semantic segmentation. Based on such strategy, we design a novel segmentation network, named Full-scale Selective Transformer (FSFormer). Specifically, our FSFormer adaptively selects partial tokens from all tokens at all scales to construct a token subset of interest for each scale. Therefore, each token only interacts with the tokens within its corresponding token subset of interest. The proposed full-scale selective fusion strategy can not only filter out the noisy information propagation but also reduce the computational costs to some extent. We evaluate our FSFormer on four challenging semantic segmentation benchmarks, including PASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the state-of-the-art methods",
    "volume": "main",
    "checked": false,
    "id": "63049ba1767693afe20c474ede9f891b376c630c",
    "citation_count": 1
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Ren_Looking_from_a_Higher-level_Perspective_Attention_and_Recognition_Enhanced_Multi-scale_ACCV_2022_paper.html": {
    "title": "Looking from a Higher-level Perspective: Attention and Recognition Enhanced Multi-scale Scene Text Segmentation",
    "abstract": "Scene text segmentation, which aims to generate pixel-level text masks, is an integral part of many fine-grained text tasks, such as text editing and text removal. Multi-scale irregular scene texts are often trapped in complex background noise around the image, and their textures are diverse and sometimes even similar to those of the background. These specific problems bring challenges that make general segmentation methods ineffective in the context of scene text. To tackle the aforementioned issues, we propose a new scene text segmentation pipeline called Attention and Recognition enhanced Multi-scale segmentation Network (ARM-Net), which consists of three main components: Text Segmentation Module (TSM) generates rectangular receptive fields of various sizes to fit scene text and integrate global information adequately; Dual Perceptual Decoder (DPD) strengthens the connection between pixels that belong to the same category from the spatial and channel perspective simultaneously during upsampling, and Recognition Enhanced Module (REM) provides text attention maps as a prior for the segmentation network, which can inherently distinguish text from background noise. Via extensive experiments, we demonstrate the effectiveness of each module of ARM-Net, and its performance surpasses that of existing state-of-the-art scene text segmentation methods. We also show that the pixel-level mask produced by our method can further improve the performance of text removal and scene text recognition",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Lee_Exp-GAN_3D-Aware_Facial_Image_Generation_with_Expression_Control_ACCV_2022_paper.html": {
    "title": "Exp-GAN: 3D-Aware Facial Image Generation with Expression Control",
    "abstract": "This paper introduces Exp-GAN, a 3D-aware facial image generator with explicit control of facial expressions. Unlike previous 3D-aware GANs, Exp-GAN supports fine-grained control over facial shapes and expressions disentangled from poses. To this ends, we propose a novel hybrid approach that adopts a 3D morphable model (3DMM) with neural textures for the facial region and a neural radiance field (NeRF) for non-facial regions with multi-view consistency. The 3DMM allows fine-grained control over facial expressions, whereas the NeRF contains volumetric features for the non-facial regions. The two features, generated separately, are combined seamlessly with our depth-based integration method that integrates the two complementary features through volume rendering. We also propose a training scheme that encourages generated images to reflect control over shapes and expressions faithfully. Experimental results show that the proposed approach successfully synthesizes realistic view consistent face images with fine-grained controls. Code is available at https://github.com/kakaobrain/expgan",
    "volume": "main",
    "checked": false,
    "id": "b5e21adc220a49cc5b0d1f5283ff458ce6faea5d",
    "citation_count": 2
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Zhang_CMT-Co_Contrastive_Learning_with_Character_Movement_Task_for_Handwritten_Text_ACCV_2022_paper.html": {
    "title": "CMT-Co: Contrastive Learning with Character Movement Task for Handwritten Text Recognition",
    "abstract": "Mainstream handwritten text recognition (HTR) approaches require large-scale labeled data for training to achieve satisfactory performance.\nRecently, contrastive learning has been introduced to perform self-supervised training on unlabeled data to improve representational capacity. It minimizes the distance between the positive pairs while maximizing their distance to the negative ones. Previous studies typically consider each frame or a fixed window of frames in a sequential feature map as a separate instance for contrastive learning. However, owing to the arbitrariness of handwriting and the diversity of word length, such modeling may contain the information of multiple consecutive characters or an over-segmented sub-character, which may confuse the model to perceive semantic clues information. To address this issue, in this paper, we design a character-level pretext task termed Character Movement Task, to assist word-level contrastive learning, namely CMT-Co. It moves the characters in a word to generate artifacts and guides the model to perceive the text content by using the moving direction and distance as supervision. In addition, we customize a data augmentation strategy specifically for handwritten text, which significantly contributes to the construction of training pairs for contrastive learning. Experiments have shown that the proposed CMT-Co achieves competitive or even superior performance compared to previous methods on public handwritten benchmarks",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://openaccess.thecvf.com/content/ACCV2022/html/Hu_Learning_to_Predict_Decomposed_Dynamic_Filters_for_Single_Image_Motion_ACCV_2022_paper.html": {
    "title": "Learning to Predict Decomposed Dynamic Filters for Single Image Motion Deblurring",
    "abstract": "This paper tackles the large motion variation problem in the single image dynamic scene deblurring task. Although fully convolutional multi-scale-based designs have recently advanced the state-of-the-art in single image motion deblurring. However, these approaches usually utilize vanilla convolution filters, which are not adapted to each spatial position. Consequently, it is hard to handle large motion blur variations at the pixel level. In this work, we propose Decomposed Dynamic Filters (DDF), a highly effective plug-and-play adaptive operator, to fulfill the goal of handling large motion blur variations across different spatial locations. In contrast to conventional dynamic convolution-based methods, which only predict either weight or offsets of the filter from the local feature at run time, in our work, both the offsets and weight are adaptively predicted from multi-scale local regions. The proposed operator comprises two components: 1) the offsets estimation module and 2) the pixel-specific filter weight generator. We incorporate the DDF into a lightweight encoder-decoder-based deblurring architecture to verify the performance gain. Extensive experiments conducted on the GoPro, HIDE, Real Blur, SIDD, and DND datasets demonstrate that the proposed method offers significant improvements over the state-of-the-art in accuracy as well as generalization capability. Code is available at: https://github.com/ZHIQIANGHU2021/DecomposedDynamicFilters",
    "volume": "main",
    "checked": false,
    "id": "71104a613283ae095345cdb4becc18303fcecff3",
    "citation_count": 22
  }
}