{
  "https://aclanthology.org/2021.emnlp-main.1": {
    "title": "AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate",
    "volume": "main",
    "abstract": "Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En↔De and WMT16 Ro→En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 En↔De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation",
    "checked": true,
    "id": "a8f9c041691db73eaa17ecc3fe93c4aac4e352b6",
    "semantic_title": "alignart: non-autoregressive neural machine translation by jointly learning to estimate alignment and translate",
    "citation_count": 40,
    "authors": [
      "Jongyoon Song",
      "Sungwon Kim",
      "Sungroh Yoon"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.2": {
    "title": "Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders",
    "volume": "main",
    "abstract": "Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an off-the-shelf MPE, then it is directly tested on zero-shot language pairs. We propose SixT, a simple yet effective model for this task. SixT leverages the MPE with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. Using this method, SixT significantly outperforms mBART, a pretrained multilingual encoder-decoder model explicitly designed for NMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test sets across 14 source languages. Furthermore, with much less training computation cost and training data, our model achieves better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines",
    "checked": true,
    "id": "4936a03cc72f0e712c8a16560e01889f4c1a1c5b",
    "semantic_title": "zero-shot cross-lingual transfer of neural machine translation with multilingual pretrained encoders",
    "citation_count": 41,
    "authors": [
      "Guanhua Chen",
      "Shuming Ma",
      "Yun Chen",
      "Li Dong",
      "Dongdong Zhang",
      "Jia Pan",
      "Wenping Wang",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.3": {
    "title": "ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora",
    "volume": "main",
    "abstract": "Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for low-resource languages. In this paper, we propose Ernie-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that Ernie-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks. The codes and pre-trained models will be made publicly available",
    "checked": true,
    "id": "66d8573a7b63055c58c3a8321062d87078fb5fce",
    "semantic_title": "ernie-m: enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
    "citation_count": 104,
    "authors": [
      "Xuan Ouyang",
      "Shuohuan Wang",
      "Chao Pang",
      "Yu Sun",
      "Hao Tian",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.4": {
    "title": "Cross Attention Augmented Transducer Networks for Simultaneous Translation",
    "volume": "main",
    "abstract": "This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. We implement CAAT with Transformer while the general CAAT architecture can also be implemented with other attention-based encoder-decoder frameworks. Experiments on both speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks show that CAAT achieves significantly better latency-quality trade-offs compared to the state-of-the-art simultaneous translation approaches",
    "checked": true,
    "id": "df4f0c97d4ee2e28bb8a88fcc9a1c865a861fc69",
    "semantic_title": "cross attention augmented transducer networks for simultaneous translation",
    "citation_count": 57,
    "authors": [
      "Dan Liu",
      "Mengge Du",
      "Xiaoxi Li",
      "Ya Li",
      "Enhong Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.5": {
    "title": "Translating Headers of Tabular Data: A Pilot Study of Schema Translation",
    "volume": "main",
    "abstract": "Schema translation is the task of automatically translating headers of tabular data from one language to another. High-quality schema translation plays an important role in cross-lingual table searching, understanding and analysis. Despite its importance, schema translation is not well studied in the community, and state-of-the-art neural machine translation models cannot work well on this task because of two intrinsic differences between plain text and tabular data: morphological difference and context difference. To facilitate the research study, we construct the first parallel dataset for schema translation, which consists of 3,158 tables with 11,979 headers written in 6 different languages, including English, Chinese, French, German, Spanish, and Japanese. Also, we propose the first schema translation model called CAST, which is a header-to-header neural machine translation model augmented with schema context. Specifically, we model a target header and its context as a directed graph to represent their entity types and relations. Then CAST encodes the graph with a relational-aware transformer and uses another transformer to decode the header in the target language. Experiments on our dataset demonstrate that CAST significantly outperforms state-of-the-art neural machine translation models. Our dataset will be released at https://github.com/microsoft/ContextualSP",
    "checked": true,
    "id": "cc4ce024b58e9ce81eb9291262898e1f7019dc35",
    "semantic_title": "translating headers of tabular data: a pilot study of schema translation",
    "citation_count": 2,
    "authors": [
      "Kunrui Zhu",
      "Yan Gao",
      "Jiaqi Guo",
      "Jian-Guang Lou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.6": {
    "title": "Towards Making the Most of Dialogue Characteristics for Neural Chat Translation",
    "volume": "main",
    "abstract": "Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the NCT model. To this end, we design four auxiliary tasks including monolingual response generation, cross-lingual response generation, next utterance discrimination, and speaker identification. Together with the main chat translation task, we optimize the enhanced NCT model through the training objectives of all these tasks. By this means, the NCT model can be enhanced by capturing the inherent dialogue characteristics, thus generating more coherent and speaker-relevant translations. Comprehensive experiments on four language directions (English<->German and English<->Chinese) verify the effectiveness and superiority of the proposed approach",
    "checked": true,
    "id": "00d4c0cdb3052ae7c788535b2c3b7b25238ad31d",
    "semantic_title": "towards making the most of dialogue characteristics for neural chat translation",
    "citation_count": 19,
    "authors": [
      "Yunlong Liang",
      "Chulun Zhou",
      "Fandong Meng",
      "Jinan Xu",
      "Yufeng Chen",
      "Jinsong Su",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.7": {
    "title": "Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining",
    "volume": "main",
    "abstract": "With the rapid increase in the volume of dialogue data from daily life, there is a growing demand for dialogue summarization. Unfortunately, training a large summarization model is generally infeasible due to the inadequacy of dialogue data with annotated summaries. Most existing works for low-resource dialogue summarization directly pretrain models in other domains, e.g., the news domain, but they generally neglect the huge difference between dialogues and conventional articles. To bridge the gap between out-of-domain pretraining and in-domain fine-tuning, in this work, we propose a multi-source pretraining paradigm to better leverage the external summary data. Specifically, we exploit large-scale in-domain non-summary data to separately pretrain the dialogue encoder and the summary decoder. The combined encoder-decoder model is then pretrained on the out-of-domain summary data using adversarial critics, aiming to facilitate domain-agnostic summarization. The experimental results on two public datasets show that with only limited training data, our approach achieves competitive performance and generalizes well in different dialogue scenarios",
    "checked": true,
    "id": "ed98ae2c7aec315488372476e964da3e3846360d",
    "semantic_title": "low-resource dialogue summarization with domain-agnostic multi-source pretraining",
    "citation_count": 32,
    "authors": [
      "Yicheng Zou",
      "Bolin Zhu",
      "Xingwu Hu",
      "Tao Gui",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.8": {
    "title": "Controllable Neural Dialogue Summarization with Personal Named Entity Planning",
    "volume": "main",
    "abstract": "In this paper, we propose a controllable neural generation framework that can flexibly guide dialogue summarization with personal named entity planning. The conditional sequences are modulated to decide what types of information or what perspective to focus on when forming summaries to tackle the under-constrained problem in summarization tasks. This framework supports two types of use cases: (1) Comprehensive Perspective, which is a general-purpose case with no user-preference specified, considering summary points from all conversational interlocutors and all mentioned persons; (2) Focus Perspective, positioning the summary based on a user-specified personal named entity, which could be one of the interlocutors or one of the persons mentioned in the conversation. During training, we exploit occurrence planning of personal named entities and coreference information to improve temporal coherence and to minimize hallucination in neural generation. Experimental results show that our proposed framework generates fluent and factually consistent summaries under various planning controls using both objective metrics and human evaluations",
    "checked": true,
    "id": "e09fbb5175044d8b481f7b512607653af857a0a2",
    "semantic_title": "controllable neural dialogue summarization with personal named entity planning",
    "citation_count": 63,
    "authors": [
      "Zhengyuan Liu",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.9": {
    "title": "Fine-grained Factual Consistency Assessment for Abstractive Summarization Models",
    "volume": "main",
    "abstract": "Factual inconsistencies existed in the output of abstractive summarization models with original documents are frequently presented. Fact consistency assessment requires the reasoning capability to find subtle clues to identify whether a model-generated summary is consistent with the original document. This paper proposes a fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC). Given a document and a summary sentence, in the first stage, SumFC selects the top-K most relevant sentences with the summary sentence from the document. In the second stage, the model performs fine-grained consistency reasoning at the sentence level, and then aggregates all sentences' consistency scores to obtain the final assessment result. We get the training data pairs by data synthesis and adopt contrastive loss of data pairs to help the model identify subtle cues. Experiment results show that SumFC has made a significant improvement over the previous state-of-the-art methods. Our experiments also indicate that SumFC distinguishes detailed differences better",
    "checked": true,
    "id": "f5d581e916613838cbadc05ab8c35ee4ea78da32",
    "semantic_title": "fine-grained factual consistency assessment for abstractive summarization models",
    "citation_count": 10,
    "authors": [
      "Sen Zhang",
      "Jianwei Niu",
      "Chuyuan Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.10": {
    "title": "Decision-Focused Summarization",
    "volume": "main",
    "abstract": "Relevance in summarization is typically de- fined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We propose a novel problem, decision-focused summarization, where the goal is to summarize relevant information for a decision. We leverage a predictive model that makes the decision based on the full text to provide valuable insights on how a decision can be inferred from text. To build a summary, we then select representative sentences that lead to similar model decisions as using the full text while accounting for textual non-redundancy. To evaluate our method (DecSum), we build a testbed where the task is to summarize the first ten reviews of a restaurant in support of predicting its future rating on Yelp. DecSum substantially outperforms text-only summarization methods and model-based explanation methods in decision faithfulness and representativeness. We further demonstrate that DecSum is the only method that enables humans to outperform random chance in predicting which restaurant will be better rated in the future",
    "checked": true,
    "id": "0fe9c9fcc19c7b682d88266e9cf9d6f97464a3d4",
    "semantic_title": "decision-focused summarization",
    "citation_count": 17,
    "authors": [
      "Chao-Chun Hsu",
      "Chenhao Tan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.11": {
    "title": "Multiplex Graph Neural Network for Extractive Text Summarization",
    "volume": "main",
    "abstract": "Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method",
    "checked": true,
    "id": "84a9b5af6b5b3c259dffa4ebb7c13e27e342a63b",
    "semantic_title": "multiplex graph neural network for extractive text summarization",
    "citation_count": 43,
    "authors": [
      "Baoyu Jing",
      "Zeyu You",
      "Tao Yang",
      "Wei Fan",
      "Hanghang Tong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.12": {
    "title": "A Thorough Evaluation of Task-Specific Pretraining for Summarization",
    "volume": "main",
    "abstract": "Task-agnostic pretraining objectives like masked language models or corrupted span prediction are applicable to a wide range of NLP downstream tasks (Raffel et al.,2019), but are outperformed by task-specific pretraining objectives like predicting extracted gap sentences on summarization (Zhang et al.,2020). We compare three summarization specific pretraining objectives with the task agnostic corrupted span prediction pretraining in controlled study. We also extend our study to a low resource and zero shot setup, to understand how many training examples are needed in order to ablate the task-specific pretraining without quality loss. Our results show that task-agnostic pretraining is sufficient for most cases which hopefully reduces the need for costly task-specific pretraining. We also report new state-of-the-art number for two summarization task using a T5 model with 11 billion parameters and an optimal beam search length penalty",
    "checked": true,
    "id": "7e4d5dad222877e0b526de63b37e548cd4b2c0ae",
    "semantic_title": "a thorough evaluation of task-specific pretraining for summarization",
    "citation_count": 30,
    "authors": [
      "Sascha Rothe",
      "Joshua Maynez",
      "Shashi Narayan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.13": {
    "title": "HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization",
    "volume": "main",
    "abstract": "To capture the semantic graph structure from raw text, most existing summarization approaches are built on GNNs with a pre-trained model. However, these methods suffer from cumbersome procedures and inefficient computations for long-text documents. To mitigate these issues, this paper proposes HetFormer, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization. Specifically, we model different types of semantic nodes in raw text as a potential heterogeneous graph and directly learn heterogeneous relationships (edges) among nodes by Transformer. Extensive experiments on both single- and multi-document summarization tasks show that HetFormer achieves state-of-the-art performance in Rouge F1 while using less memory and fewer parameters",
    "checked": true,
    "id": "34042e2680e475510a1030b54165a81534ad88d3",
    "semantic_title": "hetformer: heterogeneous transformer with sparse attention for long-text extractive summarization",
    "citation_count": 29,
    "authors": [
      "Ye Liu",
      "Jianguo Zhang",
      "Yao Wan",
      "Congying Xia",
      "Lifang He",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.14": {
    "title": "Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context",
    "volume": "main",
    "abstract": "Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space as transitional embedding based models do. In terms of the local view, we first build a graph structure based on the document where phrases are regarded as vertices and the edges are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the graph structure. Finally, we further combine the modeling of global and local context for ranking. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010) and compare with existing state-of-the-art models. The results show that our model outperforms most models while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks",
    "checked": true,
    "id": "f9cb7e5fda46c03a1c16eb6e0890d2e408ef8225",
    "semantic_title": "unsupervised keyphrase extraction by jointly modeling local and global context",
    "citation_count": 62,
    "authors": [
      "Xinnian Liang",
      "Shuangzhi Wu",
      "Mu Li",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.15": {
    "title": "Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning",
    "volume": "main",
    "abstract": "Distantly supervised relation extraction is widely used in the construction of knowledge bases due to its high efficiency. However, the automatically obtained instances are of low quality with numerous irrelevant words. In addition, the strong assumption of distant supervision leads to the existence of noisy sentences in the sentence bags. In this paper, we propose a novel Multi-Layer Revision Network (MLRN) which alleviates the effects of word-level noise by emphasizing inner-sentence correlations before extracting relevant information within sentences. Then, we devise a balanced and noise-resistant Confidence-based Multi-Instance Learning (CMIL) method to filter out noisy sentences as well as assign proper weights to relevant ones. Extensive experiments on two New York Times (NYT) datasets demonstrate that our approach achieves significant improvements over the baselines",
    "checked": true,
    "id": "215fa5aecdfd5494d526aaea8dadfcfc6d9b29fd",
    "semantic_title": "distantly supervised relation extraction using multi-layer revision network and confidence-based multi-instance learning",
    "citation_count": 8,
    "authors": [
      "Xiangyu Lin",
      "Tianyi Liu",
      "Weijia Jia",
      "Zhiguo Gong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.16": {
    "title": "Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification",
    "volume": "main",
    "abstract": "Table-based fact verification task aims to verify whether the given statement is supported by the given semi-structured table. Symbolic reasoning with logical operations plays a crucial role in this task. Existing methods leverage programs that contain rich logical information to enhance the verification process. However, due to the lack of fully supervised signals in the program generation process, spurious programs can be derived and employed, which leads to the inability of the model to catch helpful logical operations. To address the aforementioned problems, in this work, we formulate the table-based fact verification task as an evidence retrieval and reasoning framework, proposing the Logic-level Evidence Retrieval and Graph-based Verification network (LERGV). Specifically, we first retrieve logic-level program-like evidence from the given table and statement as supplementary evidence for the table. After that, we construct a logic-level graph to capture the logical relations between entities and functions in the retrieved evidence, and design a graph-based verification network to perform logic-level graph-based reasoning based on the constructed graph to classify the final entailment relation. Experimental results on the large-scale benchmark TABFACT show the effectiveness of the proposed approach",
    "checked": true,
    "id": "318a79780d6ba921a94d4b6099de57854459acc6",
    "semantic_title": "logic-level evidence retrieval and graph-based verification network for table-based fact verification",
    "citation_count": 20,
    "authors": [
      "Qi Shi",
      "Yu Zhang",
      "Qingyu Yin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.17": {
    "title": "A Partition Filter Network for Joint Entity and Relation Extraction",
    "volume": "main",
    "abstract": "In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where feature encoding is decomposed into two steps: partition and filter. In our encoder, we leverage two gates: entity and relation gate, to segment neurons into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of task-specific features is dependent upon each other. Experiment results on six public datasets show that our model performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/Coopercoppers/PFN",
    "checked": true,
    "id": "917f4a2da2e5f7b45c1f8170421d36770360b569",
    "semantic_title": "a partition filter network for joint entity and relation extraction",
    "citation_count": 143,
    "authors": [
      "Zhiheng Yan",
      "Chong Zhang",
      "Jinlan Fu",
      "Qi Zhang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.18": {
    "title": "TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network",
    "volume": "main",
    "abstract": "To alleviate label scarcity in Named Entity Recognition (NER) task, distantly supervised NER methods are widely applied to automatically label data and identify entities. Although the human effort is reduced, the generated incomplete and noisy annotations pose new challenges for learning effective neural models. In this paper, we propose a novel dictionary extension method which extracts new entities through the type expanded model. Moreover, we design a multi-granularity boundary-aware network which detects entity boundaries from both local and global perspectives. We conduct experiments on different types of datasets, the results show that our model outperforms previous state-of-the-art distantly supervised systems and even surpasses the supervised models",
    "checked": true,
    "id": "7b211c86a44aaac59ccecee59c080ffc9079dbb3",
    "semantic_title": "tebner: domain specific named entity recognition with type expanded boundary-aware network",
    "citation_count": 16,
    "authors": [
      "Zheng Fang",
      "Yanan Cao",
      "Tai Li",
      "Ruipeng Jia",
      "Fang Fang",
      "Yanmin Shang",
      "Yuhai Lu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.19": {
    "title": "Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge",
    "volume": "main",
    "abstract": "In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the aspects in the context and determine their importance based on the public knowledge base. In this way, the contextual sentiment clues can be explicitly tracked in ACSA for the aspects in the light of these aspect-related words. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the aspect from external affective commonsense knowledge. Then, we employ Beta Distribution to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct graphs for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods",
    "checked": true,
    "id": "cf26887f44445feff85e003ec8f9303074bb3cb1",
    "semantic_title": "beta distribution guided aspect-aware graph for aspect category sentiment analysis with affective knowledge",
    "citation_count": 23,
    "authors": [
      "Bin Liang",
      "Hang Su",
      "Rongdi Yin",
      "Lin Gui",
      "Min Yang",
      "Qin Zhao",
      "Xiaoqi Yu",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.20": {
    "title": "DILBERT: Customized Pre-Training for Domain Adaptation with Category Shift, with an Application to Aspect Extraction",
    "volume": "main",
    "abstract": "The rise of pre-trained language models has yielded substantial progress in the vast majority of Natural Language Processing (NLP) tasks. However, a generic approach towards the pre-training procedure can naturally be sub-optimal in some cases. Particularly, fine-tuning a pre-trained language model on a source domain and then applying it to a different target domain, results in a sharp performance decline of the eventual classifier for many source-target domain pairs. Moreover, in some NLP tasks, the output categories substantially differ between domains, making adaptation even more challenging. This, for example, happens in the task of aspect extraction, where the aspects of interest of reviews of, e.g., restaurants or electronic devices may be very different. This paper presents a new fine-tuning scheme for BERT, which aims to address the above challenges. We name this scheme DILBERT: Domain Invariant Learning with BERT, and customize it for aspect extraction in the unsupervised domain adaptation setting. DILBERT harnesses the categorical information of both the source and the target domains to guide the pre-training process towards a more domain and category invariant representation, thus closing the gap between the domains. We show that DILBERT yields substantial improvements over state-of-the-art baselines while using a fraction of the unlabeled data, particularly in more challenging domain adaptation setups",
    "checked": true,
    "id": "10b0ef66f8010fef53c3aa51c43b73e121ba7e6d",
    "semantic_title": "dilbert: customized pre-training for domain adaptation with category shift, with an application to aspect extraction",
    "citation_count": 30,
    "authors": [
      "Entony Lekhtman",
      "Yftah Ziser",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.21": {
    "title": "Improving Multimodal fusion via Mutual Dependency Maximisation",
    "volume": "main",
    "abstract": "Multimodal sentiment analysis is a trending area of research, and multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different unimodal representations into a synthetic one. So far, a consequent effort has been made on developing complex architectures allowing the fusion of these modalities. However, such systems are mainly trained by minimising simple losses such as L1 or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model",
    "checked": true,
    "id": "f3d86c41d4e423adf6ecc7e6699fdaa707164797",
    "semantic_title": "improving multimodal fusion via mutual dependency maximisation",
    "citation_count": 31,
    "authors": [
      "Pierre Colombo",
      "Emile Chapuis",
      "Matthieu Labeau",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.22": {
    "title": "Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training",
    "volume": "main",
    "abstract": "Aspect-based sentiment analysis aims to identify the sentiment polarity of a specific aspect in product reviews. We notice that about 30% of reviews do not contain obvious opinion words, but still convey clear human-aware sentiment orientation, which is known as implicit sentiment. However, recent neural network-based approaches paid little attention to implicit sentiment entailed in the reviews. To overcome this issue, we adopt Supervised Contrastive Pre-training on large-scale sentiment-annotated corpora retrieved from in-domain language resources. By aligning the representation of implicit sentiment expressions to those with the same sentiment label, the pre-training process leads to better capture of both implicit and explicit sentiment orientation towards aspects in reviews. Experimental results show that our method achieves state-of-the-art performance on SemEval2014 benchmarks, and comprehensive analysis validates its effectiveness on learning implicit sentiment",
    "checked": true,
    "id": "99d5d4148550beed9269f164037488eeee3c4c69",
    "semantic_title": "learning implicit sentiment in aspect-based sentiment analysis with supervised contrastive pre-training",
    "citation_count": 119,
    "authors": [
      "Zhengyan Li",
      "Yicheng Zou",
      "Chong Zhang",
      "Qi Zhang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.23": {
    "title": "Progressive Self-Training with Discriminator for Aspect Term Extraction",
    "volume": "main",
    "abstract": "Aspect term extraction aims to extract aspect terms from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient annotated data. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce noise. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we refine the conventional self-training to progressive self-training. Specifically, the base model infers pseudo-labels on a progressive subset at each iteration, where samples in the subset become harder and more numerous as the iteration proceeds. The other is that we use a discriminator to filter the noisy pseudo-labels. Experimental results on four SemEval datasets show that our model significantly outperforms the previous baselines and achieves state-of-the-art performance",
    "checked": true,
    "id": "b8e175bb94edbb2346fb14aefcc4641f6f09168d",
    "semantic_title": "progressive self-training with discriminator for aspect term extraction",
    "citation_count": 39,
    "authors": [
      "Qianlong Wang",
      "Zhiyuan Wen",
      "Qin Zhao",
      "Min Yang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.24": {
    "title": "Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification",
    "volume": "main",
    "abstract": "Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to word embeddings, which cannot address the spurious association problem. In this work, we propose an end-to-end reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification. Our approach has three characteristics:1) the generator automatically generates massive and diverse antonymous sentences; 2) the discriminator contains a original-side sentiment predictor and an antonymous-side sentiment predictor, which jointly evaluate the quality of the generated sample and help the generator iteratively generate higher-quality antonymous samples; 3) the discriminator is directly used as the final sentiment classifier without the need to build an extra one. Extensive experiments show that our approach outperforms strong data augmentation baselines on several benchmark sentiment classification datasets. Further analysis confirms our approach's advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification",
    "checked": true,
    "id": "01aa29292d3da33163c56e3c3c23ac933680b95b",
    "semantic_title": "reinforced counterfactual data augmentation for dual sentiment classification",
    "citation_count": 25,
    "authors": [
      "Hao Chen",
      "Rui Xia",
      "Jianfei Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.25": {
    "title": "Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles",
    "volume": "main",
    "abstract": "An individual's variation in writing style is often a function of both social and personal attributes. While structured social variation has been extensively studied, e.g., gender based variation, far less is known about how to characterize individual styles due to their idiosyncratic nature. We introduce a new approach to studying idiolects through a massive cross-author comparison to identify and encode stylistic features. The neural model achieves strong performance at authorship identification on short texts and through an analogy-based probing task, showing that the learned representations exhibit surprising regularities that encode qualitative and quantitative shifts of idiolectal styles. Through text perturbation, we quantify the relative contributions of different linguistic elements to idiolectal variation. Furthermore, we provide a description of idiolects through measuring inter- and intra-author variation, showing that variation in idiolects is often distinctive yet consistent",
    "checked": true,
    "id": "afb717aed8de457ef3c9a994b954e44c110a15ab",
    "semantic_title": "idiosyncratic but not arbitrary: learning idiolects in online registers reveals distinctive yet consistent individual styles",
    "citation_count": 24,
    "authors": [
      "Jian Zhu",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.26": {
    "title": "Narrative Theory for Computational Narrative Understanding",
    "volume": "main",
    "abstract": "Over the past decade, the field of natural language processing has developed a wide array of computational methods for reasoning about narrative, including summarization, commonsense inference, and event detection. While this work has brought an important empirical lens for examining narrative, it is by and large divorced from the large body of theoretical work on narrative within the humanities, social and cognitive sciences. In this position paper, we introduce the dominant theoretical frameworks to the NLP community, situate current research in NLP within distinct narratological traditions, and argue that linking computational work in NLP to theory opens up a range of new empirical questions that would both help advance our understanding of narrative and open up new practical applications",
    "checked": true,
    "id": "44d2dc8e5d821c60c0adf531a55678ddf4658fcc",
    "semantic_title": "narrative theory for computational narrative understanding",
    "citation_count": 99,
    "authors": [
      "Andrew Piper",
      "Richard Jean So",
      "David Bamman"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.27": {
    "title": "(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys",
    "volume": "main",
    "abstract": "Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture \"stance\" as measured by public opinion polls. We demonstrate this by directly comparing an individual's self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that recall is high for both \"Pro'' and \"Anti'' stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance: temporal inconsistencies, differences in constructs, and measurement errors from both survey respondents and annotators. By presenting a framework for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures",
    "checked": true,
    "id": "a7d4da5e331c80f34830804e20d619f0ceef48b8",
    "semantic_title": "(mis)alignment between stance expressed in social media data and public opinion surveys",
    "citation_count": 29,
    "authors": [
      "Kenneth Joseph",
      "Sarah Shugars",
      "Ryan Gallagher",
      "Jon Green",
      "Alexi Quintana Mathé",
      "Zijian An",
      "David Lazer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.28": {
    "title": "How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?",
    "volume": "main",
    "abstract": "As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn to distinguish between core features and data artifacts. While models trained on this type of data have shown promising out-of-domain generalizability, it is still unclear what the sources of such improvements are. We investigate the benefits of CAD for social NLP models by focusing on three social computing constructs — sentiment, sexism, and hate speech. Assessing the performance of models trained with and without CAD across different types of datasets, we find that while models trained on CAD show lower in-domain performance, they generalize better out-of-domain. We unpack this apparent discrepancy using machine explanations and find that CAD reduces model reliance on spurious features. Leveraging a novel typology of CAD to analyze their relationship with model performance, we find that CAD which acts on the construct directly or a diverse set of CAD leads to higher performance",
    "checked": true,
    "id": "88ee971d2b00ba0b374aab81daac42e7cc7b8d4d",
    "semantic_title": "how does counterfactually augmented data impact models for social computing constructs?",
    "citation_count": 27,
    "authors": [
      "Indira Sen",
      "Mattia Samory",
      "Fabian Flöck",
      "Claudia Wagner",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.29": {
    "title": "Latent Hatred: A Benchmark for Understanding Implicit Hate Speech",
    "volume": "main",
    "abstract": "Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue",
    "checked": true,
    "id": "518ca6f8fe502fbacae88937b694e8a4ef0a95be",
    "semantic_title": "latent hatred: a benchmark for understanding implicit hate speech",
    "citation_count": 261,
    "authors": [
      "Mai ElSherief",
      "Caleb Ziems",
      "David Muchlinski",
      "Vaishnavi Anupindi",
      "Jordyn Seybolt",
      "Munmun De Choudhury",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.30": {
    "title": "Distilling Linguistic Context for Language Model Compression",
    "volume": "main",
    "abstract": "A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. Unlike other recent distillation techniques for the language models, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes but also in combination with DynaBERT, the recently proposed adaptive size pruning method",
    "checked": true,
    "id": "e79957010ec7c5c221888e368624289cf301146e",
    "semantic_title": "distilling linguistic context for language model compression",
    "citation_count": 38,
    "authors": [
      "Geondo Park",
      "Gyeongman Kim",
      "Eunho Yang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.31": {
    "title": "Dynamic Knowledge Distillation for Pre-trained Language Models",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods",
    "checked": true,
    "id": "4b5e4948a572bd8d5045fdb532fa1391cb0b51eb",
    "semantic_title": "dynamic knowledge distillation for pre-trained language models",
    "citation_count": 50,
    "authors": [
      "Lei Li",
      "Yankai Lin",
      "Shuhuai Ren",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.32": {
    "title": "Few-Shot Text Generation with Natural Language Instructions",
    "volume": "main",
    "abstract": "Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GenPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GenPET gives consistent improvements over strong baselines in few-shot settings",
    "checked": true,
    "id": "1841cf23c65ff2f27f21ba0d2268c3445f20332f",
    "semantic_title": "few-shot text generation with natural language instructions",
    "citation_count": 149,
    "authors": [
      "Timo Schick",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.33": {
    "title": "SOM-NCSCM : An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map",
    "volume": "main",
    "abstract": "Sentence Compression (SC), which aims to shorten sentences while retaining important words that express the essential meanings, has been studied for many years in many languages, especially in English. However, improvements on Chinese SC task are still quite few due to several difficulties: scarce of parallel corpora, different segmentation granularity of Chinese sentences, and imperfect performance of syntactic analyses. Furthermore, entire neural Chinese SC models have been under-investigated so far. In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answering system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. Experimental results show that our SOM-NCSCM can significantly benefit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on Chinese SC task",
    "checked": true,
    "id": "8e5cc809ee95c0cc8bed8803254aa563e50d6b8d",
    "semantic_title": "som-ncscm : an efficient neural chinese sentence compression model enhanced with self-organizing map",
    "citation_count": 5,
    "authors": [
      "Kangli Zi",
      "Shi Wang",
      "Yu Liu",
      "Jicun Li",
      "Yanan Cao",
      "Cungen Cao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.34": {
    "title": "Efficient Multi-Task Auxiliary Learning: Selecting Auxiliary Data by Feature Similarity",
    "volume": "main",
    "abstract": "Multi-task auxiliary learning utilizes a set of relevant auxiliary tasks to improve the performance of a primary task. A common usage is to manually select multiple auxiliary tasks for multi-task learning on all data, which raises two issues: (1) selecting beneficial auxiliary tasks for a primary task is nontrivial; (2) when the auxiliary datasets are large, training on all data becomes time-expensive and impractical. Therefore, this paper focuses on addressing these problems and proposes a time-efficient sampling method to select the data that is most relevant to the primary task. The proposed method allows us to only train on the most beneficial sub-datasets from the auxiliary tasks, achieving efficient multi-task auxiliary learning. The experiments on three benchmark datasets (RTE, MRPC, STS-B) show that our method significantly outperforms random sampling and ST-DNN. Also, by applying our method, the model can surpass fully-trained MT-DNN on RTE, MRPC, STS-B, using only 50%, 66%, and 1% of data, respectively",
    "checked": true,
    "id": "be5cac3655e1433eab2d3a993b7ce9d94c05bfe3",
    "semantic_title": "efficient multi-task auxiliary learning: selecting auxiliary data by feature similarity",
    "citation_count": 18,
    "authors": [
      "Po-Nien Kung",
      "Sheng-Siang Yin",
      "Yi-Cheng Chen",
      "Tse-Hsuan Yang",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.35": {
    "title": "GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation",
    "volume": "main",
    "abstract": "Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To tackle this limited-data problem, previous methods focus on better modeling the distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal technique that augments existing data to train better OOS detectors operating in low-data regimes. GOLD generates pseudo-labeled candidates using samples from an auxiliary dataset and keeps only the most beneficial candidates for training through a novel filtering mechanism. In experiments across three target benchmarks, the top GOLD model outperforms all existing methods on all key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median baseline performance. We also analyze the unique properties of OOS data to identify key factors for optimally applying our proposed method",
    "checked": true,
    "id": "002bdeeb00007247cbb78ee45a089a12baa70397",
    "semantic_title": "gold: improving out-of-scope detection in dialogues using data augmentation",
    "citation_count": 31,
    "authors": [
      "Derek Chen",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.36": {
    "title": "Graph Based Network with Contextualized Representations of Turns in Dialogue",
    "volume": "main",
    "abstract": "Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because dialogues have the characteristics of high personal pronoun occurrences and low information density, and since most relational facts in dialogues are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the state-of-the-art models on most of the benchmark datasets. Our code is available at https://github.com/BlackNoodle/TUCORE-GCN",
    "checked": true,
    "id": "7bdadd8a3bf2de3cac4acd03a4626fb4c057bd50",
    "semantic_title": "graph based network with contextualized representations of turns in dialogue",
    "citation_count": 71,
    "authors": [
      "Bongseok Lee",
      "Yong Suk Choi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.37": {
    "title": "Automatically Exposing Problems with Neural Dialog Models",
    "volume": "main",
    "abstract": "Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these problems are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as hate speech, while leaving systematic problems undercover. In this paper, we propose two methods including reinforcement learning to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models",
    "checked": true,
    "id": "697cb0d25869e83b8304b3feb7a9e5dbce85b182",
    "semantic_title": "automatically exposing problems with neural dialog models",
    "citation_count": 9,
    "authors": [
      "Dian Yu",
      "Kenji Sagae"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.38": {
    "title": "Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News",
    "volume": "main",
    "abstract": "Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from hyperlinks in online news: When referring to a significant real-world event, writers often add a hyperlink to another article covering this event. We demonstrate that collecting hyperlinks which point to the same article(s) produces extensive and high-quality CDCR data and create a corpus of 2M documents and 2.7M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that models trained on small subsets of HyperCoref are highly competitive, with performance similar to models trained on gold-standard data. With our work, we free CDCR research from depending on costly human-annotated training data and open up possibilities for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages",
    "checked": true,
    "id": "0e78abaa9ae82dd8facd97461a0810e6227219c0",
    "semantic_title": "event coreference data (almost) for free: mining hyperlinks from online news",
    "citation_count": 6,
    "authors": [
      "Michael Bugert",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.39": {
    "title": "Inducing Stereotypical Character Roles from Plot Structure",
    "volume": "main",
    "abstract": "Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in narratives: they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters' roles in the overall narrative. We present a fully unsupervised k-means clustering approach for learning stereotypical roles given only structural plot information. We demonstrate the technique on Vladimir Propp's structural theory of Russian folktales (captured in the extended ProppLearner corpus, with 46 tales), showing that our approach can induce six out of seven of Propp's dramatis personae with F1 measures of up to 0.70 (0.58 average), with an additional category for minor characters. We have explored various feature sets and variations of a cluster evaluation method. The best-performing feature set comprises plot functions, unigrams, tf-idf weights, and embeddings over coreference chain heads. Roles that are mentioned more often (Hero, Villain), or have clearly distinct plot patterns (Princess) are more strongly differentiated than less frequent or distinct roles (Dispatcher, Helper, Donor). Detailed error analysis suggests that the quality of the coreference chain and plot functions annotations are critical for this task. We provide all our data and code for reproducibility",
    "checked": true,
    "id": "f1f2986e2e3807e2701cc5befe349387d37242ac",
    "semantic_title": "inducing stereotypical character roles from plot structure",
    "citation_count": 6,
    "authors": [
      "Labiba Jahan",
      "Rahul Mittal",
      "Mark Finlayson"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.40": {
    "title": "Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification",
    "volume": "main",
    "abstract": "As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In this work, we show that a multitask learning approach can combine discourse datasets from similar and diverse domains to improve discourse classification. We show an improvement of 4.9% Micro F1-score over current state-of-the-art benchmarks on the NewsDiscourse dataset, one of the largest discourse datasets recently published, due in part to label correlations across tasks, which improve performance for underrepresented classes. We also offer an extensive review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in our setting",
    "checked": true,
    "id": "8d555bb74cb9fc9b778dd6b1116fb3b0141cb9ba",
    "semantic_title": "multitask semi-supervised learning for class-imbalanced discourse classification",
    "citation_count": 26,
    "authors": [
      "Alexander Spangher",
      "Jonathan May",
      "Sz-Rung Shiang",
      "Lingjia Deng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.41": {
    "title": "Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models",
    "volume": "main",
    "abstract": "We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman's rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman's rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as .702. Moreover, we find Spearman's rho between racial bias and name frequency in BERT of .492, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts",
    "checked": true,
    "id": "705554c3532b7be9c1eb7c993132ffb940282e1b",
    "semantic_title": "low frequency names exhibit bias and overfitting in contextualizing language models",
    "citation_count": 51,
    "authors": [
      "Robert Wolfe",
      "Aylin Caliskan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.42": {
    "title": "Mitigating Language-Dependent Ethnic Bias in BERT",
    "volume": "main",
    "abstract": "In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show that these methods effectively alleviate the ethnic bias. Which of the two methods works better depends on the amount of NLP resources available for that language. We additionally experiment with Arabic and Greek to verify that our proposed methods work for a wider variety of languages",
    "checked": true,
    "id": "1aa1d6b29ad6fcef78d1eefacb2a7fd75e68c2c0",
    "semantic_title": "mitigating language-dependent ethnic bias in bert",
    "citation_count": 103,
    "authors": [
      "Jaimeen Ahn",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.43": {
    "title": "Adversarial Scrubbing of Demographic Information for Text Classification",
    "volume": "main",
    "abstract": "Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target task. In this paper, we present an adversarial learning framework \"Adversarial Scrubber\" (AdS), to debias contextual representations. We perform theoretical analysis to show that our framework converges without leaking demographic information under certain conditions. We extend previous evaluation techniques by evaluating debiasing performance using Minimum Description Length (MDL) probing. Experimental evaluations on 8 datasets show that AdS generates representations with minimal information about demographic attributes while being maximally informative about the target task",
    "checked": true,
    "id": "0ea8a7d99f3ced724fb89dcd631432a542bf89d7",
    "semantic_title": "adversarial scrubbing of demographic information for text classification",
    "citation_count": 14,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Sayan Ghosh",
      "Yiyuan Li",
      "Junier Oliva",
      "Shashank Srivastava",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.44": {
    "title": "Open-domain clarification question generation without question examples",
    "volume": "main",
    "abstract": "An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in dialogue. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model's ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers",
    "checked": true,
    "id": "9b621c0bcd2029006b389bc51395fb6604f9a855",
    "semantic_title": "open-domain clarification question generation without question examples",
    "citation_count": 24,
    "authors": [
      "Julia White",
      "Gabriel Poesia",
      "Robert Hawkins",
      "Dorsa Sadigh",
      "Noah Goodman"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.45": {
    "title": "Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting",
    "volume": "main",
    "abstract": "In this paper, we propose Sequence Span Rewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require sentence rewriting (e.g., text summarization, question generation, grammatical error correction, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks",
    "checked": true,
    "id": "124b25a74691a993673359288637716d83899a06",
    "semantic_title": "improving sequence-to-sequence pre-training via sequence span rewriting",
    "citation_count": 16,
    "authors": [
      "Wangchunshu Zhou",
      "Tao Ge",
      "Canwen Xu",
      "Ke Xu",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.46": {
    "title": "Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data",
    "volume": "main",
    "abstract": "Existing text classification methods mainly focus on a fixed label set, whereas many real-world applications require extending to new fine-grained classes as the number of samples per label increases. To accommodate such requirements, we introduce a new problem called coarse-to-fine grained classification, which aims to perform fine-grained classification on coarsely annotated data. Instead of asking for new fine-grained human annotations, we opt to leverage label surface names as the only human guidance and weave in rich pre-trained generative language models into the iterative weak supervision strategy. Specifically, we first propose a label-conditioned fine-tuning formulation to attune these generators for our task. Furthermore, we devise a regularization objective based on the coarse-fine label constraints derived from our problem setting, giving us even further improvements over the prior formulation. Our framework uses the fine-tuned generative models to sample pseudo-training data for training the classifier, and bootstraps on real unlabeled data for model refinement. Extensive experiments and case studies on two real-world datasets demonstrate superior performance over SOTA zero-shot classification baselines",
    "checked": true,
    "id": "da53c6d525ed27bf2e9453eeafeb7c85e9a56a1e",
    "semantic_title": "coarse2fine: fine-grained text classification on coarsely-grained annotated data",
    "citation_count": 22,
    "authors": [
      "Dheeraj Mekala",
      "Varun Gangal",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.47": {
    "title": "Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries",
    "volume": "main",
    "abstract": "We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning",
    "checked": true,
    "id": "57651d65078818821234d13544ac1f29858dcd67",
    "semantic_title": "text2mol: cross-modal molecule retrieval with natural language queries",
    "citation_count": 138,
    "authors": [
      "Carl Edwards",
      "ChengXiang Zhai",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.48": {
    "title": "Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus",
    "volume": "main",
    "abstract": "We consider the hierarchical representation of documents as graphs and use geometric deep learning to classify them into different categories. While graph neural networks can efficiently handle the variable structure of hierarchical documents using the permutation invariant message passing operations, we show that we can gain extra performance improvements using our proposed selective graph pooling operation that arises from the fact that some parts of the hierarchy are invariable across different documents. We applied our model to classify clinical trial (CT) protocols into completed and terminated categories. We use bag-of-words based, as well as pre-trained transformer-based embeddings to featurize the graph nodes, achieving f1-scoresaround 0.85 on a publicly available large scale CT registry of around 360K protocols. We further demonstrate how the selective pooling can add insights into the CT termination status prediction. We make the source code and dataset splits accessible",
    "checked": true,
    "id": "9dd4833ac99b9f7cd73077d2cce679e8ef548015",
    "semantic_title": "classification of hierarchical text using geometric deep learning: the case of clinical trials corpus",
    "citation_count": 7,
    "authors": [
      "Sohrab Ferdowsi",
      "Nikolay Borissov",
      "Julien Knafou",
      "Poorya Amini",
      "Douglas Teodoro"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.49": {
    "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
    "volume": "main",
    "abstract": "Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity split, and from 35% to 81% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results",
    "checked": true,
    "id": "ed535e93d5b5a8b689e861e9c6083a806d1535c2",
    "semantic_title": "the devil is in the detail: simple tricks improve systematic generalization of transformers",
    "citation_count": 140,
    "authors": [
      "Róbert Csordás",
      "Kazuki Irie",
      "Juergen Schmidhuber"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.50": {
    "title": "Artificial Text Detection via Examining the Topology of Attention Maps",
    "volume": "main",
    "abstract": "The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10% on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the features reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to NLP tasks, specifically the ones that incorporate surface and structural information",
    "checked": true,
    "id": "2af03a9bff2e6f14deca8f0b7bb243a56efd182a",
    "semantic_title": "artificial text detection via examining the topology of attention maps",
    "citation_count": 52,
    "authors": [
      "Laida Kushnareva",
      "Daniil Cherniavskii",
      "Vladislav Mikhailov",
      "Ekaterina Artemova",
      "Serguei Barannikov",
      "Alexander Bernstein",
      "Irina Piontkovskaya",
      "Dmitri Piontkovski",
      "Evgeny Burnaev"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.51": {
    "title": "Active Learning by Acquiring Contrastive Examples",
    "volume": "main",
    "abstract": "Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies",
    "checked": true,
    "id": "6b1462ee55b8a82bcb32d30c3bf0a4bc84f95c10",
    "semantic_title": "active learning by acquiring contrastive examples",
    "citation_count": 199,
    "authors": [
      "Katerina Margatina",
      "Giorgos Vernikos",
      "Loïc Barrault",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.52": {
    "title": "Conditional Poisson Stochastic Beams",
    "volume": "main",
    "abstract": "Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)'s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings",
    "checked": true,
    "id": "bd6018632a360cb567da8e50e1717ff526503845",
    "semantic_title": "conditional poisson stochastic beams",
    "citation_count": 11,
    "authors": [
      "Clara Meister",
      "Afra Amini",
      "Tim Vieira",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.53": {
    "title": "Building Adaptive Acceptability Classifiers for Neural NLG",
    "volume": "main",
    "abstract": "We propose a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches. An NLG response is considered acceptable if it is both semantically correct and grammatical. We don't make use of any human references making the classifiers suitable for runtime deployment. Training data for the classifiers is obtained using a 2-stage approach of first generating synthetic data using a combination of existing and new model-based approaches followed by a novel validation framework to filter and sort the synthetic data into acceptable and unacceptable classes. Our 2-stage approach adapts to a wide range of data representations and does not require additional data beyond what the NLG models are trained on. It is also independent of the underlying NLG model architecture, and is able to generate more realistic samples close to the distribution of the NLG model-generated responses. We present results on 5 datasets (WebNLG, Cleaned E2E, ViGGO, Alarm, and Weather) with varying data representations. We compare our framework with existing techniques that involve synthetic data generation using simple sentence transformations and/or model-based techniques, and show that building acceptability classifiers using data that resembles the generation model outputs followed by a validation framework outperforms the existing techniques, achieving state-of-the-art results. We also show that our techniques can be used in few-shot settings using self-training",
    "checked": true,
    "id": "4de04d8f15bc933104445dd444d9313d4e48cf37",
    "semantic_title": "building adaptive acceptability classifiers for neural nlg",
    "citation_count": 13,
    "authors": [
      "Soumya Batra",
      "Shashank Jain",
      "Peyman Heidari",
      "Ankit Arun",
      "Catharine Youngs",
      "Xintong Li",
      "Pinar Donmez",
      "Shawn Mei",
      "Shiunzu Kuo",
      "Vikas Bhardwaj",
      "Anuj Kumar",
      "Michael White"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.54": {
    "title": "Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences",
    "volume": "main",
    "abstract": "In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines",
    "checked": true,
    "id": "22e5af80e9fd64b817efd2ec50c23b3b454f5a0d",
    "semantic_title": "moral stories: situated reasoning about norms, intents, actions, and their consequences",
    "citation_count": 136,
    "authors": [
      "Denis Emelin",
      "Ronan Le Bras",
      "Jena D. Hwang",
      "Maxwell Forbes",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.55": {
    "title": "Truth-Conditional Captions for Time Series Data",
    "volume": "main",
    "abstract": "In this paper, we explore the task of automatically generating natural language descriptions of salient patterns in a time series, such as stock prices of a company over a week. A model for this task should be able to extract high-level patterns such as presence of a peak or a dip. While typical contemporary neural models with attention mechanisms can generate fluent output descriptions for this task, they often generate factually incorrect descriptions. We propose a computational model with a truth-conditional architecture which first runs small learned programs on the input time series, then identifies the programs/patterns which hold true for the given input, and finally conditions on *only* the chosen valid program (rather than the input time series) to generate the output text description. A program in our model is constructed from modules, which are small neural networks that are designed to capture numerical patterns and temporal information. The modules are shared across multiple programs, enabling compositionality as well as efficient learning of module parameters. The modules, as well as the composition of the modules, are unobserved in data, and we learn them in an end-to-end fashion with the only training signal coming from the accompanying natural language text descriptions. We find that the proposed model is able to generate high-precision captions even though we consider a small and simple space of module types",
    "checked": true,
    "id": "3d4781c86e5b49124e307b4ad6ef2fceeeee92f2",
    "semantic_title": "truth-conditional captions for time series data",
    "citation_count": 9,
    "authors": [
      "Harsh Jhamtani",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.56": {
    "title": "Injecting Entity Types into Entity-Guided Text Generation",
    "volume": "main",
    "abstract": "Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role of entity in NLG, in this paper, we aim to model the entity type in the decoding phase to generate contextual words accurately. We develop a novel NLG model to produce a target sequence based on a given list of entities. Our model has a multi-step decoder that injects the entity types into the process of entity mention generation. Experiments on two public news datasets demonstrate type injection performs better than existing type embedding concatenation baselines",
    "checked": true,
    "id": "ef46b392f470bd8dca1de5a49596f5557bfc59fb",
    "semantic_title": "injecting entity types into entity-guided text generation",
    "citation_count": 21,
    "authors": [
      "Xiangyu Dong",
      "Wenhao Yu",
      "Chenguang Zhu",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.57": {
    "title": "Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation",
    "volume": "main",
    "abstract": "Recent work on multilingual AMR-to-text generation has exclusively focused on data augmentation strategies that utilize silver AMR. However, this assumes a high quality of generated AMRs, potentially limiting the transferability to the target task. In this paper, we investigate different techniques for automatically generating AMR annotations, where we aim to study which source of information yields better multilingual results. Our models trained on gold AMR with silver (machine translated) sentences outperform approaches which leverage generated silver AMR. We find that combining both complementary sources of information further improves multilingual AMR-to-text generation. Our models surpass the previous state of the art for German, Italian, Spanish, and Chinese by a large margin",
    "checked": true,
    "id": "80086ec0d4cf23e1069eec82099ffacd20817ec7",
    "semantic_title": "smelting gold and silver for improved multilingual amr-to-text generation",
    "citation_count": 11,
    "authors": [
      "Leonardo F. R. Ribeiro",
      "Jonas Pfeiffer",
      "Yue Zhang",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.58": {
    "title": "Learning Compact Metrics for MT",
    "volume": "main",
    "abstract": "Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on related tasks suggest that these models are most efficient when they are large, which is costly and impractical for evaluation. We investigate the trade-off between multilinguality and model capacity with RemBERT, a state-of-the-art multilingual language model, using data from the WMT Metrics Shared Task. We present a series of experiments which show that model size is indeed a bottleneck for cross-lingual transfer, then demonstrate how distillation can help addressing this bottleneck, by leveraging synthetic data generation and transferring knowledge from one teacher to multiple students trained on related languages. Our method yields up to 10.5% improvement over vanilla fine-tuning and reaches 92.6% of RemBERT's performance using only a third of its parameters",
    "checked": true,
    "id": "dfd104dd0ff28b1bde2fbd4c4d6d3ccb4761f639",
    "semantic_title": "learning compact metrics for mt",
    "citation_count": 103,
    "authors": [
      "Amy Pu",
      "Hyung Won Chung",
      "Ankur Parikh",
      "Sebastian Gehrmann",
      "Thibault Sellam"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.59": {
    "title": "The Impact of Positional Encodings on Multilingual Compression",
    "volume": "main",
    "abstract": "In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is: sinusoidal encodings were explicitly designed to facilitate compositionality by allowing linear projections over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, compositionality becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the inductive bias to effectively learn cross-lingual alignment. In other words, while sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models",
    "checked": true,
    "id": "33c831326bb47b2ba2031fd7213b6918d23eb01e",
    "semantic_title": "the impact of positional encodings on multilingual compression",
    "citation_count": 5,
    "authors": [
      "Vinit Ravishankar",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.60": {
    "title": "Disentangling Representations of Text by Masking Transformers",
    "volume": "main",
    "abstract": "Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as — and often better than —previously proposed methods based on variational autoencoders and adversarial training",
    "checked": true,
    "id": "ec60638d97361d3e03156d5c7e90b73da6302ce5",
    "semantic_title": "disentangling representations of text by masking transformers",
    "citation_count": 21,
    "authors": [
      "Xiongyi Zhang",
      "Jan-Willem van de Meent",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.61": {
    "title": "Exploring the Role of BERT Token Representations to Explain Sentence Probing Results",
    "volume": "main",
    "abstract": "Several studies have been carried out on revealing linguistic features captured by BERT. This is usually achieved by training a diagnostic classifier on the representations obtained from different layers of BERT. The subsequent classification accuracy is then interpreted as the ability of the model in encoding the corresponding linguistic property. Despite providing insights, these studies have left out the potential role of token representations. In this paper, we provide a more in-depth analysis on the representation space of BERT in search for distinct and meaningful subspaces that can explain the reasons behind these probing results. Based on a set of probing tasks and with the help of attribution methods we show that BERT tends to encode meaningful knowledge in specific token representations (which are often ignored in standard classification setups), allowing the model to detect syntactic and semantic abnormalities, and to distinctively separate grammatical number and tense subspaces",
    "checked": true,
    "id": "b265827019f420b44c79fd87be1cc6000329c762",
    "semantic_title": "exploring the role of bert token representations to explain sentence probing results",
    "citation_count": 27,
    "authors": [
      "Hosein Mohebbi",
      "Ali Modarressi",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.62": {
    "title": "Do Long-Range Language Models Actually Use Long-Range Context?",
    "volume": "main",
    "abstract": "Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines)",
    "checked": true,
    "id": "f75d05e759447c2aedb7097728f29f9a520d9bc1",
    "semantic_title": "do long-range language models actually use long-range context?",
    "citation_count": 85,
    "authors": [
      "Simeng Sun",
      "Kalpesh Krishna",
      "Andrew Mattarella-Micke",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.63": {
    "title": "The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color",
    "volume": "main",
    "abstract": "Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human's perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research",
    "checked": true,
    "id": "1360dc6d21ccbf2c23701c9ddd7c6ab8d04e4531",
    "semantic_title": "the world of an octopus: how reporting bias influences a language model's perception of color",
    "citation_count": 34,
    "authors": [
      "Cory Paik",
      "Stéphane Aroca-Ouellette",
      "Alessandro Roncone",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.64": {
    "title": "SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers",
    "volume": "main",
    "abstract": "We introduce SelfExplain, a novel self-explaining model that explains a text classifier's predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SelfExplain facilitates interpretability without sacrificing performance. Most importantly, explanations from SelfExplain show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines",
    "checked": true,
    "id": "3aba582b62d1abfcd95264e6c7b32aab4c9db4b8",
    "semantic_title": "selfexplain: a self-explaining architecture for neural text classifiers",
    "citation_count": 71,
    "authors": [
      "Dheeraj Rajagopal",
      "Vidhisha Balachandran",
      "Eduard H Hovy",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.65": {
    "title": "Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories",
    "volume": "main",
    "abstract": "Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience annotation using chapter-aligned summaries from the Shmoop corpus for classic literary works. Our evaluation against this data demonstrates that our salience detection model improves performance over and above a non-knowledgebase and memory augmented language model, both of which are crucial to this improvement",
    "checked": true,
    "id": "8e5e94417f5495c5b017ff73aaccdc876b10120d",
    "semantic_title": "memory and knowledge augmented language models for inferring salience in long-form stories",
    "citation_count": 21,
    "authors": [
      "David Wilmot",
      "Frank Keller"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.66": {
    "title": "Semantic Novelty Detection in Natural Language Descriptions",
    "volume": "main",
    "abstract": "This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says \"A man is walking a chicken in the park\", it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the problem. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective model (called GAT-MA) to solve the problem and also contributes a new dataset. Experimental evaluation shows that GAT-MA outperforms 11 baselines by large margins",
    "checked": true,
    "id": "0ddff69fa1e7bb49b277136fe50c3df62a504f1d",
    "semantic_title": "semantic novelty detection in natural language descriptions",
    "citation_count": 9,
    "authors": [
      "Nianzu Ma",
      "Alexander Politowicz",
      "Sahisnu Mazumder",
      "Jiahua Chen",
      "Bing Liu",
      "Eric Robertson",
      "Scott Grigsby"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.67": {
    "title": "Jump-Starting Item Parameters for Adaptive Language Tests",
    "volume": "main",
    "abstract": "A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed ‘cold start' estimation of item difficulties without piloting, we devise a multi-task generalized linear model with BERT features to jump-start these estimates, rapidly improving their quality with as few as 500 test-takers and a small sample of item exposures (≈6 each) from a large item bank (≈4,000 items). Our joint model provides a principled way to compare test-taker proficiency, item difficulty, and language proficiency frameworks like the Common European Framework of Reference (CEFR). This also enables new item difficulty estimates without piloting them first, which in turn limits item exposure and thus enhances test item security. Finally, using operational data from the Duolingo English Test, a high-stakes English proficiency test, we find that the difficulty estimates derived using this method correlate strongly with lexico-grammatical features that correlate with reading complexity",
    "checked": true,
    "id": "c030bf54bc974f7a1393d06cae0d6dffef53407b",
    "semantic_title": "jump-starting item parameters for adaptive language tests",
    "citation_count": 31,
    "authors": [
      "Arya D. McCarthy",
      "Kevin P. Yancey",
      "Geoffrey T. LaFlair",
      "Jesse Egbert",
      "Manqian Liao",
      "Burr Settles"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.68": {
    "title": "Voice Query Auto Completion",
    "volume": "main",
    "abstract": "Query auto completion (QAC) is the task of predicting a search engine user's final query from their intermediate, incomplete query. In this paper, we extend QAC to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often don't form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC approaches on intermediate transcriptions to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best method obtains an 18% relative improvement in mean reciprocal rank over previous methods",
    "checked": true,
    "id": "6b6e13bd552d1ad640b3fca34049559dc2b3a560",
    "semantic_title": "voice query auto completion",
    "citation_count": 2,
    "authors": [
      "Raphael Tang",
      "Karun Kumar",
      "Kendra Chalkley",
      "Ji Xin",
      "Liming Zhang",
      "Wenyan Li",
      "Gefei Yang",
      "Yajie Mao",
      "Junho Shin",
      "Geoffrey Craig Murray",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.69": {
    "title": "CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification",
    "volume": "main",
    "abstract": "Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation",
    "checked": true,
    "id": "c39eb0a1a50f9754793bc4095ca35102301d28cc",
    "semantic_title": "cophe: a count-preserving hierarchical evaluation metric in large-scale multi-label text classification",
    "citation_count": 7,
    "authors": [
      "Matúš Falis",
      "Hang Dong",
      "Alexandra Birch",
      "Beatrice Alex"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.70": {
    "title": "Learning Universal Authorship Representations",
    "volume": "main",
    "abstract": "Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using statistical methods. Recently, authorship representations learned using neural networks have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such representations learned in a particular domain transfer to other domains? Or are these representations inherently entangled with domain-specific features? To study these questions, we conduct the first large-scale study of cross-domain transfer for authorship verification considering zero-shot transfers involving three disparate domains: Amazon reviews, fanfiction short stories, and Reddit comments. We find that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that influence generalization and propose simple but effective methods to improve transfer",
    "checked": true,
    "id": "c71f40033cb0f8e76ef28e504240ae14317a5094",
    "semantic_title": "learning universal authorship representations",
    "citation_count": 68,
    "authors": [
      "Rafael A. Rivera-Soto",
      "Olivia Elizabeth Miano",
      "Juanita Ordonez",
      "Barry Y. Chen",
      "Aleem Khan",
      "Marcus Bishop",
      "Nicholas Andrews"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.71": {
    "title": "Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining",
    "volume": "main",
    "abstract": "Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of chaining and knowledge from \"percept\", \"concept\", and \"language\" to infer how verbs extend their frames to form new compositions with existing and novel nouns. We evaluate SFEM rigorously on the 1) modalities of knowledge and 2) categorization models of chaining, in a syntactically parsed English corpus over the past 150 years. We show that multimodal SFEM predicts newly emerged verb syntax and arguments substantially better than competing models using purely linguistic or unimodal knowledge. We find support for an exemplar view of chaining as opposed to a prototype view and reveal how the joint approach of multimodal chaining may be fundamental to the creation of literal and figurative language uses including metaphor and metonymy",
    "checked": true,
    "id": "272a45401ae7ad05d33ccbb3efea647c0042f892",
    "semantic_title": "predicting emergent linguistic compositions through time: syntactic frame extension via multimodal chaining",
    "citation_count": 8,
    "authors": [
      "Lei Yu",
      "Yang Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.72": {
    "title": "Frequency Effects on Syntactic Rule Learning in Transformers",
    "volume": "main",
    "abstract": "Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT's performance on English subject–verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT's behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items",
    "checked": true,
    "id": "3962f108081b22c7e54b413f47ba6f2c16f2cc05",
    "semantic_title": "frequency effects on syntactic rule learning in transformers",
    "citation_count": 67,
    "authors": [
      "Jason Wei",
      "Dan Garrette",
      "Tal Linzen",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.73": {
    "title": "A surprisal–duration trade-off across and within the world's languages",
    "volume": "main",
    "abstract": "While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal–duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal–duration trade-off in operation, both across and within the world's languages",
    "checked": true,
    "id": "e6586ac159921f5876c90f8c6668666ee1de10e6",
    "semantic_title": "a surprisal–duration trade-off across and within the world's languages",
    "citation_count": 29,
    "authors": [
      "Tiago Pimentel",
      "Clara Meister",
      "Elizabeth Salesky",
      "Simone Teufel",
      "Damián Blasi",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.74": {
    "title": "Revisiting the Uniform Information Density Hypothesis",
    "volume": "main",
    "abstract": "The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal—or lack thereof—should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID's predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document—a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel",
    "checked": true,
    "id": "680e61a17e27a1e8e121276c7ec53fc4fd40babb",
    "semantic_title": "revisiting the uniform information density hypothesis",
    "citation_count": 78,
    "authors": [
      "Clara Meister",
      "Tiago Pimentel",
      "Patrick Haller",
      "Lena Jäger",
      "Ryan Cotterell",
      "Roger Levy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.75": {
    "title": "Condenser: a Pre-training Architecture for Dense Retrieval",
    "volume": "main",
    "abstract": "Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs' internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks",
    "checked": true,
    "id": "9bbdcc03d872987eef9165f4a63c3878a5b05189",
    "semantic_title": "condenser: a pre-training architecture for dense retrieval",
    "citation_count": 272,
    "authors": [
      "Luyu Gao",
      "Jamie Callan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.76": {
    "title": "Monitoring geometrical properties of word embeddings for detecting the emergence of new topics",
    "volume": "main",
    "abstract": "Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and language evolution, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of weak signals at the word level. We propose to monitor the behavior of words representation in an embedding space and use one of its geometrical properties to characterize the emergence of topics. As evaluation is typically hard for this kind of task, we present a framework for quantitative evaluation and show positive results that outperform state-of-the-art methods. Our method is evaluated on two public datasets of press and scientific articles",
    "checked": true,
    "id": "b85628fbc0b82f766bb048ffbc6097d0de1f4f60",
    "semantic_title": "monitoring geometrical properties of word embeddings for detecting the emergence of new topics",
    "citation_count": 2,
    "authors": [
      "Clément Christophe",
      "Julien Velcin",
      "Jairo Cugliari",
      "Manel Boumghar",
      "Philippe Suignard"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.77": {
    "title": "Contextualized Query Embeddings for Conversational Search",
    "volume": "main",
    "abstract": "This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval modules. Despite its effectiveness, such a pipeline often includes multiple neural models that require long inference times. In addition, independently optimizing each module ignores dependencies among them. To address these shortcomings, we propose to integrate conversational query reformulation directly into a dense retrieval model. To aid in this goal, we create a dataset with pseudo-relevance labels for conversational search to overcome the lack of training data and to explore different training strategies. We demonstrate that our model effectively rewrites conversational queries as dense representations in conversational search and open-domain question answering datasets. Finally, after observing that our model learns to adjust the L2 norm of query token embeddings, we leverage this property for hybrid retrieval and to support error analysis",
    "checked": true,
    "id": "dd0e36831fa19da2e56fa925397407961a506bb6",
    "semantic_title": "contextualized query embeddings for conversational search",
    "citation_count": 63,
    "authors": [
      "Sheng-Chieh Lin",
      "Jheng-Hong Yang",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.78": {
    "title": "Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval",
    "volume": "main",
    "abstract": "The semantic matching capabilities of neural information retrieval can ameliorate synonymy and polysemy problems of symbolic approaches. However, neural models' dense representations are more suitable for re-ranking, due to their inefficiency. Sparse representations, either in symbolic or latent form, are more efficient with an inverted index. Taking the merits of the sparse and dense representations, we propose an ultra-high dimensional (UHD) representation scheme equipped with directly controllable sparsity. UHD's large capacity and minimal noise and interference among the dimensions allow for binarized representations, which are highly efficient for storage and search. Also proposed is a bucketing method, where the embeddings from multiple layers of BERT are selected/merged to represent diverse linguistic aspects. We test our models with MS MARCO and TREC CAR, showing that our models outperforms other sparse models",
    "checked": true,
    "id": "aacc51b75d910031d8b34476e6a343d5eed73fc2",
    "semantic_title": "ultra-high dimensional sparse representations with binarization for efficient text retrieval",
    "citation_count": 20,
    "authors": [
      "Kyoung-Rok Jang",
      "Junmo Kang",
      "Giwon Hong",
      "Sung-Hyon Myaeng",
      "Joohee Park",
      "Taewon Yoon",
      "Heecheol Seo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.79": {
    "title": "IR like a SIR: Sense-enhanced Information Retrieval for Multiple Languages",
    "volume": "main",
    "abstract": "With the advent of contextualized embeddings, attention towards neural ranking approaches for Information Retrieval increased considerably. However, two aspects have remained largely neglected: i) queries usually consist of few keywords only, which increases ambiguity and makes their contextualization harder, and ii) performing neural ranking on non-English documents is still cumbersome due to shortage of labeled datasets. In this paper we present SIR (Sense-enhanced Information Retrieval) to mitigate both problems by leveraging word sense information. At the core of our approach lies a novel multilingual query expansion mechanism based on Word Sense Disambiguation that provides sense definitions as additional semantic information for the query. Importantly, we use senses as a bridge across languages, thus allowing our model to perform considerably better than its supervised and unsupervised alternatives across French, German, Italian and Spanish languages on several CLEF benchmarks, while being trained on English Robust04 data only. We release SIR at https://github.com/SapienzaNLP/sir",
    "checked": true,
    "id": "da322bfa5352c19b540756e4b7fc146a6f0d8b02",
    "semantic_title": "ir like a sir: sense-enhanced information retrieval for multiple languages",
    "citation_count": 12,
    "authors": [
      "Rexhina Blloshmi",
      "Tommaso Pasini",
      "Niccolò Campolungo",
      "Somnath Banerjee",
      "Roberto Navigli",
      "Gabriella Pasi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.80": {
    "title": "Neural Attention-Aware Hierarchical Topic Model",
    "volume": "main",
    "abstract": "Neural topic models (NTMs) apply deep neural networks to topic modelling. Despite their success, NTMs generally ignore two important aspects: (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets",
    "checked": true,
    "id": "9db03509f2a8917dc4135c0a7dfc4ff7274f533d",
    "semantic_title": "neural attention-aware hierarchical topic model",
    "citation_count": 17,
    "authors": [
      "Yuan Jin",
      "He Zhao",
      "Ming Liu",
      "Lan Du",
      "Wray Buntine"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.81": {
    "title": "Relational World Knowledge Representation in Contextual Language Models: A Review",
    "volume": "main",
    "abstract": "Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold: (1) We provide a high-level, extensible taxonomy for knowledge representation in LMs; (2) Within our taxonomy, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations",
    "checked": true,
    "id": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4",
    "semantic_title": "relational world knowledge representation in contextual language models: a review",
    "citation_count": 52,
    "authors": [
      "Tara Safavi",
      "Danai Koutra"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.82": {
    "title": "Certified Robustness to Programmable Transformations in LSTMs",
    "volume": "main",
    "abstract": "Deep neural networks for natural language processing are fragile in the face of adversarial examples—small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and extensions of LSTMs) and training models that can be efficiently certified. Our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string transformations. Our evaluation shows that (1) our approach can train models that are more robust to combinations of string transformations than those produced using existing techniques; (2) our approach can show high certification accuracy of the resulting models",
    "checked": true,
    "id": "546daf45b98dcaa1c7b438ec04b2a8b744f279d7",
    "semantic_title": "certified robustness to programmable transformations in lstms",
    "citation_count": 23,
    "authors": [
      "Yuhao Zhang",
      "Aws Albarghouthi",
      "Loris D’Antoni"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.83": {
    "title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
    "volume": "main",
    "abstract": "Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details at https://github.com/IBM/regen",
    "checked": true,
    "id": "251bdaff7521e60fe81fc375acfd34951c7f13ea",
    "semantic_title": "regen: reinforcement learning for text and knowledge base generation using pretrained language models",
    "citation_count": 21,
    "authors": [
      "Pierre Dognin",
      "Inkit Padhi",
      "Igor Melnyk",
      "Payel Das"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.84": {
    "title": "Contrastive Out-of-Distribution Detection for Pretrained Transformers",
    "volume": "main",
    "abstract": "Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model's penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research",
    "checked": true,
    "id": "a9b04a3e0cf5766df9b3af8c442f2d85ac5e2c7e",
    "semantic_title": "contrastive out-of-distribution detection for pretrained transformers",
    "citation_count": 98,
    "authors": [
      "Wenxuan Zhou",
      "Fangyu Liu",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.85": {
    "title": "MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks",
    "volume": "main",
    "abstract": "An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners' beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks",
    "checked": true,
    "id": "e1739af6e85bc57b43dc6b750b95912538cd1fef",
    "semantic_title": "mindcraft: theory of mind modeling for situated dialogue in collaborative tasks",
    "citation_count": 67,
    "authors": [
      "Cristian-Paul Bara",
      "Sky CH-Wang",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.86": {
    "title": "Detecting Speaker Personas from Conversational Texts",
    "volume": "main",
    "abstract": "Personas are useful for dialogue response prediction. However, the personas used in current studies are pre-defined and hard to obtain before a conversation. To tackle this issue, we study a new task, named Speaker Persona Detection (SPD), which aims to detect speaker personas based on the plain conversational text. In this task, a best-matched persona is searched out from candidates given the conversational text. This is a many-to-many semantic matching task because both contexts and personas in SPD are composed of multiple sentences. The long-term dependency and the dynamic redundancy among these sentences increase the difficulty of this task. We build a dataset for SPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate several baseline models and propose utterance-to-profile (U2P) matching networks for this task. The U2P models operate at a fine granularity which treat both contexts and personas as sets of multiple sequences. Then, each sequence pair is scored and an interpretable overall score is obtained for a context-persona pair through aggregation. Evaluation results show that the U2P models outperform their baseline counterparts significantly",
    "checked": true,
    "id": "75546a8cb582d0d9c76eb949144c02ccea4c4fc3",
    "semantic_title": "detecting speaker personas from conversational texts",
    "citation_count": 16,
    "authors": [
      "Jia-Chen Gu",
      "Zhenhua Ling",
      "Yu Wu",
      "Quan Liu",
      "Zhigang Chen",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.87": {
    "title": "Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking",
    "volume": "main",
    "abstract": "Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English -> Chinese, Chinese -> English) and Multilingual WoZ (English -> German, English -> Italian) datasets. We achieve impressive improvements (> 20% on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10% of the target language task data and zero-shot setup respectively",
    "checked": true,
    "id": "fa349e881c72825a8082a1c80e85583dba6d2840",
    "semantic_title": "cross-lingual intermediate fine-tuning improves dialogue state tracking",
    "citation_count": 13,
    "authors": [
      "Nikita Moghe",
      "Mark Steedman",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.88": {
    "title": "ConvFiT: Conversational Fine-Tuning of Pretrained Language Models",
    "volume": "main",
    "abstract": "Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups",
    "checked": true,
    "id": "42f5579224bc024ff24e12ced5c0084e1962987b",
    "semantic_title": "convfit: conversational fine-tuning of pretrained language models",
    "citation_count": 39,
    "authors": [
      "Ivan Vulić",
      "Pei-Hao Su",
      "Samuel Coope",
      "Daniela Gerz",
      "Paweł Budzianowski",
      "Iñigo Casanueva",
      "Nikola Mrkšić",
      "Tsung-Hsien Wen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.89": {
    "title": "We've had this conversation before: A Novel Approach to Measuring Dialog Similarity",
    "volume": "main",
    "abstract": "Dialog is a core building block of human natural language interactions. It contains multi-party utterances used to convey information from one party to another in a dynamic and evolving manner. The ability to compare dialogs is beneficial in many real world use cases, such as conversation analytics for contact center calls and virtual agent design. We propose a novel adaptation of the edit distance metric to the scenario of dialog similarity. Our approach takes into account various conversation aspects such as utterance semantics, conversation flow, and the participants. We evaluate this new approach and compare it to existing document similarity measures on two publicly available datasets. The results demonstrate that our method outperforms the other approaches in capturing dialog flow, and is better aligned with the human perception of conversation similarity",
    "checked": true,
    "id": "752a43a4e316a37bad44cee02249b1932deba3f2",
    "semantic_title": "we've had this conversation before: a novel approach to measuring dialog similarity",
    "citation_count": 6,
    "authors": [
      "Ofer Lavi",
      "Ella Rabinovich",
      "Segev Shlomov",
      "David Boaz",
      "Inbal Ronen",
      "Ateret Anaby Tavor"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.90": {
    "title": "Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU",
    "volume": "main",
    "abstract": "Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs",
    "checked": true,
    "id": "536b8a132f193471ec61ce7b7be6e1e17aa7fcce",
    "semantic_title": "towards incremental transformers: an empirical analysis of transformer models for incremental nlu",
    "citation_count": 21,
    "authors": [
      "Patrick Kahardipraja",
      "Brielen Madureira",
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.91": {
    "title": "Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding",
    "volume": "main",
    "abstract": "With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such feedback is often available in real-world dialog systems, however, the modularized architecture commonly used in large-scale systems prevents the direct application of such algorithms. In this paper, we study the feedback attribution problem that arises when using counterfactual bandit learning for multi-domain spoken language understanding. We introduce an experimental setup to simulate the problem on small-scale public datasets, propose attribution methods inspired by multi-agent reinforcement learning and evaluate them against multiple baselines. We find that while directly using overall feedback leads to disastrous performance, our proposed attribution methods can allow training competitive models from user feedback",
    "checked": true,
    "id": "03af44c8185790fc2ca56842505233135d0edd9b",
    "semantic_title": "feedback attribution for counterfactual bandit learning in multi-domain spoken language understanding",
    "citation_count": 7,
    "authors": [
      "Tobias Falke",
      "Patrick Lehnen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.92": {
    "title": "Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction",
    "volume": "main",
    "abstract": "Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per relation (17% points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, allowing to report the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are specially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases",
    "checked": true,
    "id": "85061c524fdd5ec75f06a3329352621bb8d05f43",
    "semantic_title": "label verbalization and entailment for effective zero and few-shot relation extraction",
    "citation_count": 126,
    "authors": [
      "Oscar Sainz",
      "Oier Lopez de Lacalle",
      "Gorka Labaka",
      "Ander Barrena",
      "Eneko Agirre"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.93": {
    "title": "Extend, don't rebuild: Phrasing conditional graph modification as autoregressive sequence labelling",
    "volume": "main",
    "abstract": "Deriving and modifying graphs from natural language text has become a versatile basis technology for information extraction with applications in many subfields, such as semantic parsing or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original graph and then generating the modified one based on this encoding. In this work, we show that we can considerably increase performance on this problem by phrasing it as graph extension instead of graph generation. We propose the first model for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in accuracy over the state-of-the-art between 13 and 24 percentage points. Furthermore, we introduce a novel data set from the biomedical domain which has much larger linguistic variability and more complex graphs than the scene graph modification data sets. For this data set, the state-of-the art fails to generalize, while our model can produce meaningful predictions",
    "checked": true,
    "id": "47f3f165f05d9cd2eca29d1559cb65824641db6b",
    "semantic_title": "extend, don't rebuild: phrasing conditional graph modification as autoregressive sequence labelling",
    "citation_count": 4,
    "authors": [
      "Leon Weber",
      "Jannes Münchmeyer",
      "Samuele Garda",
      "Ulf Leser"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.94": {
    "title": "Zero-Shot Information Extraction as a Unified Text-to-Triple Translation",
    "volume": "main",
    "abstract": "We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set",
    "checked": true,
    "id": "baa8b6fc12a2b410c69c2621dada73339b96da48",
    "semantic_title": "zero-shot information extraction as a unified text-to-triple translation",
    "citation_count": 36,
    "authors": [
      "Chenguang Wang",
      "Xiao Liu",
      "Zui Chen",
      "Haoyun Hong",
      "Jie Tang",
      "Dawn Song"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.95": {
    "title": "Learning Logic Rules for Document-Level Relation Extraction",
    "volume": "main",
    "abstract": "Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats logic rules as latent variables and consists of two modules: a rule generator and a relation extractor. The rule generator is to generate logic rules potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated logic rules. Those two modules can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing logic rules into neural networks, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that significantly outperforms several strong baselines in terms of relation performance and logical consistency. Our code is available at https://github.com/rudongyu/LogiRE",
    "checked": true,
    "id": "c05a7fbf8d6cf5d631b323cdd734309c3e296801",
    "semantic_title": "learning logic rules for document-level relation extraction",
    "citation_count": 37,
    "authors": [
      "Dongyu Ru",
      "Changzhi Sun",
      "Jiangtao Feng",
      "Lin Qiu",
      "Hao Zhou",
      "Weinan Zhang",
      "Yong Yu",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.96": {
    "title": "A Large-Scale Dataset for Empathetic Response Generation",
    "volume": "main",
    "abstract": "Recent development in NLP shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where emotion plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a large-scale silver dataset consisting of 1M dialogues annotated with 32 fine-grained emotions, eight empathetic response intents, and the Neutral category. To achieve this goal, we have developed a novel data curation pipeline starting with a small seed of manually annotated data and eventually scaling it to a satisfactory size. We compare its quality against a state-of-the-art gold dataset using both offline experiments and visual validation methods. The resultant procedure can be used to create similar datasets in the same domain as well as in other domains",
    "checked": true,
    "id": "bcec6078c01f8800b5b301f99a50baaa23be2b72",
    "semantic_title": "a large-scale dataset for empathetic response generation",
    "citation_count": 58,
    "authors": [
      "Anuradha Welivita",
      "Yubo Xie",
      "Pearl Pu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.97": {
    "title": "The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation",
    "volume": "main",
    "abstract": "Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text",
    "checked": true,
    "id": "d706645fbbc6edfad5fb642b1dfc3019fcabbd99",
    "semantic_title": "the perils of using mechanical turk to evaluate open-ended text generation",
    "citation_count": 113,
    "authors": [
      "Marzena Karpinska",
      "Nader Akoury",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.98": {
    "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus",
    "volume": "main",
    "abstract": "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet",
    "checked": true,
    "id": "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3",
    "semantic_title": "documenting large webtext corpora: a case study on the colossal clean crawled corpus",
    "citation_count": 467,
    "authors": [
      "Jesse Dodge",
      "Maarten Sap",
      "Ana Marasović",
      "William Agnew",
      "Gabriel Ilharco",
      "Dirk Groeneveld",
      "Margaret Mitchell",
      "Matt Gardner"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.99": {
    "title": "AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages",
    "volume": "main",
    "abstract": "Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages",
    "checked": true,
    "id": "ab364a00a356e5cb8c9e9b144b57e0517bce56f6",
    "semantic_title": "afromt: pretraining strategies and reproducible benchmarks for translation of 8 african languages",
    "citation_count": 33,
    "authors": [
      "Machel Reid",
      "Junjie Hu",
      "Graham Neubig",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.100": {
    "title": "Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer",
    "volume": "main",
    "abstract": "While the field of style transfer (ST) has been growing rapidly, it has been hampered by a lack of standardized practices for automatic evaluation. In this paper, we evaluate leading automatic metrics on the oft-researched task of formality style transfer. Unlike previous evaluations, which focus solely on English, we expand our focus to Brazilian-Portuguese, French, and Italian, making this work the first multilingual evaluation of metrics in ST. We outline best practices for automatic evaluation in (formality) style transfer and identify several models that correlate well with human judgments and are robust across languages. We hope that this work will help accelerate development in ST, where human evaluation is often challenging to collect",
    "checked": true,
    "id": "5bc5ac4f2e6c34e92b6160545e638d0e4dded78e",
    "semantic_title": "evaluating the evaluation metrics for style transfer: a case study in multilingual formality transfer",
    "citation_count": 31,
    "authors": [
      "Eleftheria Briakou",
      "Sweta Agrawal",
      "Joel Tetreault",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.101": {
    "title": "MS-Mentions: Consistently Annotating Entity Mentions in Materials Science Procedural Text",
    "volume": "main",
    "abstract": "Material science synthesis procedures are a promising domain for scientific NLP, as proper modeling of these recipes could provide insight into new ways of creating materials. However, a fundamental challenge in building information extraction models for material science synthesis procedures is getting accurate labels for the materials, operations, and other entities of those procedures. We present a new corpus of entity mention annotations over 595 Material Science synthesis procedural texts (157,488 tokens), which greatly expands the training data available for the Named Entity Recognition task. We outline a new label inventory designed to provide consistent annotations and a new annotation approach intended to maximize the consistency and annotation speed of domain experts. Inter-annotator agreement studies and baseline models trained upon the data suggest that the corpus provides high-quality annotations of these mention types. This corpus helps lay a foundation for future high-quality modeling of synthesis procedures",
    "checked": true,
    "id": "9c403ca58853fbb223f6e9fce446bb638f291692",
    "semantic_title": "ms-mentions: consistently annotating entity mentions in materials science procedural text",
    "citation_count": 15,
    "authors": [
      "Tim O’Gorman",
      "Zach Jensen",
      "Sheshera Mysore",
      "Kevin Huang",
      "Rubayyat Mahbub",
      "Elsa Olivetti",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.102": {
    "title": "Understanding Politics via Contextualized Discourse Processing",
    "volume": "main",
    "abstract": "Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events. These representations are contextualized by tweets, press releases, issues, news articles, and participating entities. Our model processes several documents at once and generates composed representations for multiple entities over several issues or events. Via qualitative and quantitative empirical analysis, we show that these representations are meaningful and effective",
    "checked": true,
    "id": "16d8dc4a40a3928d637fd943c64fed2ae727ff48",
    "semantic_title": "understanding politics via contextualized discourse processing",
    "citation_count": 20,
    "authors": [
      "Rajkumar Pujari",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.103": {
    "title": "Conundrums in Event Coreference Resolution: Making Sense of the State of the Art",
    "volume": "main",
    "abstract": "Despite recent promising results on the application of span-based models for event reference interpretation, there is a lack of understanding of what has been improved. We present an empirical analysis of a state-of-the-art span-based event reference systems with the goal of providing the general NLP audience with a better understanding of the state of the art and reference researchers with directions for future research",
    "checked": true,
    "id": "66e4837d4c6089ba65348dcaa7fa40c4111245c0",
    "semantic_title": "conundrums in event coreference resolution: making sense of the state of the art",
    "citation_count": 21,
    "authors": [
      "Jing Lu",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.104": {
    "title": "Weakly supervised discourse segmentation for multiparty oral conversations",
    "volume": "main",
    "abstract": "Discourse segmentation, the first step of discourse analysis, has been shown to improve results for text summarization, translation and other NLP tasks. While segmentation models for written text tend to perform well, they are not directly applicable to spontaneous, oral conversation, which has linguistic features foreign to written text. Segmentation is less studied for this type of language, where annotated data is scarce, and existing corpora more heterogeneous. We develop a weak supervision approach to adapt, using minimal annotation, a state of the art discourse segmenter trained on written text to French conversation transcripts. Supervision is given by a latent model bootstrapped by manually defined heuristic rules that use linguistic and acoustic information. The resulting model improves the original segmenter, especially in contexts where information on speaker turns is lacking or noisy, gaining up to 13% in F-score. Evaluation is performed on data like those used to define our heuristic rules, but also on transcripts from two other corpora",
    "checked": true,
    "id": "abec91d25bb86ec1d033b08f78300982ee8577ca",
    "semantic_title": "weakly supervised discourse segmentation for multiparty oral conversations",
    "citation_count": 5,
    "authors": [
      "Lila Gravellier",
      "Julie Hunter",
      "Philippe Muller",
      "Thomas Pellegrini",
      "Isabelle Ferrané"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.105": {
    "title": "Narrative Embedding: Re-Contextualization Through Attention",
    "volume": "main",
    "abstract": "Narrative analysis is becoming increasingly important for a number of linguistic tasks including summarization, knowledge extraction, and question answering. We present a novel approach for narrative event representation using attention to re-contextualize events across the whole story. Comparing to previous analysis we find an unexpected attachment of event semantics to predicate tokens within a popular transformer model. We test the utility of our approach on narrative completion prediction, achieving state of the art performance on Multiple Choice Narrative Cloze and scoring competitively on the Story Cloze Task",
    "checked": true,
    "id": "139840d3767787035d698d6c2f8df323acdd909d",
    "semantic_title": "narrative embedding: re-contextualization through attention",
    "citation_count": 4,
    "authors": [
      "Sean Wilner",
      "Daniel Woolridge",
      "Madeleine Glick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.106": {
    "title": "Focus on what matters: Applying Discourse Coherence Theory to Cross Document Coreference",
    "volume": "main",
    "abstract": "Performing event and entity coreference resolution across documents vastly increases the number of candidate mentions, making it intractable to do the full n2 pairwise comparisons. Existing approaches simplify by considering coreference only within document clusters, but this fails to handle inter-cluster coreference, common in many applications. As a result cross-document coreference algorithms are rarely applied to downstream tasks. We draw on an insight from discourse coherence theory: potential coreferences are constrained by the reader's discourse focus. We model the entities/events in a reader's focus as a neighborhood within a learned latent embedding space which minimizes the distance between mentions and the centroids of their gold coreference clusters. We then use these neighborhoods to sample only hard negatives to train a fine-grained classifier on mention pairs and their local discourse features. Our approach achieves state-of-the-art results for both events and entities on the ECB+, Gun Violence, Football Coreference, and Cross-Domain Cross-Document Coreference corpora. Furthermore, training on multiple corpora improves average performance across all datasets by 17.2 F1 points, leading to a robust coreference resolution model that is now feasible to apply to downstream tasks",
    "checked": true,
    "id": "9d8789537259e60929ab3201e114ff2f46bd20c0",
    "semantic_title": "focus on what matters: applying discourse coherence theory to cross document coreference",
    "citation_count": 21,
    "authors": [
      "William Held",
      "Dan Iter",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.107": {
    "title": "Salience-Aware Event Chain Modeling for Narrative Understanding",
    "volume": "main",
    "abstract": "Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction remains a challenging problem. We posit that this is due to the nature of the texts from which chains are discovered. Natural language text interleaves a narrative of concrete, salient events with background information, contextualization, opinion, and other elements that are important for a variety of necessary discourse and pragmatics acts but are not part of the principal chain of events being communicated. We introduce methods for extracting this principal chain from natural language text, by filtering away non-salient events and supportive sentences. We demonstrate the effectiveness of our methods at isolating critical event chains by comparing their effect on downstream tasks. We show that by pre-training large language models on our extracted chains, we obtain improvements in two tasks that benefit from a clear understanding of event chains: narrative prediction and event-based temporal question answering. The demonstrated improvements and ablative studies confirm that our extraction method isolates critical event chains",
    "checked": true,
    "id": "9f9667c06c94cc854eb55f16cfad2b9db7da49cd",
    "semantic_title": "salience-aware event chain modeling for narrative understanding",
    "citation_count": 25,
    "authors": [
      "Xiyang Zhang",
      "Muhao Chen",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.108": {
    "title": "Asking It All: Generating Contextualized Questions for any Semantic Role",
    "volume": "main",
    "abstract": "Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We develop a two-stage model for this task, which first produces a context-independent question prototype for each role and then revises it to be contextually appropriate for the passage. Unlike most existing approaches to question generation, our approach does not require conditioning on existing answers in the text. Instead, we condition on the type of information to inquire about, regardless of whether the answer appears explicitly in the text, could be inferred from it, or should be sought elsewhere. Our evaluation demonstrates that we generate diverse and well-formed questions for a large, broad-coverage ontology of predicates and roles",
    "checked": true,
    "id": "601398838250a4e69c69cc339d65f5c51e727ad1",
    "semantic_title": "asking it all: generating contextualized questions for any semantic role",
    "citation_count": 36,
    "authors": [
      "Valentina Pyatkin",
      "Paul Roit",
      "Julian Michael",
      "Yoav Goldberg",
      "Reut Tsarfaty",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.109": {
    "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
    "volume": "main",
    "abstract": "Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during \"identity fine-tuning\". We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders",
    "checked": true,
    "id": "2435c04832d486975304a094e55ecbab8acf8a5f",
    "semantic_title": "fast, effective, and self-supervised: transforming masked language models into universal lexical and sentence encoders",
    "citation_count": 122,
    "authors": [
      "Fangyu Liu",
      "Ivan Vulić",
      "Anna Korhonen",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.110": {
    "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models",
    "volume": "main",
    "abstract": "While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets",
    "checked": true,
    "id": "e55391a9406245584b3e5b3225dad2e171b9a06b",
    "semantic_title": "rulebert: teaching soft rules to pre-trained language models",
    "citation_count": 33,
    "authors": [
      "Mohammed Saeed",
      "Naser Ahmadi",
      "Preslav Nakov",
      "Paolo Papotti"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.111": {
    "title": "Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?",
    "volume": "main",
    "abstract": "In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use fine-tuning on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage",
    "checked": true,
    "id": "db742f7c4f7edd24e77482560df7d09c9033b3da",
    "semantic_title": "stepmothers are mean and academics are pretentious: what do pretrained language models learn about you?",
    "citation_count": 29,
    "authors": [
      "Rochelle Choenni",
      "Ekaterina Shutova",
      "Robert van Rooij"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.112": {
    "title": "ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension",
    "volume": "main",
    "abstract": "Supervised systems have nowadays become the standard recipe for Word Sense Disambiguation (WSD), with Transformer-based language models as their primary ingredient. However, while these systems have certainly attained unprecedented performances, virtually all of them operate under the constraining assumption that, given a context, each word can be disambiguated individually with no account of the other sense choices. To address this limitation and drop this assumption, we propose CONtinuous SEnse Comprehension (ConSeC), a novel approach to WSD: leveraging a recent re-framing of this task as a text extraction problem, we adapt it to our formulation and introduce a feedback loop strategy that allows the disambiguation of a target word to be conditioned not only on its context but also on the explicit senses assigned to nearby words. We evaluate ConSeC and examine how its components lead it to surpass all its competitors and set a new state of the art on English WSD. We also explore how ConSeC fares in the cross-lingual setting, focusing on 8 languages with various degrees of resource availability, and report significant improvements over prior systems. We release our code at https://github.com/SapienzaNLP/consec",
    "checked": true,
    "id": "ed96071efbb223764f104fc003d85f45517176c8",
    "semantic_title": "consec: word sense disambiguation as continuous sense comprehension",
    "citation_count": 70,
    "authors": [
      "Edoardo Barba",
      "Luigi Procopio",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.113": {
    "title": "Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning",
    "volume": "main",
    "abstract": "Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even matching or surpassing human performance in some benchmarks. Recently, some of these advances have been called into question: so called data artifacts in the training data have been made evident as spurious correlations and shallow shortcuts that in some cases are leveraging these outstanding results. In this paper we seek to further pursue this analysis into the realm of commonsense related language processing tasks. We undertake a study on different prominent benchmarks that involve commonsense reasoning, along a number of key stress experiments, thus seeking to gain insight on whether the models are learning transferable generalizations intrinsic to the problem at stake or just taking advantage of incidental shortcuts in the data items. The results obtained indicate that most datasets experimented with are problematic, with models resorting to non-robust features and appearing not to be learning and generalizing towards the overall tasks intended to be conveyed or exemplified by the datasets",
    "checked": true,
    "id": "5b91b9b3a28cd774fcacad2f21130fe731bddb41",
    "semantic_title": "shortcutted commonsense: data spuriousness in deep learning of commonsense reasoning",
    "citation_count": 51,
    "authors": [
      "Ruben Branco",
      "António Branco",
      "João António Rodrigues",
      "João Ricardo Silva"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.114": {
    "title": "When differential privacy meets NLP: The devil is in the detail",
    "volume": "main",
    "abstract": "Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users' original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder's dimension and that the amount of utterances that are not privatized could easily reach 100% of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in NLP rely on formal guarantees, these should be outlined in full and put under detailed scrutiny",
    "checked": true,
    "id": "2fb44f1317bc51a1e011a5a44d817ad9104e29e8",
    "semantic_title": "when differential privacy meets nlp: the devil is in the detail",
    "citation_count": 36,
    "authors": [
      "Ivan Habernal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.115": {
    "title": "Achieving Model Robustness through Discrete Adversarial Training",
    "volume": "main",
    "abstract": "Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving robustness has been limited to offline augmentation only. Concretely, given a trained model, attacks are used to generate perturbed (adversarial) examples, and the model is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every training step, adapting to the changing nature of the model. We propose (i) a new discrete attack, based on best-first search, and (ii) random sampling attacks that unlike prior work are not based on expensive search-based procedures. Surprisingly, we find that random sampling leads to impressive gains in robustness, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher training cost, significantly improving robustness on three datasets. Last, we show that our new attack substantially improves robustness compared to prior methods",
    "checked": true,
    "id": "146f6a424ccadfbccd48ab5488f52935d4a677b9",
    "semantic_title": "achieving model robustness through discrete adversarial training",
    "citation_count": 29,
    "authors": [
      "Maor Ivgi",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.116": {
    "title": "Debiasing Methods in Natural Language Understanding Make Bias More Accessible",
    "volume": "main",
    "abstract": "Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model's inner representations. We propose a general probing-based framework that allows for post-hoc interpretation of biases in language models, and use an information-theoretic approach to measure the extractability of certain biases from the model's representations. We experiment with several NLU datasets and known biases, and show that, counter-intuitively, the more a language model is pushed towards a debiased regime, the more bias is actually encoded in its inner representations",
    "checked": true,
    "id": "10bc2ba3533bca85b75cb09dcc100809fc3221ea",
    "semantic_title": "debiasing methods in natural language understanding make bias more accessible",
    "citation_count": 24,
    "authors": [
      "Michael Mendelson",
      "Yonatan Belinkov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.117": {
    "title": "Evaluating the Robustness of Neural Language Models to Input Perturbations",
    "volume": "main",
    "abstract": "High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems' robustness",
    "checked": true,
    "id": "3b451fa663704f927e1ec602d7c0845a9826922d",
    "semantic_title": "evaluating the robustness of neural language models to input perturbations",
    "citation_count": 107,
    "authors": [
      "Milad Moradi",
      "Matthias Samwald"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.118": {
    "title": "How much pretraining data do language models need to learn syntax?",
    "volume": "main",
    "abstract": "Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications: part-of-speech tagging, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost",
    "checked": true,
    "id": "4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c",
    "semantic_title": "how much pretraining data do language models need to learn syntax?",
    "citation_count": 32,
    "authors": [
      "Laura Pérez-Mayos",
      "Miguel Ballesteros",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.119": {
    "title": "Sorting through the noise: Testing robustness of information processing in pre-trained language models",
    "volume": "main",
    "abstract": "Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by examining robustness of models' ability to deploy relevant context information in the face of distracting content. We present models with cloze tasks requiring use of critical context information, and introduce distracting content to test how robustly the models retain and use that critical information for prediction. We also systematically manipulate the nature of these distractors, to shed light on dynamics of models' use of contextual cues. We find that although models appear in simple contexts to make predictions based on understanding and applying relevant facts from prior context, the presence of distracting but irrelevant content has clear impact in confusing model predictions. In particular, models appear particularly susceptible to factors of semantic similarity and word position. The findings are consistent with the conclusion that LM predictions are driven in large part by superficial contextual cues, rather than by robust representations of context meaning",
    "checked": true,
    "id": "0981ce872d31a665882e7677d608351ff5f1de6b",
    "semantic_title": "sorting through the noise: testing robustness of information processing in pre-trained language models",
    "citation_count": 38,
    "authors": [
      "Lalchand Pandia",
      "Allyson Ettinger"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.120": {
    "title": "Contrastive Explanations for Model Interpretability",
    "volume": "main",
    "abstract": "Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions",
    "checked": true,
    "id": "5c599dc162bfd33abf390ba00474453b54ddf60f",
    "semantic_title": "contrastive explanations for model interpretability",
    "citation_count": 99,
    "authors": [
      "Alon Jacovi",
      "Swabha Swayamdipta",
      "Shauli Ravfogel",
      "Yanai Elazar",
      "Yejin Choi",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.121": {
    "title": "On the Transferability of Adversarial Attacks against Neural Text Classifier",
    "volume": "main",
    "abstract": "Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one model can fool another model. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including network architecture, tokenization scheme, word embedding, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a genetic algorithm to find an ensemble of models that can be used to induce adversarial examples to fool almost all existing models. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples",
    "checked": true,
    "id": "a10b3deb75cb6a20a4561966881d688ce199beb5",
    "semantic_title": "on the transferability of adversarial attacks against neural text classifier",
    "citation_count": 26,
    "authors": [
      "Liping Yuan",
      "Xiaoqing Zheng",
      "Yi Zhou",
      "Cho-Jui Hsieh",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.122": {
    "title": "Conditional probing: measuring usable information beyond a baseline",
    "volume": "main",
    "abstract": "Probing experiments investigate the extent to which neural representations make properties—like part-of-speech—predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word embeddings. Instead of using baselines as a point of comparison, we're interested in measuring information that is contained in the representation but not in the baseline. For example, current methods can detect when a representation is more useful than the word identity (a baseline) for predicting part-of-speech; however, they cannot detect when the representation is predictive of just the aspects of part-of-speech not explainable by the word identity. In this work, we extend a theory of usable information called V-information and propose conditional probing, which explicitly conditions on the information in the baseline. In a case study, we find that after conditioning on non-contextual word embeddings, properties like part-of-speech are accessible at deeper layers of a network than previously thought",
    "checked": true,
    "id": "dffb0f028298ca2017255090ebd3453dce6e9ee0",
    "semantic_title": "conditional probing: measuring usable information beyond a baseline",
    "citation_count": 58,
    "authors": [
      "John Hewitt",
      "Kawin Ethayarajh",
      "Percy Liang",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.123": {
    "title": "GFST: Gender-Filtered Self-Training for More Accurate Gender in Translation",
    "volume": "main",
    "abstract": "Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate GFST on translation from English into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of GFST on several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation",
    "checked": true,
    "id": "f01ee20ea9f5bb25a843a8f51834d1aa29b532d2",
    "semantic_title": "gfst: gender-filtered self-training for more accurate gender in translation",
    "citation_count": 15,
    "authors": [
      "Prafulla Kumar Choubey",
      "Anna Currey",
      "Prashant Mathur",
      "Georgiana Dinu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.124": {
    "title": "Wikily\" Supervised Neural Translation Tailored to Cross-Lingual Tasks",
    "volume": "main",
    "abstract": "We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily translation models to unsupervised image captioning, and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a wikily translation of the English captioning data. Our captioning results on Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as an artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers",
    "checked": true,
    "id": "33e486f1dc63244ae1bdd83be89379b7c8c846a2",
    "semantic_title": "wikily\" supervised neural translation tailored to cross-lingual tasks",
    "citation_count": 6,
    "authors": [
      "Mohammad Sadegh Rasooli",
      "Chris Callison-Burch",
      "Derry Tanti Wijaya"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.125": {
    "title": "mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs",
    "volume": "main",
    "abstract": "Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, machine translation, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, named entity recognition, question answering, and abstractive summarization. Experimental results show that the proposed mT6 improves cross-lingual transferability over mT5",
    "checked": true,
    "id": "279a19b9eba7afd513394c7a733834b0f41f97fb",
    "semantic_title": "mt6: multilingual pretrained text-to-text transformer with translation pairs",
    "citation_count": 74,
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Shuming Ma",
      "Shaohan Huang",
      "Saksham Singhal",
      "Xian-Ling Mao",
      "Heyan Huang",
      "Xia Song",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.126": {
    "title": "Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training",
    "volume": "main",
    "abstract": "Pre-trained multilingual language encoders, such as multilingual BERT and XLM-R, show great potential for zero-shot cross-lingual transfer. However, these multilingual encoders do not precisely align words and phrases across languages. Especially, learning alignments in the multilingual embedding space usually requires sentence-level or word-level parallel corpora, which are expensive to be obtained for low-resource languages. An alternative is to make the multilingual encoders more robust; when fine-tuning the encoder using downstream task, we train the encoder to tolerate noise in the contextual embedding spaces such that even if the representations of different languages are not aligned well, the model can still achieve good performance on zero-shot cross-lingual transfer. In this work, we propose a learning strategy for training robust models by drawing connections between adversarial examples and the failure cases of zero-shot cross-lingual transfer. We adopt two widely used robust training methods, adversarial training and randomized smoothing, to train the desired robust model. The experimental results demonstrate that robust training improves zero-shot cross-lingual transfer on text classification tasks. The improvement is more significant in the generalized cross-lingual transfer setting, where the pair of input sentences belong to two different languages",
    "checked": true,
    "id": "c45c627aa40a982788e1a1e3b1b2a2091b2a71d0",
    "semantic_title": "improving zero-shot cross-lingual transfer learning via robust training",
    "citation_count": 35,
    "authors": [
      "Kuan-Hao Huang",
      "Wasi Ahmad",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.127": {
    "title": "Speechformer: Reducing Information Loss in Direct Speech Translation",
    "volume": "main",
    "abstract": "Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer's quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful linguistic information is not accessible to higher-level layers in the architecture. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial lossy compression and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (en→de/es/nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario",
    "checked": true,
    "id": "a8e61923b1404eb7ae286e4ec407a3e3f925f788",
    "semantic_title": "speechformer: reducing information loss in direct speech translation",
    "citation_count": 24,
    "authors": [
      "Sara Papi",
      "Marco Gaido",
      "Matteo Negri",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.128": {
    "title": "Is \"moby dick\" a Whale or a Bird? Named Entities and Terminology in Speech Translation",
    "volume": "main",
    "abstract": "Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected them, also due to the dearth of publicly available resources tailored to their specific evaluation. To fill this gap, we i) present the first systematic analysis of the behavior of state-of-the-art ST systems in translating NEs and terminology, and ii) release NEuRoparl-ST, a novel benchmark built from European Parliament speeches annotated with NEs and terminology. Our experiments on the three language directions covered by our benchmark (en→es/fr/it) show that ST systems correctly translate 75–80% of terms and 65–70% of NEs, with very low performance (37–40%) on person names",
    "checked": true,
    "id": "96d9badfc79c8c083c2d091ebaed2f2bb4ca6789",
    "semantic_title": "is \"moby dick\" a whale or a bird? named entities and terminology in speech translation",
    "citation_count": 10,
    "authors": [
      "Marco Gaido",
      "Susana Rodríguez",
      "Matteo Negri",
      "Luisa Bentivogli",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.129": {
    "title": "HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints",
    "volume": "main",
    "abstract": "Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don't filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: Hindi,Gujarati,Tamil-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings",
    "checked": true,
    "id": "0f968312b389bf47b41b3514686823d2b7239b3f",
    "semantic_title": "hintedbt: augmenting back-translation with quality and transliteration hints",
    "citation_count": 10,
    "authors": [
      "Sahana Ramnath",
      "Melvin Johnson",
      "Abhirut Gupta",
      "Aravindan Raghuveer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.130": {
    "title": "Translation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation",
    "volume": "main",
    "abstract": "In simultaneous machine translation, finding an agent with the optimal action sequence of reads and writes that maintain a high level of translation quality while minimizing the average lag in producing target tokens remains an extremely challenging problem. We propose a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences. These oracle sequences can then be used to train a supervised model for action generation at inference time. Our approach provides an alternative to current heuristic methods in simultaneous translation by introducing a new training objective, which is easier to train than previous attempts at training the agent using reinforcement learning techniques for this task. Our experimental results show that our novel training method for action generation produces much higher quality translations while minimizing the average lag in simultaneous translation",
    "checked": true,
    "id": "6df3e69cd4cda0a48fa8819196fed9061855ce79",
    "semantic_title": "translation-based supervision for policy generation in simultaneous neural machine translation",
    "citation_count": 11,
    "authors": [
      "Ashkan Alinejad",
      "Hassan S. Shavarani",
      "Anoop Sarkar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.131": {
    "title": "Nearest Neighbour Few-Shot Learning for Cross-lingual Classification",
    "volume": "main",
    "abstract": "Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have led to significant performance gains on a wide range of cross-lingual NLP tasks, success on many downstream tasks still relies on the availability of sufficient annotated data. Traditional fine-tuning of pre-trained models using only a few target samples can cause over-fitting. This can be quite limiting as most languages in the world are under-resourced. In this work, we investigate cross-lingual adaptation using a simple nearest-neighbor few-shot (<15 samples) inference technique for classification tasks. We experiment using a total of 16 distinct languages across two NLP tasks- XNLI and PAWS-X. Our approach consistently improves traditional fine-tuning using only a handful of labeled samples in target locales. We also demonstrate its generalization capability across tasks",
    "checked": true,
    "id": "4137ee5f7e831b77dcc5662dec83780b52d8adab",
    "semantic_title": "nearest neighbour few-shot learning for cross-lingual classification",
    "citation_count": 14,
    "authors": [
      "M Saiful Bari",
      "Batool Haider",
      "Saab Mansour"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.132": {
    "title": "Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation",
    "volume": "main",
    "abstract": "We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead",
    "checked": true,
    "id": "3e32139deb17761a25075f8839daa61ad5992fc9",
    "semantic_title": "cross-attention is all you need: adapting pretrained transformers for machine translation",
    "citation_count": 121,
    "authors": [
      "Mozhdeh Gheini",
      "Xiang Ren",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.133": {
    "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent",
    "volume": "main",
    "abstract": "The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (ℓ2 norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such \"saturated\" networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers",
    "checked": true,
    "id": "f10a04a77fd1cd719792de374a60f3fd03f6b944",
    "semantic_title": "effects of parameter norm growth during transformer training: inductive bias from gradient descent",
    "citation_count": 38,
    "authors": [
      "William Merrill",
      "Vivek Ramanujan",
      "Yoav Goldberg",
      "Roy Schwartz",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.134": {
    "title": "Foreseeing the Benefits of Incidental Supervision",
    "volume": "main",
    "abstract": "Real-world applications often require improved models by leveraging *a range of cheap incidental supervision signals*. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations – all having statistical associations with gold annotations but not exactly the same. However, we currently lack a principled way to measure the benefits of these signals to a given target task, and the common practice of evaluating these benefits is through exhaustive experiments with various models and hyperparameters. This paper studies whether we can, *in a single framework, quantify the benefits of various types of incidental signals for a given target task without going through combinatorial experiments*. We propose a unified PAC-Bayesian motivated informativeness measure, PABI, that characterizes the uncertainty reduction provided by incidental supervision signals. We demonstrate PABI's effectiveness by quantifying the value added by various types of incidental signals to sequence tagging tasks. Experiments on named entity recognition (NER) and question answering (QA) show that PABI's predictions correlate well with learning performance, providing a promising way to determine, ahead of learning, which supervision signals would be beneficial",
    "checked": true,
    "id": "0cdc27a99c1520c2ec604b97470ae75227e096ee",
    "semantic_title": "foreseeing the benefits of incidental supervision",
    "citation_count": 11,
    "authors": [
      "Hangfeng He",
      "Mingyuan Zhang",
      "Qiang Ning",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.135": {
    "title": "Competency Problems: On Finding and Removing Artifacts in Language Data",
    "volume": "main",
    "abstract": "Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have \"spurious\" instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word \"amazing\" on its own should not give information about a sentiment label independent of the context in which it appears, which could include negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of creating data for competency problems when human bias is taken into account, showing that realistic datasets will increasingly deviate from competency problems as dataset size increases. This analysis gives us a simple statistical test for dataset artifacts, which we use to show more subtle biases than were described in prior work, including demonstrating that models are inappropriately affected by these less extreme biases. Our theoretical treatment of this problem also allows us to analyze proposed solutions, such as making local edits to dataset instances, and to give recommendations for future data collection and model design efforts that target competency problems",
    "checked": true,
    "id": "023fc86c932fbc36702a6ad11c94ba419e1d8d88",
    "semantic_title": "competency problems: on finding and removing artifacts in language data",
    "citation_count": 112,
    "authors": [
      "Matt Gardner",
      "William Merrill",
      "Jesse Dodge",
      "Matthew Peters",
      "Alexis Ross",
      "Sameer Singh",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.136": {
    "title": "Knowledge-Aware Meta-learning for Low-Resource Text Classification",
    "volume": "main",
    "abstract": "Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new task. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when they are not well-supported by training tasks. This paper studies a low-resource text classification problem and bridges the gap between meta-training and meta-testing tasks by leveraging the external knowledge bases. Specifically, we propose KGML to introduce additional representation for each sentence learned from the extracted sentence-specific knowledge graph. The extensive experiments on three datasets demonstrate the effectiveness of KGML under both supervised adaptation and unsupervised adaptation settings",
    "checked": true,
    "id": "10ad8ec872bbee0ddafe67dc8e5154f79a88c802",
    "semantic_title": "knowledge-aware meta-learning for low-resource text classification",
    "citation_count": 11,
    "authors": [
      "Huaxiu Yao",
      "Ying-xin Wu",
      "Maruan Al-Shedivat",
      "Eric Xing"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.137": {
    "title": "Sentence Bottleneck Autoencoders from Transformer Language Models",
    "volume": "main",
    "abstract": "Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction. Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder. We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models",
    "checked": true,
    "id": "071b1c7bb2f45011967bc08de32ee21d409e02de",
    "semantic_title": "sentence bottleneck autoencoders from transformer language models",
    "citation_count": 29,
    "authors": [
      "Ivan Montero",
      "Nikolaos Pappas",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.138": {
    "title": "Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning",
    "volume": "main",
    "abstract": "We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After data augmentation is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our model outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70% of computational memory compared to the baseline model",
    "checked": true,
    "id": "82c6310c3b57d8120f35b7a6b6e8779f3987a1ef",
    "semantic_title": "efficient contrastive learning via novel data augmentation and curriculum learning",
    "citation_count": 22,
    "authors": [
      "Seonghyeon Ye",
      "Jiseon Kim",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.139": {
    "title": "CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation",
    "volume": "main",
    "abstract": "Growing interests have been attracted in Conversational Recommender Systems (CRS), which explore user preference through conversational interactions in order to make appropriate recommendation. However, there is still a lack of ability in existing CRS to (1) traverse multiple reasoning paths over background knowledge to introduce relevant items and attributes, and (2) arrange selected entities appropriately under current system intents to control response generation. To address these issues, we propose CR-Walker in this paper, a model that performs tree-structured reasoning on a knowledge graph, and generates informative dialog acts to guide language generation. The unique scheme of tree-structured reasoning views the traversed entity at each hop as part of dialog acts to facilitate language generation, which links how entities are selected and expressed. Automatic and human evaluations show that CR-Walker can arrive at more accurate recommendation, and generate more informative and engaging responses",
    "checked": true,
    "id": "f368d63bae659ebd887af07db6b72d0951355f1d",
    "semantic_title": "cr-walker: tree-structured graph reasoning and dialog acts for conversational recommendation",
    "citation_count": 62,
    "authors": [
      "Wenchang Ma",
      "Ryuichi Takanobu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.140": {
    "title": "DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization",
    "volume": "main",
    "abstract": "Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts",
    "checked": true,
    "id": "4979495965d1a2239082b4b9fd5f335a288309b7",
    "semantic_title": "dialki: knowledge identification in conversational systems through dialogue-document contextualization",
    "citation_count": 31,
    "authors": [
      "Zeqiu Wu",
      "Bo-Ru Lu",
      "Hannaneh Hajishirzi",
      "Mari Ostendorf"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.141": {
    "title": "Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text",
    "volume": "main",
    "abstract": "Communicating with humans is challenging for AIs because it requires a shared understanding of the world, complex semantics (e.g., metaphors or analogies), and at times multi-modal gestures (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of Iconary, a collaborative game of drawing and guessing based on Pictionary, that poses a novel challenge for the research community. In Iconary, a Guesser tries to identify a phrase that a Drawer is drawing by composing icons, and the Drawer iteratively revises the drawing to help the Guesser in response. This back-and-forth often uses canonical scenes, visual metaphor, or icon compositions to express challenging words, making it an ideal test for mixing language and visual/symbolic communication in AI. We propose models to play Iconary and train them on over 55,000 games between human players. Our models are skillful players and are able to employ world knowledge in language models to play with words unseen during training",
    "checked": true,
    "id": "e3d5d70005bcc9d36dc84a969b92e6f7518defa9",
    "semantic_title": "iconary: a pictionary-based game for testing multimodal communication with drawings and text",
    "citation_count": 5,
    "authors": [
      "Christopher Clark",
      "Jordi Salvador",
      "Dustin Schwenk",
      "Derrick Bonafilia",
      "Mark Yatskar",
      "Eric Kolve",
      "Alvaro Herrasti",
      "Jonghyun Choi",
      "Sachin Mehta",
      "Sam Skjonsberg",
      "Carissa Schoenick",
      "Aaron Sarnat",
      "Hannaneh Hajishirzi",
      "Aniruddha Kembhavi",
      "Oren Etzioni",
      "Ali Farhadi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.142": {
    "title": "Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems",
    "volume": "main",
    "abstract": "As the labeling cost for different modules in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different modules with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available",
    "checked": true,
    "id": "89c352a0920ecf6ca29bf58a1e7ae459b655254f",
    "semantic_title": "self-training improves pre-training for few-shot learning in task-oriented dialog systems",
    "citation_count": 32,
    "authors": [
      "Fei Mi",
      "Wanhao Zhou",
      "Lingjing Kong",
      "Fengyu Cai",
      "Minlie Huang",
      "Boi Faltings"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.143": {
    "title": "Contextual Rephrase Detection for Reducing Friction in Dialogue Systems",
    "volume": "main",
    "abstract": "For voice assistants like Alexa, Google Assistant, and Siri, correctly interpreting users' intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or user errors such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users' implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multi-turn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including the user's implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models",
    "checked": true,
    "id": "cce608e7af52c9de4041fffccffbe11d08cacd8c",
    "semantic_title": "contextual rephrase detection for reducing friction in dialogue systems",
    "citation_count": 11,
    "authors": [
      "Zhuoyi Wang",
      "Saurabh Gupta",
      "Jie Hao",
      "Xing Fan",
      "Dingcheng Li",
      "Alexander Hanbo Li",
      "Chenlei Guo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.144": {
    "title": "Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning",
    "volume": "main",
    "abstract": "In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct self-supervised contrastive pre-training on collected intent datasets, which implicitly learns to discriminate semantically similar utterances without using any labels. We then perform few-shot intent detection together with supervised contrastive learning, which explicitly pulls utterances from the same intent closer and pushes utterances across different intents farther. Experimental results show that our proposed method achieves state-of-the-art performance on three challenging intent detection datasets under 5-shot and 10-shot settings",
    "checked": true,
    "id": "891b9d4b7e833e0fe69f55dd60f5521265208996",
    "semantic_title": "few-shot intent detection via contrastive pre-training and fine-tuning",
    "citation_count": 88,
    "authors": [
      "Jianguo Zhang",
      "Trung Bui",
      "Seunghyun Yoon",
      "Xiang Chen",
      "Zhiwei Liu",
      "Congying Xia",
      "Quan Hung Tran",
      "Walter Chang",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.145": {
    "title": "It doesn't look good for a date\": Transforming Critiques into Preferences for Conversational Recommendation Systems",
    "volume": "main",
    "abstract": "Conversations aimed at determining good recommendations are iterative in nature. People often express their preferences in terms of a critique of the current recommendation (e.g., \"It doesn't look good for a date\"), requiring some degree of common sense for a preference to be inferred. In this work, we present a method for transforming a user critique into a positive preference (e.g., \"I prefer more romantic\") in order to retrieve reviews pertaining to potentially better recommendations (e.g., \"Perfect for a romantic dinner\"). We leverage a large neural language model (LM) in a few-shot setting to perform critique-to-preference transformation, and we test two methods for retrieving recommendations: one that matches embeddings, and another that fine-tunes an LM for the task. We instantiate this approach in the restaurant domain and evaluate it using a new dataset of restaurant critiques. In an ablation study, we show that utilizing critique-to-preference transformation improves recommendations, and that there are at least three general cases that explain this improved performance",
    "checked": true,
    "id": "ab49cb6378c733b5be7e5d431e484f7d71203d91",
    "semantic_title": "it doesn't look good for a date\": transforming critiques into preferences for conversational recommendation systems",
    "citation_count": 3,
    "authors": [
      "Victor Bursztyn",
      "Jennifer Healey",
      "Nedim Lipka",
      "Eunyee Koh",
      "Doug Downey",
      "Larry Birnbaum"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.146": {
    "title": "AttentionRank: Unsupervised Keyphrase Extraction using Self and Cross Attentions",
    "volume": "main",
    "abstract": "Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention using a pre-trained language model. The self-attention is designed to determine the importance of a candidate within the context of a sentence. The cross-attention is calculated to identify the semantic relevance between a candidate and sentences within a document. We evaluate the AttentionRank on three publicly available datasets against seven baselines. The results show that the AttentionRank is an effective and robust unsupervised keyphrase extraction model on both long and short documents. Source code is available on Github",
    "checked": true,
    "id": "cd3b32e41791f74dd138909496b72e0a7132665c",
    "semantic_title": "attentionrank: unsupervised keyphrase extraction using self and cross attentions",
    "citation_count": 58,
    "authors": [
      "Haoran Ding",
      "Xiao Luo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.147": {
    "title": "Unsupervised Relation Extraction: A Variational Autoencoder Approach",
    "volume": "main",
    "abstract": "Unsupervised relation extraction works by clustering entity pairs that have the same relations in the text. Some existing variational autoencoder (VAE)-based approaches train the relation extraction model as an encoder that generates relation classifications. A decoder is trained along with the encoder to reconstruct the encoder input based on the encoder-generated relation classifications. These classifications are a latent variable so they are required to follow a pre-defined prior distribution which results in unstable training. We propose a VAE-based unsupervised relation extraction technique that overcomes this limitation by using the classifications as an intermediate variable instead of a latent variable. Specifically, classifications are conditioned on sentence input, while the latent variable is conditioned on both the classifications and the sentence input. This allows our model to connect the decoder with the encoder without putting restrictions on the classification distribution; which improves training stability. Our approach is evaluated on the NYT dataset and outperforms state-of-the-art methods",
    "checked": true,
    "id": "7f56df17229094f6f6c01499732aeef95c0fc725",
    "semantic_title": "unsupervised relation extraction: a variational autoencoder approach",
    "citation_count": 12,
    "authors": [
      "Chenhan Yuan",
      "Hoda Eldardiry"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.148": {
    "title": "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
    "volume": "main",
    "abstract": "Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to ‘fill' the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models",
    "checked": true,
    "id": "ef76276f9ef929496f03282fa85ae1bbcdc69767",
    "semantic_title": "robust retrieval augmented generation for zero-shot slot filling",
    "citation_count": 33,
    "authors": [
      "Michael Glass",
      "Gaetano Rossiello",
      "Md Faisal Mahbub Chowdhury",
      "Alfio Gliozzo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.149": {
    "title": "Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction",
    "volume": "main",
    "abstract": "Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically English. While the advance of pretrained multilingual encoders suggests an easy optimism of \"train on English, run on any language\", we find through a thorough exploration and extension of techniques that a combination of approaches, both new and old, leads to better performance than any one cross-lingual strategy in particular. We explore techniques including data projection and self-training, and how different pretrained encoders impact them. We use English-to-Arabic IE as our initial example, demonstrating strong performance in this setting for event extraction, named entity recognition, part-of-speech tagging, and dependency parsing. We then apply data projection and self-training to three tasks across eight target languages. Because no single set of techniques performs the best across all tasks, we encourage practitioners to explore various configurations of the techniques described in this work when seeking to improve on zero-shot training",
    "checked": true,
    "id": "c52c02859547d2f949a017cc434fbdc9c4c0cfa9",
    "semantic_title": "everything is all it takes: a multipronged strategy for zero-shot cross-lingual information extraction",
    "citation_count": 30,
    "authors": [
      "Mahsa Yarmohammadi",
      "Shijie Wu",
      "Marc Marone",
      "Haoran Xu",
      "Seth Ebner",
      "Guanghui Qin",
      "Yunmo Chen",
      "Jialiang Guo",
      "Craig Harman",
      "Kenton Murray",
      "Aaron Steven White",
      "Mark Dredze",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.150": {
    "title": "Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies",
    "volume": "main",
    "abstract": "Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information",
    "checked": true,
    "id": "e6a237ab883e503b10b73b3a411c0078c47c9830",
    "semantic_title": "harms of gender exclusivity and challenges in non-binary representation in language technologies",
    "citation_count": 167,
    "authors": [
      "Sunipa Dev",
      "Masoud Monajatipoor",
      "Anaelia Ovalle",
      "Arjun Subramonian",
      "Jeff Phillips",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.151": {
    "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search",
    "volume": "main",
    "abstract": "Internet search affects people's cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models",
    "checked": true,
    "id": "60c498956cb5737c4964aaca0b920592bd7f5689",
    "semantic_title": "are gender-neutral queries really gender-neutral? mitigating gender bias in image search",
    "citation_count": 98,
    "authors": [
      "Jialu Wang",
      "Yang Liu",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.152": {
    "title": "Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness",
    "volume": "main",
    "abstract": "Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in hiring decisions, regardless of whether hiring decisions are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features of human-generated text through style transfer, by automatically re-writing the text itself. Critically, our framework operationalizes the notion of obfuscated style in a flexible way that enables two distinct notions of obfuscated style: (1) a minimal notion that effectively intersects the various styles seen in training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all sensitive attributes to text, in effect, computing a union of styles. Our style-obfuscation framework can be used for multiple purposes, however, we demonstrate its effectiveness in improving the fairness of downstream classifiers. We also conduct a comprehensive study on style-pooling's effect on fluency, semantic consistency, and attribute removal from text, in two and three domain style transfer",
    "checked": true,
    "id": "1d7a01914b6ba54c3ca76ecee51edc1ec1cd6984",
    "semantic_title": "style pooling: automatic text style obfuscation for improved classification fairness",
    "citation_count": 13,
    "authors": [
      "Fatemehsadat Mireshghallah",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.153": {
    "title": "Modeling Disclosive Transparency in NLP Application Descriptions",
    "volume": "main",
    "abstract": "Broader disclosive transparency—truth and clarity in communication regarding the function of AI systems—is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where \"too much information\" clouds a reader's understanding of what a system description means. Disclosive transparency's subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between transparency, confusion, and user perceptions in a corpus of real NLP system descriptions",
    "checked": true,
    "id": "55c9c459a68ce955efa9b401850f4fce36240631",
    "semantic_title": "modeling disclosive transparency in nlp application descriptions",
    "citation_count": 7,
    "authors": [
      "Michael Saxon",
      "Sharon Levy",
      "Xinyi Wang",
      "Alon Albalak",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.154": {
    "title": "Reconstruction Attack on Instance Encoding for Language Understanding",
    "volume": "main",
    "abstract": "A private learning scheme TextHide was recently proposed to protect the private text data during the training phase via so-called instance encoding. We propose a novel reconstruction attack to break TextHide by recovering the private training data, and thus unveil the privacy risks of instance encoding. We have experimentally validated the effectiveness of the reconstruction attack with two commonly-used datasets for sentence classification. Our attack would advance the development of privacy preserving machine learning in the context of natural language processing",
    "checked": true,
    "id": "443ec1236a1e6dd95ae80ea8ff9ba67e85e4530a",
    "semantic_title": "reconstruction attack on instance encoding for language understanding",
    "citation_count": 11,
    "authors": [
      "Shangyu Xie",
      "Yuan Hong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.155": {
    "title": "Fairness-aware Class Imbalanced Learning",
    "volume": "main",
    "abstract": "Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases",
    "checked": true,
    "id": "8692a65498be3ff4e134a1874ca2dccc72c394af",
    "semantic_title": "fairness-aware class imbalanced learning",
    "citation_count": 29,
    "authors": [
      "Shivashankar Subramanian",
      "Afshin Rahimi",
      "Timothy Baldwin",
      "Trevor Cohn",
      "Lea Frermann"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.156": {
    "title": "CRYPTOGRU: Low Latency Privacy-Preserving Text Analysis With GRU",
    "volume": "main",
    "abstract": "Homomorphic encryption (HE) and garbled circuit (GC) provide the protection for users' privacy. However, simply mixing the HE and GC in RNN models suffer from long inference latency due to slow activation functions. In this paper, we present a novel hybrid structure of HE and GC gated recurrent unit (GRU) network, , for low-latency secure inferences. replaces computationally expensive GC-based tanh with fast GC-based ReLU, and then quantizes sigmoid and ReLU to smaller bit-length to accelerate activations in a GRU. We evaluate with multiple GRU models trained on 4 public datasets. Experimental results show achieves top-notch accuracy and improves the secure inference latency by up to 138× over one of the state-of-the-art secure networks on the Penn Treebank dataset",
    "checked": true,
    "id": "6a0310351409ada91e209b43f310b471ada458ca",
    "semantic_title": "cryptogru: low latency privacy-preserving text analysis with gru",
    "citation_count": 15,
    "authors": [
      "Bo Feng",
      "Qian Lou",
      "Lei Jiang",
      "Geoffrey Fox"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.157": {
    "title": "Local Word Discovery for Interactive Transcription",
    "volume": "main",
    "abstract": "Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for Kunwinjku, a morphologically-complex Australian language. We combine a finite state implementation of a published grammar with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17%. Further, we find that 75% of breath groups in the test set receive at least one correct partial or full-word suggestion",
    "checked": true,
    "id": "249f5538b38456db743f5b593197a15bd47e041d",
    "semantic_title": "local word discovery for interactive transcription",
    "citation_count": 7,
    "authors": [
      "William Lane",
      "Steven Bird"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.158": {
    "title": "Segment, Mask, and Predict: Augmenting Chinese Word Segmentation with Self-Supervision",
    "volume": "main",
    "abstract": "Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the models with the fixed corpus at every iteration. The intermediate generated information is also valuable. Besides, the robustness of the previous neural methods is limited by the large-scale annotated data. There are a few noises in the annotated corpus. Limited efforts have been made by previous studies to deal with such problems. In this work, we propose a self-supervised CWS approach with a straightforward and effective architecture. First, we train a word segmentation model and use it to generate the segmentation results. Then, we use a revised masked language model (MLM) to evaluate the quality of the segmentation results based on the predictions of the MLM. Finally, we leverage the evaluations to aid the training of the segmenter by improved minimum risk training. Experimental results show that our approach outperforms previous methods on 9 different CWS datasets with single criterion training and multiple criteria training and achieves better robustness",
    "checked": true,
    "id": "3d0524a13ade09043d88ef98f74323c5d67c65f5",
    "semantic_title": "segment, mask, and predict: augmenting chinese word segmentation with self-supervision",
    "citation_count": 6,
    "authors": [
      "Mieradilijiang Maimaiti",
      "Yang Liu",
      "Yuanhang Zheng",
      "Gang Chen",
      "Kaiyu Huang",
      "Ji Zhang",
      "Huanbo Luan",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.159": {
    "title": "Minimal Supervision for Morphological Inflection",
    "volume": "main",
    "abstract": "Neural models for the various flavours of morphological reinflection tasks have proven to be extremely accurate given ample labeled data, yet labeled data may be slow and costly to obtain. In this work we aim to overcome this annotation bottleneck by bootstrapping labeled data from a seed as small as five labeled inflection tables, accompanied by a large bulk of unlabeled text. Our bootstrapping method exploits the orthographic and semantic regularities in morphological systems in a two-phased setup, where word tagging based on analogies is followed by word pairing based on distances. Our experiments with the Paradigm Cell Filling Problem over eight typologically different languages show that in languages with relatively simple morphology, orthographic regularities on their own allow inflection models to achieve respectable accuracy. Combined orthographic and semantic regularities alleviate difficulties with particularly complex morpho-phonological systems. We further show that our bootstrapping methods substantially outperform hallucination-based methods commonly used for overcoming the annotation bottleneck in morphological reinflection tasks",
    "checked": true,
    "id": "1dc912b222a56ae4f189a2e95db6e6e69625772e",
    "semantic_title": "minimal supervision for morphological inflection",
    "citation_count": 2,
    "authors": [
      "Omer Goldman",
      "Reut Tsarfaty"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.160": {
    "title": "Fast WordPiece Tokenization",
    "volume": "main",
    "abstract": "Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient algorithms for the WordPiece tokenization used in BERT, from single-word tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as maximum matching. The best known algorithms so far are O(nˆ2) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a novel algorithm whose tokenization complexity is strictly O(n). Our method is inspired by the Aho-Corasick algorithm. We introduce additional linkages on top of the trie built from the vocabulary, allowing smart transitions when the trie matching cannot continue. For general text, we further propose an algorithm that combines pre-tokenization (splitting the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text on average for general text tokenization",
    "checked": true,
    "id": "72eed6570807c1d3c9a60289c7fc6813277e1e98",
    "semantic_title": "fast wordpiece tokenization",
    "citation_count": 170,
    "authors": [
      "Xinying Song",
      "Alex Salcianu",
      "Yang Song",
      "Dave Dopson",
      "Denny Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.161": {
    "title": "You should evaluate your language model on marginal likelihood over tokenisations",
    "volume": "main",
    "abstract": "Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness",
    "checked": true,
    "id": "9635e3c008f7bfa80638ade7134a8fb0ef1b37e1",
    "semantic_title": "you should evaluate your language model on marginal likelihood over tokenisations",
    "citation_count": 27,
    "authors": [
      "Kris Cao",
      "Laura Rimell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.162": {
    "title": "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning",
    "volume": "main",
    "abstract": "Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR",
    "checked": true,
    "id": "2375e447e86f52a35655c3069bcb047e6655eeb8",
    "semantic_title": "broaden the vision: geo-diverse visual commonsense reasoning",
    "citation_count": 57,
    "authors": [
      "Da Yin",
      "Liunian Harold Li",
      "Ziniu Hu",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.163": {
    "title": "Reference-Centric Models for Grounded Collaborative Dialogue",
    "volume": "main",
    "abstract": "We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner's utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous state of the art for the task, obtaining a 20% relative improvement in successful task completion in self-play evaluations and a 50% relative improvement in success in human evaluations",
    "checked": true,
    "id": "5931c8ac145baf17cec9effc25c051049b7dfd4c",
    "semantic_title": "reference-centric models for grounded collaborative dialogue",
    "citation_count": 21,
    "authors": [
      "Daniel Fried",
      "Justin Chiu",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.164": {
    "title": "CrossVQA: Scalably Generating Benchmarks for Systematically Testing VQA Generalization",
    "volume": "main",
    "abstract": "One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse question-answer pairs with the desired dataset style. We use it to create CrossVQA, a collection of test splits for assessing VQA generalization based on the VQA2, VizWiz, and Open Images datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-the-art VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the language shifts. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention",
    "checked": true,
    "id": "4593c88fb33023bec84f8f443d16262810b9047a",
    "semantic_title": "crossvqa: scalably generating benchmarks for systematically testing vqa generalization",
    "citation_count": 26,
    "authors": [
      "Arjun Akula",
      "Soravit Changpinyo",
      "Boqing Gong",
      "Piyush Sharma",
      "Song-Chun Zhu",
      "Radu Soricut"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.165": {
    "title": "Visual Goal-Step Inference using wikiHow",
    "volume": "main",
    "abstract": "Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events",
    "checked": true,
    "id": "7c9e1282124c7c161962df3b78c1c3116deffdfd",
    "semantic_title": "visual goal-step inference using wikihow",
    "citation_count": 46,
    "authors": [
      "Yue Yang",
      "Artemis Panagopoulou",
      "Qing Lyu",
      "Li Zhang",
      "Mark Yatskar",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.166": {
    "title": "Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?",
    "volume": "main",
    "abstract": "We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed to study systematic generalization for grounded language understanding. First, we study which aspects of the original benchmark can be solved by commonly used methods in multi-modal research. We find that a general-purpose Transformer-based model with cross-modal attention achieves strong performance on a majority of the gSCAN splits, surprisingly outperforming more specialized approaches from prior work. Furthermore, our analysis suggests that many of the remaining errors reveal the same fundamental challenge in systematic generalization of linguistic constructs regardless of visual context. Second, inspired by this finding, we propose challenging new tasks for gSCAN by generating data to incorporate relations between objects in the visual environment. Finally, we find that current models are surprisingly data inefficient given the narrow scope of commands in gSCAN, suggesting another challenge for future work",
    "checked": true,
    "id": "d41417f22f898125c4d34f672938ebb2a3764961",
    "semantic_title": "systematic generalization on gscan: what is nearly solved and what is next?",
    "citation_count": 21,
    "authors": [
      "Linlu Qiu",
      "Hexiang Hu",
      "Bowen Zhang",
      "Peter Shaw",
      "Fei Sha"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.167": {
    "title": "Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models",
    "volume": "main",
    "abstract": "A method for creating a vision-and-language (V&L) model is to extend a language model through structural modifications and V&L pre-training. Such an extension aims to make a V&L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well this is achieved, we propose to evaluate V&L models using an NLU benchmark (GLUE). We compare five V&L models, including single-stream and dual-stream models, trained with the same pre-training. Dual-stream models, with their higher modality independence achieved by approximately doubling the number of parameters, are expected to preserve the NLU capability better. Our main finding is that the dual-stream scores are not much different than the single-stream scores, contrary to expectation. Further analysis shows that pre-training causes the performance drop in NLU tasks with few exceptions. These results suggest that adopting a single-stream structure and devising the pre-training could be an effective method for improving the maintenance of language knowledge in V&L extensions",
    "checked": true,
    "id": "b1905ffd714d5e7e14eff99ced51214c2d22a3bd",
    "semantic_title": "effect of visual extensions on natural language understanding in vision-and-language models",
    "citation_count": 21,
    "authors": [
      "Taichi Iki",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.168": {
    "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
    "volume": "main",
    "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter",
    "checked": true,
    "id": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
    "semantic_title": "neural path hunter: reducing hallucination in dialogue systems via path grounding",
    "citation_count": 140,
    "authors": [
      "Nouha Dziri",
      "Andrea Madotto",
      "Osmar Zaïane",
      "Avishek Joey Bose"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.169": {
    "title": "Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems",
    "volume": "main",
    "abstract": "Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The proposed model comprises a multi-concept planning module that learns to identify multiple associated concepts from a concept graph and a customized Insertion Transformer that performs concept-guided non-autoregressive generation to complete a response. The experimental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art baselines in both automatic and human evaluations with substantially faster inference speed",
    "checked": true,
    "id": "a2b00396b893740c445fdad3c62eeb645817611c",
    "semantic_title": "thinking clearly, talking fast: concept-guided non-autoregressive generation for open-domain dialogue systems",
    "citation_count": 30,
    "authors": [
      "Yicheng Zou",
      "Zhihua Liu",
      "Xingwu Hu",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.170": {
    "title": "Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes",
    "volume": "main",
    "abstract": "Empathy is a complex cognitive ability based on the reasoning of others' affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other's emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in text require sub-utterance level annotations, which can be demanding. Taking inspiration from social cognition, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on pragmatics to make dialogue models focus on targeted words in the input during generation. Our method is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best-performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation",
    "checked": true,
    "id": "ef7e5ba42e52b5584352c09ddf10977e3528e0c9",
    "semantic_title": "perspective-taking and pragmatics for generating empathetic responses focused on emotion causes",
    "citation_count": 71,
    "authors": [
      "Hyunwoo Kim",
      "Byeongchang Kim",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.171": {
    "title": "Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration",
    "volume": "main",
    "abstract": "Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantic of ontology and enhance the interrelation between slots with masked hierarchical attention. In state value decoding stage, we solve the out-of-vocabulary problem by combining generation method and extraction method together. We evaluate the performance of our model on two representative datasets, MultiWOZ in English and CrossWOZ in Chinese. The results show that our model yields a significant performance gain over current state-of-the-art state tracking model and it is more robust to out-of-vocabulary problem compared with other methods",
    "checked": true,
    "id": "3313b0ce2093013eb5e068ec4a9525a330b3f321",
    "semantic_title": "generation and extraction combined dialogue state tracking with hierarchical ontology integration",
    "citation_count": 11,
    "authors": [
      "Xinmeng Li",
      "Qian Li",
      "Wansen Wu",
      "Quanjun Yin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.172": {
    "title": "CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation",
    "volume": "main",
    "abstract": "Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this task usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation",
    "checked": true,
    "id": "2a4f240bbc33900208a0c37467f17a6933d2eb1e",
    "semantic_title": "colv: a collaborative latent variable model for knowledge-grounded dialogue generation",
    "citation_count": 26,
    "authors": [
      "Haolan Zhan",
      "Lei Shen",
      "Hongshen Chen",
      "Hainan Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.173": {
    "title": "A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation",
    "volume": "main",
    "abstract": "Neural conversation models have shown great potentials towards generating fluent and informative responses by introducing external background knowledge. Nevertheless, it is laborious to construct such knowledge-grounded dialogues, and existing models usually perform poorly when transfer to new domains with limited training samples. Therefore, building a knowledge-grounded dialogue system under the low-resource setting is a still crucial issue. In this paper, we propose a novel three-stage learning framework based on weakly supervised learning which benefits from large scale ungrounded dialogues and unstructured knowledge base. To better cooperate with this framework, we devise a variant of Transformer with decoupled decoder which facilitates the disentangled learning of response generation and knowledge incorporation. Evaluation results on two benchmarks indicate that our approach can outperform other state-of-the-art methods with less training data, and even in zero-resource scenario, our approach still performs well",
    "checked": true,
    "id": "9e3c70b39a864792e6c1779681968e5bfcb34acc",
    "semantic_title": "a three-stage learning framework for low-resource knowledge-grounded dialogue generation",
    "citation_count": 32,
    "authors": [
      "Shilei Liu",
      "Xiaofeng Zhao",
      "Bochao Li",
      "Feiliang Ren",
      "Longhui Zhang",
      "Shujuan Yin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.174": {
    "title": "Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue",
    "volume": "main",
    "abstract": "Recent years has witnessed the remarkable success in end-to-end task-oriented dialog system, especially when incorporating external knowledge information. However, the quality of most existing models' generated response is still limited, mainly due to their lack of fine-grained reasoning on deterministic knowledge (w.r.t. conceptual tokens), which makes them difficult to capture the concept shifts and identify user's real intention in cross-task scenarios. To address these issues, we propose a novel intention mechanism to better model deterministic entity knowledge. Based on such a mechanism, we further propose an intention reasoning network (IR-Net), which consists of joint and multi-hop reasoning, to obtain intention-aware representations of conceptual tokens that can be used to capture the concept shifts involved in task-oriented conversations, so as to effectively identify user's intention and generate more accurate responses. Experimental results verify the effectiveness of IR-Net, showing that it achieves the state-of-the-art performance on two representative multi-domain dialog datasets",
    "checked": true,
    "id": "70ef8cbc0783559205b671e6f79b89daff4c7e10",
    "semantic_title": "intention reasoning network for multi-domain end-to-end task-oriented dialogue",
    "citation_count": 5,
    "authors": [
      "Zhiyuan Ma",
      "Jianjun Li",
      "Zezheng Zhang",
      "Guohui Li",
      "Yongjing Cheng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.175": {
    "title": "More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge",
    "volume": "main",
    "abstract": "Despite achieving remarkable performance, previous knowledge-enhanced works usually only use a single-source homogeneous knowledge base of limited knowledge coverage. Thus, they often degenerate into traditional methods because not all dialogues can be linked with knowledge entries. This paper proposes a novel dialogue generation model, MSKE-Dialog, to solve this issue with three unique advantages: (1) Rather than only one, MSKE-Dialog can simultaneously leverage multiple heterogeneous knowledge sources (it includes but is not limited to commonsense knowledge facts, text knowledge, infobox knowledge) to improve the knowledge coverage; (2) To avoid the topic conflict among the context and different knowledge sources, we propose a Multi-Reference Selection to better select context/knowledge; (3) We propose a Multi-Reference Generation to generate informative responses by referring to multiple generation references at the same time. Extensive evaluations on a Chinese dataset show the superior performance of this work against various state-of-the-art approaches. To our best knowledge, this work is the first to use the multi-source heterogeneous knowledge in the open-domain knowledge-enhanced dialogue generation",
    "checked": true,
    "id": "71108e404768e34d902a5c89a1423f34b0fd84d7",
    "semantic_title": "more is better: enhancing open-domain dialogue generation via multi-source heterogeneous knowledge",
    "citation_count": 23,
    "authors": [
      "Sixing Wu",
      "Ying Li",
      "Minghui Wang",
      "Dawei Zhang",
      "Yang Zhou",
      "Zhonghai Wu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.176": {
    "title": "Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks",
    "volume": "main",
    "abstract": "Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively",
    "checked": true,
    "id": "7140be4f8f973c3a6a9503c76227e8d752c41065",
    "semantic_title": "domain-lifelong learning for dialogue state tracking via knowledge preservation networks",
    "citation_count": 12,
    "authors": [
      "Qingbin Liu",
      "Pengfei Cao",
      "Cao Liu",
      "Jiansong Chen",
      "Xunliang Cai",
      "Fan Yang",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.177": {
    "title": "CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling",
    "volume": "main",
    "abstract": "Conversational semantic role labeling (CSRL) is believed to be a crucial step towards dialogue understanding. However, it remains a major challenge for existing CSRL parser to handle conversational structural information. In this paper, we present a simple and effective architecture for CSRL which aims to address this problem. Our model is based on a conversational structure aware graph network which explicitly encodes the speaker dependent information. We also propose a multi-task learning method to further improve the model. Experimental results on benchmark datasets show that our model with our proposed training objectives significantly outperforms previous baselines",
    "checked": true,
    "id": "e7d744a3b2897a8f16c397493639716074599561",
    "semantic_title": "csagn: conversational structure aware graph network for conversational semantic role labeling",
    "citation_count": 8,
    "authors": [
      "Han Wu",
      "Kun Xu",
      "Linqi Song"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.178": {
    "title": "Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks",
    "volume": "main",
    "abstract": "Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in Natural Language Processing. Previous investigations prove that introducing a further pre-training phase between pre-training and fine-tuning phases to adapt the model on the domain-specific unlabeled data can bring positive effects. However, most of these further pre-training works just keep running the conventional pre-training task, e.g., masked language model, which can be regarded as the domain adaptation to bridge the data distribution gap. After observing diverse downstream tasks, we suggest that different tasks may also need a further pre-training phase with appropriate training tasks to bridge the task formulation gap. To investigate this, we carry out a study for improving multiple task-oriented dialogue downstream tasks through designing various tasks at the further pre-training phase. The experiment shows that different downstream tasks prefer different further pre-training tasks, which have intrinsic correlation and most further pre-training tasks significantly improve certain target tasks rather than all. Our investigation indicates that it is of great importance and effectiveness to design appropriate further pre-training tasks modeling specific information that benefit downstream tasks. Besides, we present multiple constructive empirical conclusions for enhancing task-oriented dialogues",
    "checked": true,
    "id": "d719a6349610c859b13de32673431ffffc28c469",
    "semantic_title": "different strokes for different folks: investigating appropriate further pre-training approaches for diverse dialogue tasks",
    "citation_count": 6,
    "authors": [
      "Yao Qiu",
      "Jinchao Zhang",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.179": {
    "title": "Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation",
    "volume": "main",
    "abstract": "Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external knowledge base to generate appropriate responses. In real-world practical, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval. To deal with this problem, instead of introducing knowledge base as the input, we force the model to learn a better semantic representation by predicting the information in the knowledge base, only based on the input context. Specifically, with the help of a knowledge base, we introduce two auxiliary training objectives: 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings",
    "checked": true,
    "id": "7af90c66039e5a4a93501090f12b3a7324f83c91",
    "semantic_title": "knowledge enhanced fine-tuning for better handling unseen entities in dialogue generation",
    "citation_count": 23,
    "authors": [
      "Leyang Cui",
      "Yu Wu",
      "Shujie Liu",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.180": {
    "title": "An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model",
    "volume": "main",
    "abstract": "Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this study, we analyze the weaknesses of the open-domain Korean Multi-turn response selection models and publish an adversarial dataset to evaluate these weaknesses. We also suggest a strategy to build a robust model in this adversarial environment",
    "checked": true,
    "id": "0a729d180bdf126bf0f350d7ac7ec2b2157eefa6",
    "semantic_title": "an evaluation dataset and strategy for building robust multi-turn response selection model",
    "citation_count": 5,
    "authors": [
      "Kijong Han",
      "Seojin Lee",
      "Dong-hun Lee"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.181": {
    "title": "Unsupervised Conversation Disentanglement through Co-Training",
    "volume": "main",
    "abstract": "Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which is expensive to obtain in practice. In this work, we explore training a conversation disentanglement model without referencing any human annotations. Our method is built upon the deep co-training algorithm, which consists of two neural networks: a message-pair classifier and a session classifier. The former is responsible of retrieving local relations between two messages while the latter categorizes a message to a session by capturing context-aware information. Both the two networks are initialized respectively with pseudo data built from the unannotated corpus. During the deep co-training process, we use the session classifier as a reinforcement learning component to learn a session assigning policy by maximizing the local rewards given by the message-pair classifier. For the message-pair classifier, we enrich its training data by retrieving message pairs with high confidence from the disentangled sessions predicted by the session classifier. Experimental results on the large Movie Dialogue Dataset demonstrate that our proposed approach achieves competitive performance compared to previous supervised methods. Further experiments show that the predicted disentangled conversations can promote the performance on the downstream task of multi-party response selection",
    "checked": true,
    "id": "890a9c6345cd2d0ca95c76d870b7d3f754671bb5",
    "semantic_title": "unsupervised conversation disentanglement through co-training",
    "citation_count": 25,
    "authors": [
      "Hui Liu",
      "Zhan Shi",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.182": {
    "title": "Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System",
    "volume": "main",
    "abstract": "Consistency Identification has obtained remarkable success on open-domain dialogue, which can be used for preventing inconsistent response generation. However, in contrast to the rapid development in open-domain dialogue, few efforts have been made to the task-oriented dialogue direction. In this paper, we argue that consistency problem is more urgent in task-oriented domain. To facilitate the research, we introduce CI-ToD, a novel dataset for Consistency Identification in Task-oriented Dialog system. In addition, we not only annotate the single label to enable the model to judge whether the system response is contradictory, but also provide more fine-grained labels (i.e., Dialogue History Inconsistency, User Query Inconsistency and Knowledge Base Inconsistency) to encourage model to know what inconsistent sources lead to it. Empirical results show that state-of-the-art methods only achieve 51.3%, which is far behind the human performance of 93.2%, indicating that there is ample room for improving consistency identification ability. Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide guidance for future directions. All datasets and models are publicly available at https://github.com/yizhen20133868/CI-ToD",
    "checked": true,
    "id": "a72bc8e6f910004ba52a82298967854d0cf26aa2",
    "semantic_title": "don't be contradicted with anything! ci-tod: towards benchmarking consistency for task-oriented dialogue system",
    "citation_count": 20,
    "authors": [
      "Libo Qin",
      "Tianbao Xie",
      "Shijue Huang",
      "Qiguang Chen",
      "Xiao Xu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.183": {
    "title": "Transferable Persona-Grounded Dialogues via Grounded Minimal Edits",
    "volume": "main",
    "abstract": "Grounded dialogue models generate responses that are grounded on certain concepts. Limited by the distribution of grounded dialogue data, models trained on such data face the transferability challenges in terms of the data distribution and the type of grounded concepts. To address the challenges, we propose the grounded minimal editing framework, which minimally edits existing responses to be grounded on the given concept. Focusing on personas, we propose Grounded Minimal Editor (GME), which learns to edit by disentangling and recombining persona-related and persona-agnostic parts of the response. To evaluate persona-grounded minimal editing, we present the PersonaMi-nEdit dataset, and experimental results show that GME outperforms competitive baselines by a large margin. To evaluate the transferability, we experiment on the test set of BlendedSkillTalk and show that GME can edit dialogue models' responses to largely improve their persona consistency while preserving the use of knowledge and empathy",
    "checked": true,
    "id": "23965e7fdcb0410ec0480f06c148722b123742cf",
    "semantic_title": "transferable persona-grounded dialogues via grounded minimal edits",
    "citation_count": 17,
    "authors": [
      "Chen Henry Wu",
      "Yinhe Zheng",
      "Xiaoxi Mao",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.184": {
    "title": "EARL: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning",
    "volume": "main",
    "abstract": "Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these models have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce knowledge graphs to informative conversation generation. Unlike traditional approaches that parameterize the specific representation for each entity, EARL utilizes the context of conversations and the relational structure of knowledge graphs to learn the category representation for entities, which is generalized to incorporating unseen entities in knowledge graphs into conversation generation. Automatic and manual evaluations demonstrate that our model can generate more informative, coherent, and natural responses than baseline models",
    "checked": true,
    "id": "0a789ce4347c9afdffe0a1351724f55916703032",
    "semantic_title": "earl: informative knowledge-grounded conversation generation with entity-agnostic representation learning",
    "citation_count": 14,
    "authors": [
      "Hao Zhou",
      "Minlie Huang",
      "Yong Liu",
      "Wei Chen",
      "Xiaoyan Zhu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.185": {
    "title": "DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings",
    "volume": "main",
    "abstract": "Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman's correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided",
    "checked": true,
    "id": "2adb04e0430d153c76236a852473dbeae1a7efaa",
    "semantic_title": "dialoguecse: dialogue-based contrastive learning of sentence embeddings",
    "citation_count": 40,
    "authors": [
      "Che Liu",
      "Rui Wang",
      "Jinghua Liu",
      "Jian Sun",
      "Fei Huang",
      "Luo Si"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.186": {
    "title": "Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings",
    "volume": "main",
    "abstract": "Dominant sentence ordering models can be classified into pairwise ordering models and set-to-sequence models. However, there is little attempt to combine these two types of models, which inituitively possess complementary advantages. In this paper, we propose a novel sentence ordering framework which introduces two classifiers to make better use of pairwise orderings for graph-based sentence ordering (Yin et al. 2019, 2021). Specially, given an initial sentence-entity graph, we first introduce a graph-based classifier to predict pairwise orderings between linked sentences. Then, in an iterative manner, based on the graph updated by previously predicted high-confident pairwise orderings, another classifier is used to predict the remaining uncertain pairwise orderings. At last, we adapt a GRN-based sentence ordering model (Yin et al. 2019, 2021) on the basis of final graph. Experiments on five commonly-used datasets demonstrate the effectiveness and generality of our model. Particularly, when equipped with BERT (Devlin et al. 2019) and FHDecoder (Yin et al. 2020), our model achieves state-of-the-art performance. Our code is available at https://github.com/DeepLearnXMU/IRSEG",
    "checked": true,
    "id": "f2fb67d7d5d31908b40186f06b82a8af7884eb2f",
    "semantic_title": "improving graph-based sentence ordering with iteratively predicted pairwise orderings",
    "citation_count": 8,
    "authors": [
      "Shaopeng Lai",
      "Ante Wang",
      "Fandong Meng",
      "Jie Zhou",
      "Yubin Ge",
      "Jiali Zeng",
      "Junfeng Yao",
      "Degen Huang",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.187": {
    "title": "Not Just Classification: Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation",
    "volume": "main",
    "abstract": "Implicit discourse relation recognition (IDRR) is a critical task in discourse analysis. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a generation task and further propose a method joint modeling of the classification and generation. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems",
    "checked": true,
    "id": "7e99a472f6df8aae309c8aa700cf7c5d0875983f",
    "semantic_title": "not just classification: recognizing implicit discourse relation on joint modeling of classification and generation",
    "citation_count": 21,
    "authors": [
      "Feng Jiang",
      "Yaxin Fan",
      "Xiaomin Chu",
      "Peifeng Li",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.188": {
    "title": "A Language Model-based Generative Classifier for Sentence-level Discourse Parsing",
    "volume": "main",
    "abstract": "Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover, since this enables LMGC to make ready the representations for labels, unseen in the pre-training step, we can effectively use a pre-trained language model in LMGC. Experimental results on the RST-DT dataset show that our LMGC achieved the state-of-the-art F1 score of 96.72 in discourse segmentation. It further achieved the state-of-the-art relation F1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively, in sentence-level discourse parsing",
    "checked": true,
    "id": "cc53d235f7757983b17d9d2a1b63b0a94c9b695a",
    "semantic_title": "a language model-based generative classifier for sentence-level discourse parsing",
    "citation_count": 13,
    "authors": [
      "Ying Zhang",
      "Hidetaka Kamigaito",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.189": {
    "title": "Multimodal Phased Transformer for Sentiment Analysis",
    "volume": "main",
    "abstract": "Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our model with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90% reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency",
    "checked": true,
    "id": "75a868a12bd4b12b806fbffa7cfd66a2751bbaec",
    "semantic_title": "multimodal phased transformer for sentiment analysis",
    "citation_count": 45,
    "authors": [
      "Junyan Cheng",
      "Iordanis Fostiropoulos",
      "Barry Boehm",
      "Mohammad Soleymani"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.190": {
    "title": "Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations",
    "volume": "main",
    "abstract": "Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably leads to fundamental information loss. In this paper, we propose a novel HMTC framework that considers both vertical and horizontal category correlations. Specifically, we first design a loosely coupled graph convolutional neural network as the representation extractor to obtain representations for words, documents, and, more importantly, level-wise representations for categories, which are not considered in previous works. Then, the learned category representations are adopted to capture the vertical dependencies among levels of category hierarchy and model the horizontal correlations. Finally, based on the document embeddings and category embeddings, we design a hybrid algorithm to predict the categories of the entire hierarchical structure. Extensive experiments conducted on real-world HMTC datasets validate the effectiveness of the proposed framework with significant improvements over the baselines",
    "checked": true,
    "id": "ebad443633a01ba8f1c3c3ea040156912ed72f9b",
    "semantic_title": "hierarchical multi-label text classification with horizontal and vertical category correlations",
    "citation_count": 19,
    "authors": [
      "Linli Xu",
      "Sijie Teng",
      "Ruoyu Zhao",
      "Junliang Guo",
      "Chi Xiao",
      "Deqiang Jiang",
      "Bo Ren"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.191": {
    "title": "RankNAS: Efficient Neural Architecture Search by Pairwise Ranking",
    "volume": "main",
    "abstract": "This paper addresses the efficiency challenge of Neural Architecture Search (NAS) by formulating the task as a ranking problem. Previous methods require numerous training examples to estimate the accurate performance of architectures, although the actual goal is to find the distinction between \"good\" and \"bad\" candidates. Here we do not resort to performance predictors. Instead, we propose a performance ranking method (RankNAS) via pairwise ranking. It enables efficient architecture search using much fewer training examples. Moreover, we develop an architecture selection method to prune the search space and concentrate on more promising candidates. Extensive experiments on machine translation and language modeling tasks show that RankNAS can design high-performance architectures while being orders of magnitude faster than state-of-the-art NAS systems",
    "checked": true,
    "id": "255c526f78bbfec35d4ed73b6dd8eacd9ef2c0b3",
    "semantic_title": "ranknas: efficient neural architecture search by pairwise ranking",
    "citation_count": 11,
    "authors": [
      "Chi Hu",
      "Chenglong Wang",
      "Xiangnan Ma",
      "Xia Meng",
      "Yinqiao Li",
      "Tong Xiao",
      "Jingbo Zhu",
      "Changliang Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.192": {
    "title": "FLiText: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks",
    "volume": "main",
    "abstract": "In natural language processing (NLP), state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate that these frameworks are not suitable for lightweight models such as TextCNN, LSTM and etc. In this work, we develop a new SSL framework called FLiText, which stands for Faster and Lighter semi-supervised Text classification. FLiText introduces an inspirer network together with the consistency regularization framework, which leverages a generalized regular constraint on the lightweight models for efficient SSL. As a result, FLiText obtains new SOTA performance for lightweight models across multiple SSL benchmarks on text classification. Compared with existing SOTA SSL methods on TextCNN, FLiText improves the accuracy of lightweight model TextCNN from 51.00% to 90.49% on IMDb, 39.8% to 58.06% on Yelp-5, and from 55.3% to 65.08% on Yahoo! Answer. In addition, compared with the fully supervised method on the full dataset, FLiText just uses less than 1% of labeled data to improve the accuracy by 6.59%, 3.94%, and 3.22% on the datasets of IMDb, Yelp-5, and Yahoo! Answer respectively",
    "checked": true,
    "id": "54793533142d6ec5e0e03d0f5ec8fd4796311b54",
    "semantic_title": "flitext: a faster and lighter semi-supervised text classification with convolution networks",
    "citation_count": 19,
    "authors": [
      "Chen Liu",
      "Zhang Mengchao",
      "Fu Zhibing",
      "Panpan Hou",
      "Yu Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.193": {
    "title": "Evaluating Debiasing Techniques for Intersectional Biases",
    "volume": "main",
    "abstract": "Bias is pervasive for NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider ‘gerrymandering' groups which comprise not only single attributes, but also intersectional groups. We evaluate a form of bias-constrained model which is new to NLP, as well an extension of the iterative nullspace projection technique which can handle multiple identities",
    "checked": true,
    "id": "258e5b18b0f2bb027080ed347e856e6940a20ce5",
    "semantic_title": "evaluating debiasing techniques for intersectional biases",
    "citation_count": 50,
    "authors": [
      "Shivashankar Subramanian",
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn",
      "Lea Frermann"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.194": {
    "title": "Definition Modelling for Appropriate Specificity",
    "volume": "main",
    "abstract": "Definition generation techniques aim to generate a definition of a target word or phrase given a context. In previous studies, researchers have faced various issues such as the out-of-vocabulary problem and over/under-specificity problems. Over-specific definitions present narrow word meanings, whereas under-specific definitions present general and context-insensitive meanings. Herein, we propose a method for definition generation with appropriate specificity. The proposed method addresses the aforementioned problems by leveraging a pre-trained encoder-decoder model, namely Text-to-Text Transfer Transformer, and introducing a re-ranking mechanism to model specificity in definitions. Experimental results on standard evaluation datasets indicate that our method significantly outperforms the previous state-of-the-art method. Moreover, manual evaluation confirms that our method effectively addresses the over/under-specificity problems",
    "checked": true,
    "id": "dc0020044410b57680bf0af6b2f45aca835ebdce",
    "semantic_title": "definition modelling for appropriate specificity",
    "citation_count": 32,
    "authors": [
      "Han Huang",
      "Tomoyuki Kajiwara",
      "Yuki Arase"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.195": {
    "title": "Transductive Learning for Unsupervised Text Style Transfer",
    "volume": "main",
    "abstract": "Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of parallel corpus hinders the ability of these inductive learning methods on this task. As a result, it is likely to cause severe inconsistent style expressions, like ‘the salad is rude'. To tackle this problem, we propose a novel transductive learning approach in this paper, based on a retrieval-based context-aware style representation. Specifically, an attentional encoder-decoder with a retriever framework is utilized. It involves top-K relevant sentences in the target style in the transfer process. In this way, we can learn a context-aware style embedding to alleviate the above inconsistency problem. In this paper, both sparse (BM25) and dense retrieval functions (MIPS) are used, and two objective functions are designed to facilitate joint learning. Experimental results show that our method outperforms several strong baselines. The proposed transductive learning approach is general and effective to the task of unsupervised style transfer, and we will apply it to the other two typical methods in the future",
    "checked": true,
    "id": "00563a98bb3421e4b1a7c9993314a182b1891331",
    "semantic_title": "transductive learning for unsupervised text style transfer",
    "citation_count": 30,
    "authors": [
      "Fei Xiao",
      "Liang Pang",
      "Yanyan Lan",
      "Yan Wang",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.196": {
    "title": "Integrating Semantic Scenario and Word Relations for Abstractive Sentence Summarization",
    "volume": "main",
    "abstract": "Recently graph-based methods have been adopted for Abstractive Text Summarization. However, existing graph-based methods only consider either word relations or structure information, which neglect the correlation between them. To simultaneously capture the word relations and structure information from sentences, we propose a novel Dual Graph network for Abstractive Sentence Summarization. Specifically, we first construct semantic scenario graph and semantic word relation graph based on FrameNet, and subsequently learn their representations and design graph fusion method to enhance their correlation and obtain better semantic representation for summary generation. Experimental results show our model outperforms existing state-of-the-art methods on two popular benchmark datasets, i.e., Gigaword and DUC 2004",
    "checked": true,
    "id": "a73bd4c57e641c711f5efb858dccc72a75f90858",
    "semantic_title": "integrating semantic scenario and word relations for abstractive sentence summarization",
    "citation_count": 11,
    "authors": [
      "Yong Guan",
      "Shaoru Guo",
      "Ru Li",
      "Xiaoli Li",
      "Hu Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.197": {
    "title": "Coupling Context Modeling with Zero Pronoun Recovering for Document-Level Natural Language Generation",
    "volume": "main",
    "abstract": "Natural language generation (NLG) tasks on pro-drop languages are known to suffer from zero pronoun (ZP) problems, and the problems remain challenging due to the scarcity of ZP-annotated NLG corpora. In this case, we propose a highly adaptive two-stage approach to couple context modeling with ZP recovering to mitigate the ZP problem in NLG tasks. Notably, we frame the recovery process in a task-supervised fashion where the ZP representation recovering capability is learned during the NLG task learning process, thus our method does not require NLG corpora annotated with ZPs. For system enhancement, we learn an adversarial bot to adjust our model outputs to alleviate the error propagation caused by mis-recovered ZPs. Experiments on three document-level NLG tasks, i.e., machine translation, question answering, and summarization, show that our approach can improve the performance to a great extent, and the improvement on pronoun translation is very impressive",
    "checked": true,
    "id": "ccd763025ec8459fe2e77be8220176393e6c5a35",
    "semantic_title": "coupling context modeling with zero pronoun recovering for document-level natural language generation",
    "citation_count": 9,
    "authors": [
      "Xin Tan",
      "Longyin Zhang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.198": {
    "title": "Adaptive Bridge between Training and Inference for Dialogue Generation",
    "volume": "main",
    "abstract": "Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario. In real human dialogue, there are many appropriate responses for the same context, not only with different expressions, but also with different topics. Therefore, due to the much bigger gap between various ground-truth responses and the generated synthetic response, exposure bias is more challenging in dialogue generation task. What's more, as MLE encourages the model to only learn the common words among different ground-truth responses, but ignores the interesting and specific parts, exposure bias may further lead to the common response generation problem, such as \"I don't know\" and \"HaHa?\" In this paper, we propose a novel adaptive switching mechanism, which learns to automatically transit between ground-truth learning and generated learning regarding the word-level matching score, such as the cosine similarity. Experimental results on both Chinese STC dataset and English Reddit dataset, show that our adaptive method achieves a significant improvement in terms of metric-based evaluation and human evaluation, as compared with the state-of-the-art exposure bias approaches. Further analysis on NMT task also shows that our model can achieve a significant improvement",
    "checked": true,
    "id": "8576cd42e5d9193ad91f4854826b04468b8fd176",
    "semantic_title": "adaptive bridge between training and inference for dialogue generation",
    "citation_count": 8,
    "authors": [
      "Haoran Xu",
      "Hainan Zhang",
      "Yanyan Zou",
      "Hongshen Chen",
      "Zhuoye Ding",
      "Yanyan Lan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.199": {
    "title": "ConRPG: Paraphrase Generation using Contexts as Regularizer",
    "volume": "main",
    "abstract": "A long-standing issue with paraphrase generation is the lack of reliable supervision signals. In this paper, we propose a new unsupervised paradigm for paraphrase generation based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods: (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs; (2) the combination of the huge amount of paraphrase candidates and further diversity-promoting filtering yields paraphrases with more lexical and syntactic diversity; and (3) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the proposed paradigm significantly outperforms existing paraphrase approaches in both supervised and unsupervised setups",
    "checked": true,
    "id": "98b2fbfe770658f779c84147903925d3410cb0e1",
    "semantic_title": "conrpg: paraphrase generation using contexts as regularizer",
    "citation_count": 25,
    "authors": [
      "Yuxian Meng",
      "Xiang Ao",
      "Qing He",
      "Xiaofei Sun",
      "Qinghong Han",
      "Fei Wu",
      "Chun Fan",
      "Jiwei Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.200": {
    "title": "Building the Directed Semantic Graph for Coherent Long Text Generation",
    "volume": "main",
    "abstract": "Generating long text conditionally depending on the short input text has recently attracted more and more research efforts. Most existing approaches focus more on introducing extra knowledge to supplement the short input text, but ignore the coherence issue of the generated texts. To address aforementioned research issue, this paper proposes a novel two-stage approach to generate coherent long text. Particularly, we first build a document-level path for each output text with each sentence embedding as its node, and a revised self-organising map (SOM) is proposed to cluster similar nodes of a family of document-level paths to construct the directed semantic graph. Then, three subgraph alignment methods are proposed to extract the maximum matching paths or subgraphs. These directed subgraphs are considered to well preserve extra but relevant content to the short input text, and then they are decoded by the employed pre-trained model to generate coherent long text. Extensive experiments have been performed on three real-world datasets, and the promising results demonstrate that the proposed approach is superior to the state-of-the-art approaches w.r.t. a number of evaluation criteria",
    "checked": true,
    "id": "df67d44bd561c2ab9636bfd598156ee72c58d23b",
    "semantic_title": "building the directed semantic graph for coherent long text generation",
    "citation_count": 2,
    "authors": [
      "Ziao Wang",
      "Xiaofeng Zhang",
      "Hongwei Du"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.201": {
    "title": "Iterative GNN-based Decoder for Question Generation",
    "volume": "main",
    "abstract": "Natural question generation (QG) aims to generate questions from a passage, and generated questions are answered from the passage. Most models with state-of-the-art performance model the previously generated text at each decoding step. However, (1) they ignore the rich structure information that is hidden in the previously generated text. (2) they ignore the impact of copied words on the passage. We perceive that information in previously generated words serves as auxiliary information in subsequent generation. To address these problems, we design the Iterative Graph Network-based Decoder (IGND) to model the previous generation using a Graph Neural Network at each decoding step. Moreover, our graph model captures dependency relations in the passage that boost the generation. Experimental results demonstrate that our model outperforms the state-of-the-art models with sentence-level QG tasks on SQuAD and MARCO datasets",
    "checked": true,
    "id": "f6ff24caa5459f6f418a7eee60a8de08c3166eb9",
    "semantic_title": "iterative gnn-based decoder for question generation",
    "citation_count": 27,
    "authors": [
      "Zichu Fei",
      "Qi Zhang",
      "Yaqian Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.202": {
    "title": "Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data",
    "volume": "main",
    "abstract": "Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate (rather than extracting) keyphrases, thus this task is reduced to keyphrase-question-answer triplet joint generation. Accordingly, we propose a multi-agent communication model to generate and optimize the question and keyphrases iteratively, and then apply the generated question and keyphrases to guide the generation of answers. To establish a solid benchmark, we build our model on the strong generative pre-training model. Experimental results show that our model makes great breakthroughs in the question-answer pair generation task. Moreover, we make a comprehensive analysis on our model, suggesting new directions for this challenging task",
    "checked": true,
    "id": "42568f7bf5dd040e04dcf176fe2d86df5c4beb86",
    "semantic_title": "asking questions like educational experts: automatically generating question-answer pairs on real-world examination data",
    "citation_count": 25,
    "authors": [
      "Fanyi Qu",
      "Xin Jia",
      "Yunfang Wu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.203": {
    "title": "Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data",
    "volume": "main",
    "abstract": "Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with nonparallel data. We propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (VAE) which can generate texts in a specified syntactic structure. Particularly, we design a two-stage learning method to effectively train the model using non-parallel data. The conditional VAE is trained to reconstruct the input sentence according to the given input and its syntactic structure. Furthermore, to improve the syntactic controllability and semantic consistency of the pre-trained conditional VAE, we fine-tune it using syntax controlling and cycle reconstruction learning objectives, and employ Gumbel-Softmax to combine these new learning objectives. Experiment results demonstrate that the proposed model trained only on non-parallel data is capable of generating diverse paraphrases with specified syntactic structure. Additionally, we validate the effectiveness of our method for generating syntactically adversarial examples on the sentiment analysis task",
    "checked": true,
    "id": "88a532a427ce48670462fac52535e4d189e8f096",
    "semantic_title": "syntactically-informed unsupervised paraphrasing with non-parallel data",
    "citation_count": 9,
    "authors": [
      "Erguang Yang",
      "Mingtong Liu",
      "Deyi Xiong",
      "Yujie Zhang",
      "Yao Meng",
      "Changjian Hu",
      "Jinan Xu",
      "Yufeng Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.204": {
    "title": "Exploring Task Difficulty for Few-Shot Relation Extraction",
    "volume": "main",
    "abstract": "Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing models do not distinguish hard tasks from easy ones in the learning process. In this paper, we introduce a novel approach based on contrastive learning that learns better representations by exploiting relation label information. We further design a method that allows the model to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "50da4158ab3a604e18f82862aab173d82c2c2714",
    "semantic_title": "exploring task difficulty for few-shot relation extraction",
    "citation_count": 79,
    "authors": [
      "Jiale Han",
      "Bo Cheng",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.205": {
    "title": "MuVER: Improving First-Stage Entity Retrieval with Multi-View Entity Representations",
    "volume": "main",
    "abstract": "Entity retrieval, which aims at disambiguating mentions to canonical entities from massive KBs, is essential for many tasks in natural language processing. Recent progress in entity retrieval shows that the dual-encoder structure is a powerful and efficient framework to nominate candidates if entities are only identified by descriptions. However, they ignore the property that meanings of entity mentions diverge in different contexts and are related to various portions of descriptions, which are treated equally in previous works. In this work, we propose Multi-View Entity Representations (MuVER), a novel approach for entity retrieval that constructs multi-view representations for entity descriptions and approximates the optimal view for mentions via a heuristic searching method. Our method achieves the state-of-the-art performance on ZESHEL and improves the quality of candidates on three standard Entity Linking datasets",
    "checked": true,
    "id": "f7ac2956642e58b8ed6290d0060763bbb5ddd21e",
    "semantic_title": "muver: improving first-stage entity retrieval with multi-view entity representations",
    "citation_count": 17,
    "authors": [
      "Xinyin Ma",
      "Yong Jiang",
      "Nguyen Bach",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.206": {
    "title": "Treasures Outside Contexts: Improving Event Detection via Global Statistics",
    "volume": "main",
    "abstract": "Event detection (ED) aims at identifying event instances of specified types in given texts, which has been formalized as a sequence labeling task. As far as we know, existing neural-based ED models make decisions relying entirely on the contextual semantic features of each word in the inputted text, which we find is easy to be confused by the varied contexts in the test stage. To this end, we come up with the idea of introducing a set of statistical features from word-event co-occurrence frequencies in the entire training set to cooperate with contextual features. Specifically, we propose a Semantic and Statistic-Joint Discriminative Network (SS-JDN) consisting of a semantic feature extractor, a statistical feature extractor, and a joint event discriminator. In experiments, SS-JDN effectively exceeds ten recent strong baselines on ACE2005 and KBP2015 datasets. Further, we perform extensive experiments to comprehensively probe SS-JDN",
    "checked": true,
    "id": "127af186fc54a2f5ba1898064898f3db57b64e8d",
    "semantic_title": "treasures outside contexts: improving event detection via global statistics",
    "citation_count": 17,
    "authors": [
      "Rui Li",
      "Wenlin Zhao",
      "Cheng Yang",
      "Sen Su"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.207": {
    "title": "Uncertain Local-to-Global Networks for Document-Level Event Factuality Identification",
    "volume": "main",
    "abstract": "Event factuality indicates the degree of certainty about whether an event occurs in the real world. Existing studies mainly focus on identifying event factuality at sentence level, which easily leads to conflicts between different mentions of the same event. To this end, we study the problem of document-level event factuality identification, which determines the event factuality from the view of a document. For this task, we need to consider two important characteristics: Local Uncertainty and Global Structure, which can be utilized to improve performance. In this paper, we propose an Uncertain Local-to-Global Network (ULGN) to make use of these two characteristics. Specifically, we devise a Local Uncertainty Estimation module to model the uncertainty of local information. Moreover, we propose an Uncertain Information Aggregation module to leverage the global structure for integrating the local information. Experimental results demonstrate the effectiveness of our proposed method, outperforming the previous state-of-the-art model by 8.4% and 11.45% of F1 score on two widely used datasets",
    "checked": true,
    "id": "d00f47c071e621f40033e76260104fd43f8be0d8",
    "semantic_title": "uncertain local-to-global networks for document-level event factuality identification",
    "citation_count": 13,
    "authors": [
      "Pengfei Cao",
      "Yubo Chen",
      "Yuqing Yang",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.208": {
    "title": "A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling",
    "volume": "main",
    "abstract": "Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a table feature for each relation. Then two kinds of global associations are mined from the generated table features. Next, the mined global associations are integrated into the table feature of each relation. This \"generate-mine-integrate\" process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation's table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed model on three benchmark datasets. Experimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: https://github.com/neukg/GRTE",
    "checked": true,
    "id": "680e7557681900e17045685b5681a918037f1acf",
    "semantic_title": "a novel global feature-oriented relational triple extraction model based on table filling",
    "citation_count": 77,
    "authors": [
      "Feiliang Ren",
      "Longhui Zhang",
      "Shujuan Yin",
      "Xiaofeng Zhao",
      "Shilei Liu",
      "Bochao Li",
      "Yaduo Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.209": {
    "title": "Structure-Augmented Keyphrase Generation",
    "volume": "main",
    "abstract": "This paper studies the keyphrase generation (KG) task for scenarios where structure plays an important role. For example, a scientific publication consists of a short title and a long body, where the title can be used for de-emphasizing unimportant details in the body. Similarly, for short social media posts (, tweets), scarce context can be augmented from titles, though often missing. Our contribution is generating/augmenting structure then injecting these information in the encoding, using existing keyphrases of other documents, complementing missing/incomplete titles. We propose novel structure-augmented document encoding approaches that consist of the following two phases: The first phase, generating structure, extends the given document with related but absent keyphrases, augmenting missing context. The second phase, encoding structure, builds a graph of keyphrases and the given document to obtain the structure-aware representation of the augmented text. Our empirical results validate that our proposed structure augmentation and augmentation-aware encoding/decoding can improve KG for both scenarios, outperforming the state-of-the-art",
    "checked": true,
    "id": "b5356c79d6e8f9279a4f384a34702e7f096cfc50",
    "semantic_title": "structure-augmented keyphrase generation",
    "citation_count": 15,
    "authors": [
      "Jihyuk Kim",
      "Myeongho Jeong",
      "Seungtaek Choi",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.210": {
    "title": "An Empirical Study on Multiple Information Sources for Zero-Shot Fine-Grained Entity Typing",
    "volume": "main",
    "abstract": "Auxiliary information from multiple sources has been demonstrated to be effective in zero-shot fine-grained entity typing (ZFET). However, there lacks a comprehensive understanding about how to make better use of the existing information sources and how they affect the performance of ZFET. In this paper, we empirically study three kinds of auxiliary information: context consistency, type hierarchy and background knowledge (e.g., prototypes and descriptions) of types, and propose a multi-source fusion model (MSF) targeting these sources. The performance obtains up to 11.42% and 22.84% absolute gains over state-of-the-art baselines on BBN and Wiki respectively with regard to macro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarity among them",
    "checked": true,
    "id": "dbf616c42c4ba3a60eb4459de603335f3a83b0a4",
    "semantic_title": "an empirical study on multiple information sources for zero-shot fine-grained entity typing",
    "citation_count": 15,
    "authors": [
      "Yi Chen",
      "Haiyun Jiang",
      "Lemao Liu",
      "Shuming Shi",
      "Chuang Fan",
      "Min Yang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.211": {
    "title": "DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling",
    "volume": "main",
    "abstract": "Incorporating lexical knowledge into deep learning models has been proved to be very effective for sequence labeling tasks. However, previous works commonly have difficulty dealing with large-scale dynamic lexicons which often cause excessive matching noise and problems of frequent updates. In this paper, we propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence labeling tasks. Instead of leveraging embeddings of words in the lexicon as in conventional methods, we adopt word-agnostic tag embeddings to avoid re-training the representation while updating the lexicon. Moreover, we employ an effective supervised lexical knowledge denoising method to smooth out matching noise. Finally, we introduce a col-wise attention based knowledge fusion mechanism to guarantee the pluggability of the proposed framework. Experiments on ten datasets of three tasks show that the proposed framework achieves new SOTA, even with very large scale lexicons",
    "checked": true,
    "id": "29795cb98a802ca7c9f63f64300bb6554e05b389",
    "semantic_title": "dylex: incorporating dynamic lexicons into bert for sequence labeling",
    "citation_count": 6,
    "authors": [
      "Baojun Wang",
      "Zhao Zhang",
      "Kun Xu",
      "Guang-Yuan Hao",
      "Yuyang Zhang",
      "Lifeng Shang",
      "Linlin Li",
      "Xiao Chen",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.212": {
    "title": "MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction",
    "volume": "main",
    "abstract": "Neural relation extraction models have shown promising results in recent years; however, the model performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of the relation itself, is often neglected for prediction. In this work, we propose a framework considering both label-agnostic and label-aware semantic mapping information for low resource relation extraction. We show that incorporating the above two types of mapping information in both pretraining and fine-tuning can significantly improve the model performance on low-resource relation extraction tasks",
    "checked": true,
    "id": "35f26a3c3f9013e419b47c928d92c333a0e09aa3",
    "semantic_title": "mapre: an effective semantic mapping approach for low-resource relation extraction",
    "citation_count": 42,
    "authors": [
      "Manqing Dong",
      "Chunguang Pan",
      "Zhipeng Luo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.213": {
    "title": "Heterogeneous Graph Neural Networks for Keyphrase Generation",
    "volume": "main",
    "abstract": "The encoder–decoder framework achieves state-of-the-art results in keyphrase generation (KG) tasks by predicting both present keyphrases that appear in the source document and absent keyphrases that do not. However, relying solely on the source document can result in generating uncontrollable and inaccurate absent keyphrases. To address these problems, we propose a novel graph-based method that can capture explicit knowledge from related references. Our model first retrieves some document-keyphrases pairs similar to the source document from a pre-defined index as references. Then a heterogeneous graph is constructed to capture relations with different levels of granularity of the source document and its retrieved references. To guide the decoding process, a hierarchical attention and copy mechanism is introduced, which directly copies appropriate words from both source document and its references based on their relevance and significance. The experimental results on multiple KG benchmarks show that the proposed model achieves significant improvements against other baseline models, especially with regard to the absent keyphrase prediction",
    "checked": true,
    "id": "268220bd8554050be752e1f3c1899941a96ed33a",
    "semantic_title": "heterogeneous graph neural networks for keyphrase generation",
    "citation_count": 24,
    "authors": [
      "Jiacheng Ye",
      "Ruijian Cai",
      "Tao Gui",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.214": {
    "title": "Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction",
    "volume": "main",
    "abstract": "Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1) implicit knowledge transfer, which enables knowledge transfer from other tasks, by building a unified training framework in the MRC formulation, and 2) explicit data augmentation, which can explicitly generate new training examples, by treating MRC models as an annotator. The extensive experiments have justified the effectiveness of our approach — it not only obtains state-of-the-art performance on two benchmarks, but also demonstrates superior results in a data-low scenario",
    "checked": true,
    "id": "648a53e10c8caf276c8bc2e6777ba4baec76fac6",
    "semantic_title": "machine reading comprehension as data augmentation: a case study on implicit event argument extraction",
    "citation_count": 62,
    "authors": [
      "Jian Liu",
      "Yufeng Chen",
      "Jinan Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.215": {
    "title": "Importance Estimation from Multiple Perspectives for Keyphrase Extraction",
    "volume": "main",
    "abstract": "Keyphrase extraction is a fundamental task in Natural Language Processing, which usually contains two main parts: candidate keyphrase extraction and keyphrase importance estimation. From the view of human understanding documents, we typically measure the importance of phrase according to its syntactic accuracy, information saliency, and concept consistency simultaneously. However, most existing keyphrase extraction approaches only focus on the part of them, which leads to biased results. In this paper, we propose a new approach to estimate the importance of keyphrase from multiple perspectives (called as KIEMP) and further improve the performance of keyphrase extraction. Specifically, KIEMP estimates the importance of phrase with three modules: a chunking module to measure its syntactic accuracy, a ranking module to check its information saliency, and a matching module to judge the concept (i.e., topic) consistency between phrase and the whole document. These three modules are seamlessly jointed together via an end-to-end multi-task learning model, which is helpful for three parts to enhance each other and balance the effects of three perspectives. Experimental results on six benchmark datasets show that KIEMP outperforms the existing state-of-the-art keyphrase extraction approaches in most cases",
    "checked": true,
    "id": "51819d708e00c0fe7ca3e8befc4f8665ac12ad0e",
    "semantic_title": "importance estimation from multiple perspectives for keyphrase extraction",
    "citation_count": 32,
    "authors": [
      "Mingyang Song",
      "Liping Jing",
      "Lin Xiao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.216": {
    "title": "Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction",
    "volume": "main",
    "abstract": "Low-resource Relation Extraction (LRE) aims to extract relation facts from limited labeled corpora when human annotation is scarce. Existing works either utilize self-training scheme to generate pseudo labels that will cause the gradual drift problem, or leverage meta-learning scheme which does not solicit feedback explicitly. To alleviate selection bias due to the lack of feedback loops in existing LRE learning paradigms, we developed a Gradient Imitation Reinforcement Learning method to encourage pseudo label data to imitate the gradient descent direction on labeled data and bootstrap its optimization capability through trial and error. We also propose a framework called GradLRE, which handles two major scenarios in low-resource relation extraction. Besides the scenario where unlabeled data is sufficient, GradLRE handles the situation where no unlabeled data is available, by exploiting a contextualized augmentation method to generate data. Experimental results on two public datasets demonstrate the effectiveness of GradLRE on low resource relation extraction when comparing with baselines",
    "checked": true,
    "id": "4b13cec9b33288452d6412777153e732123fccd0",
    "semantic_title": "gradient imitation reinforcement learning for low resource relation extraction",
    "citation_count": 61,
    "authors": [
      "Xuming Hu",
      "Chenwei Zhang",
      "Yawen Yang",
      "Xiaohe Li",
      "Li Lin",
      "Lijie Wen",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.217": {
    "title": "Low-resource Taxonomy Enrichment with Pretrained Language Models",
    "volume": "main",
    "abstract": "Taxonomies are symbolic representations of hierarchical relationships between terms or entities. While taxonomies are useful in broad applications, manually updating or maintaining them is labor-intensive and difficult to scale in practice. Conventional supervised methods for this enrichment task fail to find optimal parents of new terms in low-resource settings where only small taxonomies are available because of overfitting to hierarchical relationships in the taxonomies. To tackle the problem of low-resource taxonomy enrichment, we propose Musubu, an efficient framework for taxonomy enrichment in low-resource settings with pretrained language models (LMs) as knowledge bases to compensate for the shortage of information. Musubu leverages an LM-based classifier to determine whether or not inputted term pairs have hierarchical relationships. Musubu also utilizes Hearst patterns to generate queries to leverage implicit knowledge from the LM efficiently for more accurate prediction. We empirically demonstrate the effectiveness of our method in extensive experiments on taxonomies from both a SemEval task and real-world retailer datasets",
    "checked": true,
    "id": "eef481d09b34f3804a5a29d161eae9086201c8c6",
    "semantic_title": "low-resource taxonomy enrichment with pretrained language models",
    "citation_count": 33,
    "authors": [
      "Kunihiro Takeoka",
      "Kosuke Akimoto",
      "Masafumi Oyamada"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.218": {
    "title": "Entity Relation Extraction as Dependency Parsing in Visually Rich Documents",
    "volume": "main",
    "abstract": "Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e.,semantic entity), while the relations in-between are largely unexplored. In this paper, we adapt the popular dependency parsing model, the biaffine parser, to this entity relation extraction task. Being different from the original dependency parsing model which recognizes dependency relations between words, we identify relations between groups of words with layout information instead. We have compared different representations of the semantic entity, different VRD encoders, and different relation decoders. For the model training, we explore multi-task learning to combine entity labeling and relation extraction tasks; and for the evaluation, we conduct experiments on different datasets with filtering and augmentation. The results demonstrate that our proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the real-world application, our model has been applied to the in-house customs data, achieving reliable performance in the production setting",
    "checked": true,
    "id": "b34c2f1a9dcf1852cd83e9ef6a370df9204713d1",
    "semantic_title": "entity relation extraction as dependency parsing in visually rich documents",
    "citation_count": 33,
    "authors": [
      "Yue Zhang",
      "Zhang Bo",
      "Rui Wang",
      "Junjie Cao",
      "Chen Li",
      "Zuyi Bao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.219": {
    "title": "Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction",
    "volume": "main",
    "abstract": "Joint entity and relation extraction is challenging due to the complex interaction of interaction between named entity recognition and relation extraction. Although most existing works tend to jointly train these two tasks through a shared network, they fail to fully utilize the interdependence between entity types and relation types. In this paper, we design a novel synchronous dual network (SDN) with cross-type attention via separately and interactively considering the entity types and relation types. On the one hand, SDN adopts two isomorphic bi-directional type-attention LSTM to encode the entity type enhanced representations and the relation type enhanced representations, respectively. On the other hand, SDN explicitly models the interdependence between entity types and relation types via cross-type attention mechanism. In addition, we also propose a new multi-task learning strategy via modeling the interaction of two types of information. Experiments on NYT and WebNLG datasets verify the effectiveness of the proposed model, achieving state-of-the-art performance",
    "checked": true,
    "id": "d6c6d2bd018b44bd42721410a9d5a768df58243f",
    "semantic_title": "synchronous dual network with cross-type attention for joint entity and relation extraction",
    "citation_count": 10,
    "authors": [
      "Hui Wu",
      "Xiaodong Shi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.220": {
    "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder",
    "volume": "main",
    "abstract": "Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/",
    "checked": true,
    "id": "5be217c678916543884c354654263f27c0a6bd9f",
    "semantic_title": "less is more: pretrain a strong siamese encoder for dense text retrieval using a weak decoder",
    "citation_count": 75,
    "authors": [
      "Shuqi Lu",
      "Di He",
      "Chenyan Xiong",
      "Guolin Ke",
      "Waleed Malik",
      "Zhicheng Dou",
      "Paul Bennett",
      "Tie-Yan Liu",
      "Arnold Overwijk"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.221": {
    "title": "TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification",
    "volume": "main",
    "abstract": "Recent studies have shown that prompts improve the performance of large pre-trained language models for few-shot text classification. Yet, it is unclear how the prompting knowledge can be transferred across similar NLP tasks for the purpose of mutual reinforcement. Based on continuous prompt embeddings, we propose TransPrompt, a transferable prompting framework for few-shot learning across similar tasks. In TransPrompt, we employ a multi-task meta-knowledge acquisition procedure to train a meta-learner that captures cross-task transferable knowledge. Two de-biasing techniques are further designed to make it more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to target tasks with high accuracy. Extensive experiments show that TransPrompt outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance on previously unseen tasks; and TransPrompt also outperforms strong fine-tuning baselines when learning with full training sets",
    "checked": true,
    "id": "90df8ca8bb6dda68a7a7996b06bbfd9a742461e5",
    "semantic_title": "transprompt: towards an automatic transferable prompting framework for few-shot text classification",
    "citation_count": 59,
    "authors": [
      "Chengyu Wang",
      "Jianing Wang",
      "Minghui Qiu",
      "Jun Huang",
      "Ming Gao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.222": {
    "title": "Weakly-supervised Text Classification Based on Keyword Graph",
    "volume": "main",
    "abstract": "Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts. However, existing methods treat keywords independently, thus ignore the correlation among them, which should be useful if properly exploited. In this paper, we propose a novel framework called ClassKG to explore keyword-keyword correlation on keyword graph by GNN. Our framework is an iterative process. In each iteration, we first construct a keyword graph, so the task of assigning pseudo labels is transformed to annotating keyword subgraphs. To improve the annotation quality, we introduce a self-supervised task to pretrain a subgraph annotator, and then finetune it. With the pseudo labels generated by the subgraph annotator, we then train a text classifier to classify the unlabeled texts. Finally, we re-extract keywords from the classified texts. Extensive experiments on both long-text and short-text datasets show that our method substantially outperforms the existing ones",
    "checked": true,
    "id": "15a6ae89b2bc959d4a5b48a8ce590526b50f1c98",
    "semantic_title": "weakly-supervised text classification based on keyword graph",
    "citation_count": 57,
    "authors": [
      "Lu Zhang",
      "Jiandong Ding",
      "Yi Xu",
      "Yingyao Liu",
      "Shuigeng Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.223": {
    "title": "Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation",
    "volume": "main",
    "abstract": "News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users' historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole model, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the user model and news representations from the server, and send their locally computed gradients to the server for aggregation. The server updates its global user model with the aggregated gradients, and further updates its news model to infer updated news representations. Since the local gradients may contain private information, we propose a secure aggregation method to aggregate gradients in a privacy-preserving way. Experiments on two real-world datasets show that our method can reduce the computation and communication cost on clients while keep promising model performance",
    "checked": true,
    "id": "fed646cd7a95cbee0d84e8be34fe514c384b7adb",
    "semantic_title": "efficient-fedrec: efficient federated learning framework for privacy-preserving news recommendation",
    "citation_count": 70,
    "authors": [
      "Jingwei Yi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Ruixuan Liu",
      "Guangzhong Sun",
      "Xing Xie"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.224": {
    "title": "RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking",
    "volume": "main",
    "abstract": "In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other's relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA",
    "checked": true,
    "id": "1cef73714d8ad89e442a9635fcd3061c61067638",
    "semantic_title": "rocketqav2: a joint training method for dense passage retrieval and passage re-ranking",
    "citation_count": 260,
    "authors": [
      "Ruiyang Ren",
      "Yingqi Qu",
      "Jing Liu",
      "Wayne Xin Zhao",
      "QiaoQiao She",
      "Hua Wu",
      "Haifeng Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.225": {
    "title": "Dealing with Typos for BERT-based Passage Retrieval and Ranking",
    "volume": "main",
    "abstract": "Passage retrieval and ranking is a key task in open-domain question answering and information retrieval. Current effective approaches mostly rely on pre-trained deep language model-based retrievers and rankers. These methods have been shown to effectively model the semantic matching between queries and passages, also in presence of keyword mismatch, i.e. passages that are relevant to a query but do not contain important query keywords. In this paper we consider the Dense Retriever (DR), a passage retrieval method, and the BERT re-ranker, a popular passage re-ranking method. In this context, we formally investigate how these models respond and adapt to a specific type of keyword mismatch – that caused by keyword typos occurring in queries. Through empirical investigation, we find that typos can lead to a significant drop in retrieval and ranking effectiveness. We then propose a simple typos-aware training framework for DR and BERT re-ranker to address this issue. Our experimental results on the MS MARCO passage ranking dataset show that, with our proposed typos-aware training, DR and BERT re-ranker can become robust to typos in queries, resulting in significantly improved effectiveness compared to models trained without appropriately accounting for typos",
    "checked": true,
    "id": "a628e990938ec4eacb720ff264369c84290598d5",
    "semantic_title": "dealing with typos for bert-based passage retrieval and ranking",
    "citation_count": 47,
    "authors": [
      "Shengyao Zhuang",
      "Guido Zuccon"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.226": {
    "title": "From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment",
    "volume": "main",
    "abstract": "Cross-lingual entity alignment (EA) aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing GNN-based EA methods inevitably inherit poor interpretability and low efficiency from neural networks. Motivated by the isomorphic assumption of GNN-based methods, we successfully transform the cross-lingual EA problem into an assignment problem. Based on this re-definition, we propose a frustratingly Simple but Effective Unsupervised entity alignment method (SEU) without neural networks. Extensive experiments have been conducted to show that our proposed unsupervised approach even beats advanced supervised methods across all public datasets while having high efficiency, interpretability, and stability",
    "checked": true,
    "id": "8434e169e00eb169e0f9fb3d6713c216d3b36fa3",
    "semantic_title": "from alignment to assignment: frustratingly simple unsupervised entity alignment",
    "citation_count": 74,
    "authors": [
      "Xin Mao",
      "Wenting Wang",
      "Yuanbin Wu",
      "Man Lan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.227": {
    "title": "Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval",
    "volume": "main",
    "abstract": "Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as BM25, but at the cost of large space and memory requirements. In this paper, we analyze the redundancy present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of principal component analysis (PCA), product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracy–space trade-offs, for example, 48× compression with less than 3% drop in top-100 retrieval accuracy on average or 96× compression with less than 4% drop. Code and data are available at http://pyserini.io/",
    "checked": true,
    "id": "93ddb1188e4c6197a49fe261686e676cdf466623",
    "semantic_title": "simple and effective unsupervised redundancy elimination to compress dense vectors for passage retrieval",
    "citation_count": 20,
    "authors": [
      "Xueguang Ma",
      "Minghan Li",
      "Kai Sun",
      "Ji Xin",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.228": {
    "title": "Relation Extraction with Word Graphs from N-grams",
    "volume": "main",
    "abstract": "Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for in-domain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets",
    "checked": true,
    "id": "33f11842f5aee2168409ec5c16f92ebf84af7576",
    "semantic_title": "relation extraction with word graphs from n-grams",
    "citation_count": 26,
    "authors": [
      "Han Qin",
      "Yuanhe Tian",
      "Yan Song"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.229": {
    "title": "A Bayesian Framework for Information-Theoretic Probing",
    "volume": "main",
    "abstract": "Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents—allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task",
    "checked": true,
    "id": "884c7aac3358a6f91887dd0d091759963764bedd",
    "semantic_title": "a bayesian framework for information-theoretic probing",
    "citation_count": 25,
    "authors": [
      "Tiago Pimentel",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.230": {
    "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little",
    "volume": "main",
    "abstract": "A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks—including tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge",
    "checked": true,
    "id": "4e00843bc5f60d2b9116abc4320af6d184422291",
    "semantic_title": "masked language modeling and the distributional hypothesis: order word matters pre-training for little",
    "citation_count": 250,
    "authors": [
      "Koustuv Sinha",
      "Robin Jia",
      "Dieuwke Hupkes",
      "Joelle Pineau",
      "Adina Williams",
      "Douwe Kiela"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.231": {
    "title": "What's Hidden in a One-layer Randomly Weighted Transformer?",
    "volume": "main",
    "abstract": "We demonstrate that, hidden within one-layer randomly weighted neural networks, there exist subnetworks that can achieve impressive performance, without ever modifying the weight initializations, on machine translation tasks. To find subnetworks for one-layer randomly weighted neural networks, we apply different binary masks to the same weight matrix to generate different layers. Hidden within a one-layer randomly weighted Transformer, we find that subnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed pre-trained embedding layer, the previously found subnetworks are smaller than, but can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained Transformersmall/base on IWSLT14/WMT14. Furthermore, we demonstrate the effectiveness of larger and deeper transformers in this setting, as well as the impact of different initialization methods",
    "checked": true,
    "id": "cebd54ea966172111d08b4f215a73fe84350562e",
    "semantic_title": "what's hidden in a one-layer randomly weighted transformer?",
    "citation_count": 4,
    "authors": [
      "Sheng Shen",
      "Zhewei Yao",
      "Douwe Kiela",
      "Kurt Keutzer",
      "Michael Mahoney"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.232": {
    "title": "Rethinking Denoised Auto-Encoding in Language Pre-Training",
    "volume": "main",
    "abstract": "Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6% absolute gain on GLUE benchmarks and 0.8% absolute increment on NLVR2",
    "checked": true,
    "id": "1367823f7eae056c56357989cdc397bc19cd4104",
    "semantic_title": "rethinking denoised auto-encoding in language pre-training",
    "citation_count": 7,
    "authors": [
      "Fuli Luo",
      "Pengcheng Yang",
      "Shicheng Li",
      "Xuancheng Ren",
      "Xu Sun",
      "Songfang Huang",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.233": {
    "title": "Lifelong Explainer for Lifelong Learners",
    "volume": "main",
    "abstract": "Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME, which are computationally expensive when explaining a static black-box model, are even more inefficient in the LL setting. In this paper, we propose a novel Lifelong Explanation (LLE) approach that continuously trains a student explainer under the supervision of a teacher – an arbitrary explanation algorithm – on different tasks undertaken in LL. We also leverage the Experience Replay (ER) mechanism to prevent catastrophic forgetting in the student explainer. Our experiments comparing LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 10ˆ2 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https://github.com/situsnow/LLE",
    "checked": true,
    "id": "c1dfd8baf064e56b508327331434a7affe72f5ec",
    "semantic_title": "lifelong explainer for lifelong learners",
    "citation_count": 0,
    "authors": [
      "Xuelin Situ",
      "Sameen Maruf",
      "Ingrid Zukerman",
      "Cecile Paris",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.234": {
    "title": "Linguistic Dependencies and Statistical Dependence",
    "volume": "main",
    "abstract": "Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most ≈ 0.5. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture",
    "checked": true,
    "id": "bcb651d73447d96be58db5fac6fb13324842b351",
    "semantic_title": "linguistic dependencies and statistical dependence",
    "citation_count": 15,
    "authors": [
      "Jacob Louis Hoover",
      "Wenyu Du",
      "Alessandro Sordoni",
      "Timothy J. O’Donnell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.235": {
    "title": "Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars",
    "volume": "main",
    "abstract": "In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be discussed",
    "checked": true,
    "id": "c44799b1291616544dfa48fb10b74cc4f73706e2",
    "semantic_title": "modeling human sentence processing with left-corner recurrent neural network grammars",
    "citation_count": 10,
    "authors": [
      "Ryo Yoshida",
      "Hiroshi Noji",
      "Yohei Oseki"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.236": {
    "title": "A Simple and Effective Positional Encoding for Transformers",
    "volume": "main",
    "abstract": "Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain",
    "checked": true,
    "id": "db46b0de44c5113c47f0ec5392eb91d0726497bf",
    "semantic_title": "a simple and effective positional encoding for transformers",
    "citation_count": 66,
    "authors": [
      "Pu-Chin Chen",
      "Henry Tsai",
      "Srinadh Bhojanapalli",
      "Hyung Won Chung",
      "Yin-Wen Chang",
      "Chun-Sung Ferng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.237": {
    "title": "Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models",
    "volume": "main",
    "abstract": "Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the Gaussian function to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different RPEs",
    "checked": true,
    "id": "8d6b1929b92211ad0eb3e14b2c2b41789ccf053a",
    "semantic_title": "explore better relative position embeddings from encoding perspective for transformer models",
    "citation_count": 7,
    "authors": [
      "Anlin Qu",
      "Jianwei Niu",
      "Shasha Mo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.238": {
    "title": "Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup",
    "volume": "main",
    "abstract": "Mixup is a recent regularizer for current deep classification networks. Through training a neural network on convex combinations of pairs of examples and their labels, it imposes locally linear constraints on the model's input space. However, such strict linear constraints often lead to under-fitting which degrades the effects of regularization. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing Policy (AMP), organized in a \"min-max-rand\" formulation, to relax the Locally Linear Constraints in Mixup. Specifically, AMP adds a small adversarial perturbation to the mixing coefficients rather than the examples. Thus, slight non-linearity is injected in-between the synthetic examples and synthetic labels. By training on these data, the deep networks are further regularized, and thus achieve a lower predictive error rate. Experiments on five text classification benchmarks and five backbone models have empirically shown that our methods reduce the error rate over Mixup variants in a significant margin (up to 31.3%), especially in low-resource conditions (up to 17.5%)",
    "checked": true,
    "id": "72d7a20623d5f760df36d4467aa68f5002b8e92a",
    "semantic_title": "adversarial mixing policy for relaxing locally linear constraints in mixup",
    "citation_count": 5,
    "authors": [
      "Guang Liu",
      "Yuzhao Mao",
      "Huang Hailong",
      "Gao Weiguo",
      "Li Xuan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.239": {
    "title": "Is this the end of the gold standard? A straightforward reference-less grammatical error correction metric",
    "volume": "main",
    "abstract": "It is difficult to rank and evaluate the performance of grammatical error correction (GEC) systems, as a sentence can be rewritten in numerous correct ways. A number of GEC metrics have been used to evaluate proposed GEC systems; however, each system relies on either a comparison with one or more reference texts—in what is known as the gold standard for reference-based metrics—or a separate annotated dataset to fine-tune the reference-less metric. Reference-based systems have a low correlation with human judgement, cannot capture all the ways in which a sentence can be corrected, and require substantial work to develop a test dataset. We propose a reference-less GEC evaluation system that is strongly correlated with human judgement, solves the issues related to the use of a reference, and does not need another annotated dataset for fine-tuning. The proposed system relies solely on commonly available tools. Additionally, currently available reference-less metrics do not work properly when part of a sentence is repeated as opposed to reference-based metrics. In our proposed system, we look to address issues inherent in reference-less metrics and reference-based metrics",
    "checked": true,
    "id": "d0f5491ca2110efb926252a3eb0c2b0cab1ed56b",
    "semantic_title": "is this the end of the gold standard? a straightforward reference-less grammatical error correction metric",
    "citation_count": 17,
    "authors": [
      "Md Asadul Islam",
      "Enrico Magnani"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.240": {
    "title": "Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations",
    "volume": "main",
    "abstract": "Current language models are usually trained using a self-supervised scheme, where the main focus is learning representations at the word or sentence level. However, there has been limited progress in generating useful discourse-level representations. In this work, we propose to use ideas from predictive coding theory to augment BERT-style language models with a mechanism that allows them to learn suitable discourse-level representations. As a result, our proposed approach is able to predict future sentences using explicit top-down connections that operate at the intermediate layers of the network. By experimenting with benchmarks designed to evaluate discourse-related knowledge using pre-trained sentence representations, we demonstrate that our approach improves performance in 6 out of 11 tasks by excelling in discourse relationship detection",
    "checked": true,
    "id": "a0bd5ba1c09a287b9c3f9841743fb9398195b323",
    "semantic_title": "augmenting bert-style models with predictive coding to improve discourse-level representations",
    "citation_count": 7,
    "authors": [
      "Vladimir Araujo",
      "Andrés Villa",
      "Marcelo Mendoza",
      "Marie-Francine Moens",
      "Alvaro Soto"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.241": {
    "title": "Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning",
    "volume": "main",
    "abstract": "Pre-Trained Models have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors; we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies",
    "checked": true,
    "id": "2ad898669cf696dccd9879d6e203c13aee3376e4",
    "semantic_title": "backdoor attacks on pre-trained models by layerwise weight poisoning",
    "citation_count": 146,
    "authors": [
      "Linyang Li",
      "Demin Song",
      "Xiaonan Li",
      "Jiehang Zeng",
      "Ruotian Ma",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.242": {
    "title": "GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning",
    "volume": "main",
    "abstract": "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT's contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT's early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods",
    "checked": true,
    "id": "6fbd2bcf27c3df92c824406888cd9b25ceff31e3",
    "semantic_title": "gaml-bert: improving bert early exiting by gradient aligned mutual learning",
    "citation_count": 21,
    "authors": [
      "Wei Zhu",
      "Xiaoling Wang",
      "Yuan Ni",
      "Guotong Xie"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.243": {
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "volume": "main",
    "abstract": "In this work, we explore \"prompt tuning,\" a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \"prompt ensembling.\" We release code and model checkpoints to reproduce our experiments",
    "checked": true,
    "id": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
    "semantic_title": "the power of scale for parameter-efficient prompt tuning",
    "citation_count": 4249,
    "authors": [
      "Brian Lester",
      "Rami Al-Rfou",
      "Noah Constant"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.244": {
    "title": "Scalable Font Reconstruction with Dual Latent Manifolds",
    "volume": "main",
    "abstract": "We propose a deep generative model that performs typography analysis and font reconstruction by learning disentangled manifolds of both font style and character shape. Our approach enables us to massively scale up the number of character types we can effectively model compared to previous methods. Specifically, we infer separate latent variables representing character and font via a pair of inference networks which take as input sets of glyphs that either all share a character type, or belong to the same font. This design allows our model to generalize to characters that were not observed during training time, an important task in light of the relative sparsity of most fonts. We also put forward a new loss, adapted from prior work that measures likelihood using an adaptive distribution in a projected space, resulting in more natural images without requiring a discriminator. We evaluate on the task of font reconstruction over various datasets representing character types of many languages, and compare favorably to modern style transfer systems according to both automatic and manually-evaluated metrics",
    "checked": true,
    "id": "9c2cd0c5553d4609063964a2355b838cb0ef3c17",
    "semantic_title": "scalable font reconstruction with dual latent manifolds",
    "citation_count": 5,
    "authors": [
      "Nikita Srivatsan",
      "Si Wu",
      "Jonathan Barron",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.245": {
    "title": "Neuro-Symbolic Approaches for Text-Based Policy Learning",
    "volume": "main",
    "abstract": "Text-Based Games (TBGs) have emerged as important testbeds for reinforcement learning (RL) in the natural language domain. Previous methods using LSTM-based action policies are uninterpretable and often overfit the training games showing poor performance to unseen test games. We present SymboLic Action policy for Textual Environments (SLATE), that learns interpretable action policy rules from symbolic abstractions of textual observations for improved generalization. We outline a method for end-to-end differentiable symbolic rule learning and show that such symbolic policies outperform previous state-of-the-art methods in text-based RL for the coin collector environment from 5-10x fewer training games. Additionally, our method provides human-understandable policy rules that can be readily verified for their logical consistency and can be easily debugged",
    "checked": true,
    "id": "4be5ee7015dc0efcc317a1647d925ed4af8f36ed",
    "semantic_title": "neuro-symbolic approaches for text-based policy learning",
    "citation_count": 14,
    "authors": [
      "Subhajit Chaudhury",
      "Prithviraj Sen",
      "Masaki Ono",
      "Daiki Kimura",
      "Michiaki Tatsubori",
      "Asim Munawar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.246": {
    "title": "Layer-wise Model Pruning based on Mutual Information",
    "volume": "main",
    "abstract": "Inspired by mutual information (MI) based feature selection in SVMs and logistic regression, in this paper, we propose MI-based layer-wise pruning: for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved. Starting from the top softmax layer, layer-wise pruning proceeds in a top-down fashion until reaching the bottom word embedding layer. The proposed pruning strategy offers merits over weight-based pruning techniques: (1) it avoids irregular memory access since representations and matrices can be squeezed into their smaller but dense counterparts, leading to greater speedup; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning)",
    "checked": true,
    "id": "d0b7670f0739beb65f45b3e85f5675364f94bd30",
    "semantic_title": "layer-wise model pruning based on mutual information",
    "citation_count": 19,
    "authors": [
      "Chun Fan",
      "Jiwei Li",
      "Tianwei Zhang",
      "Xiang Ao",
      "Fei Wu",
      "Yuxian Meng",
      "Xiaofei Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.247": {
    "title": "Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification",
    "volume": "main",
    "abstract": "Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels",
    "checked": true,
    "id": "77dbebb3ebf48b3e6bf1e420e4da55c95320a521",
    "semantic_title": "hierarchical heterogeneous graph representation learning for short text classification",
    "citation_count": 46,
    "authors": [
      "Yaqing Wang",
      "Song Wang",
      "Quanming Yao",
      "Dejing Dou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.248": {
    "title": "kFolden: k-Fold Ensemble for Out-Of-Distribution Detection",
    "volume": "main",
    "abstract": "Out-of-Distribution (OOD) detection is an important problem in natural language processing (NLP). In this work, we propose a simple yet effective framework kFolden, which mimics the behaviors of OOD detection during training without the use of any external data. For a task with k training labels, kFolden induces k sub-models, each of which is trained on a subset with k-1 categories with the left category masked unknown to the sub-model. Exposing an unknown label to the sub-model during training, the model is encouraged to learn to equally attribute the probability to the seen k-1 labels for the unknown label, enabling this framework to simultaneously resolve in- and out-distribution examples in a natural way via OOD simulations. Taking text classification as an archetype, we develop benchmarks for OOD detection using existing text classification datasets. By conducting comprehensive comparisons and analyses on the developed benchmarks, we demonstrate the superiority of kFolden against current methods in terms of improving OOD detection performances while maintaining improved in-domain classification accuracy",
    "checked": true,
    "id": "c1526ec26d67a4976eae17548e6c3296adc6f5ac",
    "semantic_title": "kfolden: k-fold ensemble for out-of-distribution detection",
    "citation_count": 13,
    "authors": [
      "Xiaoya Li",
      "Jiwei Li",
      "Xiaofei Sun",
      "Chun Fan",
      "Tianwei Zhang",
      "Fei Wu",
      "Yuxian Meng",
      "Jun Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.249": {
    "title": "Frustratingly Simple Pretraining Alternatives to Masked Language Modeling",
    "volume": "main",
    "abstract": "Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE's parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective",
    "checked": true,
    "id": "acac699d02a972f58b091bfbef7518f0e61c8225",
    "semantic_title": "frustratingly simple pretraining alternatives to masked language modeling",
    "citation_count": 24,
    "authors": [
      "Atsuki Yamaguchi",
      "George Chrysostomou",
      "Katerina Margatina",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.250": {
    "title": "HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression",
    "volume": "main",
    "abstract": "On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https://github.com/cheneydon/hrkd",
    "checked": true,
    "id": "86a466b2c245e82c5216be072ebc28763f48c963",
    "semantic_title": "hrkd: hierarchical relational knowledge distillation for cross-domain language model compression",
    "citation_count": 7,
    "authors": [
      "Chenhe Dong",
      "Yaliang Li",
      "Ying Shen",
      "Minghui Qiu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.251": {
    "title": "Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution",
    "volume": "main",
    "abstract": "Recent studies have shown that deep neural network-based models are vulnerable to intentionally crafted adversarial examples, and various methods have been proposed to defend against adversarial word-substitution attacks for neural NLP models. However, there is a lack of systematic study on comparing different defense approaches under the same attacking setting. In this paper, we seek to fill the gap of systematic studies through comprehensive researches on understanding the behavior of neural text classifiers trained by various defense methods under representative adversarial attacks. In addition, we propose an effective method to further improve the robustness of neural text classifiers against such attacks, and achieved the highest accuracy on both clean and adversarial examples on AGNEWS and IMDB datasets by a significant margin. We hope this study could provide useful clues for future research on text adversarial defense. Codes are available at https://github.com/RockyLzy/TextDefender",
    "checked": true,
    "id": "31e46ed4722a4895a19eda37dbc02da55572783a",
    "semantic_title": "searching for an effective defender: benchmarking defense against adversarial word substitution",
    "citation_count": 76,
    "authors": [
      "Zongyi Li",
      "Jianhan Xu",
      "Jiehang Zeng",
      "Linyang Li",
      "Xiaoqing Zheng",
      "Qi Zhang",
      "Kai-Wei Chang",
      "Cho-Jui Hsieh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.252": {
    "title": "Re-embedding Difficult Samples via Mutual Information Constrained Semantically Oversampling for Imbalanced Text Classification",
    "volume": "main",
    "abstract": "Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that can generate anchor instances to help the backbone network determine the re-embedding position of a non-overlapping representation for each difficult sample. MISO consists of (1) a semantic fusion module that learns entangled semantics among difficult and majority samples with an adaptive multi-head attention mechanism, (2) a mutual information loss that forces our model to learn new representations of entangled semantics in the non-overlapping region of the minority class, and (3) a coupled adversarial encoder-decoder that fine-tunes disentangled semantic representations to remain their correlations with the minority class, and then using these disentangled semantic representations to generate anchor instances for each difficult sample. Experiments on a variety of imbalanced text classification tasks demonstrate that anchor instances help classifiers achieve significant improvements over strong baselines",
    "checked": true,
    "id": "6c2197a25a2e72731042534f9e3c96278cc8b8e3",
    "semantic_title": "re-embedding difficult samples via mutual information constrained semantically oversampling for imbalanced text classification",
    "citation_count": 14,
    "authors": [
      "Jiachen Tian",
      "Shizhan Chen",
      "Xiaowang Zhang",
      "Zhiyong Feng",
      "Deyi Xiong",
      "Shaojuan Wu",
      "Chunliu Dou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.253": {
    "title": "Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs",
    "volume": "main",
    "abstract": "Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are crucial for accurate multi-label document classification. Therefore, in this paper, we propose a novel neural network based approach for multi-label document classification, in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph, which models various types of metadata and their topological relations. The other is label heterogeneous graph, which is constructed based on both the labels' hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines",
    "checked": true,
    "id": "39baa95c5720f6b214e85a6637d2c66087487aa3",
    "semantic_title": "beyond text: incorporating metadata and label structure for multi-label document classification using heterogeneous graphs",
    "citation_count": 13,
    "authors": [
      "Chenchen Ye",
      "Linhai Zhang",
      "Yulan He",
      "Deyu Zhou",
      "Jie Wu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.254": {
    "title": "Natural Language Processing Meets Quantum Physics: A Survey and Categorization",
    "volume": "main",
    "abstract": "Recent research has investigated quantum NLP, designing algorithms that process natural language in quantum computers, and also quantum-inspired algorithms that improve NLP performance on classical computers. In this survey, we review representative methods at the intersection of NLP and quantum physics in the past ten years, categorizing them according to the use of quantum theory, the linguistic targets that are modeled, and the downstream application. The literature review ends with a discussion on the key factors to the success that has been achieved by existing work, as well as challenges ahead, with the goal of better understanding the promises and further directions",
    "checked": true,
    "id": "d03f91a0bc0d02b291134c26632955b0096ceae3",
    "semantic_title": "natural language processing meets quantum physics: a survey and categorization",
    "citation_count": 22,
    "authors": [
      "Sixuan Wu",
      "Jian Li",
      "Peng Zhang",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.255": {
    "title": "MetaTS: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision",
    "volume": "main",
    "abstract": "Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such formulation hinders the effectiveness of supervised methods due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for multiple languages. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to alleviate data scarcity by leveraging large multilingual unlabeled data. Prior teacher-student frameworks of self-training rely on rigid teaching strategies, which may hardly produce high-quality pseudo-labels for consecutive and interdependent tokens. On the contrary, MetaTS allows the teacher to dynamically adapt its pseudo-annotation strategies by the student's feedback on the generated pseudo-labeled data of each language and thus mitigate error propagation from noisy pseudo-labels. Extensive experiments on both public and real-world multilingual sequence labeling datasets empirically demonstrate the effectiveness of MetaTS",
    "checked": true,
    "id": "3da2195d841db8a061b849b50b5078189f468ea5",
    "semantic_title": "metats: meta teacher-student network for multilingual sequence labeling with minimal supervision",
    "citation_count": 11,
    "authors": [
      "Zheng Li",
      "Danqing Zhang",
      "Tianyu Cao",
      "Ying Wei",
      "Yiwei Song",
      "Bing Yin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.256": {
    "title": "Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings",
    "volume": "main",
    "abstract": "Neural Machine Translation (NMT) has shown a strong ability to utilize local context to disambiguate the meaning of words. However, it remains a challenge for NMT to leverage broader context information like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve translation performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English -> German and English -> French translation tasks",
    "checked": true,
    "id": "0e399cfd851196c846746e44092956beb78d2413",
    "semantic_title": "neural machine translation with heterogeneous topic knowledge embeddings",
    "citation_count": 5,
    "authors": [
      "Weixuan Wang",
      "Wei Peng",
      "Meng Zhang",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.257": {
    "title": "Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training",
    "volume": "main",
    "abstract": "Compared to monolingual models, cross-lingual models usually require a more expressive vocabulary to represent all languages adequately. We find that many languages are under-represented in recent cross-lingual language models due to the limited vocabulary capacity. To this end, we propose an algorithm VoCap to determine the desired vocabulary capacity of each language. However, increasing the vocabulary size significantly slows down the pre-training speed. In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VoCap benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github.com/bozheng-hit/VoCapXLM",
    "checked": true,
    "id": "dbd9fab59832369040661bb050daef6de405230c",
    "semantic_title": "allocating large vocabulary capacity for cross-lingual language model pre-training",
    "citation_count": 22,
    "authors": [
      "Bo Zheng",
      "Li Dong",
      "Shaohan Huang",
      "Saksham Singhal",
      "Wanxiang Che",
      "Ting Liu",
      "Xia Song",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.258": {
    "title": "Recurrent Attention for Neural Machine Translation",
    "volume": "main",
    "abstract": "Recent research questions the importance of the dot-product self-attention in Transformer models and shows that most attention heads learn simple positional patterns. In this paper, we push further in this research line and propose a novel substitute mechanism for self-attention: Recurrent AtteNtion (RAN) . RAN directly learns attention weights without any token-to-token interaction and further improves their capacity by layer-to-layer interaction. Across an extensive set of experiments on 10 machine translation tasks, we find that RAN models are competitive and outperform their Transformer counterpart in certain scenarios, with fewer parameters and inference time. Particularly, when apply RAN to the decoder of Transformer, there brings consistent improvements by about +0.5 BLEU on 6 translation tasks and +1.0 BLEU on Turkish-English translation task. In addition, we conduct extensive analysis on the attention weights of RAN to confirm their reasonableness. Our RAN is a promising alternative to build more effective and efficient NMT models",
    "checked": true,
    "id": "903fb78627f336841e7689c84d7b2576e9331f0a",
    "semantic_title": "recurrent attention for neural machine translation",
    "citation_count": 11,
    "authors": [
      "Jiali Zeng",
      "Shuangzhi Wu",
      "Yongjing Yin",
      "Yufan Jiang",
      "Mu Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.259": {
    "title": "Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating noise in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by various augmented methods. Those models provide supervision signals to each other. The experimental results show that our method outperforms the existing state of the art by 3.05 and 4.24 percentage points on two benchmark datasets, respectively. The code will be made open sourced on github",
    "checked": true,
    "id": "689db3512b67c010c44c425ff61e26f5c97e0edd",
    "semantic_title": "learning from multiple noisy augmented data sets for better cross-lingual spoken language understanding",
    "citation_count": 5,
    "authors": [
      "Yingmei Guo",
      "Linjun Shou",
      "Jian Pei",
      "Ming Gong",
      "Mingxing Xu",
      "Zhiyong Wu",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.260": {
    "title": "Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation",
    "volume": "main",
    "abstract": "Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in the text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntactic enhancement in machine translation. The importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on widely adopted WMT14 and WMT16 English to German and English to Czech language machine translation validate the RHE effectiveness",
    "checked": true,
    "id": "48bf1d216339ee13ed8e518eb67bf5fef9b001df",
    "semantic_title": "enlivening redundant heads in multi-head self-attention for machine translation",
    "citation_count": 14,
    "authors": [
      "Tianfu Zhang",
      "Heyan Huang",
      "Chong Feng",
      "Longbing Cao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.261": {
    "title": "Unsupervised Neural Machine Translation with Universal Grammar",
    "volume": "main",
    "abstract": "Machine translation usually relies on parallel corpora to provide parallel signals for training. The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised machine translation, the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation. Chomsky's Universal Grammar theory postulates that grammar is an innate form of knowledge to humans and is governed by universal principles and constraints. Therefore, in this paper, we seek to leverage such shared grammar clues to provide more explicit language parallel signals to enhance the training of unsupervised machine translation models. Through experiments on multiple typical language pairs, we demonstrate the effectiveness of our proposed approaches",
    "checked": true,
    "id": "60264a55c720642ab74da1eca9075c2bb8614b14",
    "semantic_title": "unsupervised neural machine translation with universal grammar",
    "citation_count": 5,
    "authors": [
      "Zuchao Li",
      "Masao Utiyama",
      "Eiichiro Sumita",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.262": {
    "title": "Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation",
    "volume": "main",
    "abstract": "Recently a number of approaches have been proposed to improve translation performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply \"one translation per discourse\" in NMT, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears. Then we encourage the translation of those words within a link to be consistent in two ways. On the one hand, when encoding sentences within a document we properly share context information of those words. On the other hand, we propose an auxiliary loss function to better constrain that their translation should be consistent. Experimental results on Chinese↔English and English→French translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores, but also greatly improves lexical consistency in translation",
    "checked": true,
    "id": "eda63cf4668e0471c23d6797fc3d315afce43084",
    "semantic_title": "encouraging lexical translation consistency for document-level neural machine translation",
    "citation_count": 24,
    "authors": [
      "Xinglin Lyu",
      "Junhui Li",
      "Zhengxian Gong",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.263": {
    "title": "Improving Neural Machine Translation by Bidirectional Training",
    "volume": "main",
    "abstract": "We present a simple and effective pretraining strategy – bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the training samples from \"src→tgt\" to \"src+tgt→tgt+src\" without any complicated model modifications. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experimental results show that BiT pushes the SOTA neural machine translation performance across 15 translation tasks on 8 language pairs (data sizes range from 160K to 38M) significantly higher. Encouragingly, our proposed model can complement existing data manipulation strategies, i.e. back translation, data distillation, and data diversification. Extensive analyses show that our approach functions as a novel bilingual code-switcher, obtaining better bilingual alignment",
    "checked": true,
    "id": "4f71fd26be06f479099b17af95d408c7ab272830",
    "semantic_title": "improving neural machine translation by bidirectional training",
    "citation_count": 30,
    "authors": [
      "Liang Ding",
      "Di Wu",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.264": {
    "title": "Scheduled Sampling Based on Decoding Steps for Neural Machine Translation",
    "volume": "main",
    "abstract": "Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However, vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps. Namely, it simulates an inference scene with uniform error rates, which disobeys the real inference scene, where larger decoding steps usually have higher error rates due to error accumulations. To alleviate the above discrepancy, we propose scheduled sampling methods based on decoding steps, increasing the selection chance of predicted tokens with the growth of decoding steps. Consequently, we can more realistically simulate the inference scene during training, thus better bridging the gap between training and inference. Moreover, we investigate scheduled sampling based on both training steps and decoding steps for further improvements. Experimentally, our approaches significantly outperform the Transformer baseline and vanilla scheduled sampling on three large-scale WMT tasks. Additionally, our approaches also generalize well to the text summarization task on two popular benchmarks",
    "checked": true,
    "id": "a5cc803d709442a24cf996d6769b41efa3f632f0",
    "semantic_title": "scheduled sampling based on decoding steps for neural machine translation",
    "citation_count": 17,
    "authors": [
      "Yijin Liu",
      "Fandong Meng",
      "Yufeng Chen",
      "Jinan Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.265": {
    "title": "Learning to Rewrite for Non-Autoregressive Neural Machine Translation",
    "volume": "main",
    "abstract": "Non-autoregressive neural machine translation, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying multiple refinement iterations. However, a serious drawback is that these approaches expose the serious weakness in recognizing the erroneous translation pieces. In this paper, we propose an architecture named RewriteNAT to explicitly learn to rewrite the erroneous translation pieces. Specifically, RewriteNAT utilizes a locator module to locate the erroneous ones, which are then revised into the correct ones by a revisor module. Towards keeping the consistency of data distribution with iterative decoding, an iterative training strategy is employed to further improve the capacity of rewriting. Extensive experiments conducted on several widely-used benchmarks show that RewriteNAT can achieve better performance while significantly reducing decoding time, compared with previous iterative decoding strategies. In particular, RewriteNAT can obtain competitive results with autoregressive translation on WMT14 En-De, En-Fr and WMT16 Ro-En translation benchmarks",
    "checked": true,
    "id": "23db287e75cd16ee9d71154a5eecf2ed20613798",
    "semantic_title": "learning to rewrite for non-autoregressive neural machine translation",
    "citation_count": 32,
    "authors": [
      "Xinwei Geng",
      "Xiaocheng Feng",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.266": {
    "title": "SHAPE: Shifted Absolute Position Embedding for Transformers",
    "volume": "main",
    "abstract": "Position representation is crucial for building position-aware representations in Transformers. Existing position representations suffer from a lack of generalization to test data with unseen lengths or high computational cost. We investigate shifted absolute position embedding (SHAPE) to address both issues. The basic idea of SHAPE is to achieve shift invariance, which is a key property of recent successful position representations, by randomly shifting absolute positions during training. We demonstrate that SHAPE is empirically comparable to its counterpart while being simpler and faster",
    "checked": true,
    "id": "64522a5b3476e9f201f6a5b3e312ef0005c562f1",
    "semantic_title": "shape: shifted absolute position embedding for transformers",
    "citation_count": 47,
    "authors": [
      "Shun Kiyono",
      "Sosuke Kobayashi",
      "Jun Suzuki",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.267": {
    "title": "Self-Supervised Quality Estimation for Machine Translation",
    "volume": "main",
    "abstract": "Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains",
    "checked": true,
    "id": "e00f5ee7155a09fec7b769ab315d6870a3db79da",
    "semantic_title": "self-supervised quality estimation for machine translation",
    "citation_count": 13,
    "authors": [
      "Yuanhang Zheng",
      "Zhixing Tan",
      "Meng Zhang",
      "Mieradilijiang Maimaiti",
      "Huanbo Luan",
      "Maosong Sun",
      "Qun Liu",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.268": {
    "title": "Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection",
    "volume": "main",
    "abstract": "This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score",
    "checked": true,
    "id": "4f9313938ab4e295add89caa1395c85b15abc577",
    "semantic_title": "generalised unsupervised domain adaptation of neural machine translation with cross-lingual data selection",
    "citation_count": 10,
    "authors": [
      "Thuy-Trang Vu",
      "Xuanli He",
      "Dinh Phung",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.269": {
    "title": "STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media",
    "volume": "main",
    "abstract": "Rumor detection on social media puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 by collecting posts and associated comments from Sina Weibo and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attention-masked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks co-attention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-the-art on Weibo dataset",
    "checked": true,
    "id": "76da4c6e58e8f2a0158ea62d9568c74a91bfd96f",
    "semantic_title": "stanker: stacking network based on level-grained attention-masked bert for rumor detection on social media",
    "citation_count": 37,
    "authors": [
      "Dongning Rao",
      "Xin Miao",
      "Zhihua Jiang",
      "Ran Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.270": {
    "title": "ActiveEA: Active Learning for Neural Entity Alignment",
    "volume": "main",
    "abstract": "Entity Alignment (EA) aims to match equivalent entities across different Knowledge Graphs (KGs) and is an essential step of KG fusion. Current mainstream methods – neural EA models – rely on training with seed alignment, i.e., a set of pre-aligned entity pairs which are very costly to annotate. In this paper, we devise a novel Active Learning (AL) framework for neural EA, aiming to create highly informative seed alignment to obtain more effective EA models with less annotation cost. Our framework tackles two main challenges encountered when applying AL to EA: (1) How to exploit dependencies between entities within the AL strategy. Most AL strategies assume that the data instances to sample are independent and identically distributed. However, entities in KGs are related. To address this challenge, we propose a structure-aware uncertainty sampling strategy that can measure the uncertainty of each entity as well as its impact on its neighbour entities in the KG. (2) How to recognise entities that appear in one KG but not in the other KG (i.e., bachelors). Identifying bachelors would likely save annotation budget. To address this challenge, we devise a bachelor recognizer paying attention to alleviate the effect of sampling bias. Empirical results show that our proposed AL strategy can significantly improve sampling quality with good generality across different datasets, EA models and amount of bachelors",
    "checked": true,
    "id": "84e5947d5744470de120347adee5e8c152363761",
    "semantic_title": "activeea: active learning for neural entity alignment",
    "citation_count": 27,
    "authors": [
      "Bing Liu",
      "Harrisen Scells",
      "Guido Zuccon",
      "Wen Hua",
      "Genghong Zhao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.271": {
    "title": "Cost-effective End-to-end Information Extraction for Semi-structured Document Images",
    "volume": "main",
    "abstract": "A real-world information extraction (IE) system for semi-structured document images often involves a long pipeline of multiple modules, whose complexity dramatically increases its development and maintenance cost. One can instead consider an end-to-end model that directly maps the input to the target output and simplify the entire process. However, such generation approach is known to lead to unstable performance if not designed carefully. Here we present our recent effort on transitioning from our existing pipeline-based IE system to an end-to-end system focusing on practical challenges that are associated with replacing and deploying the system in real, large-scale production. By carefully formulating document IE as a sequence generation task, we show that a single end-to-end IE system can be built and still achieve competent performance",
    "checked": true,
    "id": "39d23cc236dd5057df7ff1949fa4fe8e87559f6e",
    "semantic_title": "cost-effective end-to-end information extraction for semi-structured document images",
    "citation_count": 26,
    "authors": [
      "Wonseok Hwang",
      "Hyunji Lee",
      "Jinyeong Yim",
      "Geewook Kim",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.272": {
    "title": "Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning",
    "volume": "main",
    "abstract": "The recent algorithms for math word problems (MWP) neglect to use outside knowledge not present in the problems. Most of them only capture the word-level relationship and ignore to build hierarchical reasoning like the human being for mining the contextual structure between words and sentences. In this paper, we propose a Reasoning with Pre-trained Knowledge and Hierarchical Structure (RPKHS) network, which contains a pre-trained knowledge encoder and a hierarchical reasoning encoder. Firstly, our pre-trained knowledge encoder aims at reasoning the MWP by using outside knowledge from the pre-trained transformer-based models. Secondly, the hierarchical reasoning encoder is presented for seamlessly integrating the word-level and sentence-level reasoning to bridge the entity and context domain on MWP. Extensive experiments show that our RPKHS significantly outperforms state-of-the-art approaches on two large-scale commonly-used datasets, and boosts performance from 77.4% to 83.9% on Math23K, from 75.5 to 82.2% on Math23K with 5-fold cross-validation and from 83.7% to 89.8% on MAWPS. More extensive ablations are shown to demonstrate the effectiveness and interpretability of our proposed method",
    "checked": true,
    "id": "a2fff5686bdf8e25003097107516d6ac13bf8b8e",
    "semantic_title": "improving math word problems with pre-trained knowledge and hierarchical reasoning",
    "citation_count": 35,
    "authors": [
      "Weijiang Yu",
      "Yingpeng Wen",
      "Fudan Zheng",
      "Nong Xiao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.273": {
    "title": "GraphMR: Graph Neural Network for Mathematical Reasoning",
    "volume": "main",
    "abstract": "Mathematical reasoning aims to infer satisfiable solutions based on the given mathematics questions. Previous natural language processing researches have proven the effectiveness of sequence-to-sequence (Seq2Seq) or related variants on mathematics solving. However, few works have been able to explore structural or syntactic information hidden in expressions (e.g., precedence and associativity). This dissertation set out to investigate the usefulness of such untapped information for neural architectures. Firstly, mathematical questions are represented in the format of graphs within syntax analysis. The structured nature of graphs allows them to represent relations of variables or operators while preserving the semantics of the expressions. Having transformed to the new representations, we proposed a graph-to-sequence neural network GraphMR, which can effectively learn the hierarchical information of graphs inputs to solve mathematics and speculate answers. A complete experimental scenario with four classes of mathematical tasks and three Seq2Seq baselines is built to conduct a comprehensive analysis, and results show that GraphMR outperforms others in hidden information learning and mathematics resolving",
    "checked": true,
    "id": "244049b67dc1fbb3bc7a8c50d70b70d68609ec86",
    "semantic_title": "graphmr: graph neural network for mathematical reasoning",
    "citation_count": 12,
    "authors": [
      "Weijie Feng",
      "Binbin Liu",
      "Dongpeng Xu",
      "Qilong Zheng",
      "Yun Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.274": {
    "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers",
    "volume": "main",
    "abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications",
    "checked": true,
    "id": "a6d8d04962f84ae6225e72723869a002b9fc8036",
    "semantic_title": "what changes can large-scale language models bring? intensive study on hyperclova: billions-scale korean generative pretrained transformers",
    "citation_count": 124,
    "authors": [
      "Boseop Kim",
      "HyoungSeok Kim",
      "Sang-Woo Lee",
      "Gichang Lee",
      "Donghyun Kwak",
      "Jeon Dong Hyeon",
      "Sunghyun Park",
      "Sungju Kim",
      "Seonhoon Kim",
      "Dongpil Seo",
      "Heungsub Lee",
      "Minyoung Jeong",
      "Sungjae Lee",
      "Minsub Kim",
      "Suk Hyun Ko",
      "Seokhun Kim",
      "Taeyong Park",
      "Jinuk Kim",
      "Soyoung Kang",
      "Na-Hyeon Ryu",
      "Kang Min Yoo",
      "Minsuk Chang",
      "Soobin Suh",
      "Sookyo In",
      "Jinseong Park",
      "Kyungduk Kim",
      "Hiun Kim",
      "Jisu Jeong",
      "Yong Goo Yeo",
      "Donghoon Ham",
      "Dongju Park",
      "Min Young Lee",
      "Jaewook Kang",
      "Inho Kang",
      "Jung-Woo Ha",
      "Woomyoung Park",
      "Nako Sung"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.275": {
    "title": "APIRecX: Cross-Library API Recommendation via Pre-Trained Language Model",
    "volume": "main",
    "abstract": "For programmers, learning the usage of APIs (Application Programming Interfaces) of a software library is important yet difficult. API recommendation tools can help developers use APIs by recommending which APIs to be used next given the APIs that have been written. Traditionally, language models such as N-gram are applied to API recommendation. However, because the software libraries keep changing and new libraries keep emerging, new APIs are common. These new APIs can be seen as OOV (out of vocabulary) words and cannot be handled well by existing API recommendation approaches due to the lack of training data. In this paper, we propose APIRecX, the first cross-library API recommendation approach, which uses BPE to split each API call in each API sequence and pre-trains a GPT based language model. It then recommends APIs by fine-tuning the pre-trained model. APIRecX can migrate the knowledge of existing libraries to a new library, and can recommend APIs that are previously regarded as OOV. We evaluate APIRecX on six libraries and the results confirm its effectiveness by comparing with two typical API recommendation approaches",
    "checked": true,
    "id": "e940eb6464e452ecfdefa8a9521099e2ff022626",
    "semantic_title": "apirecx: cross-library api recommendation via pre-trained language model",
    "citation_count": 22,
    "authors": [
      "Yuning Kang",
      "Zan Wang",
      "Hongyu Zhang",
      "Junjie Chen",
      "Hanmo You"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.276": {
    "title": "GMH: A General Multi-hop Reasoning Model for KG Completion",
    "volume": "main",
    "abstract": "Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current models typically perform short distance reasoning. However, the long-distance reasoning is also vital with the ability to connect the superficially unrelated entities. To the best of our knowledge, there lacks a general framework that approaches multi-hop reasoning in mixed long-short distance reasoning scenarios. We argue that there are two key issues for a general multi-hop reasoning model: i) where to go, and ii) when to stop. Therefore, we propose a general model which resolves the issues with three modules: 1) the local-global knowledge module to estimate the possible paths, 2) the differentiated action dropout module to explore a diverse set of paths, and 3) the adaptive stopping search module to avoid over searching. The comprehensive results on three datasets demonstrate the superiority of our model with significant improvements against baselines in both short and long distance reasoning scenarios",
    "checked": true,
    "id": "6df6629d80ddd412144f71b093dc06b4e06d897f",
    "semantic_title": "gmh: a general multi-hop reasoning model for kg completion",
    "citation_count": 14,
    "authors": [
      "Yao Zhang",
      "Hongru Liang",
      "Adam Jatowt",
      "Wenqiang Lei",
      "Xin Wei",
      "Ning Jiang",
      "Zhenglu Yang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.277": {
    "title": "BPM_MT: Enhanced Backchannel Prediction Model using Multi-Task Learning",
    "volume": "main",
    "abstract": "Backchannel (BC), a short reaction signal of a listener to a speaker's utterances, helps to improve the quality of the conversation. Several studies have been conducted to predict BC in conversation; however, the utilization of advanced natural language processing techniques using lexical information presented in the utterances of a speaker has been less considered. To address this limitation, we present a BC prediction model called BPM_MT (Backchannel prediction model with multitask learning), which utilizes KoBERT, a pre-trained language model. The BPM_MT simultaneously carries out two tasks at learning: 1) BC category prediction using acoustic and lexical features, and 2) sentiment score prediction based on sentiment cues. BPM_MT exhibited 14.24% performance improvement compared to the existing baseline in the four BC categories: continuer, understanding, empathic response, and No BC. In particular, for empathic response category, a performance improvement of 17.14% was achieved",
    "checked": true,
    "id": "74496101ce4927c481874738ff23493385e7beab",
    "semantic_title": "bpm_mt: enhanced backchannel prediction model using multi-task learning",
    "citation_count": 19,
    "authors": [
      "Jin Yea Jang",
      "San Kim",
      "Minyoung Jung",
      "Saim Shin",
      "Gahgene Gweon"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.278": {
    "title": "Graphine: A Dataset for Graph-aware Terminology Definition Generation",
    "volume": "main",
    "abstract": "Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition dataset hinders the process toward definition generation. In this paper, we present a large-scale terminology definition dataset Graphine covering 2,010,648 terminology definition pairs, spanning 227 biomedical subdisciplines. Terminologies in each subdiscipline further form a directed acyclic graph, opening up new avenues for developing graph-aware text generation models. We then proposed a novel graph-aware definition generation model Graphex that integrates transformer with graph neural network. Our model outperforms existing text generation models by exploiting the graph structure of terminologies. We further demonstrated how Graphine can be used to evaluate pretrained language models, compare graph representation learning methods and predict sentence granularity. We envision Graphine to be a unique resource for definition generation and many other NLP tasks in biomedicine",
    "checked": true,
    "id": "4504c3a28efcddbb49c4e6bd2e24280ac0441687",
    "semantic_title": "graphine: a dataset for graph-aware terminology definition generation",
    "citation_count": 14,
    "authors": [
      "Zequn Liu",
      "Shukai Wang",
      "Yiyang Gu",
      "Ruiyi Zhang",
      "Ming Zhang",
      "Sheng Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.279": {
    "title": "Leveraging Order-Free Tag Relations for Context-Aware Recommendation",
    "volume": "main",
    "abstract": "Tag recommendation relies on either a ranking function for top-k tags or an autoregressive generation method. However, the previous methods neglect one of two seemingly conflicting yet desirable characteristics of a tag set: orderlessness and inter-dependency. While the ranking approach fails to address the inter-dependency among tags when they are ranked, the autoregressive approach fails to take orderlessness into account because it is designed to utilize sequential relations among tokens. We propose a sequence-oblivious generation method for tag recommendation, in which the next tag to be generated is independent of the order of the generated tags and the order of the ground truth tags occurring in training data. Empirical results on two different domains, Instagram and Stack Overflow, show that our method is significantly superior to the previous approaches",
    "checked": true,
    "id": "b79667ed76256cfa41738e575f13c30792d35609",
    "semantic_title": "leveraging order-free tag relations for context-aware recommendation",
    "citation_count": 1,
    "authors": [
      "Junmo Kang",
      "Jeonghwan Kim",
      "Suwon Shin",
      "Sung-Hyon Myaeng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.280": {
    "title": "End-to-End Conversational Search for Online Shopping with Utterance Transfer",
    "volume": "main",
    "abstract": "Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline",
    "checked": true,
    "id": "5841743b05a5cbab84c81194620eeb0321493cd2",
    "semantic_title": "end-to-end conversational search for online shopping with utterance transfer",
    "citation_count": 12,
    "authors": [
      "Liqiang Xiao",
      "Jun Ma",
      "Xin Luna Dong",
      "Pascual Martínez-Gómez",
      "Nasser Zalmout",
      "Chenwei Zhang",
      "Tong Zhao",
      "Hao He",
      "Yaohui Jin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.281": {
    "title": "Self-Supervised Curriculum Learning for Spelling Error Correction",
    "volume": "main",
    "abstract": "Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model's performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1)",
    "checked": true,
    "id": "397ba48d1afa1f339120ba26368c30994f63559f",
    "semantic_title": "self-supervised curriculum learning for spelling error correction",
    "citation_count": 13,
    "authors": [
      "Zifa Gan",
      "Hongfei Xu",
      "Hongying Zan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.282": {
    "title": "Fix-Filter-Fix: Intuitively Connect Any Models for Effective Bug Fixing",
    "volume": "main",
    "abstract": "Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input buggy code (called unchanged fix) in NMT-based approaches due to high similarity between buggy and fixed code (e.g., the difference may only appear in one particular line). Obviously, unchanged fix is not the correct fix because it is the same as the buggy code that needs to be fixed. Based on these, we propose an intuitive yet effective general framework (called Fix-Filter-Fix or Fˆ3) for bug fixing. Fˆ3 connects models with our filter mechanism to filter out the last model's unchanged fix to the next. We propose an Fˆ3 theory that can quantitatively and accurately calculate the Fˆ3 lifting effect. To evaluate, we implement the Seq2Seq Transformer (ST) and the AST2Seq Transformer (AT) to form some basic Fˆ3 instances, called Fˆ3_ST+AT and Fˆ3_AT+ST. Comparing them with single model approaches and many model connection baselines across four datasets validates the effectiveness and generality of Fˆ3 and corroborates our findings and methodology",
    "checked": true,
    "id": "9b4fd87e149a7a16383148712858f3981f2fcadd",
    "semantic_title": "fix-filter-fix: intuitively connect any models for effective bug fixing",
    "citation_count": 5,
    "authors": [
      "Haiwen Hong",
      "Jingfeng Zhang",
      "Yin Zhang",
      "Yao Wan",
      "Yulei Sui"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.283": {
    "title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
    "volume": "main",
    "abstract": "Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark",
    "checked": true,
    "id": "0fc0e93100a25ea2b561d92d1d15dc2e1cc6fbff",
    "semantic_title": "neuro-symbolic reinforcement learning with first-order logic",
    "citation_count": 39,
    "authors": [
      "Daiki Kimura",
      "Masaki Ono",
      "Subhajit Chaudhury",
      "Ryosuke Kohita",
      "Akifumi Wachi",
      "Don Joven Agravante",
      "Michiaki Tatsubori",
      "Asim Munawar",
      "Alexander Gray"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.284": {
    "title": "Biomedical Concept Normalization by Leveraging Hypernyms",
    "volume": "main",
    "abstract": "Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art model on the NCBI dataset",
    "checked": true,
    "id": "ea580eadbc457c55196501a733b8f39858907a46",
    "semantic_title": "biomedical concept normalization by leveraging hypernyms",
    "citation_count": 7,
    "authors": [
      "Cheng Yan",
      "Yuanzhe Zhang",
      "Kang Liu",
      "Jun Zhao",
      "Yafei Shi",
      "Shengping Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.285": {
    "title": "Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically",
    "volume": "main",
    "abstract": "Integrating knowledge into text is a promising way to enrich text representation, especially in the medical field. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with medical literature hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from medical literature and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and dynamic routing, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with medical literature hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKLcan more accurately associate knowledge with medical literature than mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the medical literature, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub",
    "checked": true,
    "id": "da1ea98dfd47f2af3d6931e84b89490d6e36f38d",
    "semantic_title": "leveraging capsule routing to associate knowledge with medical literature hierarchically",
    "citation_count": 1,
    "authors": [
      "Xin Liu",
      "Qingcai Chen",
      "Junying Chen",
      "Wenxiu Zhou",
      "Tingyu Liu",
      "Xinlan Yang",
      "Weihua Peng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.286": {
    "title": "Label-Enhanced Hierarchical Contextualized Representation for Sequential Metaphor Identification",
    "volume": "main",
    "abstract": "Recent metaphor identification approaches mainly consider the contextual text features within a sentence or introduce external linguistic features to the model. But they usually ignore the extra information that the data can provide, such as the contextual metaphor information and broader discourse information. In this paper, we propose a model augmented with hierarchical contextualized representation to extract more information from both sentence-level and discourse-level. At the sentence level, we leverage the metaphor information of words that except the target word in the sentence to strengthen the reasoning ability of our model via a novel label-enhanced contextualized representation. At the discourse level, the position-aware global memory network is adopted to learn long-range dependency among the same words within a discourse. Finally, our model combines the representations obtained from these two parts. The experiment results on two tasks of the VUA dataset show that our model outperforms every other state-of-the-art method that also does not use any external knowledge except what the pre-trained language model contains",
    "checked": true,
    "id": "ad561520e2bf8133902f54b02b627183d206f04a",
    "semantic_title": "label-enhanced hierarchical contextualized representation for sequential metaphor identification",
    "citation_count": 7,
    "authors": [
      "Shuqun Li",
      "Liang Yang",
      "Weidong He",
      "Shiqi Zhang",
      "Jingjie Zeng",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.287": {
    "title": "SpellBERT: A Lightweight Pretrained Model for Chinese Spelling Check",
    "volume": "main",
    "abstract": "Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a mapping between correct characters and its visually similar or phonetically similar misuses but the mapping may be out-of-domain. To that end, we propose SpellBERT, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these features with character representations, we devise masked language model alike pre-training tasks. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set",
    "checked": true,
    "id": "c92f38290af09b09b609b1685ae69f672b8ce6f5",
    "semantic_title": "spellbert: a lightweight pretrained model for chinese spelling check",
    "citation_count": 43,
    "authors": [
      "Tuo Ji",
      "Hang Yan",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.288": {
    "title": "Automated Generation of Accurate & Fluent Medical X-ray Reports",
    "volume": "main",
    "abstract": "Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical facts. Our generated medical reports, on the other hand, are fluent and, more importantly, clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm that contains three complementary modules: taking the chest X-ray images and clinical history document of patients as inputs, our classification module produces an internal checklist of disease-related topics, referred to as enriched disease embedding; the embedding representation is then passed to our transformer-based generator, to produce the medical report; meanwhile, our generator also creates a weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics. Empirical evaluations demonstrate very promising results achieved by our approach on commonly-used metrics concerning language fluency and clinical accuracy. Moreover, noticeable performance gains are consistently observed when additional input information is available, such as the clinical document and extra scans from different views",
    "checked": true,
    "id": "5b74d72befc080e827c3aa4d33428590f0f1a74e",
    "semantic_title": "automated generation of accurate & fluent medical x-ray reports",
    "citation_count": 41,
    "authors": [
      "Hoang Nguyen",
      "Dong Nie",
      "Taivanbat Badamdorj",
      "Yujie Liu",
      "Yingying Zhu",
      "Jason Truong",
      "Li Cheng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.289": {
    "title": "Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism",
    "volume": "main",
    "abstract": "In this paper, we propose a new ranking model DR-BERT, which improves the Document Retrieval (DR) task by a task-adaptive training process and a Segmented Token Recovery Mechanism (STRM). In the task-adaptive training, we first pre-train DR-BERT to be domain-adaptive and then make the two-phase fine-tuning. In the first-phase fine-tuning, the model learns query-document matching patterns regarding different query types in a pointwise way. Next, in the second-phase fine-tuning, the model learns document-level ranking features and ranks documents with regard to a given query in a listwise manner. Such pointwise plus listwise fine-tuning enables the model to minimize errors in the document ranking by incorporating ranking-specific supervisions. Meanwhile, the model derived from pointwise fine-tuning is also used to reduce noise in the training data of the listwise fine-tuning. On the other hand, we present STRM which can compute OOV word representation and contextualization more precisely in BERT-based models. As an effective strategy in DR-BERT, STRM improves the matching perfromance of OOV words between a query and a document. Notably, our DR-BERT model keeps in the top three on the MS MARCO leaderboard since May 20, 2020",
    "checked": true,
    "id": "ee8205b349a243f9ac8feceafa3c032961076445",
    "semantic_title": "enhancing document ranking with task-adaptive training and segmented token recovery mechanism",
    "citation_count": 1,
    "authors": [
      "Xingwu Sun",
      "Yanling Cui",
      "Hongyin Tang",
      "Fuzheng Zhang",
      "Beihong Jin",
      "Shi Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.290": {
    "title": "Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification",
    "volume": "main",
    "abstract": "Scientific claim verification can help the researchers to easily find the target scientific papers with the sentence evidence from a large corpus for the given claim. Some existing works propose pipeline models on the three tasks of abstract retrieval, rationale selection and stance prediction. Such works have the problems of error propagation among the modules in the pipeline and lack of sharing valuable information among modules. We thus propose an approach, named as ARSJoint, that jointly learns the modules for the three tasks with a machine reading comprehension framework by including claim information. In addition, we enhance the information exchanges and constraints among tasks by proposing a regularization term between the sentence attention scores of abstract retrieval and the estimated outputs of rational selection. The experimental results on the benchmark dataset SciFact show that our approach outperforms the existing works",
    "checked": true,
    "id": "9ac5bf05caf955a04ea9d20b7d2f1e79000115d3",
    "semantic_title": "abstract, rationale, stance: a joint model for scientific claim verification",
    "citation_count": 29,
    "authors": [
      "Zhiwei Zhang",
      "Jiyi Li",
      "Fumiyo Fukumoto",
      "Yanming Ye"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.291": {
    "title": "A Fine-Grained Domain Adaption Model for Joint Word Segmentation and POS Tagging",
    "volume": "main",
    "abstract": "Domain adaption for word segmentation and POS tagging is a challenging problem for Chinese lexical processing. Self-training is one promising solution for it, which struggles to construct a set of high-quality pseudo training instances for the target domain. Previous work usually assumes a universal source-to-target adaption to collect such pseudo corpus, ignoring the different gaps from the target sentences to the source domain. In this work, we start from joint word segmentation and POS tagging, presenting a fine-grained domain adaption method to model the gaps accurately. We measure the gaps by one simple and intuitive metric, and adopt it to develop a pseudo target domain corpus based on fine-grained subdomains incrementally. A novel domain-mixed representation learning model is proposed accordingly to encode the multiple subdomains effectively. The whole process is performed progressively for both corpus construction and model training. Experimental results on a benchmark dataset show that our method can gain significant improvements over a vary of baselines. Extensive analyses are performed to show the advantages of our final domain adaption model as well",
    "checked": true,
    "id": "ce29e0aa6e6569f137d1d248ec497a63c65235fe",
    "semantic_title": "a fine-grained domain adaption model for joint word segmentation and pos tagging",
    "citation_count": 6,
    "authors": [
      "Peijie Jiang",
      "Dingkun Long",
      "Yueheng Sun",
      "Meishan Zhang",
      "Guangwei Xu",
      "Pengjun Xie"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.292": {
    "title": "Answering Open-Domain Questions of Varying Reasoning Steps from Text",
    "volume": "main",
    "abstract": "We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks—retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents—in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability. Instead, we design a system that can answer open-domain questions on any text collection without prior knowledge of reasoning complexity. To emulate this setting, we construct a new benchmark, called BeerQA, by combining existing one- and two-step datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process. We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark. We make the new benchmark available at https://beerqa.github.io/",
    "checked": true,
    "id": "c2482d0c49c2cfb1a50c24fb2177182893a9d1a0",
    "semantic_title": "answering open-domain questions of varying reasoning steps from text",
    "citation_count": 57,
    "authors": [
      "Peng Qi",
      "Haejun Lee",
      "Tg Sido",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.293": {
    "title": "Adaptive Information Seeking for Open-Domain Question Answering",
    "volume": "main",
    "abstract": "Information seeking is an essential step for open-domain question answering to efficiently gather evidence from a large corpus. Recently, iterative approaches have been proven to be effective for complex questions, by recursively retrieving new evidence at each step. However, almost all existing iterative approaches use predefined strategies, either applying the same retrieval function multiple times or fixing the order of different retrieval functions, which cannot fulfill the diverse requirements of various questions. In this paper, we propose a novel adaptive information-seeking strategy for open-domain question answering, namely AISO. Specifically, the whole retrieval and answer process is modeled as a partially observed Markov decision process, where three types of retrieval operations (e.g., BM25, DPR, and hyperlink) and one answer operation are defined as actions. According to the learned policy, AISO could adaptively select a proper retrieval action to seek the missing evidence at each step, based on the collected evidence and the reformulated query, or directly output the answer when the evidence set is sufficient for the question. Experiments on SQuAD Open and HotpotQA fullwiki, which serve as single-hop and multi-hop open-domain QA benchmarks, show that AISO outperforms all baseline methods with predefined strategies in terms of both retrieval and answer evaluations",
    "checked": true,
    "id": "7a025a90cc36e668fb69f980ee809cd7d752854e",
    "semantic_title": "adaptive information seeking for open-domain question answering",
    "citation_count": 40,
    "authors": [
      "Yunchang Zhu",
      "Liang Pang",
      "Yanyan Lan",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.294": {
    "title": "Mapping probability word problems to executable representations",
    "volume": "main",
    "abstract": "While solving math word problems automatically has received considerable attention in the NLP community, few works have addressed probability word problems specifically. In this paper, we employ and analyse various neural models for answering such word problems. In a two-step approach, the problem text is first mapped to a formal representation in a declarative language using a sequence-to-sequence model, and then the resulting representation is executed using a probabilistic programming system to provide the answer. Our best performing model incorporates general-domain contextualised word representations that were finetuned using transfer learning on another in-domain dataset. We also apply end-to-end models to this task, which bring out the importance of the two-step approach in obtaining correct solutions to probability problems",
    "checked": true,
    "id": "6d38487e0e644f78d8827513c89230f239faa11f",
    "semantic_title": "mapping probability word problems to executable representations",
    "citation_count": 7,
    "authors": [
      "Simon Suster",
      "Pieter Fivez",
      "Pietro Totis",
      "Angelika Kimmig",
      "Jesse Davis",
      "Luc de Raedt",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.295": {
    "title": "Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations",
    "volume": "main",
    "abstract": "Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines' ability to understand human language. Multiple-choice MRC is one of the most studied tasks in MRC due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained model and reveal how the model arrives at the prediction. One of the most important interpretation forms is to attribute model decisions to input features. Based on post-hoc interpretation methods, we assess attributions of paragraphs in multiple-choice MRC and improve the model by punishing the illogical attributions. Our method can improve model performance without any external information and model structure change. Furthermore, we also analyze how and why such a self-training method works",
    "checked": true,
    "id": "50b22d9e1dffdd73b205f5726bfcaaec42b9ebf8",
    "semantic_title": "enhancing multiple-choice machine reading comprehension by punishing illogical interpretations",
    "citation_count": 9,
    "authors": [
      "Yiming Ju",
      "Yuanzhe Zhang",
      "Zhixing Tian",
      "Kang Liu",
      "Xiaohuan Cao",
      "Wenting Zhao",
      "Jinlong Li",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.296": {
    "title": "Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models",
    "volume": "main",
    "abstract": "The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete",
    "checked": true,
    "id": "375b9b36ef68678185f2b6e4dbbbe7bbfad6535a",
    "semantic_title": "large-scale relation learning for question answering over knowledge bases with pre-trained language models",
    "citation_count": 35,
    "authors": [
      "Yuanmeng Yan",
      "Rumei Li",
      "Sirui Wang",
      "Hongzhi Zhang",
      "Zan Daoguang",
      "Fuzheng Zhang",
      "Wei Wu",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.297": {
    "title": "Phrase Retrieval Learns Passage Retrieval, Too",
    "volume": "main",
    "abstract": "Dense retrieval methods have shown great promise over sparse retrieval methods in a range of NLP problems. Among them, dense phrase retrieval—the most fine-grained retrieval unit—is appealing because phrases can be directly used as the output for question answering and slot filling tasks. In this work, we follow the intuition that retrieving phrases naturally entails retrieving larger text blocks and study whether phrase retrieval can serve as the basis for coarse-level retrieval including passages and documents. We first observe that a dense phrase-retrieval system, without any retraining, already achieves better passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage retrievers, which also helps achieve superior end-to-end QA performance with fewer passages. Then, we provide an interpretation for why phrase-level supervision helps learn better fine-grained entailment compared to passage-level supervision, and also show that phrase retrieval can be improved to achieve competitive performance in document-retrieval tasks such as entity linking and knowledge-grounded dialogue. Finally, we demonstrate how phrase filtering and vector quantization can reduce the size of our index by 4-10x, making dense phrase retrieval a practical and versatile solution in multi-granularity retrieval",
    "checked": true,
    "id": "3912c7224da9b0a999d57a0df33c44a24c6f751b",
    "semantic_title": "phrase retrieval learns passage retrieval, too",
    "citation_count": 48,
    "authors": [
      "Jinhyuk Lee",
      "Alexander Wettig",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.298": {
    "title": "Neural Natural Logic Inference for Interpretable Question Answering",
    "volume": "main",
    "abstract": "Many open-domain question answering problems can be cast as a textual entailment task, where a question and candidate answers are concatenated to form hypotheses. A QA system then determines if the supporting knowledge bases, regarded as potential premises, entail the hypotheses. In this paper, we investigate a neural-symbolic QA approach that integrates natural logic reasoning within deep learning architectures, towards developing effective and yet explainable question answering models. The proposed model gradually bridges a hypothesis and candidate premises following natural logic inference steps to build proof paths. Entailment scores between the acquired intermediate hypotheses and candidate premises are measured to determine if a premise entails the hypothesis. As the natural logic reasoning process forms a tree-like, hierarchical structure, we embed hypotheses and premises in a Hyperbolic space rather than Euclidean space to acquire more precise representations. Empirically, our method outperforms prior work on answering multiple-choice science questions, achieving the best results on two publicly available datasets. The natural logic inference process inherently provides evidence to help explain the prediction process",
    "checked": true,
    "id": "8b4efc283268b54bf13d2e1c3edfcc0a21abaa4e",
    "semantic_title": "neural natural logic inference for interpretable question answering",
    "citation_count": 16,
    "authors": [
      "Jihao Shi",
      "Xiao Ding",
      "Li Du",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.299": {
    "title": "Smoothing Dialogue States for Open Conversational Machine Reading",
    "volume": "main",
    "abstract": "Conversational machine reading (CMR) requires machines to communicate with humans through multi-turn interactions between two salient dialogue states of decision making and question generation processes. In open CMR settings, as the more realistic scenario, the retrieved background knowledge would be noisy, which results in severe challenges in the information transmission. Existing studies commonly train independent or pipeline systems for the two subtasks. However, those methods are trivial by using hard-label decisions to activate question generation, which eventually hinders the model performance. In this work, we propose an effective gating strategy by smoothing the two dialogue states in only one decoder and bridge decision making and question generation to provide a richer dialogue state reference. Experiments on the OR-ShARC dataset show the effectiveness of our method, which achieves new state-of-the-art results",
    "checked": true,
    "id": "52c5c9575ebd990ed34867708dd42aa8ba9d561f",
    "semantic_title": "smoothing dialogue states for open conversational machine reading",
    "citation_count": 6,
    "authors": [
      "Zhuosheng Zhang",
      "Siru Ouyang",
      "Hai Zhao",
      "Masao Utiyama",
      "Eiichiro Sumita"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.300": {
    "title": "FinQA: A Dataset of Numerical Reasoning over Financial Data",
    "volume": "main",
    "abstract": "The sheer volume of financial statements makes it difficult for humans to access and analyze a business's financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset – the first of its kind – should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA",
    "checked": true,
    "id": "99053e3a708fc27709c9dab33110dc98b187c158",
    "semantic_title": "finqa: a dataset of numerical reasoning over financial data",
    "citation_count": 371,
    "authors": [
      "Zhiyu Chen",
      "Wenhu Chen",
      "Charese Smiley",
      "Sameena Shah",
      "Iana Borova",
      "Dylan Langdon",
      "Reema Moussa",
      "Matt Beane",
      "Ting-Hao Huang",
      "Bryan Routledge",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.301": {
    "title": "FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation",
    "volume": "main",
    "abstract": "Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings; they can fabricate explanations even for incorrect predictions, they are difficult to adapt to long input documents, and their training requires a large amount of labeled data. In this paper, we develop FiD-Ex, which addresses these shortcomings for seq2seq models by: 1) introducing sentence markers to eliminate explanation fabrication by encouraging extractive generation, 2) using the fusion-in-decoder architecture to handle long input contexts, and 3) intermediate fine-tuning on re-structured open domain QA datasets to improve few-shot performance. FiD-Ex significantly improves over prior work in terms of explanation metrics and task accuracy on five tasks from the ERASER explainability benchmark in both fully supervised and few-shot settings",
    "checked": true,
    "id": "17d4681b29b79c4ee5029ae39acabfdf9946bd77",
    "semantic_title": "fid-ex: improving sequence-to-sequence models for extractive rationale generation",
    "citation_count": 28,
    "authors": [
      "Kushal Lakhotia",
      "Bhargavi Paranjape",
      "Asish Ghoshal",
      "Scott Yih",
      "Yashar Mehdad",
      "Srini Iyer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.302": {
    "title": "RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models",
    "volume": "main",
    "abstract": "To audit the robustness of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in Wikidata; at the context level, we use pre-trained language models (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best model has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the robustness of NER models",
    "checked": true,
    "id": "4264b337c0e16953badf68c7f881b927c80d5161",
    "semantic_title": "rockner: a simple method to create adversarial examples for evaluating the robustness of named entity recognition models",
    "citation_count": 42,
    "authors": [
      "Bill Yuchen Lin",
      "Wenyang Gao",
      "Jun Yan",
      "Ryan Moreno",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.303": {
    "title": "Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI",
    "volume": "main",
    "abstract": "Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability",
    "checked": true,
    "id": "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886",
    "semantic_title": "diagnosing the first-order logical reasoning ability through logicnli",
    "citation_count": 72,
    "authors": [
      "Jidong Tian",
      "Yitian Li",
      "Wenqing Chen",
      "Liqiang Xiao",
      "Hao He",
      "Yaohui Jin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.304": {
    "title": "Constructing a Psychometric Testbed for Fair Natural Language Processing",
    "volume": "main",
    "abstract": "Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this paper we describe our efforts to construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed which encompasses survey-based psychometric measures and accompanying user-generated text from 8,502 respondents. Our testbed also encompasses self-reported demographic information, including race, sex, age, income, and education - thereby affording opportunities for measuring bias and benchmarking fairness of text classification methods. We report preliminary results on use of the text to predict/categorize users' survey response labels - and on the fairness of these models. We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness",
    "checked": true,
    "id": "c5c8d5efb4acc854955859115969aac9a783cdca",
    "semantic_title": "constructing a psychometric testbed for fair natural language processing",
    "citation_count": 13,
    "authors": [
      "Ahmed Abbasi",
      "David Dobolyi",
      "John P. Lalor",
      "Richard G. Netemeyer",
      "Kendall Smith",
      "Yi Yang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.305": {
    "title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval",
    "volume": "main",
    "abstract": "We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval. Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains ~32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ retrieval models built on top of BM25 and BERT, among which the best model achieves 48.8 under P@5, indicating a great challenge presented by COUGH and encouraging future research for further improvement. Our COUGH dataset is available at https://github.com/sunlab-osu/covid-faq",
    "checked": true,
    "id": "4a7066fb8ad3f5efbd35b55cdda2026ef0d9e0b0",
    "semantic_title": "cough: a challenge dataset and models for covid-19 faq retrieval",
    "citation_count": 17,
    "authors": [
      "Xinliang Frederick Zhang",
      "Heming Sun",
      "Xiang Yue",
      "Simon Lin",
      "Huan Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.306": {
    "title": "Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context",
    "volume": "main",
    "abstract": "This paper presents a Chinese dataset for evaluating pretrained language models on Word Prediction given Long-term Context (Chinese WPLC). We propose both automatic and manual selection strategies tailored to Chinese to guarantee that target words in passages collected from over 69K novels can only be predicted with long-term context beyond the scope of sentences containing the target words. Dataset analysis reveals that the types of target words range from common nouns to Chinese 4-character idioms. We also observe that linguistic relations between target words and long-range context exhibit diversity, including lexical match, synonym, summary and reasoning. Experiment results show that the Chinese pretrained language model PanGu-𝛼 is 45 points behind human in terms of top-1 word prediction accuracy, indicating that Chinese WPLC is a challenging dataset. The dataset is publicly available at https://git.openi.org.cn/PCL-Platform.Intelligence/Chinese_WPLC",
    "checked": true,
    "id": "45612980cad4d8a7a013b214c310989e26d24948",
    "semantic_title": "chinese wplc: a chinese dataset for evaluating pretrained language models on word prediction given long-range context",
    "citation_count": 8,
    "authors": [
      "Huibin Ge",
      "Chenxi Sun",
      "Deyi Xiong",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.307": {
    "title": "WinoLogic: A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge",
    "volume": "main",
    "abstract": "The recent success of neural language models (NLMs) on the Winograd Schema Challenge has called for further investigation of the commonsense reasoning ability of these models. Previous diagnostic datasets rely on crowd-sourcing which fails to provide coherent commonsense crucial for solving WSC problems. To better evaluate NLMs, we propose a logic-based framework that focuses on high-quality commonsense knowledge. Specifically, we identify and collect formal knowledge formulas verified by theorem provers and translate such formulas into natural language sentences. Based on these true knowledge sentences, adversarial false ones are generated. We propose a new dataset named WinoLogic with these sentences. Given a problem in WinoLogic, NLMs need to decide whether the plausible knowledge sentences could correctly solve the corresponding WSC problems in a zero-shot setting. We also ask human annotators to validate WinoLogic to ensure it is human-agreeable. Experiments show that NLMs still struggle to comprehend commonsense knowledge as humans do, indicating that their reasoning ability could have been overestimated",
    "checked": true,
    "id": "b6588d413efc23122f690ace43a28db693d857c2",
    "semantic_title": "winologic: a zero-shot logic-based diagnostic dataset for winograd schema challenge",
    "citation_count": 15,
    "authors": [
      "Weinan He",
      "Canming Huang",
      "Yongmei Liu",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.308": {
    "title": "Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution",
    "volume": "main",
    "abstract": "Masked language models (MLMs) have contributed to drastic performance improvements with regard to zero anaphora resolution (ZAR). To further improve this approach, in this study, we made two proposals. The first is a new pretraining task that trains MLMs on anaphoric relations with explicit supervision, and the second proposal is a new finetuning method that remedies a notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese ZAR demonstrated that our two proposals boost the state-of-the-art performance, and our detailed analysis provides new insights on the remaining challenges",
    "checked": true,
    "id": "28d1abc9466a3eab69b0efaaa03d4a9972d72ded",
    "semantic_title": "pseudo zero pronoun resolution improves zero anaphora resolution",
    "citation_count": 10,
    "authors": [
      "Ryuto Konno",
      "Shun Kiyono",
      "Yuichiroh Matsubayashi",
      "Hiroki Ouchi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.309": {
    "title": "Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast",
    "volume": "main",
    "abstract": "In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple dot product. Pre-trained language models are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He et al., 2020) to further improve the quality of alignment. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe andSchwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets",
    "checked": true,
    "id": "99ed8739706d8ebfdbd01bfb87f02a294490cc56",
    "semantic_title": "aligning cross-lingual sentence representations with dual momentum contrast",
    "citation_count": 14,
    "authors": [
      "Liang Wang",
      "Wei Zhao",
      "Jingming Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.310": {
    "title": "Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers",
    "volume": "main",
    "abstract": "This paper investigates continual learning for semantic parsing. In this setting, a neural semantic parser learns tasks sequentially without accessing full training data from previous tasks. Direct application of the SOTA continual learning algorithms to this problem fails to achieve comparable performance with re-training models with all seen tasks because they have not considered the special properties of structured outputs yielded by semantic parsers. Therefore, we propose TotalRecall, a continual learning method designed for neural semantic parsers from two aspects: i) a sampling method for memory replay that diversifies logical form templates and balances distributions of parse actions in a memory; ii) a two-stage training method that significantly improves generalization capability of the parsers across tasks. We conduct extensive experiments to study the research problems involved in continual semantic parsing and demonstrate that a neural semantic parser trained with TotalRecall achieves superior performance than the one trained directly with the SOTA continual learning algorithms and achieve a 3-6 times speedup compared to re-training from scratch",
    "checked": true,
    "id": "b661d3397e4ce5df84a5dd1dbe2f3d82f6995861",
    "semantic_title": "total recall: a customized continual learning method for neural semantic parsers",
    "citation_count": 15,
    "authors": [
      "Zhuang Li",
      "Lizhen Qu",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.311": {
    "title": "Exophoric Pronoun Resolution in Dialogues with Topic Regularization",
    "volume": "main",
    "abstract": "Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the dialogue text, they can often be disambiguated by the general topics of the dialogue. Motivated by this, we propose to jointly leverage the local context and global topics of dialogues to solve the out-of-text PCR problem. Extensive experiments demonstrate the effectiveness of adding topic regularization for resolving exophoric pronouns",
    "checked": true,
    "id": "6906f13b203408eb772ee7d2fa36aa287d6cb7f7",
    "semantic_title": "exophoric pronoun resolution in dialogues with topic regularization",
    "citation_count": 4,
    "authors": [
      "Xintong Yu",
      "Hongming Zhang",
      "Yangqiu Song",
      "Changshui Zhang",
      "Kun Xu",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.312": {
    "title": "Context-Aware Interaction Network for Question Matching",
    "volume": "main",
    "abstract": "Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations. However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information. We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship. Specifically, each interaction block includes (1) a context-aware cross-attention mechanism to effectively integrate contextual information when aligning two sequences, and (2) a gate fusion layer to flexibly interpolate aligned representations. We apply multiple stacked interaction blocks to produce alignments at different levels and gradually refine the attention results. Experiments on two question matching datasets and detailed analyses demonstrate the effectiveness of our model",
    "checked": true,
    "id": "5690c75c73325dd3b9af37f463e07e3a4f3fca35",
    "semantic_title": "context-aware interaction network for question matching",
    "citation_count": 16,
    "authors": [
      "Zhe Hu",
      "Zuohui Fu",
      "Yu Yin",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.313": {
    "title": "TEMP: Taxonomy Expansion with Dynamic Margin Loss through Taxonomy-Paths",
    "volume": "main",
    "abstract": "As an essential form of knowledge representation, taxonomies are widely used in various downstream natural language processing tasks. However, with the continuously rising of new concepts, many existing taxonomies are unable to maintain coverage by manual expansion. In this paper, we propose TEMP, a self-supervised taxonomy expansion method, which predicts the position of new concepts by ranking the generated taxonomy-paths. For the first time, TEMP employs pre-trained contextual encoders in taxonomy construction and hypernym detection problems. Experiments prove that pre-trained contextual embeddings are able to capture hypernym-hyponym relations. To learn more detailed differences between taxonomy-paths, we train the model with dynamic margin loss by a novel dynamic margin function. Extensive evaluations exhibit that TEMP outperforms prior state-of-the-art taxonomy expansion approaches by 14.3% in accuracy and 15.8% in mean reciprocal rank on three public benchmarks",
    "checked": true,
    "id": "8c7887456585b7bec8be2c62e70bef0d1fba2a47",
    "semantic_title": "temp: taxonomy expansion with dynamic margin loss through taxonomy-paths",
    "citation_count": 31,
    "authors": [
      "Zichen Liu",
      "Hongyuan Xu",
      "Yanlong Wen",
      "Ning Jiang",
      "HaiYing Wu",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.314": {
    "title": "A Graph-Based Neural Model for End-to-End Frame Semantic Parsing",
    "volume": "main",
    "abstract": "Frame semantic parsing is a semantic analysis task based on FrameNet which has received great attention recently. The task usually involves three subtasks sequentially: (1) target identification, (2) frame classification and (3) semantic role labeling. The three subtasks are closely related while previous studies model them individually, which ignores their intern connections and meanwhile induces error propagation problem. In this work, we propose an end-to-end neural model to tackle the task jointly. Concretely, we exploit a graph-based method, regarding frame semantic parsing as a graph construction problem. All predicates and roles are treated as graph nodes, and their relations are taken as graph edges. Experiment results on two benchmark datasets of frame semantic parsing show that our method is highly competitive, resulting in better performance than pipeline models",
    "checked": true,
    "id": "21c9ba5ed60f4efb27ba238f98f34a2e24ab974d",
    "semantic_title": "a graph-based neural model for end-to-end frame semantic parsing",
    "citation_count": 8,
    "authors": [
      "ZhiChao Lin",
      "Yueheng Sun",
      "Meishan Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.315": {
    "title": "Virtual Data Augmentation: A Robust and General Framework for Fine-tuning Pre-trained Models",
    "volume": "main",
    "abstract": "Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the robustness of PLMs. However, it is still challenging to augment semantically relevant examples with sufficient diversity. In this work, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on the original token embeddings, we construct a multinomial mixture for augmenting virtual data embeddings, where a masked language model guarantees the semantic relevance and the Gaussian noise provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects. Extensive experiments on six datasets show that our approach is able to improve the robustness of PLMs and alleviate the performance degradation under adversarial attacks. Our codes and data are publicly available at bluehttps://github.com/RUCAIBox/VDA",
    "checked": true,
    "id": "c44ec5fe53b0349b7239ab1c08135f7cb0f1c96d",
    "semantic_title": "virtual data augmentation: a robust and general framework for fine-tuning pre-trained models",
    "citation_count": 8,
    "authors": [
      "Kun Zhou",
      "Wayne Xin Zhao",
      "Sirui Wang",
      "Fuzheng Zhang",
      "Wei Wu",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.316": {
    "title": "CATE: A Contrastive Pre-trained Model for Metaphor Detection with Semi-supervised Learning",
    "volume": "main",
    "abstract": "Metaphors are ubiquitous in natural language, and detecting them requires contextual reasoning about whether a semantic incongruence actually exists. Most existing work addresses this problem using pre-trained contextualized models. Despite their success, these models require a large amount of labeled data and are not linguistically-based. In this paper, we proposed a ContrAstive pre-Trained modEl (CATE) for metaphor detection with semi-supervised learning. Our model first uses a pre-trained model to obtain a contextual representation of target words and employs a contrastive objective to promote an increased distance between target words' literal and metaphorical senses based on linguistic theories. Furthermore, we propose a simple strategy to collect large-scale candidate instances from the general corpus and generalize the model via self-training. Extensive experiments show that CATE achieves better performance against state-of-the-art baselines on several benchmark datasets",
    "checked": true,
    "id": "1393e1e572706f4e8950922ae26575b8b0a70863",
    "semantic_title": "cate: a contrastive pre-trained model for metaphor detection with semi-supervised learning",
    "citation_count": 19,
    "authors": [
      "Zhenxi Lin",
      "Qianli Ma",
      "Jiangyue Yan",
      "Jieyu Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.317": {
    "title": "To be Closer: Learning to Link up Aspects with Opinions",
    "volume": "main",
    "abstract": "Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (CITATION). However, the trees obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between aspects and corresponding opinion words by learning an aspect-centric tree structure. The aspect and opinion words are expected to be closer along such tree structure compared to the standard dependency parse tree. The learning process allows the tree structure to adaptively correlate the aspect and opinion words, enabling us to better identify the polarity in the ABSA task. We conduct experiments on five aspect-based sentiment datasets, and the proposed model significantly outperforms recent strong baselines. Furthermore, our thorough analysis demonstrates the average distance between aspect and opinion words are shortened by at least 19% on the standard SemEval Restaurant14 (CITATION) dataset",
    "checked": true,
    "id": "629b9e5cc6839e60c7e18cfe752d682ff2787999",
    "semantic_title": "to be closer: learning to link up aspects with opinions",
    "citation_count": 39,
    "authors": [
      "Yuxiang Zhou",
      "Lejian Liao",
      "Yang Gao",
      "Zhanming Jie",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.318": {
    "title": "Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model",
    "volume": "main",
    "abstract": "Aspect-based sentiment analysis (ABSA) task consists of three typical subtasks: aspect term extraction, opinion term extraction, and sentiment polarity classification. These three subtasks are usually performed jointly to save resources and reduce the error propagation in the pipeline. However, most of the existing joint models only focus on the benefits of encoder sharing between subtasks but ignore the difference. Therefore, we propose a joint ABSA model, which not only enjoys the benefits of encoder sharing but also focuses on the difference to improve the effectiveness of the model. In detail, we introduce a dual-encoder design, in which a pair encoder especially focuses on candidate aspect-opinion pair classification, and the original encoder keeps attention on sequence labeling. Empirical results show that our proposed model shows robustness and significantly outperforms the previous state-of-the-art on four benchmark datasets",
    "checked": true,
    "id": "806185a0459803112d4df53a65bfd75dc9cfd928",
    "semantic_title": "seeking common but distinguishing difference, a joint aspect-based sentiment analysis model",
    "citation_count": 26,
    "authors": [
      "Hongjiang Jing",
      "Zuchao Li",
      "Hai Zhao",
      "Shu Jiang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.319": {
    "title": "Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph",
    "volume": "main",
    "abstract": "Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In this paper, we tackle the APE task by a mutual guidance framework, which could utilize the information of an argument in one passage to guide the identification of arguments that can form pairs with it in another passage. In this manner, two passages can mutually guide each other in the process of APE. Furthermore, we propose an inter-sentence relation graph to effectively model the inter-relations between two sentences and thus facilitates the extraction of argument pairs. Our proposed method can better represent the holistic argument-level semantics and thus explicitly capture the complex correlations between argument pairs. Experimental results show that our approach significantly outperforms the current state-of-the-art model",
    "checked": true,
    "id": "ba95134cb249695c1ff5985845fdb2f69b2dfe28",
    "semantic_title": "argument pair extraction with mutual guidance and inter-sentence relation graph",
    "citation_count": 18,
    "authors": [
      "Jianzhu Bao",
      "Bin Liang",
      "Jingyi Sun",
      "Yice Zhang",
      "Min Yang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.320": {
    "title": "Emotion Inference in Multi-Turn Conversations with Addressee-Aware Module and Ensemble Strategy",
    "volume": "main",
    "abstract": "Emotion inference in multi-turn conversations aims to predict the participant's emotion in the next upcoming turn without knowing the participant's response yet, and is a necessary step for applications such as dialogue planning. However, it is a severe challenge to perceive and reason about the future feelings of participants, due to the lack of utterance information from the future. Moreover, it is crucial for emotion inference to capture the characteristics of emotional propagation in conversations, such as persistence and contagiousness. In this study, we focus on investigating the task of emotion inference in multi-turn conversations by modeling the propagation of emotional states among participants in the conversation history, and propose an addressee-aware module to automatically learn whether the participant keeps the historical emotional state or is affected by others in the next upcoming turn. In addition, we propose an ensemble strategy to further enhance the model performance. Empirical studies on three different benchmark conversation datasets demonstrate the effectiveness of the proposed model over several strong baselines",
    "checked": true,
    "id": "f41151ffb2dd2e7032957dc01e70a9d2bbf952df",
    "semantic_title": "emotion inference in multi-turn conversations with addressee-aware module and ensemble strategy",
    "citation_count": 11,
    "authors": [
      "Dayu Li",
      "Xiaodan Zhu",
      "Yang Li",
      "Suge Wang",
      "Deyu Li",
      "Jian Liao",
      "Jianxing Zheng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.321": {
    "title": "Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories",
    "volume": "main",
    "abstract": "Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained model is required to have a good understanding of the contextual information, especially the particular patterns that suggest the sentiment polarity. However, these patterns typically vary in different sentences, especially when the sentences come from different sources (domains), which makes ABSA still very challenging. Although combining labeled data across different sources (domains) is a promising solution to address the challenge, in practical applications, these labeled data are usually stored at different locations and might be inaccessible to each other due to privacy or legal concerns (e.g., the data are owned by different companies). To address this issue and make the best use of all labeled data, we propose a novel ABSA model with federated learning (FL) adopted to overcome the data isolation limitations and incorporate topic memory (TM) proposed to take the cases of data from diverse sources (domains) into consideration. Particularly, TM aims to identify different isolated data sources due to data inaccessibility by providing useful categorical information for localized predictions. Experimental results on a simulated environment for FL with three nodes demonstrate the effectiveness of our approach, where TM-FL outperforms different baselines including some well-designed FL frameworks",
    "checked": true,
    "id": "b755f885d913367f1a2457af863484937e7fb3e4",
    "semantic_title": "improving federated learning for aspect-based sentiment analysis via topic memories",
    "citation_count": 22,
    "authors": [
      "Han Qin",
      "Guimin Chen",
      "Yuanhe Tian",
      "Yan Song"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.322": {
    "title": "Comparative Opinion Quintuple Extraction from Product Reviews",
    "volume": "main",
    "abstract": "As an important task in opinion mining, comparative opinion mining aims to identify comparative sentences from product reviews, extract the comparative elements, and obtain the corresponding comparative opinion tuples. However, most previous studies simply regarded comparative tuple extraction as comparative element extraction, but ignored the fact that many comparative sentences may contain multiple comparisons. The comparative opinion tuples defined in these studies also failed to explicitly provide comparative preferences. To address these limitations, in this work we first introduce a new Comparative Opinion Quintuple Extraction (COQE) task, to identify comparative sentences from product reviews and extract all comparative opinion quintuples (Subject, Object, Comparative Aspect, Comparative Opinion, Comparative Preference). Secondly, based on the existing comparative opinion mining corpora, we make supplementary annotations and construct three datasets for the COQE task. Finally, we benchmark the COQE task by proposing a new BERT-based multi-stage approach as well as three baseline systems extended from previous methods. %The new approach significantly outperforms three baseline systems on three datasets and represents a strong benchmark for COQE. Experimental results show that the new approach significantly outperforms three baseline systems on three datasets for the COQE task",
    "checked": true,
    "id": "22b10eab291d589678249338a0156ccabf63b6a4",
    "semantic_title": "comparative opinion quintuple extraction from product reviews",
    "citation_count": 25,
    "authors": [
      "Ziheng Liu",
      "Rui Xia",
      "Jianfei Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.323": {
    "title": "CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations",
    "volume": "main",
    "abstract": "Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these models are facing challenges of overfitting with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs: masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, emotion classification, sentiment analysis, and speaker verification. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021",
    "checked": true,
    "id": "bdf1cfedb9f0867d755589b9eea226d10643fb90",
    "semantic_title": "ctal: pre-training cross-modal transformer for audio-and-language representations",
    "citation_count": 19,
    "authors": [
      "Hang Li",
      "Wenbiao Ding",
      "Yu Kang",
      "Tianqiao Liu",
      "Zhongqin Wu",
      "Zitao Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.324": {
    "title": "Relation-aware Video Reading Comprehension for Temporal Language Grounding",
    "volume": "main",
    "abstract": "Temporal language grounding in videos aims to localize the temporal span relevant to the given query sentence. Previous methods treat it either as a boundary regression task or a span extraction task. This paper will formulate temporal language grounding into video reading comprehension and propose a Relation-aware Network (RaNet) to address it. This framework aims to select a video moment choice from the predefined answer set with the aid of coarse-and-fine choice-query interaction and choice-choice relation construction. A choice-query interactor is proposed to match the visual and textual information simultaneously in sentence-moment and token-moment levels, leading to a coarse-and-fine cross-modal interaction. Moreover, a novel multi-choice relation constructor is introduced by leveraging graph convolution to capture the dependencies among video moment choices for the best choice selection. Extensive experiments on ActivityNet-Captions, TACoS, and Charades-STA demonstrate the effectiveness of our solution. Codes will be available at https://github.com/Huntersxsx/RaNet",
    "checked": true,
    "id": "918ef8da1f3642dce54c77bfffb2f1270bef058c",
    "semantic_title": "relation-aware video reading comprehension for temporal language grounding",
    "citation_count": 48,
    "authors": [
      "Jialin Gao",
      "Xin Sun",
      "Mengmeng Xu",
      "Xi Zhou",
      "Bernard Ghanem"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.325": {
    "title": "Mutual-Learning Improves End-to-End Speech Translation",
    "volume": "main",
    "abstract": "A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a machine translation (MT) task to improve the speech translation (ST) task. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher model. Therefore, We hypothesis that the knowledge distillation-based approaches are sub-optimal. In this paper, we propose an alternative–a trainable mutual-learning scenario, where the MT and the ST models are collaboratively trained and are considered as peers, rather than teacher/student. This allows us to improve the performance of end-to-end ST more effectively than with a teacher-student paradigm. As a side benefit, performance of the MT model also improves. Experimental results show that in our mutual-learning scenario, models can effectively utilise the auxiliary information from peer models and achieve compelling results on Must-C dataset",
    "checked": true,
    "id": "c5eb527195354a6ee690c13d95df6a9bb8bb3647",
    "semantic_title": "mutual-learning improves end-to-end speech translation",
    "citation_count": 27,
    "authors": [
      "Jiawei Zhao",
      "Wei Luo",
      "Boxing Chen",
      "Andrew Gilman"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.326": {
    "title": "Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization",
    "volume": "main",
    "abstract": "Multimodal abstractive summarization (MAS) models that summarize videos (vision modality) and their corresponding transcripts (text modality) are able to extract the essential information from massive multimodal data on the Internet. Recently, large-scale generative pre-trained language models (GPLMs) have been shown to be effective in text generation tasks. However, existing MAS models cannot leverage GPLMs' powerful generation ability. To fill this research gap, we aim to study two research questions: 1) how to inject visual information into GPLMs without hurting their generation ability; and 2) where is the optimal place in GPLMs to inject the visual information? In this paper, we present a simple yet effective method to construct vision guided (VG) GPLMs for the MAS task using attention-based add-on layers to incorporate visual information while maintaining their original text generation ability. Results show that our best model significantly surpasses the prior state-of-the-art model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset, and our vision guidance method contributes 83.6% of the overall improvement. Furthermore, we conduct thorough ablation studies to analyze the effectiveness of various modality fusion methods and fusion locations",
    "checked": true,
    "id": "ec33756aa312a33a5979d4d542da2eeabaa8d1a0",
    "semantic_title": "vision guided generative pre-trained language models for multimodal abstractive summarization",
    "citation_count": 74,
    "authors": [
      "Tiezheng Yu",
      "Wenliang Dai",
      "Zihan Liu",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.327": {
    "title": "Natural Language Video Localization with Learnable Moment Proposals",
    "volume": "main",
    "abstract": "Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposal-free models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank methods have inferior performance than proposal-free counterparts. In this paper, we argue that the performance of propose-and-rank models are underestimated due to the predefined manners: 1) Hand-designed rules are hard to guarantee the complete coverage of targeted segments. 2) Densely sampled candidate moments cause redundant computation and degrade the performance of ranking process. To this end, we propose a novel model termed LPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable moment proposals. The position and length of these proposals are dynamically adjusted during training process. Moreover, a boundary-aware loss has been proposed to leverage frame-level information and further improve performance. Extensive ablations on two challenging NLVL benchmarks have demonstrated the effectiveness of LPNet over existing state-of-the-art methods",
    "checked": true,
    "id": "dad19370f9713bbce9f7df77c7ee000e011e3b8f",
    "semantic_title": "natural language video localization with learnable moment proposals",
    "citation_count": 43,
    "authors": [
      "Shaoning Xiao",
      "Long Chen",
      "Jian Shao",
      "Yueting Zhuang",
      "Jun Xiao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.328": {
    "title": "Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments",
    "volume": "main",
    "abstract": "In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle ‘off the path' scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent's location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new metric that measures the number of sub-instructions the agent has completed during navigation",
    "checked": true,
    "id": "3e7b660242f12e5b4c977c604c19176df9e4789c",
    "semantic_title": "language-aligned waypoint (law) supervision for vision-and-language navigation in continuous environments",
    "citation_count": 59,
    "authors": [
      "Sonia Raychaudhuri",
      "Saim Wani",
      "Shivansh Patel",
      "Unnat Jain",
      "Angel Chang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.329": {
    "title": "How to leverage the multimodal EHR data for better medical prediction?",
    "volume": "main",
    "abstract": "Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of deep learning. Specifically, the data produced in the hospital admissions are monitored by the EHR system, which includes structured data like daily body temperature and unstructured data like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific EHR data, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different data from various views are all beneficial to the medical tasks and how to best utilize these data remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from EHR and propose a method to integrate these data, we also comprehensively study the different models and the data leverage methods for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features",
    "checked": true,
    "id": "86c30901285d92affd9ddd07f42ceda2bbde01d2",
    "semantic_title": "how to leverage the multimodal ehr data for better medical prediction?",
    "citation_count": 45,
    "authors": [
      "Bo Yang",
      "Lijun Wu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.330": {
    "title": "Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer",
    "volume": "main",
    "abstract": "Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents. However, constructing a coherent and informative summary is difficult using a pre-trained BERT-based encoder since it is not explicitly trained for representing the information of sentences in a document. We propose a nested tree-based extractive summarization model on RoBERTa (NeRoBERTa), where nested tree structures consist of syntactic and discourse trees in a given document. Experimental results on the CNN/DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE. Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state-of-the-art models",
    "checked": true,
    "id": "0909774b5bf4806dabffa701ba97cdc32cce45ea",
    "semantic_title": "considering nested tree structure in sentence extractive summarization with pre-trained transformer",
    "citation_count": 23,
    "authors": [
      "Jingun Kwon",
      "Naoki Kobayashi",
      "Hidetaka Kamigaito",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.331": {
    "title": "Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization",
    "volume": "main",
    "abstract": "Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization, which leverages Frame semantics to model sentences from both intra-sentence level and inter-sentence level, facilitating the text summarization task. In particular, intra-sentence level semantics leverage Frames and Frame Elements to model internal semantic structure within a sentence, while inter-sentence level semantics leverage Frame-to-Frame relations to model relationships among sentences. Extensive experiments on two benchmark corpus CNN/DM and NYT demonstrate that our model outperforms six state-of-the-art methods significantly",
    "checked": true,
    "id": "03d9039dc627595b293cb7a9fb26d37dd16726f1",
    "semantic_title": "frame semantic-enhanced sentence modeling for sentence-level extractive text summarization",
    "citation_count": 10,
    "authors": [
      "Yong Guan",
      "Shaoru Guo",
      "Ru Li",
      "Xiaoli Li",
      "Hongye Tan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.332": {
    "title": "CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees",
    "volume": "main",
    "abstract": "Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation. However, existing approaches fail to fully capture the rich information in ASTs because of the large size/depth of ASTs. In this paper, we propose a novel model CAST that hierarchically splits and reconstructs ASTs. First, we hierarchically split a large AST into a set of subtrees and utilize a recursive neural network to encode the subtrees. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs to get the representation of the complete AST. Finally, AST representation, together with source code embedding obtained by a vanilla code token encoder, is used for code summarization. Extensive experiments, including the ablation study and the human evaluation, on benchmarks have demonstrated the power of CAST. To facilitate reproducibility, our code and data are available at https://github.com/DeepSoftwareAnalytics/CAST",
    "checked": true,
    "id": "51654d8317478fe1497678b15e36ac99d73a65b2",
    "semantic_title": "cast: enhancing code summarization with hierarchical splitting and reconstruction of abstract syntax trees",
    "citation_count": 66,
    "authors": [
      "Ensheng Shi",
      "Yanlin Wang",
      "Lun Du",
      "Hongyu Zhang",
      "Shi Han",
      "Dongmei Zhang",
      "Hongbin Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.333": {
    "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection",
    "volume": "main",
    "abstract": "Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub-graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub-graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architecture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks",
    "checked": true,
    "id": "1595b2452a4d58c1a21e53ad74a8abab6dde0a0a",
    "semantic_title": "sgsum:transforming multi-document summarization into sub-graph selection",
    "citation_count": 23,
    "authors": [
      "Moye Chen",
      "Wei Li",
      "Jiachen Liu",
      "Xinyan Xiao",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.334": {
    "title": "Event Graph based Sentence Fusion",
    "volume": "main",
    "abstract": "Sentence fusion is a conditional generation task that merges several related sentences into a coherent one, which can be deemed as a summary sentence. The importance of sentence fusion has long been recognized by communities in natural language generation, especially in text summarization. It remains challenging for a state-of-the-art neural abstractive summarization model to generate a well-integrated summary sentence. In this paper, we explore the effective sentence fusion method in the context of text summarization. We propose to build an event graph from the input sentences to effectively capture and organize related events in a structured way and use the constructed event graph to guide sentence fusion. In addition to make use of the attention over the content of sentences and graph nodes, we further develop a graph flow attention mechanism to control the fusion process via the graph structure. When evaluated on sentence fusion data built from two summarization datasets, CNN/DaliyMail and Multi-News, our model shows to achieve state-of-the-art performance in terms of Rouge and other metrics like fusion rate and faithfulness",
    "checked": true,
    "id": "8c0a33184637723a3e6628d00598809efa9f8ef2",
    "semantic_title": "event graph based sentence fusion",
    "citation_count": 7,
    "authors": [
      "Ruifeng Yuan",
      "Zili Wang",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.335": {
    "title": "Transformer-based Lexically Constrained Headline Generation",
    "volume": "main",
    "abstract": "This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a headline including a given phrase by providing the encoder with additional information corresponding to the given phrase. However, these methods cannot always include the phrase in the generated headline. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated headline, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous strategies",
    "checked": true,
    "id": "3550fec363e02c99198cf195412eaec74062ccee",
    "semantic_title": "transformer-based lexically constrained headline generation",
    "citation_count": 12,
    "authors": [
      "Kosuke Yamada",
      "Yuta Hitomi",
      "Hideaki Tamori",
      "Ryohei Sasano",
      "Naoaki Okazaki",
      "Kentaro Inui",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.336": {
    "title": "Learn to Copy from the Copying History: Correlational Copy Network for Abstractive Summarization",
    "volume": "main",
    "abstract": "The copying mechanism has had considerable success in abstractive summarization, facilitating models to directly copy words from the input text to the output summary. Existing works mostly employ encoder-decoder attention, which applies copying at each time step independently of the former ones. However, this may sometimes lead to incomplete copying. In this paper, we propose a novel copying scheme named Correlational Copying Network (CoCoNet) that enhances the standard copying mechanism by keeping track of the copying history. It thereby takes advantage of prior copying distributions and, at each time step, explicitly encourages the model to copy the input word that is relevant to the previously copied one. In addition, we strengthen CoCoNet through pre-training with suitable corpora that simulate the copying behaviors. Experimental results show that CoCoNet can copy more accurately and achieves new state-of-the-art performances on summarization benchmarks, including CNN/DailyMail for news summarization and SAMSum for dialogue summarization. The code and checkpoint will be publicly available",
    "checked": true,
    "id": "3544532e9cfdf386b82cd999650dec54f4457e1f",
    "semantic_title": "learn to copy from the copying history: correlational copy network for abstractive summarization",
    "citation_count": 11,
    "authors": [
      "Haoran Li",
      "Song Xu",
      "Peng Yuan",
      "Yujia Wang",
      "Youzheng Wu",
      "Xiaodong He",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.337": {
    "title": "Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization",
    "volume": "main",
    "abstract": "Neural abstractive summarization systems have gained significant progress in recent years. However, abstractive summarization often produce inconsisitent statements or false facts. How to automatically generate highly abstract yet factually correct summaries? In this paper, we proposed an efficient weak-supervised adversarial data augmentation approach to form the factual consistency dataset. Based on the artificial dataset, we train an evaluation model that can not only make accurate and robust factual consistency discrimination but is also capable of making interpretable factual errors tracing by backpropagated gradient distribution on token embeddings. Experiments and analysis conduct on public annotated summarization and factual consistency datasets demonstrate our approach effective and reasonable",
    "checked": true,
    "id": "9e0e84f120cab51b8dfd0525dba552d2b3fb8042",
    "semantic_title": "gradient-based adversarial factual consistency evaluation for abstractive summarization",
    "citation_count": 6,
    "authors": [
      "Zhiyuan Zeng",
      "Jiaze Chen",
      "Weiran Xu",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.338": {
    "title": "Word Reordering for Zero-shot Cross-lingual Structured Prediction",
    "volume": "main",
    "abstract": "Adapting word order from one language to another is a key problem in cross-lingual structured prediction. Current sentence encoders (e.g., RNN, Transformer with position embeddings) are usually word order sensitive. Even with uniform word form representations (MUSE, mBERT), word order discrepancies may hurt the adaptation of models. In this paper, we build structured prediction models with bag-of-words inputs, and introduce a new reordering module to organizing words following the source language order, which learns task-specific reordering strategies from a general-purpose order predictor model. Experiments on zero-shot cross-lingual dependency parsing, POS tagging, and morphological tagging show that our model can significantly improve target language performances, especially for languages that are distant from the source language",
    "checked": true,
    "id": "2a59f588102d1b4b345f35d2d5051047d4b17b58",
    "semantic_title": "word reordering for zero-shot cross-lingual structured prediction",
    "citation_count": 5,
    "authors": [
      "Tao Ji",
      "Yong Jiang",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Yuanbin Wu",
      "Xiaoling Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.339": {
    "title": "A Unified Encoding of Structures in Transition Systems",
    "volume": "main",
    "abstract": "Transition systems usually contain various dynamic structures (e.g., stacks, buffers). An ideal transition-based model should encode these structures completely and efficiently. Previous works relying on templates or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a transition system. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding transition states with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods",
    "checked": true,
    "id": "4be2a1bcd4a972ca2030d1624f942e7a391aa08c",
    "semantic_title": "a unified encoding of structures in transition systems",
    "citation_count": 0,
    "authors": [
      "Tao Ji",
      "Yong Jiang",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Yuanbin Wu",
      "Xiaoling Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.340": {
    "title": "Improving Unsupervised Question Answering via Summarization-Informed Question Generation",
    "volume": "main",
    "abstract": "Question Generation (QG) is the task of generating a plausible question for a given <passage, answer> pair. Template-based QG uses linguistically-informed heuristics to transform declarative sentences into interrogatives, whereas supervised QG uses existing Question Answering (QA) datasets to train a system to generate a question given a passage and an answer. A disadvantage of the heuristic approach is that the generated questions are heavily tied to their declarative counterparts. A disadvantage of the supervised approach is that they are heavily tied to the domain/language of the QA dataset used as training data. In order to overcome these shortcomings, we propose a distantly-supervised QG method which uses questions generated heuristically from summaries as a source of training data for a QG system. We make use of freely available news summary data, transforming declarative summary sentences into appropriate questions using heuristics informed by dependency parsing, named entity recognition and semantic role labeling. The resulting questions are then combined with the original news articles to train an end-to-end neural QG model. We extrinsically evaluate our approach using unsupervised QA: our QG model is used to generate synthetic QA pairs for training a QA model. Experimental results show that, trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model substantially outperforms previous unsupervised models on three in-domain datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach",
    "checked": true,
    "id": "1210d0d15ee1d67341267d62bb98dfcba18020a2",
    "semantic_title": "improving unsupervised question answering via summarization-informed question generation",
    "citation_count": 44,
    "authors": [
      "Chenyang Lyu",
      "Lifeng Shang",
      "Yvette Graham",
      "Jennifer Foster",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.341": {
    "title": "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph",
    "volume": "main",
    "abstract": "Multi-hop Question Answering (QA) is a challenging task because it requires precise reasoning with entity relations at every step towards the answer. The relations can be represented in terms of labels in knowledge graph (e.g., spouse) or text in text corpus (e.g., they have been married for 26 years). Existing models usually infer the answer by predicting the sequential relation path or aggregating the hidden graph features. The former is hard to optimize, and the latter lacks interpretability. In this paper, we propose TransferNet, an effective and transparent model for multi-hop QA, which supports both label and text relations in a unified framework. TransferNet jumps across entities at multiple steps. At each step, it attends to different parts of the question, computes activated scores for relations, and then transfer the previous entity scores along activated relations in a differentiable way. We carry out extensive experiments on three datasets and demonstrate that TransferNet surpasses the state-of-the-art models by a large margin. In particular, on MetaQA, it achieves 100% accuracy in 2-hop and 3-hop questions. By qualitative analysis, we show that TransferNet has transparent and interpretable intermediate results",
    "checked": true,
    "id": "53cffcd498d78b68c7c22f6fec5760be4c8a368c",
    "semantic_title": "transfernet: an effective and transparent framework for multi-hop question answering over relation graph",
    "citation_count": 116,
    "authors": [
      "Jiaxin Shi",
      "Shulin Cao",
      "Lei Hou",
      "Juanzi Li",
      "Hanwang Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.342": {
    "title": "Topic Transferable Table Question Answering",
    "volume": "main",
    "abstract": "Weakly-supervised table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT's pretraining corpus. In this work we simulate the practical topic shift scenario by designing novel challenge benchmarks WikiSQL-TS and WikiTable-TS, consisting of train-dev-test splits in five distinct topic groups, based on the popular WikiSQL and WikiTable-Questions datasets. We empirically show that, despite pre-training on large open-domain text, performance of models degrades significantly when they are evaluated on unseen topics. In response, we propose T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation framework for TableQA comprising of: (1) topic-specific vocabulary injection into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2) based natural language question generation pipeline focused on generating topic-specific training data, and (3) a logical form re-ranker. We show that T3QA provides a reasonably good baseline for our topic shift benchmarks. We believe our topic split benchmarks will lead to robust TableQA solutions that are better suited for practical deployment",
    "checked": true,
    "id": "55e226144b34f4efdf0e028e302e94658e0a1224",
    "semantic_title": "topic transferable table question answering",
    "citation_count": 11,
    "authors": [
      "Saneem Chemmengath",
      "Vishwajeet Kumar",
      "Samarth Bharadwaj",
      "Jaydeep Sen",
      "Mustafa Canim",
      "Soumen Chakrabarti",
      "Alfio Gliozzo",
      "Karthik Sankaranarayanan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.343": {
    "title": "WebSRC: A Dataset for Web-Based Structural Reading Comprehension",
    "volume": "main",
    "abstract": "Web search is an essential way for humans to obtain information, but it's still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a web page and a question about it, the task is to find an answer from the web page. This task requires a system not only to understand the semantics of texts but also the structure of the web page. Moreover, we proposed WebSRC, a novel Web-based Structural Reading Comprehension dataset. WebSRC consists of 400K question-answer pairs, which are collected from 6.4K web pages with corresponding HTML source code, screenshots, and metadata. Each question in WebSRC requires a certain structural understanding of a web page to answer, and the answer is either a text span on the web page or yes/no. We evaluate various strong baselines on our dataset to show the difficulty of our task. We also investigate the usefulness of structural information and visual features. Our dataset and baselines have been publicly available",
    "checked": true,
    "id": "54dd1600ad9e0b2d5a2899c3b7a7a403087823ae",
    "semantic_title": "websrc: a dataset for web-based structural reading comprehension",
    "citation_count": 99,
    "authors": [
      "Xingyu Chen",
      "Zihan Zhao",
      "Lu Chen",
      "JiaBao Ji",
      "Danyang Zhang",
      "Ao Luo",
      "Yuxuan Xiong",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.344": {
    "title": "Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language",
    "volume": "main",
    "abstract": "Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100% accuracy. Cryptonite is a challenging task for current models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6% accuracy, on par with the accuracy of a rule-based clue solver (8.6%)",
    "checked": true,
    "id": "8575d181953d63646d3f0453910ea30ff13e09af",
    "semantic_title": "cryptonite: a cryptic crossword benchmark for extreme ambiguity in language",
    "citation_count": 18,
    "authors": [
      "Avia Efrat",
      "Uri Shaham",
      "Dan Kilman",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.345": {
    "title": "End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs",
    "volume": "main",
    "abstract": "Recently, end-to-end (E2E) trained models for question answering over knowledge graphs (KGQA) have delivered promising results using only a weakly supervised dataset. However, these models are trained and evaluated in a setting where hand-annotated question entities are supplied to the model, leaving the important and non-trivial task of entity resolution (ER) outside the scope of E2E learning. In this work, we extend the boundaries of E2E learning for KGQA to include the training of an ER component. Our model only needs the question text and the answer entities to train, and delivers a stand-alone QA model that does not require an additional ER component to be supplied during runtime. Our approach is fully differentiable, thanks to its reliance on a recent method for building differentiable KGs (Cohen et al., 2020). We evaluate our E2E trained model on two public datasets and show that it comes close to baseline models that use hand-annotated entities",
    "checked": true,
    "id": "15d643f4c27d373aa46f26a760051e76fde81dc2",
    "semantic_title": "end-to-end entity resolution and question answering using differentiable knowledge graphs",
    "citation_count": 24,
    "authors": [
      "Amir Saffari",
      "Armin Oliya",
      "Priyanka Sen",
      "Tom Ayoola"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.346": {
    "title": "Improving Query Graph Generation for Complex Question Answering over Knowledge Base",
    "volume": "main",
    "abstract": "Most of the existing Knowledge-based Question Answering (KBQA) methods first learn to map the given question to a query graph, and then convert the graph to an executable query to find the answer. The query graph is typically expanded progressively from the topic entity based on a sequence prediction model. In this paper, we propose a new solution to query graph generation that works in the opposite manner: we start with the entire knowledge base and gradually shrink it to the desired query graph. This approach improves both the efficiency and the accuracy of query graph generation, especially for complex multi-hop questions. Experimental results show that our method achieves state-of-the-art performance on ComplexWebQuestion (CWQ) dataset",
    "checked": true,
    "id": "3206f0b9df53438121d02f6b384f707a32c5f71c",
    "semantic_title": "improving query graph generation for complex question answering over knowledge base",
    "citation_count": 15,
    "authors": [
      "Kechen Qin",
      "Cheng Li",
      "Virgil Pavlu",
      "Javed Aslam"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.347": {
    "title": "DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer",
    "volume": "main",
    "abstract": "Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence",
    "checked": true,
    "id": "61c67b52a0377617d1bcf4b0c5d875c2aa974fa7",
    "semantic_title": "discodvt: generating long text with discourse-aware discrete variational transformer",
    "citation_count": 23,
    "authors": [
      "Haozhe Ji",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.348": {
    "title": "Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations",
    "volume": "main",
    "abstract": "There is an increasing interest in the use of mathematical word problem (MWP) generation in educational assessment. Different from standard natural question generation, MWP generation needs to maintain the underlying mathematical operations between quantities and variables, while at the same time ensuring the relevance between the output and the given topic. To address above problem, we develop an end-to-end neural model to generate diverse MWPs in real-world scenarios from commonsense knowledge graph and equations. The proposed model (1) learns both representations from edge-enhanced Levi graphs of symbolic equations and commonsense knowledge; (2) automatically fuses equation and commonsense knowledge information via a self-planning module when generating the MWPs. Experiments on an educational gold-standard set and a large-scale generated MWP set show that our approach is superior on the MWP generation task, and it outperforms the SOTA models in terms of both automatic evaluation metrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e., equation relevance, topic relevance, and language coherence. To encourage reproducible results, we make our code and MWP dataset public available at https://github.com/tal-ai/MaKE_EMNLP2021",
    "checked": true,
    "id": "0c19221c7b41865030cb277373e4861d7ee00e63",
    "semantic_title": "mathematical word problem generation from commonsense knowledge graph and equations",
    "citation_count": 30,
    "authors": [
      "Tianqiao Liu",
      "Qiang Fang",
      "Wenbiao Ding",
      "Hang Li",
      "Zhongqin Wu",
      "Zitao Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.349": {
    "title": "Generic resources are what you need: Style transfer tasks without task-specific parallel training data",
    "volume": "main",
    "abstract": "Style transfer aims to rewrite a source text in a different target style while preserving its content. We propose a novel approach to this task that leverages generic resources, and without using any task-specific parallel (source–target) data outperforms existing unsupervised approaches on the two most popular style transfer tasks: formality transfer and polarity swap. In practice, we adopt a multi-step procedure which builds on a generic pre-trained sequence-to-sequence model (BART). First, we strengthen the model's ability to rewrite by further pre-training BART on both an existing collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative back-translation approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs, dynamically in the training process. Lastly, we let our best resulting model generate static synthetic pairs to be used in a supervised training regime. Besides methodology and state-of-the-art results, a core contribution of this work is a reflection on the nature of the two tasks we address, and how their differences are highlighted by their response to our approach",
    "checked": true,
    "id": "dd17b1de8f230f6717188dc8d063b027c5a025c2",
    "semantic_title": "generic resources are what you need: style transfer tasks without task-specific parallel training data",
    "citation_count": 16,
    "authors": [
      "Huiyuan Lai",
      "Antonio Toral",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.350": {
    "title": "Revisiting Pivot-Based Paraphrase Generation: Language Is Not the Only Optional Pivot",
    "volume": "main",
    "abstract": "Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the round-trip translation, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where large-scale, high-quality parallel bilingual texts are required. In this paper, we explore the feasibility of using semantic and syntactic representations as the pivot for paraphrase generation. Concretely, we transform a sentence into a variety of different semantic or syntactic representations (including AMR, UD, and latent semantic representation), and then decode the sentence back from the semantic representations. We further explore a pretraining-based approach to compress the pipeline process into an end-to-end framework. We conduct experiments comparing different approaches with different kinds of pivots. Experimental results show that taking AMR as pivot can obtain paraphrases with better quality than taking language as the pivot. The end-to-end framework can reduce semantic shift when language is used as the pivot. Besides, several unsupervised pivot-based methods can generate paraphrases with similar quality as the supervised sequence-to-sequence model, which indicates that parallel data of paraphrases may not be necessary for paraphrase generation",
    "checked": true,
    "id": "b42767ee673c530babc200cef2085d6e4348eb15",
    "semantic_title": "revisiting pivot-based paraphrase generation: language is not the only optional pivot",
    "citation_count": 8,
    "authors": [
      "Yitao Cai",
      "Yue Cao",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.351": {
    "title": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation",
    "volume": "main",
    "abstract": "Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose StructAdapt, an adapter method to encode graph structure into PLMs. Contrary to prior work, StructAdapt effectively models interactions among the nodes based on the graph connectivity, only training graph structure-aware adapter parameters. In this way, we incorporate task-specific knowledge while maintaining the topological structure of the graph. We empirically show the benefits of explicitly encoding graph structure into PLMs using StructAdapt, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters",
    "checked": true,
    "id": "43bab902791fc844d80b005e2c54bbbbe9e64f26",
    "semantic_title": "structural adapters in pretrained language models for amr-to-text generation",
    "citation_count": 73,
    "authors": [
      "Leonardo F. R. Ribeiro",
      "Yue Zhang",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.352": {
    "title": "Data-to-text Generation by Splicing Together Nearest Neighbors",
    "volume": "main",
    "abstract": "We propose to tackle data-to-text generation tasks by directly splicing together retrieved segments of text from \"neighbor\" source-target pairs. Unlike recent work that conditions on retrieved neighbors but generates text token-by-token, left-to-right, we learn a policy that directly manipulates segments of neighbor text, by inserting or replacing them in partially constructed generations. Standard techniques for training such a policy require an oracle derivation for each generation, and we prove that finding the shortest such derivation can be reduced to parsing under a particular weighted context-free grammar. We find that policies learned in this way perform on par with strong baselines in terms of automatic and human evaluation, but allow for more interpretable and controllable generation",
    "checked": true,
    "id": "8a99e1eb3285f127eed7169441679d47be7f1633",
    "semantic_title": "data-to-text generation by splicing together nearest neighbors",
    "citation_count": 9,
    "authors": [
      "Sam Wiseman",
      "Arturs Backurs",
      "Karl Stratos"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.353": {
    "title": "Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB context, which can lead to the less effective representation due to the information loss, and adversely favor KB reasoning and response generation. To tackle this issue, we explore to fully contextualize the entity representation by dynamically perceiving all the relevant entities and dialogue history. To achieve this, we propose a COntext-aware Memory Enhanced Transformer framework (COMET), which treats the KB as a sequence and leverages a novel Memory Mask to enforce the entity to only focus on its relevant entities and dialogue history, while avoiding the distraction from the irrelevant entities. Through extensive experiments, we show that our COMET framework can achieve superior performance over the state of the arts",
    "checked": true,
    "id": "399c60643a774eb1c6a113a0582a92e4b8e69e89",
    "semantic_title": "contextualize knowledge bases with transformer for end-to-end task-oriented dialogue systems",
    "citation_count": 17,
    "authors": [
      "Yanjie Gou",
      "Yinjie Lei",
      "Lingqiao Liu",
      "Yong Dai",
      "Chunxu Shen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.354": {
    "title": "Efficient Dialogue Complementary Policy Learning via Deep Q-network Policy and Episodic Memory Policy",
    "volume": "main",
    "abstract": "Deep reinforcement learning has shown great potential in training dialogue policies. However, its favorable performance comes at the cost of many rounds of interaction. Most of the existing dialogue policy methods rely on a single learning system, while the human brain has two specialized learning and memory systems, supporting to find good solutions without requiring copious examples. Inspired by the human brain, this paper proposes a novel complementary policy learning (CPL) framework, which exploits the complementary advantages of the episodic memory (EM) policy and the deep Q-network (DQN) policy to achieve fast and effective dialogue policy learning. In order to coordinate between the two policies, we proposed a confidence controller to control the complementary time according to their relative efficacy at different stages. Furthermore, memory connectivity and time pruning are proposed to guarantee the flexible and adaptive generalization of the EM policy in dialog tasks. Experimental results on three dialogue datasets show that our method significantly outperforms existing methods relying on a single learning system",
    "checked": true,
    "id": "6b587b28fe55e7c61834fa0864f5b19b1c11bced",
    "semantic_title": "efficient dialogue complementary policy learning via deep q-network policy and episodic memory policy",
    "citation_count": 10,
    "authors": [
      "Yangyang Zhao",
      "Zhenyu Wang",
      "Changxi Zhu",
      "Shihan Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.355": {
    "title": "CRFR: Improving Conversational Recommender Systems via Flexible Fragments Reasoning on Knowledge Graphs",
    "volume": "main",
    "abstract": "Although paths of user interests shift in knowledge graphs (KGs) can benefit conversational recommender systems (CRS), explicit reasoning on KGs has not been well considered in CRS, due to the complex of high-order and incomplete paths. We propose CRFR, which effectively does explicit multi-hop reasoning on KGs with a conversational context-based reinforcement learning model. Considering the incompleteness of KGs, instead of learning single complete reasoning path, CRFR flexibly learns multiple reasoning fragments which are likely contained in the complete paths of interests shift. A fragments-aware unified model is then designed to fuse the fragments information from item-oriented and concept-oriented KGs to enhance the CRS response with entities and words from the fragments. Extensive experiments demonstrate CRFR's SOTA performance on recommendation, conversation and conversation interpretability",
    "checked": true,
    "id": "c144f7ba423e59e8d79c61bafebdc7e2a7cc554c",
    "semantic_title": "crfr: improving conversational recommender systems via flexible fragments reasoning on knowledge graphs",
    "citation_count": 49,
    "authors": [
      "Jinfeng Zhou",
      "Bo Wang",
      "Ruifang He",
      "Yuexian Hou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.356": {
    "title": "DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation",
    "volume": "main",
    "abstract": "In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both English and Chinese, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation",
    "checked": true,
    "id": "c53bba3c4f3a1e912840b17dddccab3f57351f2e",
    "semantic_title": "durecdial 2.0: a bilingual parallel corpus for conversational recommendation",
    "citation_count": 56,
    "authors": [
      "Zeming Liu",
      "Haifeng Wang",
      "Zheng-Yu Niu",
      "Hua Wu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.357": {
    "title": "End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs",
    "volume": "main",
    "abstract": "We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the flowchart without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research",
    "checked": true,
    "id": "6ab36d2577f7c9487b28b2bcdf236191ba901aad",
    "semantic_title": "end-to-end learning of flowchart grounded task-oriented dialogs",
    "citation_count": 23,
    "authors": [
      "Dinesh Raghu",
      "Shantanu Agarwal",
      "Sachindra Joshi",
      "Mausam"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.358": {
    "title": "Dimensional Emotion Detection from Categorical Emotion",
    "volume": "main",
    "abstract": "We present a model to predict fine-grained emotions along the continuous dimensions of valence, arousal, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover's Distance) loss between the predicted VAD score distribution and the categorical emotion distributions sorted along VAD, and it can simultaneously classify the emotion categories and predict the VAD scores for a given sentence. We use pre-trained RoBERTa-Large and fine-tune on three different corpora with categorical labels and evaluate on EmoBank corpus with VAD scores. We show that our approach reaches comparable performance to that of the state-of-the-art classifiers in categorical emotion classification and shows significant positive correlations with the ground truth VAD scores. Also, further training with supervision of VAD labels leads to improved performance especially when dataset is small. We also present examples of predictions of appropriate emotion words that are not part of the original annotations",
    "checked": true,
    "id": "d5784f9c671a2f07b4316427b0d3c56a3d3cc027",
    "semantic_title": "dimensional emotion detection from categorical emotion",
    "citation_count": 37,
    "authors": [
      "Sungjoon Park",
      "Jiseon Kim",
      "Seonghyeon Ye",
      "Jaeyeol Jeon",
      "Hee Young Park",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.359": {
    "title": "Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification",
    "volume": "main",
    "abstract": "Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we analyse the contrastive fine-tuning of pre-trained language models on two fine-grained text classification tasks, emotion classification and sentiment analysis. We adaptively embed class relationships into a contrastive objective function to help differently weigh the positives and negatives, and in particular, weighting closely confusable negatives more than less similar negative examples. We find that Label-aware Contrastive Loss outperforms previous contrastive methods, in the presence of larger number and/or more confusable classes, and helps models to produce output distributions that are more differentiated",
    "checked": true,
    "id": "a35b5f92af58358483a75c50cead24f48b0be112",
    "semantic_title": "not all negatives are equal: label-aware contrastive loss for fine-grained text classification",
    "citation_count": 87,
    "authors": [
      "Varsha Suresh",
      "Desmond Ong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.360": {
    "title": "Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection",
    "volume": "main",
    "abstract": "Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between MATE and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches",
    "checked": true,
    "id": "ac762288143469ba425840aaed0fe661d2c2ffb1",
    "semantic_title": "joint multi-modal aspect-sentiment analysis with auxiliary cross-modal relation detection",
    "citation_count": 113,
    "authors": [
      "Xincheng Ju",
      "Dong Zhang",
      "Rong Xiao",
      "Junhui Li",
      "Shoushan Li",
      "Min Zhang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.361": {
    "title": "Solving Aspect Category Sentiment Analysis as a Text Generation Task",
    "volume": "main",
    "abstract": "Aspect category sentiment analysis has attracted increasing research attention. The dominant methods make use of pre-trained language models by learning effective aspect category-specific representations, and adding specific output layers to its pre-trained representation. We consider a more direct way of making use of pre-trained language models, by casting the ACSA tasks into natural language generation tasks, using natural language sentences to represent the output. Our method allows more direct use of pre-trained knowledge in seq2seq language models by directly following the task setting during pre-training. Experiments on several benchmarks show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings",
    "checked": true,
    "id": "e33eaa88f4b9ac8a020b6147192493bbf4cdd559",
    "semantic_title": "solving aspect category sentiment analysis as a text generation task",
    "citation_count": 75,
    "authors": [
      "Jian Liu",
      "Zhiyang Teng",
      "Leyang Cui",
      "Hanmeng Liu",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.362": {
    "title": "Semantics-Preserved Data Augmentation for Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "Both the issues of data deficiencies and semantic consistency are important for data augmentation. Most of previous methods address the first issue, but ignore the second one. In the cases of aspect-based sentiment analysis, violation of the above issues may change the aspect and sentiment polarity. In this paper, we propose a semantics-preservation data augmentation approach by considering the importance of each word in a textual sequence according to the related aspects and sentiments. We then substitute the unimportant tokens with two replacement strategies without altering the aspect-level polarity. Our approach is evaluated on several publicly available sentiment analysis datasets and the real-world stock price/risk movement prediction scenarios. Experimental results show that our methodology achieves better performances in all datasets",
    "checked": true,
    "id": "4e1e362fe8625629ea672c617ceb17374e704011",
    "semantic_title": "semantics-preserved data augmentation for aspect-based sentiment analysis",
    "citation_count": 34,
    "authors": [
      "Ting-Wei Hsu",
      "Chung-Chi Chen",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.363": {
    "title": "The Effect of Round-Trip Translation on Fairness in Sentiment Analysis",
    "volume": "main",
    "abstract": "Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip translation consistently improves classification fairness at test time (reducing up to 47% of between-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data",
    "checked": true,
    "id": "21e465f2aa410565aeaab4895fe338d0e7b1c600",
    "semantic_title": "the effect of round-trip translation on fairness in sentiment analysis",
    "citation_count": 3,
    "authors": [
      "Jonathan Gabel Christiansen",
      "Mathias Gammelgaard",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.364": {
    "title": "CHoRaL: Collecting Humor Reaction Labels from Millions of Social Media Users",
    "volume": "main",
    "abstract": "Humor detection has gained attention in recent years due to the desire to understand user-generated content with figurative language. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a framework to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides both binary labels and continuous scores of humor and non-humor. We present the largest dataset to date with labeled humor on 785K posts related to COVID-19. Additionally, we analyze the expression of COVID-related humor in social media by extracting lexico-semantic and affective features from the posts, and build humor detection models with performance similar to humans. CHoRaL enables the development of large-scale humor detection models on any topic and opens a new path to the study of humor on social media",
    "checked": true,
    "id": "f873ab118d7d44aa2cf90a9d3bf5f726e07d3001",
    "semantic_title": "choral: collecting humor reaction labels from millions of social media users",
    "citation_count": 10,
    "authors": [
      "Zixiaofan Yang",
      "Shayan Hooshmand",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.365": {
    "title": "CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization",
    "volume": "main",
    "abstract": "Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer's issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive summaries in two aspects: (1) In addition to the overall summary for the whole dialogue, role-oriented summaries are also provided to acquire different speakers' viewpoints. (2) All the summaries sum up each topic separately, thus containing the topic-level structure of the dialogue. We define tasks in CSDS as generating the overall summary and different role-oriented summaries for a given dialogue. Next, we compare various summarization methods on CSDS, and experiment results show that existing methods are prone to generate redundant and incoherent summaries. Besides, the performance becomes much worse when analyzing the performance on role-oriented summaries and topic structures. We hope that this study could benchmark Chinese dialogue summarization and benefit further studies",
    "checked": true,
    "id": "00b95d2f15eab5b14c2affef055fd3a6727eb571",
    "semantic_title": "csds: a fine-grained chinese dataset for customer service dialogue summarization",
    "citation_count": 49,
    "authors": [
      "Haitao Lin",
      "Liqun Ma",
      "Junnan Zhu",
      "Lu Xiang",
      "Yu Zhou",
      "Jiajun Zhang",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.366": {
    "title": "CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild",
    "volume": "main",
    "abstract": "Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in knowledge bases can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards knowledge acquisition in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges: Given two entities, (1) it requires finding the relevant documents that can provide clues for identifying their relations; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models",
    "checked": true,
    "id": "c14c6569771af19f911f7606206aafcbbb9c05a7",
    "semantic_title": "codred: a cross-document relation extraction dataset for acquiring knowledge in the wild",
    "citation_count": 23,
    "authors": [
      "Yuan Yao",
      "Jiaju Du",
      "Yankai Lin",
      "Peng Li",
      "Zhiyuan Liu",
      "Jie Zhou",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.367": {
    "title": "Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions",
    "volume": "main",
    "abstract": "Enabling open-domain dialogue systems to ask clarifying questions when appropriate is an important direction for improving the quality of the system response. Namely, for cases when a user request is not specific enough for a conversation system to provide an answer right away, it is desirable to ask a clarifying question to increase the chances of retrieving a satisfying answer. To address the problem of ‘asking clarifying questions in open-domain dialogues': (1) we collect and release a new dataset focused on open-domain single- and multi-turn conversations, (2) we benchmark several state-of-the-art neural baselines, and (3) we propose a pipeline consisting of offline and online steps for evaluating the quality of clarifying questions in various dialogues. These contributions are suitable as a foundation for further research",
    "checked": true,
    "id": "267d9a093ae7e8388fd1e25d2b5e4cfe91c71226",
    "semantic_title": "building and evaluating open-domain dialogue corpora with clarifying questions",
    "citation_count": 102,
    "authors": [
      "Mohammad Aliannejadi",
      "Julia Kiseleva",
      "Aleksandr Chuklin",
      "Jeff Dalton",
      "Mikhail Burtsev"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.368": {
    "title": "We Need to Talk About train-dev-test Splits",
    "volume": "main",
    "abstract": "Standard train-dev-test splits used to benchmark multiple models against each other are ubiquitously used in Natural Language Processing (NLP). In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during development, and the test set to confirm the answers to the main research question(s). However, the introduction of neural networks in NLP has led to a different use of these standard splits; the development set is now often used for model selection during the training procedure. Because of this, comparing multiple versions of the same model during development leads to overestimation on the development data. As an effect, people have started to compare an increasing amount of models on the test data, leading to faster overfitting and \"expiration\" of our test sets. We propose to use a tune-set when developing neural network methods, which can be used for model picking so that comparing the different versions of a new model can safely be done on the development data",
    "checked": true,
    "id": "fb6e8415b3f7f3cc987c4b0aec55e64b5de12698",
    "semantic_title": "we need to talk about train-dev-test splits",
    "citation_count": 21,
    "authors": [
      "Rob van der Goot"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.369": {
    "title": "PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation",
    "volume": "main",
    "abstract": "We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations: the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at: https://github.com/VinAIResearch/PhoMT",
    "checked": true,
    "id": "845cf90e126daf28a9d97453353773e6e80d0117",
    "semantic_title": "phomt: a high-quality and large-scale benchmark dataset for vietnamese-english machine translation",
    "citation_count": 26,
    "authors": [
      "Long Doan",
      "Linh The Nguyen",
      "Nguyen Luong Tran",
      "Thai Hoang",
      "Dat Quoc Nguyen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.370": {
    "title": "Lying Through One's Teeth: A Study on Verbal Leakage Cues",
    "volume": "main",
    "abstract": "Although many studies use the LIWC lexicon to show the existence of verbal leakage cues in lie detection datasets, none mention how verbal leakage cues are influenced by means of data collection, or the impact thereof on the performance of models. In this paper, we study verbal leakage cues to understand the effect of the data construction method on their significance, and examine the relationship between such cues and models' validity. The LIWC word-category dominance scores of seven lie detection datasets are used to show that audio statements and lie-based annotations indicate a greater number of strong verbal leakage cue categories. Moreover, we evaluate the validity of state-of-the-art lie detection models with cross- and in-dataset testing. Results show that in both types of testing, models trained on a dataset with more strong verbal leakage cue categories—as opposed to only a greater number of strong cues—yield superior results, suggesting that verbal leakage cues are a key factor for selecting lie detection datasets",
    "checked": true,
    "id": "b51770fffaa319dc7d01d0c59406f8aab6d58c6a",
    "semantic_title": "lying through one's teeth: a study on verbal leakage cues",
    "citation_count": 7,
    "authors": [
      "Min-Hsuan Yeh",
      "Lun-Wei Ku"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.371": {
    "title": "Multi-granularity Textual Adversarial Attack with Behavior Cloning",
    "volume": "main",
    "abstract": "Recently, the textual adversarial attack models become increasingly popular due to their successful in estimating the robustness of NLP models. However, existing works have obvious deficiencies. (1)They usually consider only a single granularity of modification strategies (e.g. word-level or sentence-level), which is insufficient to explore the holistic textual space for generation; (2) They need to query victim models hundreds of times to make a successful attack, which is highly inefficient in practice. To address such problems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to effectively generate high-quality adversarial samples with fewer queries to victim models. Furthermore, we propose a reinforcement-learning based method to train a multi-granularity attack agent through behavior cloning with the expert knowledge from our MAYA algorithm to further reduce the query times. Additionally, we also adapt the agent to attack black-box models that only output labels without confidence scores. We conduct comprehensive experiments to evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two different black-box attack settings and three benchmark datasets. Experimental results show that our models achieve overall better attacking performance and produce more fluent and grammatical adversarial samples compared to baseline models. Besides, our adversarial attack agent significantly reduces the query times in both attack settings. Our codes are released at https://github.com/Yangyi-Chen/MAYA",
    "checked": true,
    "id": "9871606b134fcf84db3cb30c906ce24c73298a7f",
    "semantic_title": "multi-granularity textual adversarial attack with behavior cloning",
    "citation_count": 33,
    "authors": [
      "Yangyi Chen",
      "Jin Su",
      "Wei Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.372": {
    "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality",
    "volume": "main",
    "abstract": "Similarity measures are a vital tool for understanding how language models represent and process language. Standard representational similarity measures such as cosine similarity and Euclidean distance have been successfully used in static word embedding models to understand how words cluster in semantic space. Recently, these measures have been applied to embeddings from contextualized models such as BERT and GPT-2. In this work, we call into question the informativity of such measures for contextualized language models. We find that a small number of rogue dimensions, often just 1-3, dominate these measures. Moreover, we find a striking mismatch between the dimensions that dominate similarity measures and those which are important to the behavior of the model. We show that simple postprocessing techniques such as standardization are able to correct for rogue dimensions and reveal underlying representational quality. We argue that accounting for rogue dimensions is essential for any similarity-based analysis of contextual language models",
    "checked": true,
    "id": "51b0c571d89bd2d39a194f60f91f0a03d74574b5",
    "semantic_title": "all bark and no bite: rogue dimensions in transformer language models obscure representational quality",
    "citation_count": 117,
    "authors": [
      "William Timkey",
      "Marten van Schijndel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.373": {
    "title": "Incorporating Residual and Normalization Layers into Analysis of Masked Language Models",
    "volume": "main",
    "abstract": "Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers' progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available",
    "checked": true,
    "id": "042c6d99bdf381b55048b8bb48c8479dbcfbcd5a",
    "semantic_title": "incorporating residual and normalization layers into analysis of masked language models",
    "citation_count": 49,
    "authors": [
      "Goro Kobayashi",
      "Tatsuki Kuribayashi",
      "Sho Yokoi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.374": {
    "title": "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer",
    "volume": "main",
    "abstract": "Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a backdoor attack method, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transfer—the attack success rates can exceed 90% without much effort. It reflects the limited ability of NLP models to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to baselines in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack",
    "checked": true,
    "id": "342f6ae2ebfc0ddddf15e8ba910eab6f2e06bf83",
    "semantic_title": "mind the style of text! adversarial and backdoor attacks based on text style transfer",
    "citation_count": 191,
    "authors": [
      "Fanchao Qi",
      "Yangyi Chen",
      "Xurui Zhang",
      "Mukai Li",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.375": {
    "title": "Sociolectal Analysis of Pretrained Language Models",
    "volume": "main",
    "abstract": "Using data from English cloze tests, in which subjects also self-reported their gender, age, education, and race, we examine performance differences of pretrained language models across demographic groups, defined by these (protected) attributes. We demonstrate wide performance gaps across demographic groups and show that pretrained language models systematically disfavor young non-white male speakers; i.e., not only do pretrained language models learn social biases (stereotypical associations) – pretrained language models also learn sociolectal biases, learning to speak more like some than like others. We show, however, that, with the exception of BERT models, larger pretrained language models reduce some the performance gaps between majority and minority groups",
    "checked": true,
    "id": "e2ba3f80aac11ebc1bc4d7be247800bee4d5fb62",
    "semantic_title": "sociolectal analysis of pretrained language models",
    "citation_count": 17,
    "authors": [
      "Sheng Zhang",
      "Xin Zhang",
      "Weiming Zhang",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.376": {
    "title": "Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes",
    "volume": "main",
    "abstract": "State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's contextual representations for nine diverse languages. We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing",
    "checked": true,
    "id": "698c2d985d2568dfb8c76f96396897cc5ca29800",
    "semantic_title": "examining cross-lingual contextual embeddings with orthogonal structural probes",
    "citation_count": 3,
    "authors": [
      "Tomasz Limisiewicz",
      "David Mareček"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.377": {
    "title": "Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement",
    "volume": "main",
    "abstract": "Many recent works have demonstrated that unsupervised sentence representations of neural networks encode syntactic information by observing that neural language models are able to predict the agreement between a verb and its subject. We take a critical look at this line of research by showing that it is possible to achieve high accuracy on this agreement task with simple surface heuristics, indicating a possible flaw in our assessment of neural networks' syntactic ability. Our fine-grained analyses of results on the long-range French object-verb agreement show that contrary to LSTMs, Transformers are able to capture a non-trivial amount of grammatical structure",
    "checked": true,
    "id": "78d68e41d190f844f65e41ba79a8bae0714b8ab2",
    "semantic_title": "are transformers a modern version of eliza? observations on french object verb agreement",
    "citation_count": 6,
    "authors": [
      "Bingzhi Li",
      "Guillaume Wisniewski",
      "Benoit Crabbé"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.378": {
    "title": "Fine-grained Entity Typing via Label Reasoning",
    "volume": "main",
    "abstract": "Conventional entity typing approaches are based on independent classification paradigms, which make them difficult to recognize inter-dependent, long-tailed and fine-grained entity types. In this paper, we argue that the implicitly entailed extrinsic and intrinsic dependencies between labels can provide critical knowledge to tackle the above challenges. To this end, we propose Label Reasoning Network(LRN), which sequentially reasons fine-grained entity labels by discovering and exploiting label dependencies knowledge entailed in the data. Specifically, LRN utilizes an auto-regressive network to conduct deductive reasoning and a bipartite attribute graph to conduct inductive reasoning between labels, which can effectively model, learn and reason complex label dependencies in a sequence-to-set, end-to-end manner. Experiments show that LRN achieves the state-of-the-art performance on standard ultra fine-grained entity typing benchmarks, and can also resolve the long tail label problem effectively",
    "checked": true,
    "id": "7f30821267a11138497107d947ea39726e4b7fbd",
    "semantic_title": "fine-grained entity typing via label reasoning",
    "citation_count": 29,
    "authors": [
      "Qing Liu",
      "Hongyu Lin",
      "Xinyan Xiao",
      "Xianpei Han",
      "Le Sun",
      "Hua Wu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.379": {
    "title": "Enhanced Language Representation with Label Knowledge for Span Extraction",
    "volume": "main",
    "abstract": "Span extraction, aiming to extract text spans (such as words or phrases) from plain text, is a fundamental process in Information Extraction. Recent works introduce the label knowledge to enhance the text representation by formalizing the span extraction task into a question answering problem (QA Formalization), which achieves state-of-the-art performance. However, such a QA Formalization does not fully exploit the label knowledge and causes a dramatic decrease in efficiency of training/inference. To address those problems, we introduce a fresh paradigm to integrate label knowledge and further propose a novel model to explicitly and efficiently integrate label knowledge into text representations. Specifically, it encodes texts and label annotations independently and then integrates label knowledge into text representation with an elaborate-designed semantics fusion module. We conduct extensive experiments on three typical span extraction tasks: flat NER, nested NER, and event detection. The empirical results show that 1) our model achieves a new state-of-the-art performance on four benchmarks, and 2) reduces training time and inference time by 76% and 77% on average, respectively, compared with the QA Formalization paradigm",
    "checked": true,
    "id": "588a9b9be073b2124b45e66f492029a74de7971b",
    "semantic_title": "enhanced language representation with label knowledge for span extraction",
    "citation_count": 29,
    "authors": [
      "Pan Yang",
      "Xin Cong",
      "Zhenyu Sun",
      "Xingwu Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.380": {
    "title": "PRIDE: Predicting Relationships in Conversations",
    "volume": "main",
    "abstract": "Automatically extracting interpersonal relationships of conversation interlocutors can enrich personal knowledge bases to enhance personalized search, recommenders and chatbots. To infer speakers' relationships from dialogues we propose PRIDE, a neural multi-label classifier, based on BERT and Transformer for creating a conversation representation. PRIDE utilizes dialogue structure and augments it with external knowledge about speaker features and conversation style. Unlike prior works, we address multi-label prediction of fine-grained relationships. We release large-scale datasets, based on screenplays of movies and TV shows, with directed relationships of conversation participants. Extensive experiments on both datasets show superior performance of PRIDE compared to the state-of-the-art baselines",
    "checked": true,
    "id": "d248773e715f42e11d5df4366a5cdd3ae0cd8afe",
    "semantic_title": "pride: predicting relationships in conversations",
    "citation_count": 13,
    "authors": [
      "Anna Tigunova",
      "Paramita Mirza",
      "Andrew Yates",
      "Gerhard Weikum"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.381": {
    "title": "Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results",
    "volume": "main",
    "abstract": "Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), PubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as nodes and relations as edges between them, but also fine-grained attributes that modify entities and their relations, for a total of 12,738 labels in the corpus. By including more label types and more than twice the label density of previous datasets, SciClaim captures causal, comparative, predictive, statistical, and proportional associations over experimental variables along with their qualifications, subtypes, and evidence. We extend work in transformer-based joint entity and relation extraction to effectively infer our schema, showing the promise of fine-grained knowledge graphs in scientific claims and beyond",
    "checked": true,
    "id": "70d0d080b40e2cb9831d72a52a0e8da854a7de39",
    "semantic_title": "extracting fine-grained knowledge graphs of scientific claims: dataset and transformer-based results",
    "citation_count": 14,
    "authors": [
      "Ian Magnusson",
      "Scott Friedman"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.382": {
    "title": "Sequential Cross-Document Coreference Resolution",
    "volume": "main",
    "abstract": "Relating entities and events in text is a key component of natural language understanding. Cross-document coreference resolution, in particular, is important for the growing interest in multi-document analysis tasks. In this work we propose a new model that extends the efficient sequential prediction paradigm for coreference resolution to cross-document settings and achieves competitive results for both entity and event coreference while providing strong evidence of the efficacy of both sequential models and higher-order inference in cross-document settings. Our model incrementally composes mentions into cluster representations and predicts links between a mention and the already constructed clusters, approximating a higher-order model. In addition, we conduct extensive ablation studies that provide new insights into the importance of various inputs and representation types in coreference",
    "checked": true,
    "id": "93f118e4f583c0ee6a97b39ef8b6bc23565229bb",
    "semantic_title": "sequential cross-document coreference resolution",
    "citation_count": 17,
    "authors": [
      "Emily Allaway",
      "Shuai Wang",
      "Miguel Ballesteros"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.383": {
    "title": "Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT",
    "volume": "main",
    "abstract": "Infusing factual knowledge into pre-trained models is fundamental for many knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions (MoP), an infusion approach that can handle a very large knowledge graph (KG) by partitioning it into smaller sub-graphs and infusing their specific knowledge into various BERT models using lightweight adapters. To leverage the overall factual knowledge for a target task, these sub-graph adapters are further fine-tuned along with the underlying BERT through a mixture layer. We evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on six downstream tasks (inc. NLI, QA, Classification), and the results show that our MoP consistently enhances the underlying BERTs in task performance, and achieves new SOTA performances on five evaluated datasets",
    "checked": true,
    "id": "8cdb9f975aaff5adb51cfa164199010bb9b9b6d1",
    "semantic_title": "mixture-of-partitions: infusing large biomedical knowledge graphs into bert",
    "citation_count": 37,
    "authors": [
      "Zaiqiao Meng",
      "Fangyu Liu",
      "Thomas Clark",
      "Ehsan Shareghi",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.384": {
    "title": "Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach",
    "volume": "main",
    "abstract": "We present models which complete missing text given transliterations of ancient Mesopotamian documents, originally written on cuneiform clay tablets (2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on contextual cues to manually fill in missing parts in the text in a subjective and time-consuming process. We identify that this challenge can be formulated as a masked language modelling task, used mostly as a pretraining objective for contextualized language models. Following, we develop several architectures focusing on the Akkadian language, the lingua franca of the time. We find that despite data scarcity (1M tokens) we can achieve state of the art performance on missing tokens prediction (89% hit@5) using a greedy decoding scheme and pretraining on data from other languages and different time periods. Finally, we conduct human evaluations showing the applicability of our models in assisting experts to transcribe texts in extinct languages",
    "checked": true,
    "id": "73ebd89b20bb1c1a93e79e4b0175e0825b0aecda",
    "semantic_title": "filling the gaps in ancient akkadian texts: a masked language modelling approach",
    "citation_count": 25,
    "authors": [
      "Koren Lazar",
      "Benny Saret",
      "Asaf Yehudai",
      "Wayne Horowitz",
      "Nathan Wasserman",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.385": {
    "title": "AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain",
    "volume": "main",
    "abstract": "During the fine-tuning phase of transfer learning, the pretrained vocabulary remains unchanged, while model parameters are updated. The vocabulary generated based on the pretrained data is suboptimal for downstream data when domain discrepancy exists. We propose to consider the vocabulary as an optimizable parameter, allowing us to update the vocabulary by expanding it with domain specific vocabulary based on a tokenization statistic. Furthermore, we preserve the embeddings of the added words from overfitting to downstream data by utilizing knowledge learned from a pretrained language model with a regularization term. Our method achieved consistent performance improvements on diverse domains (i.e., biomedical, computer science, news, and reviews)",
    "checked": true,
    "id": "03aa38229a6c853d96430edfc8f1542598f2116e",
    "semantic_title": "avocado: strategy for adapting vocabulary to downstream domain",
    "citation_count": 25,
    "authors": [
      "Jimin Hong",
      "TaeHee Kim",
      "Hyesu Lim",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.386": {
    "title": "Can We Improve Model Robustness through Secondary Attribute Counterfactuals?",
    "volume": "main",
    "abstract": "Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data augmentation (CDA) can mitigate such issues. In this paper we study how and why modeling counterfactuals over multiple attributes can go significantly further in improving model performance. We propose RDI, a context-aware methodology which takes into account the impact of secondary attributes on the model's predictions and increases sensitivity for secondary attributes over reweighted counterfactually augmented data. By implementing RDI in the context of toxicity detection, we find that accounting for secondary attributes can significantly improve robustness, with improvements in sliced accuracy on the original dataset up to 7% compared to existing robustness methods. We also demonstrate that RDI generalizes to the coreference resolution task and provide guidelines to extend this to other tasks",
    "checked": true,
    "id": "c41c3d07a10bebb4d8b6dcaea0d263ede051bda8",
    "semantic_title": "can we improve model robustness through secondary attribute counterfactuals?",
    "citation_count": 10,
    "authors": [
      "Ananth Balashankar",
      "Xuezhi Wang",
      "Ben Packer",
      "Nithum Thain",
      "Ed Chi",
      "Alex Beutel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.387": {
    "title": "Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy",
    "volume": "main",
    "abstract": "Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that they will be unable to leverage the entire relevant context of large files and packages for any given task. While there are many efforts to extend the context window, we introduce an architecture-independent approach for leveraging the syntactic hierarchies of source code for incorporating entire file-level context into a fixed-length window. Using concrete syntax trees of each source file we extract syntactic hierarchies and integrate them into context window by selectively removing from view more specific, less relevant scopes for a given task. We evaluate this approach on code generation tasks and joint translation of natural language and source code in Python programming language, achieving a new state-of-the-art in code completion and summarization for Python in the CodeXGLUE benchmark. We also introduce new CodeXGLUE benchmarks for user-experience-motivated tasks: code completion with normalized literals, method body completion/code summarization conditioned on file-level context",
    "checked": true,
    "id": "2016e814eed00b0c0a9358e193e29854e0ed526f",
    "semantic_title": "long-range modeling of source code files with ewash: extended window access by syntax hierarchy",
    "citation_count": 28,
    "authors": [
      "Colin Clement",
      "Shuai Lu",
      "Xiaoyu Liu",
      "Michele Tufano",
      "Dawn Drain",
      "Nan Duan",
      "Neel Sundaresan",
      "Alexey Svyatkovskiy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.388": {
    "title": "Can Language Models be Biomedical Knowledge Bases?",
    "volume": "main",
    "abstract": "Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing",
    "checked": true,
    "id": "4c5f4ddc68be643fb34ea969bf2c105ff7538995",
    "semantic_title": "can language models be biomedical knowledge bases?",
    "citation_count": 108,
    "authors": [
      "Mujeen Sung",
      "Jinhyuk Lee",
      "Sean Yi",
      "Minji Jeon",
      "Sungdong Kim",
      "Jaewoo Kang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.389": {
    "title": "LayoutReader: Pre-training of Text and Layout for Reading Order Detection",
    "volume": "main",
    "abstract": "Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that contains reading order, text, and layout information for 500,000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically, our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. The dataset and models are publicly available at https://aka.ms/layoutreader",
    "checked": true,
    "id": "ad2911c00a13e8a0828e34b65a36622658c545a0",
    "semantic_title": "layoutreader: pre-training of text and layout for reading order detection",
    "citation_count": 78,
    "authors": [
      "Zilong Wang",
      "Yiheng Xu",
      "Lei Cui",
      "Jingbo Shang",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.390": {
    "title": "Region under Discussion for visual dialog",
    "volume": "main",
    "abstract": "Visual Dialog is assumed to require the dialog history to generate correct responses during a dialog. However, it is not clear from previous work how dialog history is needed for visual dialog. In this paper we define what it means for a visual question to require dialog history and we release a subset of the Guesswhat?! questions for which their dialog history completely changes their responses. We propose a novel interpretable representation that visually grounds dialog history: the Region under Discussion. It constrains the image's spatial features according to a semantic representation of the history inspired by the information structure notion of Question under Discussion.We evaluate the architecture on task-specific multimodal models and the visual transformer model LXMERT",
    "checked": true,
    "id": "4656b3fbc0e5dad904001de2eca8645813871801",
    "semantic_title": "region under discussion for visual dialog",
    "citation_count": 5,
    "authors": [
      "Mauricio Mazuecos",
      "Franco M. Luque",
      "Jorge Sánchez",
      "Hernán Maina",
      "Thomas Vadora",
      "Luciana Benotti"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.391": {
    "title": "Learning grounded word meaning representations on similarity graphs",
    "volume": "main",
    "abstract": "This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the hierarchy models modality-specific word representations, conditioned to another modality, through dedicated but communicating graphs, while the higher level puts these representations together on a single graph to learn a representation jointly from both modalities. The topology of each graph models similarity relations among words, and is estimated jointly with the graph embedding. The assumption underlying this model is that words sharing similar meaning correspond to communities in an underlying graph in a low-dimensional space. We named this model Hierarchical Multi-Modal Similarity Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE to simulate human similarity judgments and concept categorization, outperforming the state of the art",
    "checked": true,
    "id": "d7c93b1dfc6a36d41deaf61cb2a82f52607a80cf",
    "semantic_title": "learning grounded word meaning representations on similarity graphs",
    "citation_count": 1,
    "authors": [
      "Mariella Dimiccoli",
      "Herwig Wendt",
      "Pau Batlle Franch"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.392": {
    "title": "WhyAct: Identifying Action Reasons in Lifestyle Vlogs",
    "volume": "main",
    "abstract": "We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video",
    "checked": true,
    "id": "f78748bd6137b0768d757657f0fdea660509f4a7",
    "semantic_title": "whyact: identifying action reasons in lifestyle vlogs",
    "citation_count": 11,
    "authors": [
      "Oana Ignat",
      "Santiago Castro",
      "Hanwen Miao",
      "Weiji Li",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.393": {
    "title": "Genre as Weak Supervision for Cross-lingual Dependency Parsing",
    "volume": "main",
    "abstract": "Recent work has shown that monolingual masked language models learn to represent data-driven notions of language variation which can be used for domain-targeted training data selection. Dataset genre labels are already frequently available, yet remain largely unexplored in cross-lingual setups. We harness this genre metadata as a weak supervision signal for targeted data selection in zero-shot dependency parsing. Specifically, we project treebank-level genre information to the finer-grained sentence level, with the goal to amplify information implicitly stored in unsupervised contextualized representations. We demonstrate that genre is recoverable from multilingual contextual embeddings and that it provides an effective signal for training data selection in cross-lingual, zero-shot scenarios. For 12 low-resource language treebanks, six of which are test-only, our genre-specific methods significantly outperform competitive baselines as well as recent embedding-based methods for data selection. Moreover, genre-based data selection provides new state-of-the-art results for three of these target languages",
    "checked": true,
    "id": "af3e19795ce37f8ed068e6602bf2b2e349ded105",
    "semantic_title": "genre as weak supervision for cross-lingual dependency parsing",
    "citation_count": 19,
    "authors": [
      "Max Müller-Eberstein",
      "Rob van der Goot",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.394": {
    "title": "On the Relation between Syntactic Divergence and Zero-Shot Performance",
    "volume": "main",
    "abstract": "We explore the link between the extent to which syntactic relations are preserved in translation and the ease of correctly constructing a parse tree in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edges—a gap we aim to address. As a test case, we take the transfer of Universal Dependencies (UD) parsing from English to a diverse set of languages and conduct two sets of experiments. In one, we analyze zero-shot performance based on the extent to which English source edges are preserved in translation. In another, we apply three linguistically motivated transformations to UD, creating more cross-lingually stable versions of it, and assess their zero-shot parsability. In order to compare parsing performance across different schemes, we perform extrinsic evaluation on the downstream task of cross-lingual relation extraction (RE) using a subset of a standard English RE benchmark translated to Russian and Korean. In both sets of experiments, our results suggest a strong relation between cross-lingual stability and zero-shot parsing performance",
    "checked": true,
    "id": "acfaf61783872c5434553c5d174dd8d6afb1d0d6",
    "semantic_title": "on the relation between syntactic divergence and zero-shot performance",
    "citation_count": 4,
    "authors": [
      "Ofir Arviv",
      "Dmitry Nikolaev",
      "Taelin Karidi",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.395": {
    "title": "Improved Latent Tree Induction with Distant Supervision via Span Constraints",
    "volume": "main",
    "abstract": "For over thirty years, researchers have developed and analyzed methods for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern systems still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of span constraints (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from Wikipedia, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset",
    "checked": true,
    "id": "b593be8ff3c09c6994657678fcde0c5adf43328e",
    "semantic_title": "improved latent tree induction with distant supervision via span constraints",
    "citation_count": 9,
    "authors": [
      "Zhiyang Xu",
      "Andrew Drozdov",
      "Jay Yoon Lee",
      "Tim O’Gorman",
      "Subendhu Rongali",
      "Dylan Finkbeiner",
      "Shilpa Suresh",
      "Mohit Iyyer",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.396": {
    "title": "Aligning Multidimensional Worldviews and Discovering Ideological Differences",
    "volume": "main",
    "abstract": "The Internet is home to thousands of communities, each with their own unique worldview and associated ideological differences. With new communities constantly emerging and serving as ideological birthplaces, battlegrounds, and bunkers, it is critical to develop a framework for understanding worldviews and ideological distinction. Most existing work, however, takes a predetermined view based on political polarization: the \"right vs. left\" dichotomy of U.S. politics. In reality, both political polarization – and worldviews more broadly – transcend one-dimensional difference, and deserve a more complete analysis. Extending the ability of word embedding models to capture the semantic and cultural characteristics of their training corpora, we propose a novel method for discovering the multifaceted ideological and worldview characteristics of communities. Using over 1B comments collected from the largest communities on Reddit.com representing ~40% of Reddit activity, we demonstrate the efficacy of this approach to uncover complex ideological differences across multiple axes of polarization",
    "checked": true,
    "id": "54efe3aadf8d793f0332c8c213a09fc7d2ff15c8",
    "semantic_title": "aligning multidimensional worldviews and discovering ideological differences",
    "citation_count": 16,
    "authors": [
      "Jeremiah Milbauer",
      "Adarsh Mathew",
      "James Evans"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.397": {
    "title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts",
    "volume": "main",
    "abstract": "Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer",
    "checked": true,
    "id": "f56cda7ee6b3cfa427d045b6cc754ec68349c511",
    "semantic_title": "just say no: analyzing the stance of neural dialogue generation in offensive contexts",
    "citation_count": 92,
    "authors": [
      "Ashutosh Baheti",
      "Maarten Sap",
      "Alan Ritter",
      "Mark Riedl"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.398": {
    "title": "Multi-Modal Open-Domain Dialogue",
    "volume": "main",
    "abstract": "Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference",
    "checked": true,
    "id": "cf58cbdaf475109da7c528e6d5d390ed97fba6b2",
    "semantic_title": "multi-modal open-domain dialogue",
    "citation_count": 44,
    "authors": [
      "Kurt Shuster",
      "Eric Michael Smith",
      "Da Ju",
      "Jason Weston"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.399": {
    "title": "A Label-Aware BERT Attention Network for Zero-Shot Multi-Intent Detection in Spoken Language Understanding",
    "volume": "main",
    "abstract": "With the early success of query-answer assistants such as Alexa and Siri, research attempts to expand system capabilities of handling service automation are now abundant. However, preliminary systems have quickly found the inadequacy in relying on simple classification techniques to effectively accomplish the automation task. The main challenge is that the dialogue often involves complexity in user's intents (or purposes) which are multiproned, subject to spontaneous change, and difficult to track. Furthermore, public datasets have not considered these complications and the general semantic annotations are lacking which may result in zero-shot problem. Motivated by the above, we propose a Label-Aware BERT Attention Network (LABAN) for zero-shot multi-intent detection. We first encode input utterances with BERT and construct a label embedded space by considering embedded semantics in intent labels. An input utterance is then classified based on its projection weights on each intent embedding in this embedded space. We show that it successfully extends to few/zero-shot setting where part of intent labels are unseen in training data, by also taking account of semantics in these unseen intent labels. Experimental results show that our approach is capable of detecting many unseen intent labels correctly. It also achieves the state-of-the-art performance on five multi-intent datasets in normal cases",
    "checked": true,
    "id": "3b5c4d29355cf6b2a1b6f646d18f06b7e4d82b73",
    "semantic_title": "a label-aware bert attention network for zero-shot multi-intent detection in spoken language understanding",
    "citation_count": 19,
    "authors": [
      "Ting-Wei Wu",
      "Ruolin Su",
      "Biing Juang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.400": {
    "title": "Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection",
    "volume": "main",
    "abstract": "Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all reply-to links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a zero-shot dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10% of the data, we achieve nearly the same performance of using the full dataset",
    "checked": true,
    "id": "957cb7b0256b9a923147fba88ea5c5c757a5813b",
    "semantic_title": "zero-shot dialogue disentanglement by self-supervised entangled response selection",
    "citation_count": 3,
    "authors": [
      "Ta-Chung Chi",
      "Alexander Rudnicky"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.401": {
    "title": "SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations",
    "volume": "main",
    "abstract": "Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user's multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K task-oriented user<->assistant dialogs (117K utterances) in the shopping domain, grounded in immersive and photo-realistic scenes. The dialogs are collection using a two-phase pipeline: (1) A novel multimodal dialog simulator generates simulated dialog flows, with an emphasis on diversity and richness of interactions, (2) Manual paraphrasing of generating utterances to draw from natural language distribution. We provide an in-depth analysis of the collected dataset, and describe in detail the four main benchmark tasks we propose for SIMMC 2.0. Our baseline model, powered by the state-of-the-art language model, shows promising results, and highlights new challenges and directions for the community to study",
    "checked": true,
    "id": "0c15e710981bb339af64a1fb2e8364b3b0e97a5f",
    "semantic_title": "simmc 2.0: a task-oriented dialog dataset for immersive multimodal conversations",
    "citation_count": 93,
    "authors": [
      "Satwik Kottur",
      "Seungwhan Moon",
      "Alborz Geramifard",
      "Babak Damavandi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.402": {
    "title": "RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging",
    "volume": "main",
    "abstract": "The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model's outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems when transferring to another dataset",
    "checked": true,
    "id": "56dc1ce4df5ce80a48743be8ebd38025bdba24bf",
    "semantic_title": "rast: domain-robust dialogue rewriting as sequence tagging",
    "citation_count": 21,
    "authors": [
      "Jie Hao",
      "Linfeng Song",
      "Liwei Wang",
      "Kun Xu",
      "Zhaopeng Tu",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.403": {
    "title": "MRF-Chat: Improving Dialogue with Markov Random Fields",
    "volume": "main",
    "abstract": "Recent state-of-the-art approaches in open-domain dialogue include training end-to-end deep-learning models to learn various conversational features like emotional content of response, symbolic transitions of dialogue contexts in a knowledge graph and persona of the agent and the user, among others. While neural models have shown reasonable results, modelling the cognitive processes that humans use when conversing with each other may improve the agent's quality of responses. A key element of natural conversation is to tailor one's response such that it accounts for concepts that the speaker and listener may or may not know and the contextual relevance of all prior concepts used in conversation. We show that a rich representation and explicit modeling of these psychological processes can improve predictions made by existing neural network models. In this work, we propose a novel probabilistic approach using Markov Random Fields (MRF) to augment existing deep-learning methods for improved next utterance prediction. Using human and automatic evaluations, we show that our augmentation approach significantly improves the performance of existing state-of-the-art retrieval models for open-domain conversational agents",
    "checked": true,
    "id": "26a15c0e1becb323f40a616003a15db48ea1581a",
    "semantic_title": "mrf-chat: improving dialogue with markov random fields",
    "citation_count": 1,
    "authors": [
      "Ishaan Grover",
      "Matthew Huggins",
      "Cynthia Breazeal",
      "Hae Won Park"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.404": {
    "title": "Dialogue State Tracking with a Language Model using Schema-Driven Prompting",
    "volume": "main",
    "abstract": "Task-oriented conversational systems often use dialogue state tracking to represent the user's intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely generative system achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting",
    "checked": true,
    "id": "9c5a5e932139621da37c93d48f8a8df40a9c61d4",
    "semantic_title": "dialogue state tracking with a language model using schema-driven prompting",
    "citation_count": 133,
    "authors": [
      "Chia-Hsuan Lee",
      "Hao Cheng",
      "Mari Ostendorf"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.405": {
    "title": "Signed Coreference Resolution",
    "volume": "main",
    "abstract": "Coreference resolution is key to many natural language processing tasks and yet has been relatively unexplored in Sign Language Processing. In signed languages, space is primarily used to establish reference. Solving coreference resolution for signed languages would not only enable higher-level Sign Language Processing systems, but also enhance our understanding of language in different modalities and of situated references, which are key problems in studying grounded language. In this paper, we: (1) introduce Signed Coreference Resolution (SCR), a new challenge for coreference modeling and Sign Language Processing; (2) collect an annotated corpus of German Sign Language with gold labels for coreference together with an annotation software for the task; (3) explore features of hand gesture, iconicity, and spatial situated properties and move forward to propose a set of linguistically informed heuristics and unsupervised models for the task; (4) put forward several proposals about ways to address the complexities of this challenge effectively",
    "checked": true,
    "id": "4904db69abdff71cefa8696a80b8043e7554a3b2",
    "semantic_title": "signed coreference resolution",
    "citation_count": 3,
    "authors": [
      "Kayo Yin",
      "Kenneth DeHaan",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.406": {
    "title": "Consistent Accelerated Inference via Confident Adaptive Transformers",
    "volume": "main",
    "abstract": "We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs – Confident Adaptive Transformers – in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks",
    "checked": true,
    "id": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d",
    "semantic_title": "consistent accelerated inference via confident adaptive transformers",
    "citation_count": 74,
    "authors": [
      "Tal Schuster",
      "Adam Fisch",
      "Tommi Jaakkola",
      "Regina Barzilay"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.407": {
    "title": "Improving and Simplifying Pattern Exploiting Training",
    "volume": "main",
    "abstract": "Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few-shot learning without any unlabeled data and introduce ADAPET, which modifies PET's objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data",
    "checked": true,
    "id": "e812919d2cd818e7262f01b32dc5e630fc825af1",
    "semantic_title": "improving and simplifying pattern exploiting training",
    "citation_count": 151,
    "authors": [
      "Derek Tam",
      "Rakesh R. Menon",
      "Mohit Bansal",
      "Shashank Srivastava",
      "Colin Raffel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.408": {
    "title": "Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data",
    "volume": "main",
    "abstract": "Unsupervised Data Augmentation (UDA) is a semisupervised technique that applies a consistency loss to penalize differences between a model's predictions on (a) observed (unlabeled) examples; and (b) corresponding ‘noised' examples produced via data augmentation. While UDA has gained popularity for text classification, open questions linger over which design decisions are necessary and how to extend the method to sequence labeling tasks. In this paper, we re-examine UDA and demonstrate its efficacy on several sequential tasks. Our main contribution is an empirical study of UDA to establish which components of the algorithm confer benefits in NLP. Notably, although prior work has emphasized the use of clever augmentation techniques including back-translation, we find that enforcing consistency between predictions assigned to observed and randomly substituted words often yields comparable (or greater) benefits compared to these more complex perturbation models. Furthermore, we find that applying UDA's consistency loss affords meaningful gains without any unlabeled data at all, i.e., in a standard supervised setting. In short, UDA need not be unsupervised to realize much of its noted benefits, and does not require complex data augmentation to be effective",
    "checked": true,
    "id": "80b747af8d86541cf53198519c8fa51109eed4f9",
    "semantic_title": "unsupervised data augmentation with naive augmentation and without unlabeled data",
    "citation_count": 24,
    "authors": [
      "David Lowell",
      "Brian Howard",
      "Zachary C. Lipton",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.409": {
    "title": "Pre-train or Annotate? Domain Adaptation with a Constrained Budget",
    "volume": "main",
    "abstract": "Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain language models. Then we evaluate the utility of different combinations of pre-training and data annotation under varying budget constraints to assess which combination strategy works best. We find that, for small budgets, spending all funds on annotation leads to the best performance; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally. We therefore suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain",
    "checked": true,
    "id": "48055cf6bf89b89f87e904a45e084d03a97e8170",
    "semantic_title": "pre-train or annotate? domain adaptation with a constrained budget",
    "citation_count": 32,
    "authors": [
      "Fan Bai",
      "Alan Ritter",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.410": {
    "title": "Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources",
    "volume": "main",
    "abstract": "Warning: this paper contains content that may be offensive or upsetting. Commonsense knowledge bases (CSKB) are increasingly used for various natural language processing tasks. Since CSKBs are mostly human-generated and may reflect societal biases, it is important to ensure that such biases are not conflated with the notion of commonsense. Here we focus on two widely used CSKBs, ConceptNet and GenericsKB, and establish the presence of bias in the form of two types of representational harms, overgeneralization of polarized perceptions and representation disparity across different demographic groups in both CSKBs. Next, we find similar representational harms for downstream models that use ConceptNet. Finally, we propose a filtering-based approach for mitigating such harms, and observe that our filtered-based approach can reduce the issues in both resources and models but leads to a performance drop, leaving room for future work to build fairer and stronger commonsense models",
    "checked": true,
    "id": "26b7eacd6aaff6c2bd1beac40b96597fb1d29a1e",
    "semantic_title": "lawyers are dishonest? quantifying representational harms in commonsense knowledge resources",
    "citation_count": 45,
    "authors": [
      "Ninareh Mehrabi",
      "Pei Zhou",
      "Fred Morstatter",
      "Jay Pujara",
      "Xiang Ren",
      "Aram Galstyan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.411": {
    "title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings",
    "volume": "main",
    "abstract": "Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated",
    "checked": true,
    "id": "09fba948316e04f0e1641926948b67b0799c8e0d",
    "semantic_title": "oscar: orthogonal subspace correction and rectification of biases in word embeddings",
    "citation_count": 56,
    "authors": [
      "Sunipa Dev",
      "Tao Li",
      "Jeff M Phillips",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.412": {
    "title": "Sentence-Permuted Paragraph Generation",
    "volume": "main",
    "abstract": "Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-to-right sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses hierarchical positional embedding and designs new procedures for training, and decoding in the sentence-permuted generation. Experiments on three paragraph generation benchmarks demonstrate PermGen generates more diverse outputs with a higher quality than existing models",
    "checked": true,
    "id": "1e12d7ca867e876b58544e1223cf43c98777a79a",
    "semantic_title": "sentence-permuted paragraph generation",
    "citation_count": 11,
    "authors": [
      "Wenhao Yu",
      "Chenguang Zhu",
      "Tong Zhao",
      "Zhichun Guo",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.413": {
    "title": "Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation",
    "volume": "main",
    "abstract": "Prior studies on text-to-text generation typically assume that the model could figure out what to attend to in the input and what to include in the output via seq2seq learning, with only the parallel training data and no additional guidance. However, it remains unclear whether current models can preserve important concepts in the source input, as seq2seq learning does not have explicit focus on the concepts and commonly used evaluation metrics also treat them equally important as other tokens. In this paper, we present a systematic analysis that studies whether current seq2seq models, especially pre-trained language models, are good enough for preserving important input concepts and to what extent explicitly guiding generation with the concepts as lexical constraints is beneficial. We answer the above questions by conducting extensive analytical experiments on four representative text-to-text generation tasks. Based on the observations, we then propose a simple yet effective framework to automatically extract, denoise, and enforce important input concepts as lexical constraints. This new method performs comparably or better than its unconstrained counterpart on automatic metrics, demonstrates higher coverage for concept preservation, and receives better ratings in the human evaluation. Our code is available at https://github.com/morningmoni/EDE",
    "checked": true,
    "id": "d6c7f34b9b8e22b4547b0e01b95fbfea58858557",
    "semantic_title": "extract, denoise and enforce: evaluating and improving concept preservation for text-to-text generation",
    "citation_count": 4,
    "authors": [
      "Yuning Mao",
      "Wenchang Ma",
      "Deren Lei",
      "Jiawei Han",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.414": {
    "title": "Paraphrase Generation: A Survey of the State of the Art",
    "volume": "main",
    "abstract": "This paper focuses on paraphrase generation,which is a widely studied natural language generation task in NLP. With the development of neural models, paraphrase generation research has exhibited a gradual shift to neural methods in the recent years. This has provided architectures for contextualized representation of an input text and generating fluent, diverseand human-like paraphrases. This paper surveys various approaches to paraphrase generation with a main focus on neural methods",
    "checked": true,
    "id": "91bd9810dd34a7e08d450c05c83dc26ccd0df503",
    "semantic_title": "paraphrase generation: a survey of the state of the art",
    "citation_count": 74,
    "authors": [
      "Jianing Zhou",
      "Suma Bhat"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.415": {
    "title": "Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?",
    "volume": "main",
    "abstract": "Exposure bias has been regarded as a central problem for auto-regressive language models (LM). It claims that teacher forcing would cause the test-time generation to be incrementally distorted due to the training-generation discrepancy. Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is. In this work, we focus on the task of open-ended language generation, propose metrics to quantify the impact of exposure bias in the aspects of quality, diversity, and consistency. Our key intuition is that if we feed ground-truth data prefixes (instead of prefixes generated by the model itself) into the model and ask it to continue the generation, the performance should become much better because the training-generation discrepancy in the prefix is removed. Both automatic and human evaluations are conducted in our experiments. On the contrary to the popular belief in exposure bias, we find that the the distortion induced by the prefix discrepancy is limited, and does not seem to be incremental during the generation. Moreover, our analysis reveals an interesting self-recovery ability of the LM, which we hypothesize to be countering the harmful effects from exposure bias",
    "checked": true,
    "id": "cb51b53d5d26537ed6d458f83b81afe0f2efa289",
    "semantic_title": "exposure bias versus self-recovery: are distortions really incremental for autoregressive text generation?",
    "citation_count": 34,
    "authors": [
      "Tianxing He",
      "Jingzhao Zhang",
      "Zhiming Zhou",
      "James Glass"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.416": {
    "title": "Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning",
    "volume": "main",
    "abstract": "Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of news articles with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate exposure bias, a common problem in natural language generation. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy",
    "checked": true,
    "id": "ec9a9ad39aa393be5330ed40b27bc428a5f84f0e",
    "semantic_title": "generating self-contained and summary-centric question answer pairs via differentiable reward imitation learning",
    "citation_count": 2,
    "authors": [
      "Li Zhou",
      "Kevin Small",
      "Yong Zhang",
      "Sandeep Atluri"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.417": {
    "title": "Unsupervised Paraphrasing with Pretrained Language Models",
    "volume": "main",
    "abstract": "Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a surface form dissimilar from the input, whenever the language model emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our model transfers to paraphrasing in other languages without any additional finetuning",
    "checked": true,
    "id": "ff5836af731bd06ea2d70186a96a3e51287d5840",
    "semantic_title": "unsupervised paraphrasing with pretrained language models",
    "citation_count": 31,
    "authors": [
      "Tong Niu",
      "Semih Yavuz",
      "Yingbo Zhou",
      "Nitish Shirish Keskar",
      "Huan Wang",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.418": {
    "title": "Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness",
    "volume": "main",
    "abstract": "Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output profanity. These outputs may potentially hurt the usability of seq2seq models and make the end-users feel offended. To address this problem, we propose a training framework with certified robustness to eliminate the causes that trigger the generation of profanity. The proposed training framework leverages merely a short list of profanity examples to prevent seq2seq models from generating a broader spectrum of profanity. The framework is composed of a pattern-eliminating training component to suppress the impact of language patterns with profanity in the training set, and a trigger-resisting training component to provide certified robustness for seq2seq models against intentionally injected profanity-triggering expressions in test samples. In the experiments, we consider two representative NLP tasks that seq2seq can be applied to, i.e., style transfer and dialogue generation. Extensive experimental results show that the proposed training framework can successfully prevent the NLP models from generating profanity",
    "checked": true,
    "id": "160299a9847bca49e2dc200ac54df4ba03b2049b",
    "semantic_title": "profanity-avoiding training framework for seq2seq models with certified robustness",
    "citation_count": 0,
    "authors": [
      "Hengtong Zhang",
      "Tianhang Zheng",
      "Yaliang Li",
      "Jing Gao",
      "Lu Su",
      "Bo Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.419": {
    "title": "Journalistic Guidelines Aware News Image Captioning",
    "volume": "main",
    "abstract": "The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this task, motivated by caption guidelines that journalists follow. Our approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC), leverages the structure of captions to improve the generation quality and guide our representation design. Experimental results, including detailed ablation studies, on two large-scale publicly available datasets show that JoGANIC substantially outperforms state-of-the-art methods both on caption generation and named entity related metrics",
    "checked": true,
    "id": "9b10c49575c5aa07b5076d2f5b27c67b68b09c9b",
    "semantic_title": "journalistic guidelines aware news image captioning",
    "citation_count": 29,
    "authors": [
      "Xuewen Yang",
      "Svebor Karaman",
      "Joel Tetreault",
      "Alejandro Jaimes"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.420": {
    "title": "AESOP: Paraphrase Generation with Adaptive Syntactic Control",
    "volume": "main",
    "abstract": "We propose to control paraphrase generation through carefully chosen target syntactic structures to generate more proper and higher quality paraphrases. Our model, AESOP, leverages a pretrained language model and adds deliberately chosen syntactical control via a retrieval-based selection module to generate fluent paraphrases. Experiments show that AESOP achieves state-of-the-art performances on semantic preservation and syntactic conformation on two benchmark datasets with ground-truth syntactic control from human-annotated exemplars. Moreover, with the retrieval-based target syntax selection module, AESOP generates paraphrases with even better qualities than the current best model using human-annotated target syntactic parses according to human evaluation. We further demonstrate the effectiveness of AESOP to improve classification models' robustness to syntactic perturbation by data augmentation on two GLUE tasks",
    "checked": true,
    "id": "64dcf1aad8233c2ea615d1e12d1f93928183d8ed",
    "semantic_title": "aesop: paraphrase generation with adaptive syntactic control",
    "citation_count": 46,
    "authors": [
      "Jiao Sun",
      "Xuezhe Ma",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.421": {
    "title": "Refocusing on Relevance: Personalization in NLG",
    "volume": "main",
    "abstract": "Many NLG tasks such as summarization, dialogue response, or open domain question answering, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user's intent or context of work is not easily recoverable based solely on that source text– a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional context, and suggest that relevance (as used in Information Retrieval) be thought of as a crucial tool for designing user-oriented text-generating tasks. We further discuss possible harms and hazards around such personalization, and argue that value-sensitive design represents a crucial path forward through these challenges",
    "checked": true,
    "id": "4d2ed16ce4654132e6968fb979b0919fa11f2378",
    "semantic_title": "refocusing on relevance: personalization in nlg",
    "citation_count": 27,
    "authors": [
      "Shiran Dudy",
      "Steven Bedrick",
      "Bonnie Webber"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.422": {
    "title": "The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction",
    "volume": "main",
    "abstract": "Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between events via arguments and argument relations. We introduce a new concept of Temporal Complex Event Schema: a graph-based schema representation that encompasses events, arguments, temporal connections and argument relations. In addition, we propose a Temporal Event Graph Model that predicts event instances following the temporal complex event schema. To build and evaluate such schemas, we release a new schema learning corpus containing 6,399 documents accompanied with event graphs, and we have manually constructed gold-standard schemas. Intrinsic evaluations by schema matching and instance graph perplexity, prove the superior quality of our probabilistic graph schema library compared to linear representations. Extrinsic evaluation on schema-guided future event prediction further demonstrates the predictive power of our event graph model, significantly outperforming human schemas and baselines by more than 17.8% on HITS@1",
    "checked": true,
    "id": "6dc8adf2318fcd0843aac1fe23c4626f937dbb07",
    "semantic_title": "the future is not one-dimensional: complex event schema induction by graph modeling for event prediction",
    "citation_count": 58,
    "authors": [
      "Manling Li",
      "Sha Li",
      "Zhenhailong Wang",
      "Lifu Huang",
      "Kyunghyun Cho",
      "Heng Ji",
      "Jiawei Han",
      "Clare Voss"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.423": {
    "title": "Learning Constraints and Descriptive Segmentation for Subevent Detection",
    "volume": "main",
    "abstract": "Event mentions in text correspond to real-world events of varying degrees of granularity. The task of subevent detection aims to resolve this granularity issue, recognizing the membership of multi-granular events in event complexes. Since knowing the span of descriptive contexts of event complexes helps infer the membership of events, we propose the task of event-based text segmentation (EventSeg) as an auxiliary task to improve the learning for subevent detection. To bridge the two tasks together, we propose an approach to learning and enforcing constraints that capture dependencies between subevent detection and EventSeg prediction, as well as guiding the model to make globally consistent inference. Specifically, we adopt Rectifier Networks for constraint learning and then convert the learned constraints to a regularization term in the loss function of the neural model. Experimental results show that the proposed method outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for subevent detection, HiEve and IC, respectively, while achieving a decent performance on EventSeg prediction",
    "checked": true,
    "id": "023c5782335f853e4d4b54f0ac26eb7eccf2702a",
    "semantic_title": "learning constraints and descriptive segmentation for subevent detection",
    "citation_count": 23,
    "authors": [
      "Haoyu Wang",
      "Hongming Zhang",
      "Muhao Chen",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.424": {
    "title": "ChemNER: Fine-Grained Chemistry Named Entity Recognition with Ontology-Guided Distant Supervision",
    "volume": "main",
    "abstract": "Scientific literature analysis needs fine-grained named entity recognition (NER) to provide a wide range of information for scientific discovery. For example, chemistry research needs to study dozens to hundreds of distinct, fine-grained entity types, making consistent and accurate annotation difficult even for crowds of domain experts. On the other hand, domain-specific ontologies and knowledge bases (KBs) can be easily accessed, constructed, or integrated, which makes distant supervision realistic for fine-grained chemistry NER. In distant supervision, training labels are generated by matching mentions in a document with the concepts in the knowledge bases (KBs). However, this kind of KB-matching suffers from two major challenges: incomplete annotation and noisy annotation. We propose ChemNER, an ontology-guided, distantly-supervised method for fine-grained chemistry NER to tackle these challenges. It leverages the chemistry type ontology structure to generate distant labels with novel methods of flexible KB-matching and ontology-guided multi-type disambiguation. It significantly improves the distant label generation for the subsequent sequence labeling model training. We also provide an expert-labeled, chemistry NER dataset with 62 fine-grained chemistry types (e.g., chemical compounds and chemical reactions). Experimental results show that ChemNER is highly effective, outperforming substantially the state-of-the-art NER methods (with .25 absolute F1 score improvement)",
    "checked": true,
    "id": "f6cb7363464ee8f15183fbb93f768ff2d78b0f8e",
    "semantic_title": "chemner: fine-grained chemistry named entity recognition with ontology-guided distant supervision",
    "citation_count": 34,
    "authors": [
      "Xuan Wang",
      "Vivian Hu",
      "Xiangchen Song",
      "Shweta Garg",
      "Jinfeng Xiao",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.425": {
    "title": "Moving on from OntoNotes: Coreference Resolution Model Transfer",
    "volume": "main",
    "abstract": "Academic neural models for coreference resolution (coref) are typically trained on a single dataset, OntoNotes, and model improvements are benchmarked on that same dataset. However, real-world applications of coref depend on the annotation guidelines and the domain of the target dataset, which often differ from those of OntoNotes. We aim to quantify transferability of coref models based on the number of annotated documents available in the target dataset. We examine eleven target datasets and find that continued training is consistently effective and especially beneficial when there are few target documents. We establish new benchmarks across several datasets, including state-of-the-art results on PreCo",
    "checked": true,
    "id": "c1ce2e7ce538116df81752a432bfabe860e6e89a",
    "semantic_title": "moving on from ontonotes: coreference resolution model transfer",
    "citation_count": 30,
    "authors": [
      "Patrick Xia",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.426": {
    "title": "Document-level Entity-based Extraction as Template Generation",
    "volume": "main",
    "abstract": "Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture cross-entity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TopK Copy, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%), binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score",
    "checked": true,
    "id": "6f7e3a6ecc8464b9edb54b863b8a745d3b4b5766",
    "semantic_title": "document-level entity-based extraction as template generation",
    "citation_count": 54,
    "authors": [
      "Kung-Hsiang Huang",
      "Sam Tang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.427": {
    "title": "Learning Prototype Representations Across Few-Shot Tasks for Event Detection",
    "volume": "main",
    "abstract": "We address the sampling bias and outlier issues in few-shot learning for event detection, a subtask of information extraction. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among classifiers across tasks to make the model more robust to outliers. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our model is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact",
    "checked": true,
    "id": "275029fb2d74af9e47a3cccd94be2ecb7c42bdba",
    "semantic_title": "learning prototype representations across few-shot tasks for event detection",
    "citation_count": 24,
    "authors": [
      "Viet Lai",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.428": {
    "title": "Lifelong Event Detection with Knowledge Transfer",
    "volume": "main",
    "abstract": "Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from unstructured data, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or semantic relatedness exist among hierarchical knowledge element types. In our proposed framework, knowledge is being transferred between learned old event types and new event types. Specifically, we update old knowledge with new event types' mentions using a self-training loss. In addition, we aggregate old event types' representations based on their similarities with new event types to initialize the new event types' representations. Experimental results show that our framework outperforms competitive baselines with a 5.1% absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30% absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel knowledge acquisition",
    "checked": true,
    "id": "83cd70c87825e5295ed3dd6eeb57add7ccd82dce",
    "semantic_title": "lifelong event detection with knowledge transfer",
    "citation_count": 33,
    "authors": [
      "Pengfei Yu",
      "Heng Ji",
      "Prem Natarajan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.429": {
    "title": "Modular Self-Supervision for Document-Level Relation Extraction",
    "volume": "main",
    "abstract": "Extracting relations across large text spans has been relatively underexplored in NLP, but it is particularly important for high-value domains such as biomedicine, where obtaining high recall of the latest findings is crucial for practical applications. Compared to conventional information extraction confined to short text spans, document-level relation extraction faces additional challenges in both inference and learning. Given longer text spans, state-of-the-art neural architectures are less effective and task-specific self-supervision such as distant supervision becomes very noisy. In this paper, we propose decomposing document-level relation extraction into relation detection and argument resolution, taking inspiration from Davidsonian semantics. This enables us to incorporate explicit discourse modeling and leverage modular self-supervision for each sub-problem, which is less noise-prone and can be further refined end-to-end via variational EM. We conduct a thorough evaluation in biomedical machine reading for precision oncology, where cross-paragraph relation mentions are prevalent. Our method outperforms prior state of the art, such as multi-scale learning and graph neural networks, by over 20 absolute F1 points. The gain is particularly pronounced among the most challenging relation instances whose arguments never co-occur in a paragraph",
    "checked": true,
    "id": "329b8ba1d128be952f97303953da6878ea61a6ec",
    "semantic_title": "modular self-supervision for document-level relation extraction",
    "citation_count": 6,
    "authors": [
      "Sheng Zhang",
      "Cliff Wong",
      "Naoto Usuyama",
      "Sarthak Jain",
      "Tristan Naumann",
      "Hoifung Poon"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.430": {
    "title": "Unsupervised Paraphrasing Consistency Training for Low Resource Named Entity Recognition",
    "volume": "main",
    "abstract": "Unsupervised consistency training is a way of semi-supervised learning that encourages consistency in model predictions between the original and augmented data. For Named Entity Recognition (NER), existing approaches augment the input sequence with token replacement, assuming annotations on the replaced positions unchanged. In this paper, we explore the use of paraphrasing as a more principled data augmentation scheme for NER unsupervised consistency training. Specifically, we convert Conditional Random Field (CRF) into a multi-label classification module and encourage consistency on the entity appearance between the original and paraphrased sequences. Experiments show that our method is especially effective when annotations are limited",
    "checked": true,
    "id": "e8599d2de6274e13805af951c5fa1e14430a4faa",
    "semantic_title": "unsupervised paraphrasing consistency training for low resource named entity recognition",
    "citation_count": 15,
    "authors": [
      "Rui Wang",
      "Ricardo Henao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.431": {
    "title": "Fine-grained Entity Typing without Knowledge Base",
    "volume": "main",
    "abstract": "Existing work on Fine-grained Entity Typing (FET) typically trains automatic models on the datasets obtained by using Knowledge Bases (KB) as distant supervision. However, the reliance on KB means this training setting can be hampered by the lack of or the incompleteness of the KB. To alleviate this limitation, we propose a novel setting for training FET models: FET without accessing any knowledge base. Under this setting, we propose a two-step framework to train FET models. In the first step, we automatically create pseudo data with fine-grained labels from a large unlabeled dataset. Then a neural network model is trained based on the pseudo data, either in an unsupervised way or using self-training under the weak guidance from a coarse-grained Named Entity Recognition (NER) model. Experimental results show that our method achieves competitive performance with respect to the models trained on the original KB-supervised datasets",
    "checked": true,
    "id": "4f763f9290ac6a2874e11092fbbb0218745f80d2",
    "semantic_title": "fine-grained entity typing without knowledge base",
    "citation_count": 8,
    "authors": [
      "Jing Qian",
      "Yibin Liu",
      "Lemao Liu",
      "Yangming Li",
      "Haiyun Jiang",
      "Haisong Zhang",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.432": {
    "title": "Adversarial Attack against Cross-lingual Knowledge Graph Alignment",
    "volume": "main",
    "abstract": "Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities in dense regions in two KGs, such that the derived perturbations are unnoticeable. Second, an attack signal amplification method is developed to reduce the gradient vanishing issues in the process of adversarial attacks for further improving the attack effectiveness",
    "checked": true,
    "id": "03f62e822e7095bc69168c85a0bab583ac23e1d3",
    "semantic_title": "adversarial attack against cross-lingual knowledge graph alignment",
    "citation_count": 12,
    "authors": [
      "Zeru Zhang",
      "Zijie Zhang",
      "Yang Zhou",
      "Lingfei Wu",
      "Sixing Wu",
      "Xiaoying Han",
      "Dejing Dou",
      "Tianshi Che",
      "Da Yan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.433": {
    "title": "Towards Realistic Few-Shot Relation Extraction",
    "volume": "main",
    "abstract": "In recent years, few-shot models have been applied successfully to a variety of NLP tasks. Han et al. (2018) introduced a few-shot learning framework for relation classification, and since then, several models have surpassed human performance on this task, leading to the impression that few-shot relation classification is solved. In this paper we take a deeper look at the efficacy of strong few-shot classification models in the more common relation extraction setting, and show that typical few-shot evaluation metrics obscure a wide variability in performance across relations. In particular, we find that state of the art few-shot relation classification models overly rely on entity type information, and propose modifications to the training routine to encourage models to better discriminate between relations involving similar entity types",
    "checked": true,
    "id": "a89c6d59b0efaa7f57192cdc78ebea6aa0e361f6",
    "semantic_title": "towards realistic few-shot relation extraction",
    "citation_count": 18,
    "authors": [
      "Sam Brody",
      "Sichao Wu",
      "Adrian Benton"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.434": {
    "title": "Data Augmentation for Cross-Domain Named Entity Recognition",
    "volume": "main",
    "abstract": "Current work in named entity recognition (NER) shows that data augmentation techniques can produce more robust models. However, most existing techniques focus on augmenting in-domain data in low-resource scenarios where annotated data is quite limited. In this work, we take this research direction to the opposite and study cross-domain data augmentation for the NER task. We investigate the possibility of leveraging data from high-resource domains by projecting it into the low-resource domains. Specifically, we propose a novel neural architecture to transform the data representation from a high-resource to a low-resource domain by learning the patterns (e.g. style, noise, abbreviations, etc.) in the text that differentiate them and a shared feature space where both domains are aligned. We experiment with diverse datasets and show that transforming the data to the low-resource domain representation achieves significant improvements over only using data from high-resource domains",
    "checked": true,
    "id": "c3b439fa8d6f52b762cda47e31c051b76437b126",
    "semantic_title": "data augmentation for cross-domain named entity recognition",
    "citation_count": 49,
    "authors": [
      "Shuguang Chen",
      "Gustavo Aguilar",
      "Leonardo Neves",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.435": {
    "title": "Incorporating medical knowledge in BERT for clinical relation extraction",
    "volume": "main",
    "abstract": "In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., Wikipedia versus clinic notes), these models may not be ideal for domain-specific tasks (e.g., extracting clinical relations). Furthermore, it may require additional medical knowledge to understand clinical text properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add medical knowledge into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2/VA 2010 clinical relation extraction dataset",
    "checked": true,
    "id": "f88dff39e5b10e4d005b87060bfee58df241d4a2",
    "semantic_title": "incorporating medical knowledge in bert for clinical relation extraction",
    "citation_count": 63,
    "authors": [
      "Arpita Roy",
      "Shimei Pan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.436": {
    "title": "ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning",
    "volume": "main",
    "abstract": "While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This **E**ffective **CON**tinual pre-training framework for **E**vent **T**emporal reasoning (ECONET) improves the PTLMs' fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks",
    "checked": true,
    "id": "6eee69031d2e11aa03a5a8fcb219cff4562863be",
    "semantic_title": "econet: effective continual pretraining of language models for event temporal reasoning",
    "citation_count": 57,
    "authors": [
      "Rujun Han",
      "Xiang Ren",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.437": {
    "title": "Learning from Noisy Labels for Entity-Centric Information Extraction",
    "volume": "main",
    "abstract": "Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with identical structures but different parameter initialization. These models are jointly optimized with the task-specific losses and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework. We release our code to the community for future research",
    "checked": true,
    "id": "dbfc17833434243e07c4629e58f3d8ed7112dbfe",
    "semantic_title": "learning from noisy labels for entity-centric information extraction",
    "citation_count": 66,
    "authors": [
      "Wenxuan Zhou",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.438": {
    "title": "Extracting Material Property Measurement Data from Scientific Articles",
    "volume": "main",
    "abstract": "Machine learning-based prediction of material properties is often hampered by the lack of sufficiently large training data sets. The majority of such measurement data is embedded in scientific literature and the ability to automatically extract these data is essential to support the development of reliable property prediction methods. In this work, we describe a methodology for developing an automatic property extraction framework using material solubility as the target property. We create a training and evaluation data set containing tags for solubility-related entities using a combination of regular expressions and manual tagging. We then compare five entity recognition models leveraging both token-level and span-level architectures on the task of classifying solute names, solubility values, and solubility units. Additionally, we explore a novel pretraining approach that leverages automated chemical name and quantity extraction tools to generate large datasets that do not rely on intensive manual tagging. Finally, we perform an analysis to identify the causes of classification errors",
    "checked": true,
    "id": "fd651b69b5683c79b368c43df4446917a8405946",
    "semantic_title": "extracting material property measurement data from scientific articles",
    "citation_count": 6,
    "authors": [
      "Gihan Panapitiya",
      "Fred Parks",
      "Jonathan Sepulveda",
      "Emily Saldanha"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.439": {
    "title": "Modeling Document-Level Context for Event Detection via Important Context Selection",
    "volume": "main",
    "abstract": "The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue, we propose a novel method to model document-level context for ED that dynamically selects relevant sentences in the document for the event prediction of the target sentence. The target sentence will be then augmented with the selected sentences and consumed entirely by transformer-based language models for improved representation learning for ED. To this end, the REINFORCE algorithm is employed to train the relevant sentence selection for ED. Several information types are then introduced to form the reward function for the training process, including ED performance, sentence similarity, and discourse relations. Our extensive experiments on multiple benchmark datasets reveal the effectiveness of the proposed model, leading to new state-of-the-art performance",
    "checked": true,
    "id": "6ef94216e9845b0031cdbd81084e613acd673578",
    "semantic_title": "modeling document-level context for event detection via important context selection",
    "citation_count": 26,
    "authors": [
      "Amir Pouran Ben Veyseh",
      "Minh Van Nguyen",
      "Nghia Ngo Trung",
      "Bonan Min",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.440": {
    "title": "Crosslingual Transfer Learning for Relation and Event Extraction via Word Category and Class Alignments",
    "volume": "main",
    "abstract": "Previous work on crosslingual Relation and Event Extraction (REE) suffers from the monolingual bias issue due to the training of models on only the source language data. An approach to overcome this issue is to use unlabeled data in the target language to aid the alignment of crosslingual representations, i.e., via fooling a language discriminator. However, as this approach does not condition on class information, a target language example of a class could be incorrectly aligned to a source language example of a different class. To address this issue, we propose a novel crosslingual alignment method that leverages class information of REE tasks for representation learning. In particular, we propose to learn two versions of representation vectors for each class in an REE task based on either source or target language examples. Representation vectors for corresponding classes will then be aligned to achieve class-aware alignment for crosslingual representations. In addition, we propose to further align representation vectors for language-universal word categories (i.e., parts of speech and dependency relations). As such, a novel filtering mechanism is presented to facilitate the learning of word category representations from contextualized representations on input texts based on adversarial learning. We conduct extensive crosslingual experiments with English, Chinese, and Arabic over REE tasks. The results demonstrate the benefits of the proposed method that significantly advances the state-of-the-art performance in these settings",
    "checked": true,
    "id": "74e7cf6c0c0070e7555d0cd46098bae5c19c44ad",
    "semantic_title": "crosslingual transfer learning for relation and event extraction via word category and class alignments",
    "citation_count": 26,
    "authors": [
      "Minh Van Nguyen",
      "Tuan Ngo Nguyen",
      "Bonan Min",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.441": {
    "title": "Corpus-based Open-Domain Event Type Induction",
    "volume": "main",
    "abstract": "Traditional event extraction methods require predefined event types and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that automatically discovers a set of event types from a given corpus. As events of the same type could be expressed in multiple ways, we propose to represent each event type as a cluster of <predicate sense, object head> pairs. Specifically, our method (1) selects salient predicates and object heads, (2) disambiguates predicate senses using only a verb sense dictionary, and (3) obtains event types by jointly embedding and clustering <predicate sense, object head> pairs in a latent spherical space. Our experiments, on three datasets from different domains, show our method can discover salient and high-quality event types, according to both automatic and human evaluations",
    "checked": true,
    "id": "c23b13179711e524b5ed53b7fc0a60c2567db897",
    "semantic_title": "corpus-based open-domain event type induction",
    "citation_count": 30,
    "authors": [
      "Jiaming Shen",
      "Yunyi Zhang",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.442": {
    "title": "PDALN: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition",
    "volume": "main",
    "abstract": "Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach – PDALN. It achieves superior domain adaptability by employing three components: (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously; (2) Multi-level Domain invariant features, derived from a multi-grained MMD (Maximum Mean Discrepancy) approach, to enable knowledge transfer across domains; (3) Advanced KD schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that PDALN can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other baselines indicates the state-of-the-art performance of PDALN",
    "checked": true,
    "id": "45efb5974a3b9288f5bbc5076a700d22df5e380c",
    "semantic_title": "pdaln: progressive domain adaptation over a pre-trained model for low-resource cross-domain named entity recognition",
    "citation_count": 23,
    "authors": [
      "Tao Zhang",
      "Congying Xia",
      "Philip S. Yu",
      "Zhiwei Liu",
      "Shu Zhao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.443": {
    "title": "Multi-Vector Attention Models for Deep Re-ranking",
    "volume": "main",
    "abstract": "Large-scale document retrieval systems often utilize two styles of neural network models which live at two different ends of the joint computation vs. accuracy spectrum. The first style is dual encoder (or two-tower) models, where the query and document representations are computed completely independently and combined with a simple dot product operation. The second style is cross-attention models, where the query and document features are concatenated in the input layer and all computation is based on the joint query-document representation. Dual encoder models are typically used for retrieval and deep re-ranking, while cross-attention models are typically used for shallow re-ranking. In this paper, we present a lightweight architecture that explores this joint cost vs. accuracy trade-off based on multi-vector attention (MVA). We thoroughly evaluate our method on the MS-MARCO passage retrieval dataset and show how to efficiently trade off retrieval accuracy with joint computation and offline document storage cost. We show that a highly compressed document representation and inexpensive joint computation can be achieved through a combination of learned pooling tokens and aggressive downprojection. Our code and model checkpoints are open-source and available on GitHub",
    "checked": true,
    "id": "b62ecfb91a5773790daab8d8981957561183d113",
    "semantic_title": "multi-vector attention models for deep re-ranking",
    "citation_count": 11,
    "authors": [
      "Giulio Zhou",
      "Jacob Devlin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.444": {
    "title": "Toward Deconfounding the Effect of Entity Demographics for Question Answering Accuracy",
    "volume": "main",
    "abstract": "The goal of question answering (QA) is to answer _any_ question. However, major QA datasets have skewed distributions over gender, profession, and nationality. Despite that skew, an analysis of model accuracy reveals little evidence that accuracy is lower for people based on gender or nationality; instead, there is more variation on professions (question topic) and question ambiguity. But QA's lack of representation could itself hide evidence of bias, necessitating QA datasets that better represent global diversity",
    "checked": true,
    "id": "574589d8359879aaa53afb71d87ca3931bd00c81",
    "semantic_title": "toward deconfounding the effect of entity demographics for question answering accuracy",
    "citation_count": 10,
    "authors": [
      "Maharshi Gor",
      "Kellie Webster",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.445": {
    "title": "Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models",
    "volume": "main",
    "abstract": "Commonsense reasoning benchmarks have been largely solved by fine-tuning language models. The downside is that fine-tuning may cause models to overfit to task-specific data and thereby forget their knowledge gained during pre-training. Recent works only propose lightweight model updates as models may already possess useful knowledge from past experience, but a challenge remains in understanding what parts and to what extent models should be refined for a given task. In this paper, we investigate what models learn from commonsense reasoning datasets. We measure the impact of three different adaptation methods on the generalization and accuracy of models. Our experiments with two models show that fine-tuning performs best, by learning both the content and the structure of the task, but suffers from overfitting and limited generalization to novel answers. We observe that alternative adaptation methods like prefix-tuning have comparable accuracy, but generalize better to unseen answers and are more robust to adversarial splits",
    "checked": true,
    "id": "011dd2058a9e38ac91b1790377fdd7d0f99d2781",
    "semantic_title": "exploring strategies for generalizable commonsense reasoning with pre-trained models",
    "citation_count": 17,
    "authors": [
      "Kaixin Ma",
      "Filip Ilievski",
      "Jonathan Francis",
      "Satoru Ozaki",
      "Eric Nyberg",
      "Alessandro Oltramari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.446": {
    "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
    "volume": "main",
    "abstract": "Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution",
    "checked": true,
    "id": "4a54d58a4b20e4f3af25cea3c188a12082a95e02",
    "semantic_title": "transformer feed-forward layers are key-value memories",
    "citation_count": 891,
    "authors": [
      "Mor Geva",
      "Roei Schuster",
      "Jonathan Berant",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.447": {
    "title": "Connecting Attributions and QA Model Behavior on Realistic Counterfactuals",
    "volume": "main",
    "abstract": "When a model attribution technique highlights a particular part of the input, a user might understand this highlight as making a statement about counterfactuals (Miller, 2019): if that part of the input were to change, the model's prediction might change as well. This paper investigates how well different attribution techniques align with this assumption on realistic counterfactuals in the case of reading comprehension (RC). RC is a particularly challenging test case, as token-level attributions that have been extensively studied in other NLP tasks such as sentiment analysis are less suitable to represent the reasoning that RC models perform. We construct counterfactual sets for three different RC settings, and through heuristics that can connect attribution methods' outputs to high-level model behavior, we can evaluate how useful different attribution methods and even different formats are for understanding counterfactuals. We find that pairwise attributions are better suited to RC than token-level attributions across these different RC settings, with our best performance coming from a modification that we propose to an existing pairwise attribution method",
    "checked": true,
    "id": "2a456fd1d47a396feef9a1a2cf140a71bbc78ad4",
    "semantic_title": "connecting attributions and qa model behavior on realistic counterfactuals",
    "citation_count": 24,
    "authors": [
      "Xi Ye",
      "Rohan Nair",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.448": {
    "title": "How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction",
    "volume": "main",
    "abstract": "After a neural sequence model encounters an unexpected token, can its behavior be predicted? We show that RNN and transformer language models exhibit structured, consistent generalization in out-of-distribution contexts. We begin by introducing two idealized models of generalization in next-word prediction: a lexical context model in which generalization is consistent with the last word observed, and a syntactic context model in which generalization is consistent with the global structure of the input. In experiments in English, Finnish, Mandarin, and random regular languages, we demonstrate that neural language models interpolate between these two forms of generalization: their predictions are well-approximated by a log-linear combination of lexical and syntactic predictive distributions. We then show that, in some languages, noise mediates the two forms of generalization: noise applied to input tokens encourages syntactic generalization, while noise in history representations encourages lexical generalization. Finally, we offer a preliminary theoretical explanation of these results by proving that the observed interpolation behavior is expected in log-linear models with a particular feature correlation structure. These results help explain the effectiveness of two popular regularization schemes and show that aspects of sequence model generalization can be understood and controlled",
    "checked": true,
    "id": "582f977cc2b06700dea6faa183c64f5d11204cfe",
    "semantic_title": "how do neural sequence models generalize? local and global cues for out-of-distribution prediction",
    "citation_count": 3,
    "authors": [
      "D. Anthony Bau",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.449": {
    "title": "Comparing Text Representations: A Theory-Driven Approach",
    "volume": "main",
    "abstract": "Much of the progress in contemporary NLP has come from learning representations, such as masked language model (MLM) contextual embeddings, that turn challenging problems into simple classification tasks. But how do we quantify and explain this effect? We adapt general tools from computational learning theory to fit the specific characteristics of text datasets and present a method to evaluate the compatibility between representations and tasks. Even though many tasks can be easily solved with simple bag-of-words (BOW) representations, BOW does poorly on hard natural language inference tasks. For one such task we find that BOW cannot distinguish between real and randomized labelings, while pre-trained MLM representations show 72x greater distinction between real and random labelings than BOW. This method provides a calibrated, quantitative measure of the difficulty of a classification-based NLP task, enabling comparisons between representations without requiring empirical evaluations that may be sensitive to initializations and hyperparameters. The method provides a fresh perspective on the patterns in a dataset and the alignment of those patterns with specific labels",
    "checked": true,
    "id": "57f47450bc93a43b1f701d86bb72512c93f61eaa",
    "semantic_title": "comparing text representations: a theory-driven approach",
    "citation_count": 7,
    "authors": [
      "Gregory Yauney",
      "David Mimno"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.450": {
    "title": "Human Rationales as Attribution Priors for Explainable Stance Detection",
    "volume": "main",
    "abstract": "As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art classifier—particularly for inputs containing challenging phenomena such as sarcasm—at no cost in predictive performance. Furthermore, we demonstrate that attention weights surpass a leading attribution method in providing faithful explanations of our model's predictions, thus serving as a computationally cheap and reliable source of attributions for our model",
    "checked": true,
    "id": "cf20c1ba36ce063c85c797eff38508ef3d542444",
    "semantic_title": "human rationales as attribution priors for explainable stance detection",
    "citation_count": 24,
    "authors": [
      "Sahil Jayaram",
      "Emily Allaway"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.451": {
    "title": "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders",
    "volume": "main",
    "abstract": "Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis",
    "checked": true,
    "id": "0a81729daba7a482a3a988320559e4cb631f985f",
    "semantic_title": "the stem cell hypothesis: dilemma behind multi-task learning with transformer encoders",
    "citation_count": 101,
    "authors": [
      "Han He",
      "Jinho D. Choi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.452": {
    "title": "Text Counterfactuals via Latent Optimization and Shapley-Guided Search",
    "volume": "main",
    "abstract": "We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model's prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals",
    "checked": true,
    "id": "15aaae9832ada788c5d0533cdddae37021af5a97",
    "semantic_title": "text counterfactuals via latent optimization and shapley-guided search",
    "citation_count": 20,
    "authors": [
      "Xiaoli Fern",
      "Quintin Pope"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.453": {
    "title": "Average\" Approximates \"First Principal Component\"? An Empirical Analysis on Representations from Neural Language Models",
    "volume": "main",
    "abstract": "Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such representations remains a mystery. In this paper, we present an empirical property of these representations—\"average\" approximates \"first principal component\". Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong baseline. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a normal distribution for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically",
    "checked": true,
    "id": "348d58a56f354c73e196a9d9633c27ba3e2bb4e8",
    "semantic_title": "average\" approximates \"first principal component\"? an empirical analysis on representations from neural language models",
    "citation_count": 4,
    "authors": [
      "Zihan Wang",
      "Chengyu Dong",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.454": {
    "title": "Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models",
    "volume": "main",
    "abstract": "Prior work has shown that structural supervision helps English language models learn generalizations about syntactic phenomena such as subject-verb agreement. However, it remains unclear if such an inductive bias would also improve language models' ability to learn grammatical dependencies in typologically different languages. Here we investigate this question in Mandarin Chinese, which has a logographic, largely syllable-based writing system; different word order; and sparser morphology than English. We train LSTMs, Recurrent Neural Network Grammars, Transformer language models, and Transformer-parameterized generative parsing models on two Mandarin Chinese datasets of different sizes. We evaluate the models' ability to learn different aspects of Mandarin grammar that assess syntactic and semantic relationships. We find suggestive evidence that structural supervision helps with representing syntactic state across intervening content and improves performance in low-data settings, suggesting that the benefits of hierarchical inductive biases in acquiring dependency relationships may extend beyond English",
    "checked": true,
    "id": "328860f16f76ec234380fbde78495dbba178097c",
    "semantic_title": "controlled evaluation of grammatical knowledge in mandarin chinese language models",
    "citation_count": 3,
    "authors": [
      "Yiwen Wang",
      "Jennifer Hu",
      "Roger Levy",
      "Peng Qian"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.455": {
    "title": "GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks",
    "volume": "main",
    "abstract": "A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS improves the performance of MT-DNN with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. GradTS is also time-saving since (1) its gradient calculations are based on single-task experiments and (2) the gradients are re-used without additional experiments when the candidate task set changes. On the 8 GLUE classification tasks, for example, GradTS costs on average 21.32% less time than AUTOSEM with comparable GPU consumption. Further, we show the robustness of GradTS across various task settings and model selections, e.g. mixed objectives among candidate tasks. The efficiency and efficacy of GradTS in these case studies illustrate its general applicability in MTL research without requiring manual task filtering or costly parameter tuning",
    "checked": true,
    "id": "14a897ae303d2c2e38617852ad6062f1bfee68ae",
    "semantic_title": "gradts: a gradient-based automatic auxiliary task selection method based on transformer networks",
    "citation_count": 8,
    "authors": [
      "Weicheng Ma",
      "Renze Lou",
      "Kai Zhang",
      "Lili Wang",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.456": {
    "title": "NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases",
    "volume": "main",
    "abstract": "Codifying commonsense knowledge in machines is a longstanding goal of artificial intelligence. Recently, much progress toward this goal has been made with automatic knowledge base (KB) construction techniques. However, such techniques focus primarily on the acquisition of positive (true) KB statements, even though negative (false) statements are often also important for discriminative reasoning over commonsense KBs. As a first step toward the latter, this paper proposes NegatER, a framework that ranks potential negatives in commonsense KBs using a contextual language model (LM). Importantly, as most KBs do not contain negatives, NegatER relies only on the positive knowledge in the LM and does not require ground-truth negative examples. Experiments demonstrate that, compared to multiple contrastive data augmentation approaches, NegatER yields negatives that are more grammatical, coherent, and informative—leading to statistically significant accuracy improvements in a challenging KB completion task and confirming that the positive knowledge in LMs can be \"re-purposed\" to generate negative knowledge",
    "checked": true,
    "id": "09308b9be4052d0f51cda2372edeb973d6051ae4",
    "semantic_title": "negater: unsupervised discovery of negatives in commonsense knowledge bases",
    "citation_count": 13,
    "authors": [
      "Tara Safavi",
      "Jing Zhu",
      "Danai Koutra"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.457": {
    "title": "Instance-adaptive training with noise-robust losses against noisy labels",
    "volume": "main",
    "abstract": "In order to alleviate the huge demand for annotated datasets for different tasks, many recent natural language processing datasets have adopted automated pipelines for fast-tracking usable data. However, model training with such datasets poses a challenge because popular optimization objectives are not robust to label noise induced in the annotation generation process. Several noise-robust losses have been proposed and evaluated on tasks in computer vision, but they generally use a single dataset-wise hyperparamter to control the strength of noise resistance. This work proposes novel instance-adaptive training frameworks to change single dataset-wise hyperparameters of noise resistance in such losses to be instance-wise. Such instance-wise noise resistance hyperparameters are predicted by special instance-level label quality predictors, which are trained along with the main classification models. Experiments on noisy and corrupted NLP datasets show that proposed instance-adaptive training frameworks help increase the noise-robustness provided by such losses, promoting the use of the frameworks and associated losses in NLP models trained with noisy data",
    "checked": true,
    "id": "2b465347af31b01e8b8345e17ab8d60bd260812f",
    "semantic_title": "instance-adaptive training with noise-robust losses against noisy labels",
    "citation_count": 8,
    "authors": [
      "Lifeng Jin",
      "Linfeng Song",
      "Kun Xu",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.458": {
    "title": "Distributionally Robust Multilingual Machine Translation",
    "volume": "main",
    "abstract": "Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language pairs. We further show how to practically optimize this objective for large translation corpora using an iterated best response scheme, which is both effective and incurs negligible additional computational cost compared to standard empirical risk minimization. We perform extensive experiments on three sets of languages from two datasets and show that our method consistently outperforms strong baseline methods in terms of average and per-language performance under both many-to-one and one-to-many translation settings",
    "checked": true,
    "id": "9b3a071734cfecb2880105d84bde6791b48bf4a3",
    "semantic_title": "distributionally robust multilingual machine translation",
    "citation_count": 24,
    "authors": [
      "Chunting Zhou",
      "Daniel Levy",
      "Xian Li",
      "Marjan Ghazvininejad",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.459": {
    "title": "Model Selection for Cross-lingual Transfer",
    "volume": "main",
    "abstract": "Transformers that are pre-trained on multilingual corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In the zero-shot transfer setting, only English training data is used, and the fine-tuned model is evaluated on another target language. While this works surprisingly well, substantial variance has been observed in target language performance between different fine-tuning runs, and in the zero-shot setup, no target-language development data is available to select among multiple fine-tuned models. Prior work has relied on English dev data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices. In this paper, we show that it is possible to select consistently better models when small amounts of annotated data are available in auxiliary pivot languages. We propose a machine learning approach to model selection that uses the fine-tuned model's own internal representations to predict its cross-lingual capabilities. In extensive experiments we find that this method consistently selects better models than English validation data across twenty five languages (including eight low-resource languages), and often achieves results that are comparable to model selection using target language development data",
    "checked": true,
    "id": "5e8180e2ceddaab161e9be55bd81d8f911967302",
    "semantic_title": "model selection for cross-lingual transfer",
    "citation_count": 12,
    "authors": [
      "Yang Chen",
      "Alan Ritter"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.460": {
    "title": "Continual Few-Shot Learning for Text Classification",
    "volume": "main",
    "abstract": "Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize sentiment, handle numbers, perform coreference, etc. Our solutions to complex problems are still far from perfect, so it is important to create systems that can learn to correct mistakes quickly, incrementally, and with little training data. In this work, we propose a continual few-shot learning (CFL) task, in which a system is challenged with a difficult phenomenon and asked to learn to correct mistakes with only a few (10 to 15) training examples. To this end, we first create benchmarks based on previously annotated data: two NLI (ANLI and SNLI) and one sentiment analysis (IMDB) datasets. Next, we present various baselines from diverse paradigms (e.g., memory-aware synapses and Prototypical networks) and compare them on few-shot learning and continual few-shot learning setups. Our contributions are in creating a benchmark suite and evaluation protocol for continual few-shot learning on the text classification tasks, and making several interesting observations on the behavior of similarity-based methods. We hope that our work serves as a useful starting point for future work on this important topic",
    "checked": true,
    "id": "98fd08bbaf7e12b6d237698356ad6f99c16535f1",
    "semantic_title": "continual few-shot learning for text classification",
    "citation_count": 12,
    "authors": [
      "Ramakanth Pasunuru",
      "Veselin Stoyanov",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.461": {
    "title": "Efficient Nearest Neighbor Language Models",
    "volume": "main",
    "abstract": "Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs",
    "checked": true,
    "id": "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d",
    "semantic_title": "efficient nearest neighbor language models",
    "citation_count": 107,
    "authors": [
      "Junxian He",
      "Graham Neubig",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.462": {
    "title": "STraTA: Self-Training with Task Augmentation for Better Few-shot Learning",
    "volume": "main",
    "abstract": "Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a novel technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeled texts. Second, STraTA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data. Our experiments demonstrate that STraTA can substantially improve sample efficiency across 12 few-shot benchmarks. Remarkably, on the SST-2 sentiment dataset, STraTA, with only 8 training examples per class, achieves comparable results to standard fine-tuning with 67K training examples. Our analyses reveal that task augmentation and self-training are both complementary and independently effective",
    "checked": true,
    "id": "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3",
    "semantic_title": "strata: self-training with task augmentation for better few-shot learning",
    "citation_count": 61,
    "authors": [
      "Tu Vu",
      "Minh-Thang Luong",
      "Quoc Le",
      "Grady Simon",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.463": {
    "title": "TADPOLE: Task ADapted Pre-Training via AnOmaLy DEtection",
    "volume": "main",
    "abstract": "The paradigm of pre-training followed by finetuning has become a standard procedure for NLP tasks, with a known problem of domain shift between the pre-training and downstream corpus. Previous works have tried to mitigate this problem with additional pre-training, either on the downstream corpus itself when it is large enough, or on a manually curated unlabeled corpus of a similar domain. In this paper, we address the problem for the case when the downstream corpus is too small for additional pre-training. We propose TADPOLE, a task adapted pre-training framework based on data selection techniques adapted from Domain Adaptation. We formulate the data selection as an anomaly detection problem that unlike existing methods works well when the downstream corpus is limited in size. It results in a scalable and efficient unsupervised technique that eliminates the need for any manual data curation. We evaluate our framework on eight tasks across four different domains: Biomedical, Computer Science, News, and Movie reviews, and compare its performance against competitive baseline techniques from the area of Domain Adaptation. Our framework outperforms all the baseline methods. On small datasets with less than 5K training examples, we get a gain of 1.82% in performance with additional pre-training for only 5% steps compared to the originally pre-trained models. It also compliments some of the other techniques such as data augmentation known for boosting performance when downstream corpus is small; highest performance is achieved when data augmentation is combined with task adapted pre-training",
    "checked": true,
    "id": "77994c9c7ec19a07173b88fb45a8900b6616dd03",
    "semantic_title": "tadpole: task adapted pre-training via anomaly detection",
    "citation_count": 4,
    "authors": [
      "Vivek Madan",
      "Ashish Khetan",
      "Zohar Karnin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.464": {
    "title": "Gradient-based Adversarial Attacks against Text Transformers",
    "volume": "main",
    "abstract": "We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs",
    "checked": true,
    "id": "59c2b4ef91d4ce23cd4f270c8750a00de9054ec2",
    "semantic_title": "gradient-based adversarial attacks against text transformers",
    "citation_count": 254,
    "authors": [
      "Chuan Guo",
      "Alexandre Sablayrolles",
      "Hervé Jégou",
      "Douwe Kiela"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.465": {
    "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
    "volume": "main",
    "abstract": "The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results",
    "checked": true,
    "id": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
    "semantic_title": "do transformer modifications transfer across implementations and applications?",
    "citation_count": 128,
    "authors": [
      "Sharan Narang",
      "Hyung Won Chung",
      "Yi Tay",
      "Liam Fedus",
      "Thibault Fevry",
      "Michael Matena",
      "Karishma Malkan",
      "Noah Fiedel",
      "Noam Shazeer",
      "Zhenzhong Lan",
      "Yanqi Zhou",
      "Wei Li",
      "Nan Ding",
      "Jake Marcus",
      "Adam Roberts",
      "Colin Raffel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.466": {
    "title": "Paired Examples as Indirect Supervision in Latent Decision Models",
    "volume": "main",
    "abstract": "Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the model is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only provides a weak indirect signal on what values the latent decisions should take. This often results in the model failing to learn to perform the intermediate tasks correctly. In this work, we introduce a way to leverage paired examples that provide stronger cues for learning latent decisions. When two related training examples share internal substructure, we add an additional training objective to encourage consistency between their latent decisions. Such an objective does not require external supervision for the values of the latent output, or even the end task, yet provides an additional training signal to that provided by individual training examples themselves. We apply our method to improve compositional question answering using neural module networks on the DROP dataset. We explore three ways to acquire paired questions in DROP: (a) discovering naturally occurring paired examples within the dataset, (b) constructing paired examples using templates, and (c) generating paired examples using a question generation model. We empirically demonstrate that our proposed approach improves both in- and out-of-distribution generalization and leads to correct latent decision predictions",
    "checked": true,
    "id": "a29c55573839087490d7469fbbcfd06728566b44",
    "semantic_title": "paired examples as indirect supervision in latent decision models",
    "citation_count": 7,
    "authors": [
      "Nitish Gupta",
      "Sameer Singh",
      "Matt Gardner",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.467": {
    "title": "Pairwise Supervised Contrastive Learning of Sentence Representations",
    "volume": "main",
    "abstract": "Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10%–13% averaged improvement on eight clustering tasks, and 5%–6% averaged improvement on seven semantic textual similarity (STS) tasks",
    "checked": true,
    "id": "296aaba05c9a790f48b501d14f82c562d202c2e5",
    "semantic_title": "pairwise supervised contrastive learning of sentence representations",
    "citation_count": 52,
    "authors": [
      "Dejiao Zhang",
      "Shang-Wen Li",
      "Wei Xiao",
      "Henghui Zhu",
      "Ramesh Nallapati",
      "Andrew O. Arnold",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.468": {
    "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning",
    "volume": "main",
    "abstract": "We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g. RoBERTa) and generation models (e.g. BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks",
    "checked": true,
    "id": "789b5441743c2e38cf4c38749ed820c0671d81b1",
    "semantic_title": "muppet: massive multi-task representations with pre-finetuning",
    "citation_count": 272,
    "authors": [
      "Armen Aghajanyan",
      "Anchit Gupta",
      "Akshat Shrivastava",
      "Xilun Chen",
      "Luke Zettlemoyer",
      "Sonal Gupta"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.469": {
    "title": "Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP",
    "volume": "main",
    "abstract": "Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from limited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automatically proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diversity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribution, some inducing significant improvements in downstream few-shot accuracy of the meta-learned models. Empirically, results on 20 downstream tasks show significant improvements in few-shot learning – adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised methods on the FewRel 2.0 benchmark",
    "checked": true,
    "id": "43b0f0d2abcafabb31222a6b5b44a085019057b5",
    "semantic_title": "diverse distributions of self-supervised tasks for meta-learning in nlp",
    "citation_count": 20,
    "authors": [
      "Trapit Bansal",
      "Karthick Prasad Gunasekaran",
      "Tong Wang",
      "Tsendsuren Munkhdalai",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.470": {
    "title": "A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations",
    "volume": "main",
    "abstract": "Language agnostic and semantic-language information isolation is an emerging research direction for multilingual representations models. We explore this problem from a novel angle of geometric algebra and semantic space. A simple but highly effective method \"Language Information Removal (LIR)\" factors out language identity information from semantic related components in multilingual representations pre-trained on multi-monolingual data. A post-training and model-agnostic method, LIR only uses simple linear operations, e.g. matrix factorization and orthogonal projection. LIR reveals that for weak-alignment multilingual systems, the principal components of semantic spaces primarily encodes language identity information. We first evaluate the LIR on a cross-lingual question answer retrieval task (LAReQA), which requires the strong alignment for the multilingual embedding space. Experiment shows that LIR is highly effectively on this task, yielding almost 100% relative improvement in MAP for weak-alignment models. We then evaluate the LIR on Amazon Reviews and XEVAL dataset, with the observation that removing language information is able to improve the cross-lingual transfer performance",
    "checked": true,
    "id": "7a11c1dce5aac45f46e9b75f83c84cea167b02a9",
    "semantic_title": "a simple and effective method to eliminate the self language bias in multilingual representations",
    "citation_count": 24,
    "authors": [
      "Ziyi Yang",
      "Yinfei Yang",
      "Daniel Cer",
      "Eric Darve"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.471": {
    "title": "A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space",
    "volume": "main",
    "abstract": "In cross-lingual language models, representations for many different languages live in the same space. Here, we investigate the linguistic and non-linguistic factors affecting sentence-level alignment in cross-lingual pretrained language models for 101 languages and 5,050 language pairs. Using BERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our corpus, we compute a task-based measure of cross-lingual alignment in the form of bitext retrieval performance, as well as four intrinsic measures of vector space alignment and isomorphism. We then examine a range of linguistic, quasi-linguistic, and training-related features as potential predictors of these alignment metrics. The results of our analyses show that word order agreement and agreement in morphological complexity are two of the strongest linguistic predictors of cross-linguality. We also note in-family training data as a stronger predictor than language-specific training data across the board. We verify some of our linguistic findings by looking at the effect of morphological segmentation on English-Inuktitut alignment, in addition to examining the effect of word order agreement on isomorphism for 66 zero-shot language pairs from a different corpus. We make the data and code for our experiments publicly available",
    "checked": true,
    "id": "52a480b785ebdf48cb357e57f0d7f655655ef02d",
    "semantic_title": "a massively multilingual analysis of cross-linguality in shared embedding space",
    "citation_count": 8,
    "authors": [
      "Alexander Jones",
      "William Yang Wang",
      "Kyle Mahowald"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.472": {
    "title": "Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing",
    "volume": "main",
    "abstract": "The availability of corpora has led to significant advances in training semantic parsers in English. Unfortunately, for languages other than English, annotated data is limited and so is the performance of the developed parsers. Recently, pretrained multilingual models have been proven useful for zero-shot cross-lingual transfer in many NLP tasks. What else does it require to apply a parser trained in English to other languages for zero-shot cross-lingual semantic parsing? Will simple language-independent features help? To this end, we experiment with six Discourse Representation Structure (DRS) semantic parsers in English, and generalize them to Italian, German and Dutch, where there are only a small number of manually annotated parses available. Extensive experiments show that despite its simplicity, adding Universal Dependency (UD) relations and Universal POS tags (UPOS) as model-agnostic features achieves surprisingly strong improvement on all parsers",
    "checked": true,
    "id": "65fc5e6657fa2e9fd1b15df6b61acbe3ab3993df",
    "semantic_title": "frustratingly simple but surprisingly strong: using language-independent features for zero-shot cross-lingual semantic parsing",
    "citation_count": 7,
    "authors": [
      "Jingfeng Yang",
      "Federico Fancellu",
      "Bonnie Webber",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.473": {
    "title": "Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings",
    "volume": "main",
    "abstract": "Simultaneous translation is vastly different from full-sentence translation, in the sense that it starts translation before the source sentence ends, with only a few words delay. However, due to the lack of large-scale, high-quality simultaneous translation datasets, most such systems are still trained on conventional full-sentence bitexts. This is far from ideal for the simultaneous scenario due to the abundance of unnecessary long-distance reorderings in those bitexts. We propose a novel method that rewrites the target side of existing full-sentence corpora into simultaneous-style translation. Experiments on Zh→En and Ja→En simultaneous translation show substantial improvements (up to +2.7 BLEU) with the addition of these generated pseudo-references",
    "checked": true,
    "id": "27902aaa105053b15b66ad2b3384b91734878edf",
    "semantic_title": "improving simultaneous translation by incorporating pseudo-references with fewer reorderings",
    "citation_count": 28,
    "authors": [
      "Junkun Chen",
      "Renjie Zheng",
      "Atsuhito Kita",
      "Mingbo Ma",
      "Liang Huang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.474": {
    "title": "Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications",
    "volume": "main",
    "abstract": "Sentence-level Quality estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications",
    "checked": true,
    "id": "129aad3e4d9845075acfe0105eac7a6dd344d9f8",
    "semantic_title": "classification-based quality estimation: small and efficient models for real-world applications",
    "citation_count": 1,
    "authors": [
      "Shuo Sun",
      "Ahmed El-Kishky",
      "Vishrav Chaudhary",
      "James Cross",
      "Lucia Specia",
      "Francisco Guzmán"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.475": {
    "title": "A Large-Scale Study of Machine Translation in Turkic Languages",
    "volume": "main",
    "abstract": "Recent advances in neural machine translation (NMT) have pushed the quality of machine translation systems to the point where they are becoming widely adopted to build competitive systems. However, there is still a large number of languages that are yet to reap the benefits of NMT. In this paper, we provide the first large-scale case study of the practical application of MT in the Turkic language family in order to realize the gains of NMT for Turkic languages under high-resource to extremely low-resource scenarios. In addition to presenting an extensive analysis that identifies the bottlenecks towards building competitive systems to ameliorate data scarcity, our study has several key contributions, including, i) a large parallel corpus covering 22 Turkic languages consisting of common public datasets in combination with new datasets of approximately 1.4 million parallel sentences, ii) bilingual baselines for 26 language pairs, iii) novel high-quality test sets in three different translation domains and iv) human evaluation scores. All models, scripts, and data will be released to the public",
    "checked": true,
    "id": "042d980c2a70e1a48aeb1139520e56ee1ee8447b",
    "semantic_title": "a large-scale study of machine translation in turkic languages",
    "citation_count": 28,
    "authors": [
      "Jamshidbek Mirzakhalov",
      "Anoop Babu",
      "Duygu Ataman",
      "Sherzod Kariev",
      "Francis Tyers",
      "Otabek Abduraufov",
      "Mammad Hajili",
      "Sardana Ivanova",
      "Abror Khaytbaev",
      "Antonio Laverghetta Jr.",
      "Bekhzodbek Moydinboyev",
      "Esra Onal",
      "Shaxnoza Pulatova",
      "Ahsan Wahab",
      "Orhan Firat",
      "Sriram Chellappan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.476": {
    "title": "Analyzing the Surprising Variability in Word Embedding Stability Across Languages",
    "volume": "main",
    "abstract": "Word embeddings are powerful representations that form the foundation of many natural language processing architectures, both in English and in other languages. To gain further insight into word embeddings, we explore their stability (e.g., overlap between the nearest neighbors of a word in different embedding spaces) in diverse languages. We discuss linguistic properties that are related to stability, drawing out insights about correlations with affixing, language gender systems, and other features. This has implications for embedding use, particularly in research that uses them to study language trends",
    "checked": true,
    "id": "2e7a2736c83955ba7bd44d4cb1034b64aa6a3902",
    "semantic_title": "analyzing the surprising variability in word embedding stability across languages",
    "citation_count": 10,
    "authors": [
      "Laura Burdick",
      "Jonathan K. Kummerfeld",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.477": {
    "title": "Rule-based Morphological Inflection Improves Neural Terminology Translation",
    "volume": "main",
    "abstract": "Current approaches to incorporating terminology constraints in machine translation (MT) typically assume that the constraint terms are provided in their correct morphological forms. This limits their application to real-world scenarios where constraint terms are provided as lemmas. In this paper, we introduce a modular framework for incorporating lemma constraints in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate lemma constraints more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs",
    "checked": true,
    "id": "e13732c7ed5c80d20a9fe0dbcd8976d83a1c444d",
    "semantic_title": "rule-based morphological inflection improves neural terminology translation",
    "citation_count": 6,
    "authors": [
      "Weijia Xu",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.478": {
    "title": "Data and Parameter Scaling Laws for Neural Machine Translation",
    "volume": "main",
    "abstract": "We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs",
    "checked": true,
    "id": "eec7ad0270eda7298c139af6e2676599f1fd53f6",
    "semantic_title": "data and parameter scaling laws for neural machine translation",
    "citation_count": 90,
    "authors": [
      "Mitchell A Gordon",
      "Kevin Duh",
      "Jared Kaplan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.479": {
    "title": "Good-Enough Example Extrapolation",
    "volume": "main",
    "abstract": "This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid inductive bias for data augmentation. To operationalize this question, I propose a simple data augmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than upsampling and other hidden-space data augmentation methods",
    "checked": true,
    "id": "e6440bdb3418e3320c361bce1c0874bd9d8afe33",
    "semantic_title": "good-enough example extrapolation",
    "citation_count": 6,
    "authors": [
      "Jason Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.480": {
    "title": "Learning to Selectively Learn for Weakly-supervised Paraphrase Generation",
    "volume": "main",
    "abstract": "Paraphrase generation is a longstanding NLP task that has diverse applications on downstream NLP tasks. However, the effectiveness of existing efforts predominantly relies on large amounts of golden labeled data. Though unsupervised endeavors have been proposed to alleviate this issue, they may fail to generate meaningful paraphrases due to the lack of supervision signals. In this work, we go beyond the existing paradigms and propose a novel approach to generate high-quality paraphrases with data of weak supervision. Specifically, we tackle the weakly-supervised paraphrase generation problem by: (1) obtaining abundant weakly-labeled parallel sentences via retrieval-based pseudo paraphrase expansion; and (2) developing a meta-learning framework to progressively select valuable samples for fine-tuning a pre-trained language model BART on the sentential paraphrasing task. We demonstrate that our approach achieves significant improvements over existing unsupervised approaches, and is even comparable in performance with supervised state-of-the-arts",
    "checked": true,
    "id": "ebfb824c87663bb091d91f3f72f3fc232fe831a6",
    "semantic_title": "learning to selectively learn for weakly-supervised paraphrase generation",
    "citation_count": 6,
    "authors": [
      "Kaize Ding",
      "Dingcheng Li",
      "Alexander Hanbo Li",
      "Xing Fan",
      "Chenlei Guo",
      "Yang Liu",
      "Huan Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.481": {
    "title": "Effective Convolutional Attention Network for Multi-label Clinical Document Classification",
    "volume": "main",
    "abstract": "Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information across the document and learn meaningful document representations that cover different ranges of texts; (2) we explore multi-layer and sum-pooling attention to extract the most informative features from these multi-scale representations; (3) we combine binary cross entropy loss and focal loss to improve performance for rare labels. We focus our evaluation study on MIMIC-III, a widely used dataset in the medical domain. Our models outperform prior work on medical coding and achieve new state-of-the-art results on multiple metrics. We also demonstrate the language independent nature of our approach by applying it to two non-English datasets. Our model outperforms prior best model and a multilingual Transformer model by a substantial margin",
    "checked": true,
    "id": "2be1cbc2cfd6e207e908f2c47b1edee672d270f2",
    "semantic_title": "effective convolutional attention network for multi-label clinical document classification",
    "citation_count": 61,
    "authors": [
      "Yang Liu",
      "Hua Cheng",
      "Russell Klopfer",
      "Matthew R. Gormley",
      "Thomas Schaaf"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.482": {
    "title": "Contrastive Code Representation Learning",
    "volume": "main",
    "abstract": "Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like code clone detection, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based RoBERTa model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training outperforms RoBERTa on an adversarial code clone detection benchmark by 39% AUROC. Surprisingly, improved adversarial robustness translates to better accuracy over natural code; ContraCode improves summarization and TypeScript type inference accuracy by 2 to 13 percentage points over competitive baselines. All source is available at https://github.com/parasj/contracode",
    "checked": true,
    "id": "021bbcefc993c389bad6c1daefd8ff92d0fc2441",
    "semantic_title": "contrastive code representation learning",
    "citation_count": 154,
    "authors": [
      "Paras Jain",
      "Ajay Jain",
      "Tianjun Zhang",
      "Pieter Abbeel",
      "Joseph Gonzalez",
      "Ion Stoica"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.483": {
    "title": "IGA: An Intent-Guided Authoring Assistant",
    "volume": "main",
    "abstract": "While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We fine-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to fill in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations confirm the quality of IGA's generated outputs, while a small-scale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into AI-assisted writing",
    "checked": true,
    "id": "7354b87a1b4c99ccd9cf25b7314927ced8b156f7",
    "semantic_title": "iga: an intent-guided authoring assistant",
    "citation_count": 18,
    "authors": [
      "Simeng Sun",
      "Wenlong Zhao",
      "Varun Manjunatha",
      "Rajiv Jain",
      "Vlad Morariu",
      "Franck Dernoncourt",
      "Balaji Vasan Srinivasan",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.484": {
    "title": "Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints",
    "volume": "main",
    "abstract": "We study the problem of generating arithmetic math word problems (MWPs) given a math equation that specifies the mathematical computation and a context that specifies the problem scenario. Existing approaches are prone to generating MWPs that are either mathematically invalid or have unsatisfactory language quality. They also either ignore the context or require manual specification of a problem template, which compromises the diversity of the generated MWPs. In this paper, we develop a novel MWP generation approach that leverages i) pre-trained language models and a context keyword selection model to improve the language quality of generated MWPs and ii) an equation consistency constraint for math equations to improve the mathematical validity of the generated MWPs. Extensive quantitative and qualitative experiments on three real-world MWP datasets demonstrate the superior performance of our approach compared to various baselines",
    "checked": true,
    "id": "f678ed31af657eefc021a3e7ee7f5848caff636a",
    "semantic_title": "math word problem generation with mathematical consistency and problem context constraints",
    "citation_count": 49,
    "authors": [
      "Zichao Wang",
      "Andrew Lan",
      "Richard Baraniuk"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.485": {
    "title": "Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep Learning",
    "volume": "main",
    "abstract": "Irrespective of the success of the deep learning-based mixed-domain transfer learning approach for solving various Natural Language Processing tasks, it does not lend a generalizable solution for detecting misinformation from COVID-19 social media data. Due to the inherent complexity of this type of data, caused by its dynamic (context evolves rapidly), nuanced (misinformation types are often ambiguous), and diverse (skewed, fine-grained, and overlapping categories) nature, it is imperative for an effective model to capture both the local and global context of the target domain. By conducting a systematic investigation, we show that: (i) the deep Transformer-based pre-trained models, utilized via the mixed-domain transfer learning, are only good at capturing the local context, thus exhibits poor generalization, and (ii) a combination of shallow network-based domain-specific models and convolutional neural networks can efficiently extract local as well as global context directly from the target data in a hierarchical fashion, enabling it to offer a more generalizable solution",
    "checked": true,
    "id": "fae499b19e89a9b05e6adf83db40c713327ed42f",
    "semantic_title": "navigating the kaleidoscope of covid-19 misinformation using deep learning",
    "citation_count": 4,
    "authors": [
      "Yuanzhi Chen",
      "Mohammad Hasan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.486": {
    "title": "Detecting Health Advice in Medical Research Literature",
    "volume": "main",
    "abstract": "Health and medical researchers often give clinical and policy recommendations to inform health practice and public health policy. However, no current health information system supports the direct retrieval of health advice. This study fills the gap by developing and validating an NLP-based prediction model for identifying health advice in research publications. We annotated a corpus of 6,000 sentences extracted from structured abstracts in PubMed publications as ‘\"strong advice\", \"weak advice\", or \"no advice\", and developed a BERT-based model that can predict, with a macro-averaged F1-score of 0.93, whether a sentence gives strong advice, weak advice, or not. The prediction model generalized well to sentences in both unstructured abstracts and discussion sections, where health advice normally appears. We also conducted a case study that applied this prediction model to retrieve specific health advice on COVID-19 treatments from LitCovid, a large COVID research literature portal, demonstrating the usefulness of retrieving health advice sentences as an advanced research literature navigation function for health researchers and the general public",
    "checked": true,
    "id": "3852e89b9d35811817476cc1c2a8d9da7b088854",
    "semantic_title": "detecting health advice in medical research literature",
    "citation_count": 10,
    "authors": [
      "Yingya Li",
      "Jun Wang",
      "Bei Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.487": {
    "title": "A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading",
    "volume": "main",
    "abstract": "Automatic short answer grading (ASAG) is the task of assessing students' short natural language responses to objective questions. It is a crucial component of new education platforms, and could support more wide-spread use of constructed response questions to replace cognitively less challenging multiple choice questions. We propose a Semantic Feature-wise transformation Relation Network (SFRN) that exploits the multiple components of ASAG datasets more effectively. SFRN captures relational knowledge among the questions (Q), reference answers or rubrics (R), and labeled student answers (A). A relation network learns vector representations for the elements of QRA triples, then combines the learned representations using learned semantic feature-wise transformations. We apply translation-based data augmentation to address the two problems of limited training data, and high data skew for multi-class ASAG tasks. Our model has up to 11% performance improvement over state-of-the-art results on the benchmark SemEval-2013 datasets, and surpasses custom approaches designed for a Kaggle challenge, demonstrating its generality",
    "checked": true,
    "id": "3fc4d9968e7d134a59b91b75c543bdc0a967b971",
    "semantic_title": "a semantic feature-wise transformation relation network for automatic short answer grading",
    "citation_count": 18,
    "authors": [
      "Zhaohui Li",
      "Yajur Tomar",
      "Rebecca J. Passonneau"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.488": {
    "title": "Evaluating Scholarly Impact: Towards Content-Aware Bibliometrics",
    "volume": "main",
    "abstract": "Quantitatively measuring the impact-related aspects of scientific, engineering, and technological (SET) innovations is a fundamental problem with broad applications. Traditional citation-based measures for assessing the impact of innovations and related entities do not take into account the content of the publications. This limits their ability to provide rigorous quality-related metrics because they cannot account for the reasons that led to a citation. We present approaches to estimate content-aware bibliometrics to quantitatively measure the scholarly impact of a publication. Our approaches assess the impact of a cited publication by the extent to which the cited publication informs the citing publication. We introduce a new metric, called \"Content Informed Index\" (CII), that uses the content of the paper as a source of distant-supervision, to quantify how much the cited-node informs the citing-node. We evaluate the weights estimated by our approach on three manually annotated datasets, where the annotations quantify the extent of information in the citation. Particularly, we evaluate how well the ranking imposed by our approach associates with the ranking imposed by the manual annotations. CII achieves up to 103% improvement in performance as compared to the second-best performing approach",
    "checked": true,
    "id": "50ecda6262a54185df418cd60a13c2c09dd6ebbb",
    "semantic_title": "evaluating scholarly impact: towards content-aware bibliometrics",
    "citation_count": 5,
    "authors": [
      "Saurav Manchanda",
      "George Karypis"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.489": {
    "title": "A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational AI Systems",
    "volume": "main",
    "abstract": "Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system across 10 domains",
    "checked": true,
    "id": "c01e4f0a098f3f1d007621b8ec6bd9f6ed064c34",
    "semantic_title": "a scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational ai systems",
    "citation_count": 21,
    "authors": [
      "Sunghyun Park",
      "Han Li",
      "Ameen Patel",
      "Sidharth Mudgal",
      "Sungjin Lee",
      "Young-Bum Kim",
      "Spyros Matsoukas",
      "Ruhi Sarikaya"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.490": {
    "title": "Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension",
    "volume": "main",
    "abstract": "How can we generate concise explanations for multi-hop Reading Comprehension (RC)? The current strategies of identifying supporting sentences can be seen as an extractive question-focused summarization of the input text. However, these extractive explanations are not necessarily concise i.e. not minimally sufficient for answering a question. Instead, we advocate for an abstractive approach, where we propose to generate a question-focused, abstractive summary of input paragraphs and then feed it to an RC system. Given a limited amount of human-annotated abstractive explanations, we train the abstractive explainer in a semi-supervised manner, where we start from the supervised model and then train it further through trial and error maximizing a conciseness-promoted reward function. Our experiments demonstrate that the proposed abstractive explainer can generate more compact explanations than an extractive explainer with limited supervision (only 2k instances) while maintaining sufficiency",
    "checked": true,
    "id": "6160d89955aeb79424cb8bf164e3a797035c1c67",
    "semantic_title": "summarize-then-answer: generating concise explanations for multi-hop reading comprehension",
    "citation_count": 16,
    "authors": [
      "Naoya Inoue",
      "Harsh Trivedi",
      "Steven Sinha",
      "Niranjan Balasubramanian",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.491": {
    "title": "FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models",
    "volume": "main",
    "abstract": "The task of learning from only a few examples (called a few-shot setting) is of key importance and relevance to a real-world setting. For question answering (QA), the current state-of-the-art pre-trained models typically need fine-tuning on tens of thousands of examples to obtain good results. Their performance degrades significantly in a few-shot setting (< 100 examples). To address this, we propose a simple fine-tuning framework that leverages pre-trained text-to-text models and is directly aligned with their pre-training framework. Specifically, we construct the input as a concatenation of the question, a mask token representing the answer span and a context. Given this input, the model is fine-tuned using the same objective as that of its pre-training objective. Through experimental studies on various few-shot configurations, we show that this formulation leads to significant gains on multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when there are only 16 training examples). The gains extend further when used with larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples) and translate well to a multilingual setting . On the multilingual TydiQA benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of upto 40 F1 points and an average of 33 F1 points in a few-shot setting (<= 64 training examples). We conduct detailed ablation studies to analyze factors contributing to these gains",
    "checked": true,
    "id": "9c81394363178ce94b1eac111df4e3ad11bdb7c2",
    "semantic_title": "fewshotqa: a simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models",
    "citation_count": 46,
    "authors": [
      "Rakesh Chada",
      "Pradeep Natarajan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.492": {
    "title": "Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval",
    "volume": "main",
    "abstract": "In the context of neural passage retrieval, we study three promising techniques: synthetic data generation, negative sampling, and fusion. We systematically investigate how these techniques contribute to the performance of the retrieval system and how they complement each other. We propose a multi-stage framework comprising of pre-training with synthetic data, fine-tuning with labeled data, and negative sampling at both stages. We study six negative sampling strategies and apply them to the fine-tuning stage and, as a noteworthy novelty, to the synthetic data that we use for pre-training. Also, we explore fusion methods that combine negatives from different strategies. We evaluate our system using two passage retrieval tasks for open-domain QA and using MS MARCO. Our experiments show that augmenting the negative contrast in both stages is effective to improve passage retrieval accuracy and, importantly, they also show that synthetic data generation and negative sampling have additive benefits. Moreover, using the fusion of different kinds allows us to reach performance that establishes a new state-of-the-art level in two of the tasks we evaluated",
    "checked": true,
    "id": "77db3d91786935f91656d4193a742220651f869f",
    "semantic_title": "multi-stage training with improved negative contrast for neural passage retrieval",
    "citation_count": 29,
    "authors": [
      "Jing Lu",
      "Gustavo Hernandez Abrego",
      "Ji Ma",
      "Jianmo Ni",
      "Yinfei Yang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.493": {
    "title": "Perhaps PTLMs Should Go to School – A Task to Assess Open Book and Closed Book QA",
    "volume": "main",
    "abstract": "Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given significant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government 2e) and humanities (U.S. History), hundreds of true/false statements based on review questions written by the textbook authors, validation/development tests based on the first eight chapters of the textbooks, blind tests based on the remaining textbook chapters, and baseline results given state-of-the-art PTLMs. Since the questions are balanced, random performance should be ~50%. T5, fine-tuned with BoolQ achieves the same performance, suggesting that the textbook's content is not pre-represented in the PTLM. Taking the exam closed book, but having read the textbook (i.e., adding the textbook to T5's pre-training), yields at best minor improvement (56%), suggesting that the PTLM may not have \"understood\" the textbook (or perhaps misunderstood the questions). Performance is better (~60%) when the exam is taken open-book (i.e., allowing the machine to automatically retrieve a paragraph and use it to answer the question)",
    "checked": true,
    "id": "ef5a79368dddf07a368347c64c289bfaaa587fa5",
    "semantic_title": "perhaps ptlms should go to school – a task to assess open book and closed book qa",
    "citation_count": 10,
    "authors": [
      "Manuel Ciosici",
      "Joe Cecil",
      "Dong-Ho Lee",
      "Alex Hedges",
      "Marjorie Freedman",
      "Ralph Weischedel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.494": {
    "title": "ReasonBERT: Pre-trained to Reason with Distant Supervision",
    "volume": "main",
    "abstract": "We present ReasonBert, a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid contexts. Unlike existing pre-training methods that only harvest learning signals from local contexts of naturally occurring texts, we propose a generalized notion of distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of reasoning are simulated, including intersecting multiple pieces of evidence, bridging from one piece of evidence to another, and detecting unanswerable cases. We conduct a comprehensive evaluation on a variety of extractive question answering datasets ranging from single-hop to multi-hop and from text-only to table-only to hybrid that require various reasoning capabilities and show that ReasonBert achieves remarkable improvement over an array of strong baselines. Few-shot experiments further demonstrate that our pre-training method substantially improves sample efficiency",
    "checked": true,
    "id": "35ab270362e3dd4a1c95b63f7e800d5f7d4b9dc0",
    "semantic_title": "reasonbert: pre-trained to reason with distant supervision",
    "citation_count": 32,
    "authors": [
      "Xiang Deng",
      "Yu Su",
      "Alyssa Lees",
      "You Wu",
      "Cong Yu",
      "Huan Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.495": {
    "title": "Single-dataset Experts for Multi-dataset Question Answering",
    "volume": "main",
    "abstract": "Many datasets have been created for training reading comprehension models, and a natural question is whether we can combine them to build models that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub- distributions and might transfer worse compared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with an ensemble of single-dataset experts, by training a collection of lightweight, dataset-specific adapter modules (Houlsby et al., 2019) that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple methods based on parameter-averaging lead to better zero-shot generalization and few-shot transfer performance, offering a strong and versatile starting point for building new reading comprehension systems",
    "checked": true,
    "id": "67dc4ba4542d5895862c8b5af5023f659c14542c",
    "semantic_title": "single-dataset experts for multi-dataset question answering",
    "citation_count": 26,
    "authors": [
      "Dan Friedman",
      "Ben Dodge",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.496": {
    "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
    "volume": "main",
    "abstract": "Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., \"Where was Arve Furset born?\"), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions",
    "checked": true,
    "id": "f7a6b57adebb5f6a10d16e120f0b0ef55aab7b2b",
    "semantic_title": "simple entity-centric questions challenge dense retrievers",
    "citation_count": 171,
    "authors": [
      "Christopher Sciavolino",
      "Zexuan Zhong",
      "Jinhyuk Lee",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.497": {
    "title": "Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization",
    "volume": "main",
    "abstract": "Question Answering (QA) tasks requiring information from multiple documents often rely on a retrieval model to identify relevant information for reasoning. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as Wikipedia, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during annotation, rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean, since the model cannot rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries, and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets, IIRC and HotpotQA. On IIRC, we show that joint modeling with marginalization improves model performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting",
    "checked": true,
    "id": "443f889a2d4da14e930586e9416068f91c93a5bb",
    "semantic_title": "mitigating false-negative contexts in multi-document question answering with retrieval marginalization",
    "citation_count": 9,
    "authors": [
      "Ansong Ni",
      "Matt Gardner",
      "Pradeep Dasigi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.498": {
    "title": "MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents",
    "volume": "main",
    "abstract": "We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as machine reading comprehension task based on a single given document or passage. In this work, we aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents. To facilitate such task, we introduce a new dataset that contains dialogues grounded in multiple documents from four different domains. We also explore modeling the dialogue-based and document-based contexts in the dataset. We present strong baseline approaches and various experimental results, aiming to support further research efforts on such a task",
    "checked": true,
    "id": "2525df3bfc0f2c1c7abda2691f9f4b2f58fc1fc6",
    "semantic_title": "multidoc2dial: modeling dialogues grounded in multiple documents",
    "citation_count": 70,
    "authors": [
      "Song Feng",
      "Siva Sankalp Patel",
      "Hui Wan",
      "Sachindra Joshi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.499": {
    "title": "GupShup: Summarizing Open-Domain Code-Switched Conversations",
    "volume": "main",
    "abstract": "Code-switching is the communication phenomenon where the speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities worldwide. Therefore, it is essential to develop techniques for understanding and summarizing these conversations. Towards this objective, we introduce the task of abstractive summarization of Hindi-English (Hi-En) code-switched conversations. We also develop the first code-switched conversation summarization dataset - GupShup, which contains over 6,800 Hi-En conversations and their corresponding human-annotated summaries in English (En) and Hi-En. We present a detailed account of the entire data collection and annotation process. We analyze the dataset using various code-switching statistics. We train state-of-the-art abstractive summarization models and report their performances using both automated metrics and human evaluation. Our results show that multi-lingual mBART and multi-view seq2seq models obtain the best performances on this new dataset. We also conduct an extensive qualitative analysis to provide insight into the models and some of their shortcomings",
    "checked": true,
    "id": "84546181c8ee493cc54b7005bb453f18244b1de1",
    "semantic_title": "gupshup: summarizing open-domain code-switched conversations",
    "citation_count": 17,
    "authors": [
      "Laiba Mehnaz",
      "Debanjan Mahata",
      "Rakesh Gosangi",
      "Uma Sushmitha Gunturi",
      "Riya Jain",
      "Gauri Gupta",
      "Amardeep Kumar",
      "Isabelle G. Lee",
      "Anish Acharya",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.500": {
    "title": "BiSECT: Learning to Split and Rephrase Sentences with Bitexts",
    "volume": "main",
    "abstract": "An important task in NLP applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this ‘split and rephrase' task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language. BiSECT contains higher quality training examples than the previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our corpus and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations",
    "checked": true,
    "id": "42ac65179b2099f8782d8aa055969bb973ccd1ec",
    "semantic_title": "bisect: learning to split and rephrase sentences with bitexts",
    "citation_count": 26,
    "authors": [
      "Joongwon Kim",
      "Mounica Maddela",
      "Reno Kriz",
      "Wei Xu",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.501": {
    "title": "Data Collection vs. Knowledge Graph Completion: What is Needed to Improve Coverage?",
    "volume": "main",
    "abstract": "This survey/position paper discusses ways to improve coverage of resources such as WordNet. Rapp estimated correlations, rho, between corpus statistics and pyscholinguistic norms. rho improves with quantity (corpus size) and quality (balance). 1M words is enough for simple estimates (unigram frequencies), but at least 100x more is required for good estimates of word associations and embeddings. Given such estimates, WordNet's coverage is remarkable. WordNet was developed on SemCor, a small sample (200k words) from the Brown Corpus. Knowledge Graph Completion (KGC) attempts to learn missing links from subsets. But Rapp's estimates of sizes suggest it would be more profitable to collect more data than to infer missing information that is not there",
    "checked": true,
    "id": "9041130aa571e446a1b00e667849d4ac924a7481",
    "semantic_title": "data collection vs. knowledge graph completion: what is needed to improve coverage?",
    "citation_count": 6,
    "authors": [
      "Kenneth Church",
      "Yuchen Bian"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.502": {
    "title": "Universal Sentence Representation Learning with Conditional Masked Language Model",
    "volume": "main",
    "abstract": "This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10% improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics",
    "checked": true,
    "id": "a15421b034d3c75043190bbe4de28540f403f99c",
    "semantic_title": "universal sentence representation learning with conditional masked language model",
    "citation_count": 59,
    "authors": [
      "Ziyi Yang",
      "Yinfei Yang",
      "Daniel Cer",
      "Jax Law",
      "Eric Darve"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.503": {
    "title": "On the Benefit of Syntactic Supervision for Cross-lingual Transfer in Semantic Role Labeling",
    "volume": "main",
    "abstract": "Although recent developments in neural architectures and pre-trained representations have greatly increased state-of-the-art model performance on fully-supervised semantic role labeling (SRL), the task remains challenging for languages where supervised SRL training data are not abundant. Cross-lingual learning can improve performance in this setting by transferring knowledge from high-resource languages to low-resource ones. Moreover, we hypothesize that annotations of syntactic dependencies can be leveraged to further facilitate cross-lingual transfer. In this work, we perform an empirical exploration of the helpfulness of syntactic supervision for crosslingual SRL within a simple multitask learning scheme. With comprehensive evaluations across ten languages (in addition to English) and three SRL benchmark datasets, including both dependency- and span-based SRL, we show the effectiveness of syntactic supervision in low-resource scenarios",
    "checked": true,
    "id": "1753348a601867ce73f3c7130361ca49fe62a42a",
    "semantic_title": "on the benefit of syntactic supervision for cross-lingual transfer in semantic role labeling",
    "citation_count": 5,
    "authors": [
      "Zhisong Zhang",
      "Emma Strubell",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.504": {
    "title": "Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models",
    "volume": "main",
    "abstract": "Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an enthymeme, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets",
    "checked": true,
    "id": "5ec0e287dabae97a59adc3d0445e9af3d991ec95",
    "semantic_title": "implicit premise generation with discourse-aware commonsense knowledge models",
    "citation_count": 14,
    "authors": [
      "Tuhin Chakrabarty",
      "Aadit Trivedi",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.505": {
    "title": "Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks",
    "volume": "main",
    "abstract": "Systematic compositionality is an essential mechanism in human language, allowing the recombination of known parts to create novel expressions. However, existing neural models have been shown to lack this basic ability in learning symbolic structures. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision. These automatically-generated sequences are more representative of the underlying compositional symbolic structures of the input data. During inference, the model jointly predicts the next action and the next tokens in the auxiliary sequences at each step. Experiments on the SCAN dataset show that our method encourages the Transformer to understand compositional structures of the command, improving its accuracy on multiple challenging splits from ≤ 10% to 100%. With only 418 (5%) training instances, our approach still achieves 97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can be induced in Transformers given minimal but proper guidance. We also show that a better result is achieved using less contextualized vectors as the attention's query, providing insights into architecture choices in achieving systematic compositionality. Finally, we show positive generalization results on the grounded-SCAN task (Ruis et al., 2020)",
    "checked": true,
    "id": "bb0ab8591d6d57c7e2bd1ec35d806b3f25277752",
    "semantic_title": "inducing transformer's compositional generalization ability via auxiliary sequence prediction tasks",
    "citation_count": 27,
    "authors": [
      "Yichen Jiang",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.506": {
    "title": "Flexible Generation of Natural Language Deductions",
    "volume": "main",
    "abstract": "An interpretable system for open-domain reasoning needs to express its reasoning process in a transparent form. Natural language is an attractive representation for this purpose — it is both highly expressive and easy for humans to understand. However, manipulating natural language statements in logically consistent ways is hard: models must cope with variation in how meaning is expressed while remaining precise. In this paper, we describe ParaPattern, a method for building models to generate deductive inferences from diverse natural language inputs without direct human supervision. We train BART-based models (Lewis et al., 2020) to generate the result of applying a particular logical operation to one or more premise statements. Crucially, we develop a largely automated pipeline for constructing suitable training examples from Wikipedia. We evaluate our models using out-of-domain sentence compositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et al., 2021) datasets as well as targeted perturbation sets. Our results show that our models are substantially more accurate and flexible than baseline systems. ParaPattern achieves 85% validity on examples of the ‘substitution' operation from EntailmentBank without the use of any in-domain training data, matching the performance of a model fine-tuned for EntailmentBank. The full source code for our method is publicly available",
    "checked": true,
    "id": "911b7539e964782670e555930b291de16fa971c5",
    "semantic_title": "flexible generation of natural language deductions",
    "citation_count": 33,
    "authors": [
      "Kaj Bostrom",
      "Xinyu Zhao",
      "Swarat Chaudhuri",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.507": {
    "title": "Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing",
    "volume": "main",
    "abstract": "Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the desirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization",
    "checked": true,
    "id": "490a020d018ae198ccc01d567f94c5493b48f9b8",
    "semantic_title": "structure-aware fine-tuning of sequence-to-sequence transformers for transition-based amr parsing",
    "citation_count": 43,
    "authors": [
      "Jiawei Zhou",
      "Tahira Naseem",
      "Ramón Fernandez Astudillo",
      "Young-Suk Lee",
      "Radu Florian",
      "Salim Roukos"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.508": {
    "title": "Think about it! Improving defeasible reasoning by first modeling the question scenario",
    "volume": "main",
    "abstract": "Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a \"mental model\" of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model first create a graph of relevant influences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS, achieves a new state-of-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a system to \"think about\" a question and explicitly model the scenario, rather than answering reflexively",
    "checked": true,
    "id": "3dcf9c900f5f28e082a2fcdea4763b6063a76f09",
    "semantic_title": "think about it! improving defeasible reasoning by first modeling the question scenario",
    "citation_count": 31,
    "authors": [
      "Aman Madaan",
      "Niket Tandon",
      "Dheeraj Rajagopal",
      "Peter Clark",
      "Yiming Yang",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.509": {
    "title": "Open Aspect Target Sentiment Classification with Natural Language Prompts",
    "volume": "main",
    "abstract": "For many business applications, we often seek to analyze sentiments associated with any arbitrary aspects of commercial products, despite having a very limited amount of labels or even without any labels at all. However, existing aspect target sentiment classification (ATSC) models are not trainable if annotated datasets are not available. Even with labeled data, they fall short of reaching satisfactory performance. To address this, we propose simple approaches that better solve ATSC with natural language prompts, enabling the task under zero-shot cases and enhancing supervised settings, especially for few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop domain, our method of reformulating ATSC as an NLI task outperforms supervised SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points. Moreover, we demonstrate that our prompts could handle implicitly stated aspects as well: our models reach about 77% accuracy on detecting sentiments for aspect categories (e.g., food), which do not necessarily appear within the text, even though we trained the models only with explicitly mentioned aspect terms (e.g., fajitas) from just 16 reviews - while the accuracy of the no-prompt baseline is only around 65%",
    "checked": true,
    "id": "973217ce9681b7627da01075f0707e812e9a55c0",
    "semantic_title": "open aspect target sentiment classification with natural language prompts",
    "citation_count": 65,
    "authors": [
      "Ronald Seoh",
      "Ian Birle",
      "Mrinal Tak",
      "Haw-Shiuan Chang",
      "Brian Pinette",
      "Alfred Hough"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.510": {
    "title": "Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica",
    "volume": "main",
    "abstract": "People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they provide. To collect labels of human perception, we curate a new dataset, Hummingbird, on top of benchmarking style datasets. We have crowd workers highlight the representative words in the text that makes them think the text has the following styles: politeness, sentiment, offensiveness, and five emotion types. We then compare these human word labels with word importance derived from a popular fine-tuned style classifier like BERT. Our results show that the BERT often finds content words not relevant to the target style as important words used in style prediction, but humans do not perceive the same way even though for some styles (e.g., positive sentiment and joy) human- and machine-identified words share significant overlap for some styles",
    "checked": true,
    "id": "ed95a7dd1ae1d4ceea54e90d10679f7e02586d20",
    "semantic_title": "does bert learn as humans perceive? understanding linguistic styles through lexica",
    "citation_count": 35,
    "authors": [
      "Shirley Anugrah Hayati",
      "Dongyeop Kang",
      "Lyle Ungar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.511": {
    "title": "Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation",
    "volume": "main",
    "abstract": "Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this task, one of the remaining challenges is the scarcity of annotations. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one model on each dataset and datasets of different domains, respectively. We show that models can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the state-of-the-art by a large margin. We publicly release our code",
    "checked": true,
    "id": "04760fc00c4ec5a4b0954a5cc0d287a18a1263ba",
    "semantic_title": "improving stance detection with multi-dataset learning and knowledge distillation",
    "citation_count": 28,
    "authors": [
      "Yingjie Li",
      "Chenye Zhao",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.512": {
    "title": "Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering",
    "volume": "main",
    "abstract": "Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples — there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the \"unknowns\" to the learned VQA model are indeed \"known\" in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the \"how many\" question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SimpleAug to turn this \"known\" knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models' performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/heendung/simpleAUG",
    "checked": true,
    "id": "467e5a2164cf78c5be70c91129e1c6e843685fb3",
    "semantic_title": "discovering the unknown knowns: turning implicit knowledge in the dataset into explicit training examples for visual question answering",
    "citation_count": 20,
    "authors": [
      "Jihyung Kil",
      "Cheng Zhang",
      "Dong Xuan",
      "Wei-Lun Chao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.513": {
    "title": "Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding",
    "volume": "main",
    "abstract": "Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without fine-tuning. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing representation generality",
    "checked": true,
    "id": "25d58d980001cdd224f7a9c95a02e88d64a71cf8",
    "semantic_title": "improving pre-trained vision-and-language embeddings for phrase grounding",
    "citation_count": 10,
    "authors": [
      "Zi-Yi Dou",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.514": {
    "title": "Sequential Randomized Smoothing for Adversarially Robust Speech Recognition",
    "volume": "main",
    "abstract": "While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be effective at defending models. However, it is difficult to apply this paradigm to ASR tasks, due to their complexity and the sequential nature of their outputs. Our paper overcomes some of these challenges by leveraging speech-specific tools like enhancement and ROVER voting to design an ASR model that is robust to perturbations. We apply adaptive versions of state-of-the-art attacks, such as the Imperceptible ASR attack, to our model, and show that our strongest defense is robust to all attacks that use inaudible noise, and can only be broken with very high distortion",
    "checked": true,
    "id": "b0754c0a3ac2a18ff6d3f721ab53401a790075e1",
    "semantic_title": "sequential randomized smoothing for adversarially robust speech recognition",
    "citation_count": 11,
    "authors": [
      "Raphael Olivier",
      "Bhiksha Raj"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.515": {
    "title": "Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video",
    "volume": "main",
    "abstract": "The combination of gestures, intonations, and textual content plays a key role in argument delivery. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to datasets containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate centric features such as clarity, content variation, body movement cues, and pauses, inspired by theories of argumentation quality. Second, we design the Multimodal ARgument Quality assessor (MARQ) – a hierarchical neural network model that summarizes the multimodal signals on long sequences and enriches the multimodal embedding with debate centric features. Our proposed MARQ model achieves an accuracy of 81.91% on the argument quality prediction task and outperforms established baseline models with an error rate reduction of 22.7%. Through ablation studies, we demonstrate the importance of multimodal cues in modeling argument quality",
    "checked": true,
    "id": "fab3283d3d52ed38ca2c7f0247e97c9485a03f1c",
    "semantic_title": "hitting your marq: multimodal argument quality assessment in long debate video",
    "citation_count": 5,
    "authors": [
      "Md Kamrul Hasan",
      "James Spann",
      "Masum Hasan",
      "Md Saiful Islam",
      "Kurtis Haut",
      "Rada Mihalcea",
      "Ehsan Hoque"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.516": {
    "title": "Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions",
    "volume": "main",
    "abstract": "Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. \"dark cube on the left\" vs. \"black cube on the left\"). In this work, we address these limitations and evaluate the impact of contextual clues in improving the performance of NMN models. First, we address the problem of fixed textual inputs by parameterizing the module arguments. This substantially reduce the number of modules in NMN by up to 75% without any loss in performance. Next we propose a method to contextualize our parameterized model to enhance the module's capacity in exploiting the visiolinguistic associations. Our model outperforms the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1% improvement in accuracy on the single-referent test set and +4.3% on the full test set. Additionally, we demonstrate that contextualization provides +11.2% and +1.7% improvements in accuracy over prior NMN models on CLOSURE and NLVR2. We further evaluate the impact of our contextualization by constructing a contrast set for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the baselines by as much as +10.4% absolute accuracy on CC-Ref+, illustrating the generalization skills of our approach",
    "checked": true,
    "id": "d68255e8210843118d641175105e69686ad5b40f",
    "semantic_title": "mind the context: the impact of contextualization in neural module networks for grounding visual referring expressions",
    "citation_count": 12,
    "authors": [
      "Arjun Akula",
      "Spandana Gella",
      "Keze Wang",
      "Song-Chun Zhu",
      "Siva Reddy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.517": {
    "title": "Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering",
    "volume": "main",
    "abstract": "Knowledge-based visual question answering (VQA) requires answering questions with external knowledge in addition to the content of images. One dataset that is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold standard knowledge corpus for retrieval. Existing work leverage different knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge. Because of varying knowledge bases, it is hard to fairly compare models' performance. To address this issue, we collect a natural language knowledge base that can be used for any VQA system. Moreover, we propose a Visual Retriever-Reader pipeline to approach knowledge-based VQA. The visual retriever aims to retrieve relevant knowledge, and the visual reader seeks to predict answers based on given knowledge. We introduce various ways to retrieve knowledge using text and images and two reader styles: classification and extraction. Both the retriever and reader are trained with weak supervision. Our experimental results show that a good retriever can significantly improve the reader's performance on the OK-VQA challenge. The code and corpus are provided in https://github.com/luomancs/retriever_reader_for_okvqa.git",
    "checked": true,
    "id": "4e92fec0a61972ae076707d0630d1333affccdfc",
    "semantic_title": "weakly-supervised visual-retriever-reader for knowledge-based question answering",
    "citation_count": 67,
    "authors": [
      "Man Luo",
      "Yankai Zeng",
      "Pratyay Banerjee",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.518": {
    "title": "NDH-Full: Learning and Evaluating Navigational Agents on Full-Length Dialogue",
    "volume": "main",
    "abstract": "Communication between human and mobile agents is getting increasingly important as such agents are widely deployed in our daily lives. Vision-and-Dialogue Navigation is one of the tasks that evaluate the agent's ability to interact with humans for assistance and navigate based on natural language responses. In this paper, we explore the Navigation from Dialogue History (NDH) task, which is based on the Cooperative Vision-and-Dialogue Navigation (CVDN) dataset, and present a state-of-the-art model which is built upon Vision-Language transformers. However, despite achieving competitive performance, we find that the agent in the NDH task is not evaluated appropriately by the primary metric – Goal Progress. By analyzing the performance mismatch between Goal Progress and other metrics (e.g., normalized Dynamic Time Warping) from our state-of-the-art model, we show that NDH's sub-path based task setup (i.e., navigating partial trajectory based on its correspondent subset of the full dialogue) does not provide the agent with enough supervision signal towards the goal region. Therefore, we propose a new task setup called NDH-Full which takes the full dialogue and the whole navigation path as one instance. We present a strong baseline model and show initial results on this new task. We further describe several approaches that we try, in order to improve the model performance (based on curriculum learning, pre-training, and data-augmentation), suggesting potential useful training methods on this new NDH-Full task",
    "checked": true,
    "id": "a00369391b59b4edfc19dd02bc2f81a93f9cb9da",
    "semantic_title": "ndh-full: learning and evaluating navigational agents on full-length dialogue",
    "citation_count": 15,
    "authors": [
      "Hyounghun Kim",
      "Jialu Li",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.519": {
    "title": "Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport",
    "volume": "main",
    "abstract": "Timeline Summarization identifies major events from a news collection and describes them following temporal order, with key dates tagged. Previous methods generally generate summaries separately for each date after they determine the key dates of events. These methods overlook the events' intra-structures (arguments) and inter-structures (event-event connections). Following a different route, we propose to represent the news articles as an event-graph, thus the summarization becomes compressing the whole graph to its salient sub-graph. The key hypothesis is that the events connected through shared arguments and temporal order depict the skeleton of a timeline, containing events that are semantically related, temporally coherent and structurally salient in the global event graph. A time-aware optimal transport distance is then introduced for learning the compression model in an unsupervised manner. We show that our approach significantly improves on the state of the art on three real-world datasets, including two public standard benchmarks and our newly collected Timeline100 dataset",
    "checked": true,
    "id": "7ed78c8ee10da27643dfce64068bea9216da58ad",
    "semantic_title": "timeline summarization based on event graph compression via time-aware optimal transport",
    "citation_count": 36,
    "authors": [
      "Manling Li",
      "Tengfei Ma",
      "Mo Yu",
      "Lingfei Wu",
      "Tian Gao",
      "Heng Ji",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.520": {
    "title": "StreamHover: Livestream Transcript Summarization and Annotation",
    "volume": "main",
    "abstract": "With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further, there has been a shortage of annotated datasets that are necessary for transcript summarization. In this paper, we present StreamHover, a framework for annotating and summarizing livestream transcripts. With a total of over 500 hours of videos annotated with both extractive and abstractive summaries, our benchmark dataset is significantly larger than currently existing annotated corpora. We explore a neural extractive summarization model that leverages vector-quantized variational autoencoder to learn latent vector representations of spoken utterances and identify salient utterances from the transcripts to form summaries. We show that our model generalizes better and improves performance over strong baselines. The results of this study provide an avenue for future research to improve summarization solutions for efficient browsing of livestreams",
    "checked": true,
    "id": "e76199600e884bdf096fa176010693d081351394",
    "semantic_title": "streamhover: livestream transcript summarization and annotation",
    "citation_count": 29,
    "authors": [
      "Sangwoo Cho",
      "Franck Dernoncourt",
      "Tim Ganter",
      "Trung Bui",
      "Nedim Lipka",
      "Walter Chang",
      "Hailin Jin",
      "Jonathan Brandt",
      "Hassan Foroosh",
      "Fei Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.521": {
    "title": "Cross-Register Projection for Headline Part of Speech Tagging",
    "volume": "main",
    "abstract": "Part of speech (POS) tagging is a familiar NLP task. State of the art taggers routinely achieve token-level accuracies of over 97% on news body text, evidence that the problem is well understood. However, the register of English news headlines, \"headlinese\", is very different from the register of long-form text, causing POS tagging models to underperform on headlines. In this work, we automatically annotate news headlines with POS tags by projecting predicted tags from corresponding sentences in news bodies. We train a multi-domain POS tagger on both long-form and headline text and show that joint training on both registers improves over training on just one or naïvely concatenating training sets. We evaluate on a newly-annotated corpus of over 5,248 English news headlines from the Google sentence compression corpus, and show that our model yields a 23% relative error reduction per token and 19% per headline. In addition, we demonstrate that better headline POS tags can improve the performance of a syntax-based open information extraction system. We make POSH, the POS-tagged Headline corpus, available to encourage research in improved NLP models for news headlines",
    "checked": true,
    "id": "404fb9d629ca932f5dd6a7053fd2936939de9e8f",
    "semantic_title": "cross-register projection for headline part of speech tagging",
    "citation_count": 2,
    "authors": [
      "Adrian Benton",
      "Hanyang Li",
      "Igor Malioutov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.522": {
    "title": "Editing Factual Knowledge in Language Models",
    "volume": "main",
    "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix ‘bugs' or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a ‘probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor",
    "checked": true,
    "id": "240b0caabb415578bdea4da7d0a32bdff2e8163f",
    "semantic_title": "editing factual knowledge in language models",
    "citation_count": 526,
    "authors": [
      "Nicola De Cao",
      "Wilker Aziz",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.523": {
    "title": "Sparse Attention with Linear Units",
    "volume": "main",
    "abstract": "Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. ‘switch off') for some queries, which is not possible with sparsified softmax alternatives",
    "checked": true,
    "id": "a7721b6523971394a8bd4bfda139122ef59b22cd",
    "semantic_title": "sparse attention with linear units",
    "citation_count": 42,
    "authors": [
      "Biao Zhang",
      "Ivan Titov",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.524": {
    "title": "Knowledge Base Completion Meets Transfer Learning",
    "volume": "main",
    "abstract": "The aim of knowledge base completion is to predict unseen facts from existing facts in knowledge bases. In this work, we introduce the first approach for transfer of knowledge from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. Such knowledge bases are a natural output of automated information extraction tools that extract structured data from unstructured text. Our main contribution is a method that can make use of a large-scale pretraining on facts, collected from unstructured text, to improve predictions on structured data from a specific domain. The introduced method is the most impactful on small datasets such as ReVerb20K, where we obtained a 6% absolute increase of mean reciprocal rank and 65% relative decrease of mean rank over the previously best method, despite not relying on large pre-trained models like BERT",
    "checked": true,
    "id": "c04a7387338b767d88335e20f6d51b55968c7915",
    "semantic_title": "knowledge base completion meets transfer learning",
    "citation_count": 6,
    "authors": [
      "Vid Kocijan",
      "Thomas Lukasiewicz"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.525": {
    "title": "SPECTRA: Sparse Structured Text Rationalization",
    "volume": "main",
    "abstract": "Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction (e.g., to control the sparsity of a text highlight or the number of alignments). In this paper, we present a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer. Our approach greatly eases training and rationale regularization, generally outperforming previous work on what comes to performance and plausibility of the extracted rationales. We further provide a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks, jointly assessing their predictive power, quality of the explanations, and model variability",
    "checked": true,
    "id": "ee95231fe084ff59c1a7fb027be58a9759cd4e6f",
    "semantic_title": "spectra: sparse structured text rationalization",
    "citation_count": 28,
    "authors": [
      "Nuno M. Guerreiro",
      "André F. T. Martins"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.526": {
    "title": "Towards Zero-Shot Knowledge Distillation for Natural Language Processing",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher's training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and adversarial training to learn the teacher's output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher's classification score (accuracy or F1) while compressing the model 30 times",
    "checked": true,
    "id": "8b28d9e3ca408b8a41d32f8bd4da7fbbd4f12a4b",
    "semantic_title": "towards zero-shot knowledge distillation for natural language processing",
    "citation_count": 28,
    "authors": [
      "Ahmad Rashid",
      "Vasileios Lioutas",
      "Abbas Ghaddar",
      "Mehdi Rezagholizadeh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.527": {
    "title": "Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach",
    "volume": "main",
    "abstract": "Adversarial regularization has been shown to improve the generalization performance of deep learning models in various natural language processing tasks. Existing works usually formulate the method as a zero-sum game, which is solved by alternating gradient descent/ascent algorithms. Such a formulation treats the adversarial and the defending players equally, which is undesirable because only the defending player contributes to the generalization performance. To address this issue, we propose Stackelberg Adversarial Regularization (SALT), which formulates adversarial regularization as a Stackelberg game. This formulation induces a competition between a leader and a follower, where the follower generates perturbations, and the leader trains the model subject to the perturbations. Different from conventional approaches, in SALT, the leader is in an advantageous position. When the leader moves, it recognizes the strategy of the follower and takes the anticipated follower's outcomes into consideration. Such a leader's advantage enables us to improve the model fitting to the unperturbed data. The leader's strategic information is captured by the Stackelberg gradient, which is obtained using an unrolling algorithm. Our experimental results on a set of machine translation and natural language understanding tasks show that SALT outperforms existing adversarial regularization baselines across all tasks. Our code is publicly available",
    "checked": true,
    "id": "8e88f835acdabf16eda88ece9c5acedac64a6274",
    "semantic_title": "adversarial regularization as stackelberg game: an unrolled optimization approach",
    "citation_count": 9,
    "authors": [
      "Simiao Zuo",
      "Chen Liang",
      "Haoming Jiang",
      "Xiaodong Liu",
      "Pengcheng He",
      "Jianfeng Gao",
      "Weizhu Chen",
      "Tuo Zhao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.528": {
    "title": "Aspect-Controllable Opinion Summarization",
    "volume": "main",
    "abstract": "Recent work on opinion summarization produces general summaries based on a set of input reviews and the popularity of opinions expressed in them. In this paper, we propose an approach that allows the generation of customized summaries based on aspect queries (e.g., describing the location and room of a hotel). Using a review corpus, we create a synthetic training dataset of (review, summary) pairs enriched with aspect controllers which are induced by a multi-instance learning model that predicts the aspects of a document at different levels of granularity. We fine-tune a pretrained model using our synthetic dataset and generate aspect-specific summaries by modifying the aspect controllers. Experiments on two benchmarks show that our model outperforms the previous state of the art and generates personalized summaries by controlling the number of aspects discussed in them",
    "checked": true,
    "id": "c4247abf33146aaacd9fe288a94a1ff1a22abce9",
    "semantic_title": "aspect-controllable opinion summarization",
    "citation_count": 77,
    "authors": [
      "Reinald Kim Amplayo",
      "Stefanos Angelidis",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.529": {
    "title": "QuestEval: Summarization Asks for Fact-based Evaluation",
    "volume": "main",
    "abstract": "Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments. In this paper, we extend previous approaches and propose a unified framework, named QuestEval. In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any ground-truth reference. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in extensive experiments",
    "checked": true,
    "id": "6ca07b95e6b68798e89888d3c2f5f43938feb419",
    "semantic_title": "questeval: summarization asks for fact-based evaluation",
    "citation_count": 281,
    "authors": [
      "Thomas Scialom",
      "Paul-Alexis Dray",
      "Sylvain Lamprier",
      "Benjamin Piwowarski",
      "Jacopo Staiano",
      "Alex Wang",
      "Patrick Gallinari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.530": {
    "title": "Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue Summarization",
    "volume": "main",
    "abstract": "Abstractive conversation summarization has received growing attention while most current state-of-the-art summarization models heavily rely on human-annotated summaries. To reduce the dependence on labeled summaries, in this work, we present a simple yet effective set of Conversational Data Augmentation (CODA) methods for semi-supervised abstractive conversation summarization, such as random swapping/deletion to perturb the discourse relations inside conversations, dialogue-acts-guided insertion to interrupt the development of conversations, and conditional-generation-based substitution to substitute utterances with their paraphrases generated based on the conversation context. To further utilize unlabeled conversations, we combine CODA with two-stage noisy self-training where we first pre-train the summarization model on unlabeled conversations with pseudo summaries and then fine-tune it on labeled conversations. Experiments conducted on the recent conversation summarization datasets demonstrate the effectiveness of our methods over several state-of-the-art data augmentation baselines",
    "checked": true,
    "id": "b519511993b63423be6e3580cf3ce63ea77e9e2f",
    "semantic_title": "simple conversational data augmentation for semi-supervised abstractive dialogue summarization",
    "citation_count": 42,
    "authors": [
      "Jiaao Chen",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.531": {
    "title": "Finding a Balanced Degree of Automation for Summary Evaluation",
    "volume": "main",
    "abstract": "Human evaluation for summarization tasks is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with human judgment. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs' presence in system summaries with a natural language inference (NLI) model. Fully automatic Lite3Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. Finally, we propose in-between metrics, Lite2.xPyramid, where we use a simple regressor to predict how well the STUs can simulate SCUs and retain SCUs that are more difficult to simulate, which provides a smooth transition and balance between automation and manual evaluation. Comparing to 15 existing metrics, we evaluate human-metric correlations on 3 existing meta-evaluation datasets and our newly collected PyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid consistently has the best summary-level correlations; Lite3Pyramid works better than or comparable to other automatic metrics; Lite2.xPyramid trades off small correlation drops for larger manual effort reduction, which can reduce costs for future data collection",
    "checked": true,
    "id": "388bd29361dce29fd009358618813f3c3f1edfed",
    "semantic_title": "finding a balanced degree of automation for summary evaluation",
    "citation_count": 47,
    "authors": [
      "Shiyue Zhang",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.532": {
    "title": "CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization",
    "volume": "main",
    "abstract": "We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by two state-of-the-art models, BART and PEGASUS, found in our new human annotations of summary errors. Experiments on XSum and CNN/Daily Mail show that our contrastive learning framework is robust across datasets and models. It consistently produces more factual summaries than strong comparisons with post error correction, entailment-based reranking, and unlikelihood training, according to QA-based factuality evaluation. Human judges echo the observation and find that our model summaries correct more errors",
    "checked": true,
    "id": "3cf51f5f36bac0cbafdb7581d8713979e0d1c17e",
    "semantic_title": "cliff: contrastive learning for improving faithfulness and factuality in abstractive summarization",
    "citation_count": 183,
    "authors": [
      "Shuyang Cao",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.533": {
    "title": "Multilingual Unsupervised Neural Machine Translation with Denoising Adapters",
    "volume": "main",
    "abstract": "We consider the problem of multilingual unsupervised machine translation, translating to and from languages that only have monolingual data by using auxiliary parallel language pairs. For this problem the standard procedure so far to leverage the monolingual data is _back-translation_, which is computationally costly and hard to tune. In this paper we propose instead to use _denoising adapters_, adapter layers with a denoising objective, on top of pre-trained mBART-50. In addition to the modularity and flexibility of such an approach we show that the resulting translations are on-par with back-translating as measured by BLEU, and furthermore it allows adding unseen languages incrementally",
    "checked": true,
    "id": "99b12d0df2b93e800207a5e4618a353912f3dff8",
    "semantic_title": "multilingual unsupervised neural machine translation with denoising adapters",
    "citation_count": 47,
    "authors": [
      "Ahmet Üstün",
      "Alexandre Berard",
      "Laurent Besacier",
      "Matthias Gallé"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.534": {
    "title": "BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation",
    "volume": "main",
    "abstract": "The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pre-trained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of a dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En→De and 38.61 for De→En on the IWSLT'14 dataset, and 31.26 for En→De and 34.94 for De→En on the WMT'14 dataset, which exceeds all published numbers",
    "checked": true,
    "id": "2607dce6dcb9043ca9cae67e25e6a24411f08c0b",
    "semantic_title": "bert, mbert, or bibert? a study on contextualized embeddings for neural machine translation",
    "citation_count": 63,
    "authors": [
      "Haoran Xu",
      "Benjamin Van Durme",
      "Kenton Murray"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.535": {
    "title": "Controlling Machine Translation for Multiple Attributes with Additive Interventions",
    "volume": "main",
    "abstract": "Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users' trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks: continuous values must be binned into discrete categories, which is unnatural for certain applications; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a model trained without annotations to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better control over a wider range of tasks compared to tagging, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable control in an already trained model after a relatively cheap fine-tuning stage",
    "checked": true,
    "id": "9b4f31c44e2e2579bc5df35a709b70036dddc710",
    "semantic_title": "controlling machine translation for multiple attributes with additive interventions",
    "citation_count": 31,
    "authors": [
      "Andrea Schioppa",
      "David Vilar",
      "Artem Sokolov",
      "Katja Filippova"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.536": {
    "title": "A Generative Framework for Simultaneous Machine Translation",
    "volume": "main",
    "abstract": "We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural sequence-to-sequence learning problem. A latent variable is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the model to explicitly balance translation quality and latency. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets",
    "checked": true,
    "id": "b05386241ac7cce9abc154f580d0a6376a77ecc8",
    "semantic_title": "a generative framework for simultaneous machine translation",
    "citation_count": 28,
    "authors": [
      "Yishu Miao",
      "Phil Blunsom",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.537": {
    "title": "It Is Not As Good As You Think! Evaluating Simultaneous Machine Translation on Interpretation Data",
    "volume": "main",
    "abstract": "Most existing simultaneous machine translation (SiMT) systems are trained and evaluated on offline translation corpora. We argue that SiMT systems should be trained and tested on real interpretation data. To illustrate this argument, we propose an interpretation test set and conduct a realistic evaluation of SiMT trained on offline translations. Our results, on our test set along with 3 existing smaller scale language pairs, highlight the difference of up-to 13.83 BLEU score when SiMT models are evaluated on translation vs interpretation data. In the absence of interpretation training data, we propose a translation-to-interpretation (T2I) style transfer method which allows converting existing offline translations into interpretation-style data, leading to up-to 2.8 BLEU improvement. However, the evaluation gap remains notable, calling for constructing large-scale interpretation corpora better suited for evaluating and developing SiMT systems",
    "checked": true,
    "id": "2ee442b040a59ee4db34851f262be1a5f0685605",
    "semantic_title": "it is not as good as you think! evaluating simultaneous machine translation on interpretation data",
    "citation_count": 12,
    "authors": [
      "Jinming Zhao",
      "Philip Arthur",
      "Gholamreza Haffari",
      "Trevor Cohn",
      "Ehsan Shareghi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.538": {
    "title": "Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation",
    "volume": "main",
    "abstract": "Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer: Language Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 for NER on average and 2.5 accuracy for NLI",
    "checked": true,
    "id": "1297f087e4d539cf7b322641f98e4a15a90b6bc1",
    "semantic_title": "boosting cross-lingual transfer via self-learning with uncertainty estimation",
    "citation_count": 14,
    "authors": [
      "Liyan Xu",
      "Xuchao Zhang",
      "Xujiang Zhao",
      "Haifeng Chen",
      "Feng Chen",
      "Jinho D. Choi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.539": {
    "title": "Levenshtein Training for Word-level Quality Estimation",
    "volume": "main",
    "abstract": "We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting",
    "checked": true,
    "id": "e68e29ea60c3803c5b425bb959a42e2376819bd4",
    "semantic_title": "levenshtein training for word-level quality estimation",
    "citation_count": 7,
    "authors": [
      "Shuoyang Ding",
      "Marcin Junczys-Dowmunt",
      "Matt Post",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.540": {
    "title": "Interactive Machine Comprehension with Dynamic Knowledge Graphs",
    "volume": "main",
    "abstract": "Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that graph representations are good inductive biases, which can serve as an agent's memory mechanism in iMRC tasks. We explore four different categories of graphs that can capture text information at various levels. We describe methods that dynamically build and update these graphs during information gathering, as well as neural models to encode graph representations in RL agents. Extensive experiments on iSQuAD suggest that graph representations can result in significant performance improvements for RL agents",
    "checked": true,
    "id": "776bb801f581d6d052e695ae5d2e1e82e525e8b3",
    "semantic_title": "interactive machine comprehension with dynamic knowledge graphs",
    "citation_count": 3,
    "authors": [
      "Xingdi Yuan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.541": {
    "title": "Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) systems are often optimized to work best for speakers with canonical speech patterns. Unfortunately, these systems perform poorly when tested on atypical speech and heavily accented speech. It has previously been shown that personalization through model fine-tuning substantially improves performance. However, maintaining such large models per speaker is costly and difficult to scale. We show that by adding a relatively small number of extra parameters to the encoder layers via so-called residual adapter, we can achieve similar adaptation gains compared to model fine-tuning, while only updating a tiny fraction (less than 0.5%) of the model parameters. We demonstrate this on two speech adaptation tasks (atypical and accented speech) and for two state-of-the-art ASR architectures",
    "checked": true,
    "id": "dac5711aea72aba9c607f049e295da5f8abb3c93",
    "semantic_title": "residual adapters for parameter-efficient asr adaptation to atypical and accented speech",
    "citation_count": 58,
    "authors": [
      "Katrin Tomanek",
      "Vicky Zayats",
      "Dirk Padfield",
      "Kara Vaillancourt",
      "Fadi Biadsy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.542": {
    "title": "Visual News: Benchmark and Challenges in News Image Captioning",
    "volume": "main",
    "abstract": "We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other metadata. Unlike the standard image captioning task, news images depict situations where people, locations, and events are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as events and entities. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our method utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images",
    "checked": true,
    "id": "60b508a92f73f7e7f14380b45f75d5f0df9548fe",
    "semantic_title": "visual news: benchmark and challenges in news image captioning",
    "citation_count": 121,
    "authors": [
      "Fuxiao Liu",
      "Yinghan Wang",
      "Tianlu Wang",
      "Vicente Ordonez"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.543": {
    "title": "Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization",
    "volume": "main",
    "abstract": "While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit narrative structure that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, consistency and relevance. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters/objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in visual quality. Finally, we provide an analysis of the linguistic and visuo-spatial information",
    "checked": true,
    "id": "ee1d0e141530190a2b0d834737096c5bfda6b304",
    "semantic_title": "integrating visuospatial, linguistic, and commonsense structure into story visualization",
    "citation_count": 63,
    "authors": [
      "Adyasha Maharana",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.544": {
    "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
    "volume": "main",
    "abstract": "We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT",
    "checked": true,
    "id": "821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0",
    "semantic_title": "videoclip: contrastive pre-training for zero-shot video-text understanding",
    "citation_count": 600,
    "authors": [
      "Hu Xu",
      "Gargi Ghosh",
      "Po-Yao Huang",
      "Dmytro Okhonko",
      "Armen Aghajanyan",
      "Florian Metze",
      "Luke Zettlemoyer",
      "Christoph Feichtenhofer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.545": {
    "title": "NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media",
    "volume": "main",
    "abstract": "Online misinformation is a prevalent societal issue, with adversaries relying on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated by the threat scenario where an image is used out of context to support a certain narrative. While some prior datasets for detecting image-text inconsistency generate samples via text manipulation, we propose a dataset where both image and text are unmanipulated but mismatched. We introduce several strategies for automatically retrieving convincing images for a given caption, capturing cases with inconsistent entities or semantic context. Our large-scale automatically generated the NewsCLIPpings Dataset: (1) demonstrates that machine-driven image repurposing is now a realistic threat, and (2) provides samples that represent challenging instances of mismatch between text and image in news that are able to mislead humans. We benchmark several state-of-the-art multimodal models on our dataset and analyze their performance across different pretraining domains and visual backbones",
    "checked": true,
    "id": "89e7626d9b94e05c4154e7f6fb6df146fa4eb8fa",
    "semantic_title": "newsclippings: automatic generation of out-of-context multimodal media",
    "citation_count": 97,
    "authors": [
      "Grace Luo",
      "Trevor Darrell",
      "Anna Rohrbach"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.546": {
    "title": "Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer",
    "volume": "main",
    "abstract": "We study Comparative Preference Classification (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which entity is preferred over the other. High-quality CPC models can significantly benefit applications such as comparative question answering and review-based recommendation. Among the existing approaches, non-deep learning methods suffer from inferior performances. The state-of-the-art graph neural network-based ED-GAT (Ma et al., 2020) only considers syntactic information while ignoring the critical semantic relations and the sentiments to the compared entities. We propose Sentiment Analysis Enhanced COmparative Network (SAECON) which improves CPC accuracy with a sentiment analyzer that learns sentiments to individual entities via domain adaptive knowledge transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset present a significant improvement on the F1 scores over the best existing CPC approaches",
    "checked": true,
    "id": "0e6a761bbac6bda2d68457a54c0c1c893d2217aa",
    "semantic_title": "powering comparative classification with sentiment analysis via domain adaptive knowledge transfer",
    "citation_count": 4,
    "authors": [
      "Zeyu Li",
      "Yilong Qin",
      "Zihan Liu",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.547": {
    "title": "Tribrid: Stance Classification with Neural Inconsistency Detection",
    "volume": "main",
    "abstract": "We study the problem of performing automatic stance classification on social media with neural architectures such as BERT. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on the downstream task (e.g., fact-checking). To improve the performance, we present a new neural architecture where the input also includes automatically generated negated perspectives over a given claim. The model is jointly learned to make simultaneously multiple predictions, which can be used either to improve the classification of the original perspective or to filter out doubtful predictions. In the first case, we propose a weakly supervised method for combining the predictions into a final one. In the second case, we show that using the confidence scores to remove doubtful predictions allows our method to achieve human-like performance over the retained information, which is still a sizable part of the original input",
    "checked": true,
    "id": "50bf2e734fedaf471b3023312b43c200c7400b1c",
    "semantic_title": "tribrid: stance classification with neural inconsistency detection",
    "citation_count": 7,
    "authors": [
      "Song Yang",
      "Jacopo Urbani"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.548": {
    "title": "SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis",
    "volume": "main",
    "abstract": "Darknet market forums are frequently used to exchange illegal goods and services between parties who use encryption to conceal their identities. The Tor network is used to host these markets, which guarantees additional anonymization from IP and location tracking, making it challenging to link across malicious users using multiple accounts (sybils). Additionally, users migrate to new forums when one is closed further increasing the difficulty of linking users across multiple forums. We develop a novel stylometry-based multitask learning approach for natural language and model interactions using graph embeddings to construct low-dimensional representations of short episodes of user activity for authorship attribution. We provide a comprehensive evaluation of our methods across four different darknet forums demonstrating its efficacy over the state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X on Recall@10",
    "checked": true,
    "id": "0fbd5adb48a0e7d0fe640988b4deac0b06b1b596",
    "semantic_title": "sysml: stylometry with structure and multitask learning: implications for darknet forum migrant analysis",
    "citation_count": 10,
    "authors": [
      "Pranav Maneriker",
      "Yuntian He",
      "Srinivasan Parthasarathy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.549": {
    "title": "Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks",
    "volume": "main",
    "abstract": "Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect emotions and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such context. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two datasets with different languages: daily conversations in English and customer service chat conversations in French. When applied to emotion classification in conversations, our method proved to be competitive even when compared to other ones",
    "checked": true,
    "id": "76a134e245367d2a1d0fc35801a549d47ec98d0a",
    "semantic_title": "few-shot emotion recognition in conversation with sequential prototypical networks",
    "citation_count": 34,
    "authors": [
      "Gaël Guibon",
      "Matthieu Labeau",
      "Hélène Flamein",
      "Luce Lefeuvre",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.550": {
    "title": "CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks",
    "volume": "main",
    "abstract": "This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks in a particular CL setting called domain incremental learning (DIL). Each task is from a different domain or product. The DIL setting is particularly suited to ASC because in testing the system needs not know the task/domain to which the test data belongs. To our knowledge, this setting has not been studied before for ASC. This paper proposes a novel model called CLASSIC. The key novelty is a contrastive continual learning method that enables both knowledge transfer across tasks and knowledge distillation from old tasks to the new task, which eliminates the need for task ids in testing. Experimental results show the high effectiveness of CLASSIC",
    "checked": true,
    "id": "a99f26f191dc3d93ad611c84db5ce8d272595355",
    "semantic_title": "classic: continual and contrastive learning of aspect sentiment classification tasks",
    "citation_count": 60,
    "authors": [
      "Zixuan Ke",
      "Bing Liu",
      "Hu Xu",
      "Lei Shu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.551": {
    "title": "Implicit Sentiment Analysis with Event-centered Text Representation",
    "volume": "main",
    "abstract": "Implicit sentiment analysis, aiming at detecting the sentiment of a sentence without sentiment words, has become an attractive research topic in recent years. In this paper, we focus on event-centric implicit sentiment analysis that utilizes the sentiment-aware event contained in a sentence to infer its sentiment polarity. Most existing methods in implicit sentiment analysis simply view noun phrases or entities in text as events or indirectly model events with sophisticated models. Since events often trigger sentiments in sentences, we argue that this task would benefit from explicit modeling of events and event representation learning. To this end, we represent an event as the combination of its event type and the event triplet <subject, predicate, object>. Based on such event representation, we further propose a novel model with hierarchical tensor-based composition mechanism to detect sentiment in text. In addition, we present a dataset for event-centric implicit sentiment analysis where each sentence is labeled with the event representation described above. Experimental results on our constructed dataset and an existing benchmark dataset show the effectiveness of the proposed approach",
    "checked": true,
    "id": "2fa9698fbd3b52831b92903b39e3d7b77875cfac",
    "semantic_title": "implicit sentiment analysis with event-centered text representation",
    "citation_count": 36,
    "authors": [
      "Deyu Zhou",
      "Jianan Wang",
      "Linhai Zhang",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.552": {
    "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
    "volume": "main",
    "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available",
    "checked": true,
    "id": "c26759e6c701201af2f62f7ee4eb68742b5bf085",
    "semantic_title": "simcse: simple contrastive learning of sentence embeddings",
    "citation_count": 3501,
    "authors": [
      "Tianyu Gao",
      "Xingcheng Yao",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.553": {
    "title": "When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection",
    "volume": "main",
    "abstract": "Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun \"wall\" has different lexical manifestations in Spanish – \"pared\" refers to an indoor wall while \"muro\" refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, Spanish and Greek, where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations",
    "checked": true,
    "id": "d06493373421c86ba33dbb8834ccb725105a665f",
    "semantic_title": "when is wall a pared and when a muro?: extracting rules governing lexical selection",
    "citation_count": 3,
    "authors": [
      "Aditi Chaudhary",
      "Kayo Yin",
      "Antonios Anastasopoulos",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.554": {
    "title": "Aligning Actions Across Recipe Graphs",
    "volume": "main",
    "abstract": "Recipe texts are an idiosyncratic form of instructional language that pose unique challenges for automatic understanding. One challenge is that a cooking step in one recipe can be explained in another recipe in different words, at a different level of abstraction, or not at all. Previous work has annotated correspondences between recipe instructions at the sentence level, often glossing over important correspondences between cooking steps across recipes. We present a novel and fully-parsed English recipe corpus, ARA (Aligned Recipe Actions), which annotates correspondences between individual actions across similar recipes with the goal of capturing information implicit for accurate recipe understanding. We represent this information in the form of recipe graphs, and we train a neural model for predicting correspondences on ARA. We find that substantial gains in accuracy can be obtained by taking fine-grained structural information about the recipes into account",
    "checked": true,
    "id": "f7e68690d30e7ba9aceb389987278bbf35c6bca7",
    "semantic_title": "aligning actions across recipe graphs",
    "citation_count": 26,
    "authors": [
      "Lucia Donatelli",
      "Theresa Schmidt",
      "Debanjali Biswas",
      "Arne Köhn",
      "Fangzhou Zhai",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.555": {
    "title": "Generating Datasets with Pretrained Language Models",
    "volume": "main",
    "abstract": "To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets",
    "checked": true,
    "id": "b769b629c8de35b16735214251d6b4e99cb55762",
    "semantic_title": "generating datasets with pretrained language models",
    "citation_count": 241,
    "authors": [
      "Timo Schick",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.556": {
    "title": "Continuous Entailment Patterns for Lexical Inference in Context",
    "volume": "main",
    "abstract": "Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design patterns that closely resemble the text seen during self-supervised pretraining because the model has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM's vocabulary, patterns can be adapted more flexibly to a PLM's idiosyncrasies. Contrasting patterns where a \"token\" can be any continuous vector from those where a discrete choice between vocabulary elements has to be made, we call our method CONtinous pAtterNs (CONAN). We evaluate CONAN on two established benchmarks for lexical inference in context (LIiC) a.k.a. predicate entailment, a challenging natural language understanding task with relatively small training data. In a direct comparison with discrete patterns, CONAN consistently leads to improved performance, setting a new state of the art. Our experiments give valuable insights on the kind of pattern that enhances a PLM's performance on LIiC and raise important questions regarding our understanding of PLMs using text patterns",
    "checked": true,
    "id": "f4dc71a476c00bdc4148fb3e01acacc6a1556041",
    "semantic_title": "continuous entailment patterns for lexical inference in context",
    "citation_count": 3,
    "authors": [
      "Martin Schmitt",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.557": {
    "title": "Numeracy enhances the Literacy of Language Models",
    "volume": "main",
    "abstract": "Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your ‘room' but not 500. Does a better grasp of numbers improve a model's understanding of other concepts and words? This paper studies the effect of using six different number encoders on the task of masked word prediction (MWP), as a proxy for evaluating literacy. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn",
    "checked": true,
    "id": "31699d03a49e38295298f1b1a53185644abba12e",
    "semantic_title": "numeracy enhances the literacy of language models",
    "citation_count": 26,
    "authors": [
      "Avijit Thawani",
      "Jay Pujara",
      "Filip Ilievski"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.558": {
    "title": "Students Who Study Together Learn Better: On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification",
    "volume": "main",
    "abstract": "While neural networks produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization to apply: a little helps reduce overfitting, but too much discards useful information. We propose Group Learning, a knowledge and model distillation approach for fact verification in which multiple student models have access to different delexicalized views of the data, but are encouraged to learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset, and outperforms state-of-the-art classifiers that rely on the original data",
    "checked": true,
    "id": "5dd4d6e5467ddb83365025e665b12a2a92741176",
    "semantic_title": "students who study together learn better: on the importance of collective knowledge distillation for domain transfer in fact verification",
    "citation_count": 1,
    "authors": [
      "Mitch Paul Mithun",
      "Sandeep Suntwal",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.559": {
    "title": "MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer",
    "volume": "main",
    "abstract": "We introduce MULTI-EURLEX, a new multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. We highlight the effect of temporal concept drift and the importance of chronological, instead of random splits. We use the dataset as a testbed for zero-shot cross-lingual transfer, where we exploit annotated training documents in one language (source) to classify documents in another language (target). We find that fine-tuning a multilingually pretrained model (XLM-ROBERTA, MT5) in a single source language leads to catastrophic forgetting of multilingual knowledge and, consequently, poor zero-shot transfer to other languages. Adaptation strategies, namely partial fine-tuning, adapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially improving zero-shot cross-lingual transfer, but their impact also depends on the pretrained model used and the size of the label set",
    "checked": true,
    "id": "e8d84c7c098f087eb935ff5eee87e0e585d6c758",
    "semantic_title": "multieurlex - a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer",
    "citation_count": 112,
    "authors": [
      "Ilias Chalkidis",
      "Manos Fergadiotis",
      "Ion Androutsopoulos"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.560": {
    "title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval",
    "volume": "main",
    "abstract": "We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single-answer retrieval is limited as it cannot reason about the set of passages jointly. In this paper, we introduce JPR, a joint passage retrieval model focusing on reranking. To model the joint probability of the retrieved passages, JPR makes use of an autoregressive reranker that selects a sequence of passages, equipped with novel training and decoding algorithms. Compared to prior approaches, JPR achieves significantly better answer coverage on three multi-answer datasets. When combined with downstream question answering, the improved retrieval enables larger answer generation models since they need to consider fewer passages, establishing a new state-of-the-art",
    "checked": true,
    "id": "baf34ac4080a365a7cec30b6877fa1a018eb31cf",
    "semantic_title": "joint passage ranking for diverse multi-answer retrieval",
    "citation_count": 43,
    "authors": [
      "Sewon Min",
      "Kenton Lee",
      "Ming-Wei Chang",
      "Kristina Toutanova",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.561": {
    "title": "Generative Context Pair Selection for Multi-hop Question Answering",
    "volume": "main",
    "abstract": "Compositional reasoning tasks such as multi-hop question answering require models to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, while being comparable to the state-of-the-art answering performance, our proposed generative passage selection model has a better performance (4.9% higher than baseline) on adversarial held-out set which tests robustness of model's multi-hop reasoning capabilities",
    "checked": true,
    "id": "553b3d53dff9a36f1266c5c97ad09e0c0e8b51d7",
    "semantic_title": "generative context pair selection for multi-hop question answering",
    "citation_count": 2,
    "authors": [
      "Dheeru Dua",
      "Cicero Nogueira dos Santos",
      "Patrick Ng",
      "Ben Athiwaratkun",
      "Bing Xiang",
      "Matt Gardner",
      "Sameer Singh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.562": {
    "title": "Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering",
    "volume": "main",
    "abstract": "Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only. We report a new state-of-the-art on four datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr)",
    "checked": true,
    "id": "486dbc28a7c6ccbfb1cbaee4222d974eda0beb38",
    "semantic_title": "synthetic data augmentation for zero-shot cross-lingual question answering",
    "citation_count": 55,
    "authors": [
      "Arij Riabi",
      "Thomas Scialom",
      "Rachel Keraron",
      "Benoît Sagot",
      "Djamé Seddah",
      "Jacopo Staiano"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.563": {
    "title": "Have You Seen That Number? Investigating Extrapolation in Question Answering Models",
    "volume": "main",
    "abstract": "Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC",
    "checked": true,
    "id": "b59947541d2ac4211c4b17554b2e16c260299bed",
    "semantic_title": "have you seen that number? investigating extrapolation in question answering models",
    "citation_count": 24,
    "authors": [
      "Jeonghwan Kim",
      "Giwon Hong",
      "Kyung-min Kim",
      "Junmo Kang",
      "Sung-Hyon Myaeng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.564": {
    "title": "Surface Form Competition: Why the Highest Probability Answer Isn't Always Right",
    "volume": "main",
    "abstract": "Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition—wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. \"computer\" and \"PC.\" Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated and uncalibrated scoring functions on all GPT-2 and GPT-3 models on a variety of multiple choice datasets",
    "checked": true,
    "id": "9d81bc8bebf1beb936427c224afb219b54a64f1e",
    "semantic_title": "surface form competition: why the highest probability answer isn't always right",
    "citation_count": 241,
    "authors": [
      "Ari Holtzman",
      "Peter West",
      "Vered Shwartz",
      "Yejin Choi",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.565": {
    "title": "Entity-Based Knowledge Conflicts in Question Answering",
    "volume": "main",
    "abstract": "Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4% - 7%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e. time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts",
    "checked": true,
    "id": "238deab37e201c57505a4a47bb854e462af79bd7",
    "semantic_title": "entity-based knowledge conflicts in question answering",
    "citation_count": 269,
    "authors": [
      "Shayne Longpre",
      "Kartik Perisetla",
      "Anthony Chen",
      "Nikhil Ramesh",
      "Chris DuBois",
      "Sameer Singh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.566": {
    "title": "Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval",
    "volume": "main",
    "abstract": "In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation (UDA). While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This significantly reduces the gap between target domain and synthetic data distribution, and reduces model overfitting to source domain. We run UDA experiments on question generation and passage retrieval from the Natural Questions domain to machine learning and biomedical domains. We find that back-training vastly outperforms self-training by a mean improvement of 7.8 BLEU-4 points on generation, and 17.6% top-20 retrieval accuracy across both domains. We further propose consistency filters to remove low-quality synthetic data before training. We also release a new domain-adaptation dataset - MLQuestions containing 35K unaligned questions, 50K unaligned passages, and 3K aligned question-passage pairs",
    "checked": true,
    "id": "7676c5e0cbd1366d23549c4a773fcfc4d21bdb0e",
    "semantic_title": "back-training excels self-training at unsupervised domain adaptation of question generation and passage retrieval",
    "citation_count": 16,
    "authors": [
      "Devang Kulshreshtha",
      "Robert Belfer",
      "Iulian Vlad Serban",
      "Siva Reddy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.567": {
    "title": "DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages",
    "volume": "main",
    "abstract": "Word meaning is notoriously difficult to capture, both synchronically and diachronically. In this paper, we describe the creation of the largest resource of graded contextualized, diachronic word meaning annotation in four different languages, based on 100,000 human semantic proximity judgments. We describe in detail the multi-round incremental annotation process, the choice for a clustering algorithm to group usages into senses, and possible – diachronic and synchronic – uses for this dataset",
    "checked": true,
    "id": "1fae23f01c34694b9124753e0d6b0ff5c55a312f",
    "semantic_title": "dwug: a large resource of diachronic word usage graphs in four languages",
    "citation_count": 51,
    "authors": [
      "Dominik Schlechtweg",
      "Nina Tahmasebi",
      "Simon Hengchen",
      "Haim Dubossarsky",
      "Barbara McGillivray"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.568": {
    "title": "I Wish I Would Have Loved This One, But I Didn't – A Multilingual Dataset for Counterfactual Detection in Product Review",
    "volume": "main",
    "abstract": "Counterfactual statements describe events that did not or cannot take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering counterfactual statements written in English, German, and Japanese languages. The dataset is unique as it contains counterfactuals in multiple languages, covers a new application area of e-commerce reviews, and provides high quality professional annotations. We train CFD models using different text representation methods and classifiers. We find that these models are robust against the selectional biases introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying machine translation on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far",
    "checked": true,
    "id": "8aea804f26e2ed8321bb57e76fb3b0b0a0964506",
    "semantic_title": "i wish i would have loved this one, but i didn't – a multilingual dataset for counterfactual detection in product review",
    "citation_count": 33,
    "authors": [
      "James O’Neill",
      "Polina Rozenshtein",
      "Ryuichi Kiryo",
      "Motoko Kubota",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.569": {
    "title": "Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework",
    "volume": "main",
    "abstract": "Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance of any model that can compare two sentences on style. We illustrate STEL with two general dimensions of style (formal/informal and simple/complex) as well as two specific characteristics of style (contrac'tion and numb3r substitution). We find that BERT-based methods outperform simple versions of commonly used style measures like 3-grams, punctuation frequency and LIWC-based approaches. We invite the addition of further tasks and task instances to STEL and hope to facilitate the improvement of style-sensitive measures",
    "checked": true,
    "id": "6b69cdcefa7cbdcc0aa95c8d341d97a1822a03ec",
    "semantic_title": "does it capture stel? a modular, similarity-based linguistic style evaluation framework",
    "citation_count": 12,
    "authors": [
      "Anna Wegmann",
      "Dong Nguyen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.570": {
    "title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts",
    "volume": "main",
    "abstract": "Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L'AMBRE – a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages",
    "checked": true,
    "id": "8b20173b98914f36302389e4c761c334fe867dcd",
    "semantic_title": "evaluating the morphosyntactic well-formedness of generated texts",
    "citation_count": 9,
    "authors": [
      "Adithya Pratapa",
      "Antonios Anastasopoulos",
      "Shruti Rijhwani",
      "Aditi Chaudhary",
      "David R. Mortensen",
      "Graham Neubig",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.571": {
    "title": "AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples",
    "volume": "main",
    "abstract": "Capturing word meaning in context and distinguishing between correspondences and variations across languages is key to building successful multilingual and cross-lingual text representation models. However, existing multilingual evaluation datasets that evaluate lexical semantics \"in-context\" have various limitations. In particular, 1) their language coverage is restricted to high-resource languages and skewed in favor of only a few language families and areas, 2) a design that makes the task solvable via superficial cues, which results in artificially inflated (and sometimes super-human) performances of pretrained encoders, and 3) no support for cross-lingual evaluation. In order to address these gaps, we present AM2iCo (Adversarial and Multilingual Meaning in Context), a wide-coverage cross-lingual and multilingual evaluation set; it aims to faithfully assess the ability of state-of-the-art (SotA) representation models to understand the identity of word meaning in cross-lingual contexts for 14 language pairs. We conduct a series of experiments in a wide range of setups and demonstrate the challenging nature of AM2iCo. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English",
    "checked": true,
    "id": "acb5e45732d76ef69b709a8eec21aa0ebae9c804",
    "semantic_title": "am2ico: evaluating word meaning in context across low-resource languages with adversarial examples",
    "citation_count": 20,
    "authors": [
      "Qianchu Liu",
      "Edoardo Maria Ponti",
      "Diana McCarthy",
      "Ivan Vulić",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.572": {
    "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP",
    "volume": "main",
    "abstract": "Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability",
    "checked": true,
    "id": "7fa273f450251523e6b7fcc2eb3fdbdfd4a30493",
    "semantic_title": "crossfit: a few-shot learning challenge for cross-task generalization in nlp",
    "citation_count": 186,
    "authors": [
      "Qinyuan Ye",
      "Bill Yuchen Lin",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.573": {
    "title": "On the Influence of Masking Policies in Intermediate Pre-training",
    "volume": "main",
    "abstract": "Current NLP models are predominantly trained through a two-stage \"pre-train then fine-tune\" pipeline. Prior work has shown that inserting an intermediate pre-training stage, using heuristic masking policies for masked language modeling (MLM), can significantly improve final performance. However, it is still unclear (1) in what cases such intermediate pre-training is helpful, (2) whether hand-crafted heuristic objectives are optimal for a given task, and (3) whether a masking policy designed for one task is generalizable beyond that task. In this paper, we perform a large-scale empirical study to investigate the effect of various masking policies in intermediate pre-training with nine selected tasks across three categories. Crucially, we introduce methods to automate the discovery of optimal masking policies via direct supervision or meta-learning. We conclude that the success of intermediate pre-training is dependent on appropriate pre-train corpus, selection of output format (i.e., masked spans or full sentence), and clear understanding of the role that MLM plays for the downstream task. In addition, we find our learned masking policies outperform the heuristic of masking named entities on TriviaQA, and policies learned from one task can positively transfer to other tasks in certain cases, inviting future research in this direction",
    "checked": true,
    "id": "1110cc433550800c3839435f797cb6e31965479e",
    "semantic_title": "on the influence of masking policies in intermediate pre-training",
    "citation_count": 12,
    "authors": [
      "Qinyuan Ye",
      "Belinda Z. Li",
      "Sinong Wang",
      "Benjamin Bolte",
      "Hao Ma",
      "Wen-tau Yih",
      "Xiang Ren",
      "Madian Khabsa"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.574": {
    "title": "ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries",
    "volume": "main",
    "abstract": "Word embeddings learn implicit biases from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from social psychology. We apply ValNorm on static word embeddings from seven languages (Chinese, English, German, Polish, Portuguese, Spanish, and Turkish) and from historical English text spanning 200 years. ValNorm achieves consistently high accuracy in quantifying the valence of non-discriminatory, non-social group word sets. Specifically, ValNorm achieves a Pearson correlation of r=0.88 for human judgment scores of valence for 399 words collected to establish pleasantness norms in English. In contrast, we measure gender stereotypes using the same set of word embeddings and find that social biases vary across languages. Our results indicate that valence associations of non-discriminatory, non-social group words represent widely-shared associations, in seven languages and over 200 years",
    "checked": true,
    "id": "9c347056ab9cb42b19e471129e0961c1ed7126a2",
    "semantic_title": "valnorm quantifies semantics to reveal consistent valence biases across languages and over centuries",
    "citation_count": 23,
    "authors": [
      "Autumn Toney",
      "Aylin Caliskan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.575": {
    "title": "Perturbation CheckLists for Evaluating NLG Evaluation Metrics",
    "volume": "main",
    "abstract": "Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics. Our templates and code are available at https://iitmnlp.github.io/EvalEval/",
    "checked": true,
    "id": "dd9d593d95cf5090007fabb91118cec8fb78c408",
    "semantic_title": "perturbation checklists for evaluating nlg evaluation metrics",
    "citation_count": 58,
    "authors": [
      "Ananya B. Sai",
      "Tanay Dixit",
      "Dev Yashpal Sheth",
      "Sreyas Mohan",
      "Mitesh M. Khapra"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.576": {
    "title": "Robust Open-Vocabulary Translation from Visual Text Representations",
    "volume": "main",
    "abstract": "Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an ‘open vocabulary.' This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that models using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German–English task where subword models degrade to 1.9",
    "checked": true,
    "id": "5c3005e22e6fb218aa76fea49971f3f991993b32",
    "semantic_title": "robust open-vocabulary translation from visual text representations",
    "citation_count": 42,
    "authors": [
      "Elizabeth Salesky",
      "David Etter",
      "Matt Post"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.577": {
    "title": "Don't Go Far Off: An Empirical Study on Neural Poetry Translation",
    "volume": "main",
    "abstract": "Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-language-family models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore, COMET) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data",
    "checked": true,
    "id": "909ba837269f787c6d439de62002d9c8d70bfc93",
    "semantic_title": "don't go far off: an empirical study on neural poetry translation",
    "citation_count": 16,
    "authors": [
      "Tuhin Chakrabarty",
      "Arkadiy Saakyan",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.578": {
    "title": "Improving Multilingual Translation by Representation and Gradient Regularization",
    "volume": "main",
    "abstract": "Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations – commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our method also works well when the small amount of direct data is not available",
    "checked": true,
    "id": "92b5460ac5dc63ccdd693c988f75fb5ba5e6287e",
    "semantic_title": "improving multilingual translation by representation and gradient regularization",
    "citation_count": 41,
    "authors": [
      "Yilin Yang",
      "Akiko Eriguchi",
      "Alexandre Muzio",
      "Prasad Tadepalli",
      "Stefan Lee",
      "Hany Hassan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.579": {
    "title": "Learning Kernel-Smoothed Machine Translation with Retrieved Examples",
    "volume": "main",
    "abstract": "How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained models are released at https://github.com/jiangqn/KSTER",
    "checked": true,
    "id": "60dff1a62aa0a84df087d7749a2b5e99e93e5fdf",
    "semantic_title": "learning kernel-smoothed machine translation with retrieved examples",
    "citation_count": 33,
    "authors": [
      "Qingnan Jiang",
      "Mingxuan Wang",
      "Jun Cao",
      "Shanbo Cheng",
      "Shujian Huang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.580": {
    "title": "Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training",
    "volume": "main",
    "abstract": "Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model's uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods",
    "checked": true,
    "id": "1827fccc93995d54b74be678d919218394430c90",
    "semantic_title": "uncertainty-aware balancing for multilingual and multi-domain neural machine translation training",
    "citation_count": 24,
    "authors": [
      "Minghao Wu",
      "Yitong Li",
      "Meng Zhang",
      "Liangyou Li",
      "Gholamreza Haffari",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.581": {
    "title": "Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy",
    "volume": "main",
    "abstract": "Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to achieve the best translation quality under arbitrary latency with only one trained model. Specifically, our method employs multi-head attention to accomplish the mixture of experts where each head is treated as a wait-k expert with its own waiting words number, and given a test latency and source inputs, the weights of the experts are accordingly adjusted to produce the best translation. Experiments on three datasets show that our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy",
    "checked": true,
    "id": "5d6c242b3477c2315676277fd2d3f17f0a4f8875",
    "semantic_title": "universal simultaneous machine translation with mixture-of-experts wait-k policy",
    "citation_count": 43,
    "authors": [
      "Shaolei Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.582": {
    "title": "How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI",
    "volume": "main",
    "abstract": "Many real-world problems require the combined application of multiple reasoning abilities—employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, \"How much would the sea level rise if all ice in the world melted?\" FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason",
    "checked": true,
    "id": "3ceca3cc39cd90515ccd73afe6539cc953762140",
    "semantic_title": "how much coffee was consumed during emnlp 2019? fermi problems: a new reasoning challenge for ai",
    "citation_count": 10,
    "authors": [
      "Ashwin Kalyan",
      "Abhinav Kumar",
      "Arjun Chandrasekaran",
      "Ashish Sabharwal",
      "Peter Clark"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.583": {
    "title": "Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering",
    "volume": "main",
    "abstract": "In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding: the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the system due to their answer confidence scores being lower than the system threshold. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision/Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower Recall, e.g., reducing computation by ~60%, while only losing ~3-4% of Recall",
    "checked": true,
    "id": "212e2272a662e1edcddb77462a0a8fdeff49c03b",
    "semantic_title": "will this question be answered? question filtering via answer model distillation for efficient question answering",
    "citation_count": 27,
    "authors": [
      "Siddhant Garg",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.584": {
    "title": "Learning with Instance Bundles for Reading Comprehension",
    "volume": "main",
    "abstract": "When training most modern reading comprehension models, all the questions associated with a context are treated as being independent from each other. However, closely related questions and their corresponding answers are not independent, and leveraging these relationships could provide a strong supervision signal to a model. Drawing on ideas from contrastive estimation, we introduce several new supervision losses that compare question-answer scores across multiple related instances. Specifically, we normalize these scores across various neighborhoods of closely contrasting questions and/or answers, adding a cross entropy loss term in addition to traditional maximum likelihood estimation. Our techniques require bundles of related question-answer pairs, which we either mine from within existing data or create using automated heuristics. We empirically demonstrate the effectiveness of training with instance bundles on two datasets—HotpotQA and ROPES—showing up to 9% absolute gains in accuracy",
    "checked": true,
    "id": "baabaf96830b8c2388ba9a128d3677e936bec9d7",
    "semantic_title": "learning with instance bundles for reading comprehension",
    "citation_count": 11,
    "authors": [
      "Dheeru Dua",
      "Pradeep Dasigi",
      "Sameer Singh",
      "Matt Gardner"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.585": {
    "title": "Explaining Answers with Entailment Trees",
    "volume": "main",
    "abstract": "Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \"rationale\"). If this could be done, new opportunities for understanding and debugging the system's reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations",
    "checked": true,
    "id": "4a56f72b9c529810ba4ecfe9eac522d87f6db81d",
    "semantic_title": "explaining answers with entailment trees",
    "citation_count": 188,
    "authors": [
      "Bhavana Dalvi",
      "Peter Jansen",
      "Oyvind Tafjord",
      "Zhengnan Xie",
      "Hannah Smith",
      "Leighanna Pipatanangkura",
      "Peter Clark"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.586": {
    "title": "SituatedQA: Incorporating Extra-Linguistic Contexts into QA",
    "volume": "main",
    "abstract": "Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SituatedQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https://situatedqa.github.io/",
    "checked": true,
    "id": "8e3b613f8a6d775a5bf97aa73328a2f4795dd407",
    "semantic_title": "situatedqa: incorporating extra-linguistic contexts into qa",
    "citation_count": 156,
    "authors": [
      "Michael Zhang",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.587": {
    "title": "ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational AI",
    "volume": "main",
    "abstract": "We present the first English corpus study on abusive language towards three conversational AI systems gathered ‘in the wild': an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more ‘nuanced' approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90%",
    "checked": true,
    "id": "93df98bf527e97f3876dfefb8b167a221b37e638",
    "semantic_title": "convabuse: data, analysis, and benchmarks for nuanced abuse detection in conversational ai",
    "citation_count": 84,
    "authors": [
      "Amanda Cercas Curry",
      "Gavin Abercrombie",
      "Verena Rieser"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.588": {
    "title": "Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules",
    "volume": "main",
    "abstract": "One of the challenges faced by conversational agents is their inability to identify unstated presumptions of their users' commands, a task trivial for humans due to their common sense. In this paper, we propose a zero-shot commonsense reasoning system for conversational agents in an attempt to achieve this. Our reasoner uncovers unstated presumptions from user commands satisfying a general template of if-(state), then-(action), because-(goal). Our reasoner uses a state-of-the-art transformer-based generative commonsense knowledge base (KB) as its source of background knowledge for reasoning. We propose a novel and iterative knowledge query mechanism to extract multi-hop reasoning chains from the neural KB which uses symbolic logic rules to significantly reduce the search space. Similar to any KBs gathered to date, our commonsense KB is prone to missing knowledge. Therefore, we propose to conversationally elicit the missing knowledge from human users with our novel dynamic question generation strategy, which generates and presents contextualized queries to human users. We evaluate the model with a user study with human users that achieves a 35% higher success rate compared to SOTA",
    "checked": true,
    "id": "55d82b6d4fc1ad7c45beb1d2fbcf82fafc5d7f4d",
    "semantic_title": "conversational multi-hop reasoning with neural commonsense knowledge and symbolic logic rules",
    "citation_count": 17,
    "authors": [
      "Forough Arabshahi",
      "Jennifer Lee",
      "Antoine Bosselut",
      "Yejin Choi",
      "Tom Mitchell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.589": {
    "title": "Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach",
    "volume": "main",
    "abstract": "Reliable automatic evaluation of dialogue systems under an interactive environment has long been overdue. An ideal environment for evaluating dialog systems, also known as the Turing test, needs to involve human interaction, which is usually not affordable for large-scale experiments. Though researchers have attempted to use metrics for language generation tasks (e.g., perplexity, BLEU) or some model-based reinforcement learning methods (e.g., self-play evaluation) for automatic evaluation, these methods only show very weak correlation with the actual human evaluation in practice. To bridge such a gap, we propose a new framework named ENIGMA for estimating human evaluation scores based on recent advances of off-policy evaluation in reinforcement learning. ENIGMA only requires a handful of pre-collected experience data, and therefore does not involve human interaction with the target policy during the evaluation, making automatic evaluations feasible. More importantly, ENIGMA is model-free and agnostic to the behavior policies for collecting the experience data, which significantly alleviates the technical difficulties of modeling complex dialogue environments and human behaviors. Our experiments show that ENIGMA significantly outperforms existing methods in terms of correlation with human evaluation scores",
    "checked": true,
    "id": "e270ee0a279a624ebbf4a59ae679b786be16c510",
    "semantic_title": "towards automatic evaluation of dialog systems: a model-free off-policy evaluation approach",
    "citation_count": 18,
    "authors": [
      "Haoming Jiang",
      "Bo Dai",
      "Mengjiao Yang",
      "Tuo Zhao",
      "Wei Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.590": {
    "title": "Continual Learning in Task-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "Continual learning in task-oriented dialogue systems allows the system to add new domains and functionalities overtime after deployment, without incurring the high cost of retraining the whole system each time. In this paper, we propose a first-ever continual learning benchmark for task-oriented dialogue systems with 37 domains to be learned continuously in both modularized and end-to-end learning settings. In addition, we implement and compare multiple existing continual learning baselines, and we propose a simple yet effective architectural method based on residual adapters. We also suggest that the upper bound performance of continual learning should be equivalent to multitask learning when data from all domain is available at once. Our experiments demonstrate that the proposed architectural method and a simple replay-based strategy perform better, by a large margin, compared to other continuous learning techniques, and only slightly worse than the multitask learning upper bound while being 20X faster in learning new domains. We also report several trade-offs in terms of parameter usage, memory size and training time, which are important in the design of a task-oriented dialogue system. The proposed benchmark is released to promote more research in this direction",
    "checked": true,
    "id": "96726a0d83c9b73f35326956a1d3b83a9b903337",
    "semantic_title": "continual learning in task-oriented dialogue systems",
    "citation_count": 133,
    "authors": [
      "Andrea Madotto",
      "Zhaojiang Lin",
      "Zhenpeng Zhou",
      "Seungwhan Moon",
      "Paul Crook",
      "Bing Liu",
      "Zhou Yu",
      "Eunjoon Cho",
      "Pascale Fung",
      "Zhiguang Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.591": {
    "title": "Multilingual and Cross-Lingual Intent Detection from Spoken Data",
    "volume": "main",
    "abstract": "We present a systematic study on multilingual and cross-lingual intent detection (ID) from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the ID task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., translation direction, impact of speech recognition, data augmentation from a related domain. We see this work as an important step towards more inclusive development and evaluation of multilingual ID from spoken data, hopefully in a much wider spectrum of languages compared to prior work",
    "checked": true,
    "id": "314e5d633cbba92a83acb0d22bb68e7ca41035e3",
    "semantic_title": "multilingual and cross-lingual intent detection from spoken data",
    "citation_count": 35,
    "authors": [
      "Daniela Gerz",
      "Pei-Hao Su",
      "Razvan Kusztos",
      "Avishek Mondal",
      "Michał Lis",
      "Eshan Singhal",
      "Nikola Mrkšić",
      "Tsung-Hsien Wen",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.592": {
    "title": "Investigating Robustness of Dialog Models to Popular Figurative Language Constructs",
    "volume": "main",
    "abstract": "Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of existing dialog models in situations where the input dialog context exhibits use of figurative language. We observe large gaps in handling of figurative language when evaluating the models on two open domain dialog datasets. When faced with dialog contexts consisting of figurative language, some models show very large drops in performance compared to contexts without figurative language. We encourage future research in dialog modeling to separately analyze and report results on figurative language in order to better test model capabilities relevant to real-world use. Finally, we propose lightweight solutions to help existing models become more robust to figurative language by simply using an external resource to translate figurative language to literal (non-figurative) forms while preserving the meaning to the best extent possible",
    "checked": true,
    "id": "dc59b104f41d41d555c5b1a1fda7d69a5e080c53",
    "semantic_title": "investigating robustness of dialog models to popular figurative language constructs",
    "citation_count": 21,
    "authors": [
      "Harsh Jhamtani",
      "Varun Gangal",
      "Eduard Hovy",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.593": {
    "title": "Effective Sequence-to-Sequence Dialogue State Tracking",
    "volume": "main",
    "abstract": "Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this problem from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more effective than auto-regressive language modeling. We also explore using Pegasus, a span prediction-based pre-training objective for text summarization, for the state tracking model. We found that pre-training for the seemingly distant summarization task works surprisingly well for dialogue state tracking. In addition, we found that while recurrent state context representation works also reasonably well, the model may have a hard time recovering from earlier mistakes. We conducted experiments on the MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations",
    "checked": true,
    "id": "0df511862b8ac84a7f6ab618652bf00d53e370bb",
    "semantic_title": "effective sequence-to-sequence dialogue state tracking",
    "citation_count": 42,
    "authors": [
      "Jeffrey Zhao",
      "Mahdis Mahdieh",
      "Ye Zhang",
      "Yuan Cao",
      "Yonghui Wu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.594": {
    "title": "MSˆ2: Multi-Document Summarization of Medical Studies",
    "volume": "main",
    "abstract": "To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MSˆ2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system's generated summaries. Data and models are available at https://github.com/allenai/ms2",
    "checked": true,
    "id": "d25121da56c9050137800c69520111b30201d1ed",
    "semantic_title": "msˆ2: multi-document summarization of medical studies",
    "citation_count": 114,
    "authors": [
      "Jay DeYoung",
      "Iz Beltagy",
      "Madeleine van Zuylen",
      "Bailey Kuehl",
      "Lucy Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.595": {
    "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
    "volume": "main",
    "abstract": "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge",
    "checked": true,
    "id": "38b0567e83386ddc294d6c81b541deacbd8e3c2a",
    "semantic_title": "clipscore: a reference-free evaluation metric for image captioning",
    "citation_count": 1694,
    "authors": [
      "Jack Hessel",
      "Ari Holtzman",
      "Maxwell Forbes",
      "Ronan Le Bras",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.596": {
    "title": "On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings",
    "volume": "main",
    "abstract": "Building compositional explanations requires models to combine two or more facts that, together, describe why the answer to a question is correct. Typically, these \"multi-hop\" explanations are evaluated relative to one (or a small number of) gold explanations. In this work, we show these evaluations substantially underestimate model performance, both in terms of the relevance of included facts, as well as the completeness of model-generated explanations, because models regularly discover and produce valid explanations that are different than gold explanations. To address this, we construct a large corpus of 126k domain-expert (science teacher) relevance ratings that augment a corpus of explanations to standardized science exam questions, discovering 80k additional relevant facts not rated as gold. We build three strong models based on different methodologies (generation, ranking, and schemas), and empirically show that while expert-augmented ratings provide better estimates of explanation quality, both original (gold) and expert-augmented automatic evaluations still substantially underestimate performance by up to 36% when compared with full manual expert judgements, with different models being disproportionately affected. This poses a significant methodological challenge to accurately evaluating explanations produced by compositional reasoning models",
    "checked": true,
    "id": "5c664b8705749d630b35701a6b2a79a1f0adcf90",
    "semantic_title": "on the challenges of evaluating compositional explanations in multi-hop inference: relevance, completeness, and expert ratings",
    "citation_count": 9,
    "authors": [
      "Peter Jansen",
      "Kelly J. Smith",
      "Dan Moreno",
      "Huitzilin Ortiz"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.597": {
    "title": "ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations",
    "volume": "main",
    "abstract": "Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines' ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce **ESTER**, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1%, 63.3% and 83.5% for token-based exact-match (**EM**), **F1** and event-based **HIT@1** scores, which are all significantly below human performances (36.0%, 79.6%, 100% respectively), highlighting our dataset as a challenging benchmark",
    "checked": true,
    "id": "c7f977f556d2060238fdc1286d057d46958afaf9",
    "semantic_title": "ester: a machine reading comprehension dataset for reasoning about event semantic relations",
    "citation_count": 46,
    "authors": [
      "Rujun Han",
      "I-Hung Hsu",
      "Jiao Sun",
      "Julia Baylon",
      "Qiang Ning",
      "Dan Roth",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.598": {
    "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms",
    "volume": "main",
    "abstract": "Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense",
    "checked": true,
    "id": "011869f932f89d047ce2bd36d73a95cc04888193",
    "semantic_title": "rica: evaluating robust inference capabilities based on commonsense axioms",
    "citation_count": 37,
    "authors": [
      "Pei Zhou",
      "Rahul Khanna",
      "Seyeon Lee",
      "Bill Yuchen Lin",
      "Daniel Ho",
      "Jay Pujara",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.599": {
    "title": "Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation",
    "volume": "main",
    "abstract": "Natural language generation (NLG) spans a broad range of tasks, each of which serves for specific objectives and desires different properties of generated text. The complexity makes automatic evaluation of NLG particularly challenging. Previous work has typically focused on a single task and developed individual evaluation metrics based on specific intuitions. In this paper, we propose a unifying perspective based on the nature of information change in NLG tasks, including compression (e.g., summarization), transduction (e.g., text rewriting), and creation (e.g., dialog). _Information alignment_ between input, context, and output text plays a common central role in characterizing the generation. With automatic alignment prediction models, we develop a family of interpretable metrics that are suitable for evaluating key aspects of different NLG tasks, often without need of gold reference data. Experiments show the uniformly designed metrics achieve stronger or comparable correlations with human judgement compared to state-of-the-art metrics in each of diverse tasks, including text summarization, style transfer, and knowledge-grounded dialog",
    "checked": true,
    "id": "43fae0a7af211d91557d115d2f82e3c46d8bf022",
    "semantic_title": "compression, transduction, and creation: a unified framework for evaluating natural language generation",
    "citation_count": 73,
    "authors": [
      "Mingkai Deng",
      "Bowen Tan",
      "Zhengzhong Liu",
      "Eric Xing",
      "Zhiting Hu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.600": {
    "title": "MATE: Multi-view Attention for Table Transformer Efficiency",
    "volume": "main",
    "abstract": "This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points",
    "checked": true,
    "id": "395aae6e7a79e5760457ca38e868acc970016230",
    "semantic_title": "mate: multi-view attention for table transformer efficiency",
    "citation_count": 97,
    "authors": [
      "Julian Eisenschlos",
      "Maharshi Gor",
      "Thomas Müller",
      "William Cohen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.601": {
    "title": "Learning with Different Amounts of Annotation: From Zero to Many Labels",
    "volume": "main",
    "abstract": "Training NLP systems typically assumes access to annotated data that has a single human label per example. Given imperfect labeling from annotators and inherent ambiguity of language, we hypothesize that single label is not sufficient to learn the spectrum of language interpretation. We explore new annotation distribution schemes, assigning multiple labels per example for a small subset of training examples. Introducing such multi label examples at the cost of annotating fewer examples brings clear gains on natural language inference task and entity typing task, even when we simply first train with a single label data and then fine tune with multi label examples. Extending a MixUp data augmentation framework, we propose a learning algorithm that can learn from training examples with different amount of annotation (with zero, one, or multiple labels). This algorithm efficiently combines signals from uneven training data and brings additional gains in low annotation budget and cross domain settings. Together, our method achieves consistent gains in two tasks, suggesting distributing labels unevenly among training examples can be beneficial for many NLP tasks",
    "checked": true,
    "id": "7ab1a02835dc9a302efcf6952be81c69573f65e3",
    "semantic_title": "learning with different amounts of annotation: from zero to many labels",
    "citation_count": 32,
    "authors": [
      "Shujian Zhang",
      "Chengyue Gong",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.602": {
    "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
    "volume": "main",
    "abstract": "Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference",
    "checked": true,
    "id": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
    "semantic_title": "when attention meets fast recurrence: training language models with reduced compute",
    "citation_count": 50,
    "authors": [
      "Tao Lei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.603": {
    "title": "Universal-KD: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation",
    "volume": "main",
    "abstract": "Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies matching in the hidden spaces of two different networks (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD cannot easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits: (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques",
    "checked": true,
    "id": "24a099c7ac954f5795456235a245b1377f66244a",
    "semantic_title": "universal-kd: attention-based output-grounded intermediate layer knowledge distillation",
    "citation_count": 23,
    "authors": [
      "Yimeng Wu",
      "Mehdi Rezagholizadeh",
      "Abbas Ghaddar",
      "Md Akmal Haidar",
      "Ali Ghodsi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.604": {
    "title": "Highly Parallel Autoregressive Entity Linking with Discriminative Correction",
    "volume": "main",
    "abstract": "Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e., a correction term which lets us directly optimize the generator's ranking. When taken together, these techniques tackle all the above issues: our model is >70 times faster and more accurate than the previous generative method, outperforming state-of-the-art approaches on the standard English dataset AIDA-CoNLL. Source code available at https://github.com/nicola-decao/efficient-autoregressive-EL",
    "checked": true,
    "id": "cbfbe1da18b559a74c818c61b3994168e5f6c1e1",
    "semantic_title": "highly parallel autoregressive entity linking with discriminative correction",
    "citation_count": 37,
    "authors": [
      "Nicola De Cao",
      "Wilker Aziz",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.605": {
    "title": "Word-Level Coreference Resolution",
    "volume": "main",
    "abstract": "Recent coreference resolution models rely heavily on span representations to find coreference links between word spans. As the number of spans is O(n2) in the length of text and the number of potential links is O(n4), various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to O(n2) and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for coreference resolution will be significantly outperformed by RoBERTa. While being highly efficient, our model performs competitively with recent coreference resolution systems on the OntoNotes benchmark",
    "checked": true,
    "id": "23b52235c5a7d5b008f646364b4fd4abb4a73721",
    "semantic_title": "word-level coreference resolution",
    "citation_count": 74,
    "authors": [
      "Vladimir Dobrovolskii"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.606": {
    "title": "A Secure and Efficient Federated Learning Framework for NLP",
    "volume": "main",
    "abstract": "In this work, we consider the problem of designing secure and efficient federated learning (FL) frameworks for NLP. Existing solutions under this literature either consider a trusted aggregator or require heavy-weight cryptographic primitives, which makes the performance significantly degraded. Moreover, many existing secure FL designs work only under the restrictive assumption that none of the clients can be dropped out from the training protocol. To tackle these problems, we propose SEFL, a secure and efficient federated learning framework that (1) eliminates the need for the trusted entities; (2) achieves similar and even better model accuracy compared with existing FL designs; (3) is resilient to client dropouts",
    "checked": true,
    "id": "42bbaf03e76a9132cd05137aa762b8dc70087bd9",
    "semantic_title": "a secure and efficient federated learning framework for nlp",
    "citation_count": 22,
    "authors": [
      "Chenghong Wang",
      "Jieren Deng",
      "Xianrui Meng",
      "Yijue Wang",
      "Ji Li",
      "Sheng Lin",
      "Shuo Han",
      "Fei Miao",
      "Sanguthevar Rajasekaran",
      "Caiwen Ding"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.607": {
    "title": "Controllable Semantic Parsing via Retrieval Augmentation",
    "volume": "main",
    "abstract": "In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving state-of-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model",
    "checked": true,
    "id": "a0e92f6e9564b8c38b6649ae71b892ddfb988faa",
    "semantic_title": "controllable semantic parsing via retrieval augmentation",
    "citation_count": 48,
    "authors": [
      "Panupong Pasupat",
      "Yuan Zhang",
      "Kelvin Guu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.608": {
    "title": "Constrained Language Models Yield Few-Shot Semantic Parsers",
    "volume": "main",
    "abstract": "We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data",
    "checked": true,
    "id": "64a1dbdd7653eaca25c78e87335ee156b6f6959e",
    "semantic_title": "constrained language models yield few-shot semantic parsers",
    "citation_count": 208,
    "authors": [
      "Richard Shin",
      "Christopher Lin",
      "Sam Thomson",
      "Charles Chen",
      "Subhro Roy",
      "Emmanouil Antonios Platanios",
      "Adam Pauls",
      "Dan Klein",
      "Jason Eisner",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.609": {
    "title": "ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning",
    "volume": "main",
    "abstract": "Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model's ability to reason and explain predictions with underlying commonsense knowledge. They also allow such models to use reasoning shortcuts and not be \"right for the right reasons\". In this work, we present ExplaGraphs, a new generative and structured commonsense-reasoning task (and an associated dataset) of explanation graph generation for stance prediction. Specifically, given a belief and an argument, a model has to predict if the argument supports or counters the belief and also generate a commonsense-augmented graph that serves as non-trivial, complete, and unambiguous explanation for the predicted stance. We collect explanation graphs through a novel Create-Verify-And-Refine graph collection framework that improves the graph quality (up to 90%) via multiple rounds of verification and refinement. A significant 79% of our graphs contain external commonsense nodes with diverse structures and reasoning depths. Next, we propose a multi-level evaluation framework, consisting of automatic metrics and human evaluation, that check for the structural and semantic correctness of the generated graphs and their degree of match with ground-truth graphs. Finally, we present several structured, commonsense-augmented, and text generation models as strong starting points for this explanation graph generation task, and observe that there is a large gap with human performance, thereby encouraging future work for this new challenging task",
    "checked": true,
    "id": "bb8d6327497eff7cfc0d2e911698eff2a729a387",
    "semantic_title": "explagraphs: an explanation graph generation task for structured commonsense reasoning",
    "citation_count": 59,
    "authors": [
      "Swarnadeep Saha",
      "Prateek Yadav",
      "Lisa Bauer",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.610": {
    "title": "Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories",
    "volume": "main",
    "abstract": "Word Sense Disambiguation (WSD) aims to automatically identify the exact meaning of one word according to its context. Existing supervised models struggle to make correct predictions on rare word senses due to limited training data and can only select the best definition sentence from one predefined word sense inventory (e.g., WordNet). To address the data sparsity problem and generalize the model to be independent of one predefined inventory, we propose a gloss alignment algorithm that can align definition sentences (glosses) with the same meaning from different sense inventories to collect rich lexical knowledge. We then train a model to identify semantic equivalence between a target word in context and one of its glosses using these aligned inventories, which exhibits strong transfer capability to many WSD tasks. Experiments on benchmark datasets show that the proposed method improves predictions on both frequent and rare word senses, outperforming prior work by 1.2% on the All-Words WSD Task and 4.3% on the Low-Shot WSD Task. Evaluation on WiC Task also indicates that our method can better capture word meanings in context",
    "checked": true,
    "id": "e368b713f0bb321154135cdbaf0dcd08510f1958",
    "semantic_title": "connect-the-dots: bridging semantics between words and definitions via aligning word sense inventories",
    "citation_count": 7,
    "authors": [
      "Wenlin Yao",
      "Xiaoman Pan",
      "Lifeng Jin",
      "Jianshu Chen",
      "Dian Yu",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.611": {
    "title": "LM-Critic: Language Models for Unsupervised Grammatical Error Correction",
    "volume": "main",
    "abstract": "Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets on multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the unsupervised setting (+7.7 F0.5) and the supervised setting (+0.5 F0.5)",
    "checked": true,
    "id": "9cac09098aa611bd9a94d080d2401840632ab16f",
    "semantic_title": "lm-critic: language models for unsupervised grammatical error correction",
    "citation_count": 50,
    "authors": [
      "Michihiro Yasunaga",
      "Jure Leskovec",
      "Percy Liang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.612": {
    "title": "Language-agnostic Representation from Multilingual Sentence Encoders for Cross-lingual Similarity Estimation",
    "volume": "main",
    "abstract": "We propose a method to distill a language-agnostic meaning embedding from a multilingual sentence encoder. By removing language-specific information from the original embedding, we retrieve an embedding that fully represents the sentence's meaning. The proposed method relies only on parallel corpora without any human annotations. Our meaning embedding allows efficient cross-lingual sentence similarity estimation by simple cosine similarity calculation. Experimental results on both quality estimation of machine translation and cross-lingual semantic textual similarity tasks reveal that our method consistently outperforms the strong baselines using the original multilingual embedding. Our method consistently improves the performance of any pre-trained multilingual sentence encoder, even in low-resource language pairs where only tens of thousands of parallel sentence pairs are available",
    "checked": true,
    "id": "c0a54963b0689fa7d76fda1063b65003c769d9b7",
    "semantic_title": "language-agnostic representation from multilingual sentence encoders for cross-lingual similarity estimation",
    "citation_count": 24,
    "authors": [
      "Nattapong Tiyajamorn",
      "Tomoyuki Kajiwara",
      "Yuki Arase",
      "Makoto Onizuka"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.613": {
    "title": "Classifying Dyads for Militarized Conflict Analysis",
    "volume": "main",
    "abstract": "Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies",
    "checked": true,
    "id": "5f776a0d34647940b6eeaed93ed9de8f651a64b1",
    "semantic_title": "classifying dyads for militarized conflict analysis",
    "citation_count": 10,
    "authors": [
      "Niklas Stoehr",
      "Lucas Torroba Hennigen",
      "Samin Ahbab",
      "Robert West",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.614": {
    "title": "Point-of-Interest Type Prediction using Text and Images",
    "volume": "main",
    "abstract": "Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI's type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in reality, the variety of modalities, as well as their semiotic relationships with one another, shape communication and interactions in social media. This paper presents a study on POI type prediction using multimodal information from text and images available at posting time. For that purpose, we enrich a currently available data set for POI type prediction with the images that accompany the text messages. Our proposed method extracts relevant information from each modality to effectively capture interactions between text and image achieving a macro F1 of 47.21 across 8 categories significantly outperforming the state-of-the-art method for POI type prediction based on text-only methods. Finally, we provide a detailed analysis to shed light on cross-modal interactions and the limitations of our best performing model",
    "checked": true,
    "id": "8bc20aaa5342b4cdc2bc8b1623ab326873f77444",
    "semantic_title": "point-of-interest type prediction using text and images",
    "citation_count": 14,
    "authors": [
      "Danae Sánchez Villegas",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.615": {
    "title": "Come hither or go away? Recognising pre-electoral coalition signals in the news",
    "volume": "main",
    "abstract": "In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the news coverage leading up to an election the (un)willingness of political parties to form a government coalition. We decompose our problem into two related, but distinct tasks: (i) predicting whether a reported statement from a politician or a journalist refers to a potential coalition and (ii) predicting the polarity of the signal – namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of multi-task learning and investigate which setup and task formulation is best suited for each sub-task. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, Germany, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline",
    "checked": true,
    "id": "5bd08e128d3f46fd1143eb3c187b1d90ac85a205",
    "semantic_title": "come hither or go away? recognising pre-electoral coalition signals in the news",
    "citation_count": 1,
    "authors": [
      "Ines Rehbein",
      "Simone Paolo Ponzetto",
      "Anna Adendorf",
      "Oke Bahnsen",
      "Lukas Stoetzer",
      "Heiner Stuckenschmidt"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.616": {
    "title": "#HowYouTagTweets: Learning User Hashtagging Preferences via Personalized Topic Attention",
    "volume": "main",
    "abstract": "Millions of hashtags are created on social media every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user's hashtagging preferences via predicting how likely they will post with a hashtag. It is hypothesized that one's interests in a hashtag are related with what they said before (user history) and the existing posts present the hashtag (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via multitask learning. In this way, user interests learned from the past can be customized to match future hashtags, which is beyond the capability of existing methods assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our model significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics",
    "checked": true,
    "id": "44fa1bb612392858d1e700a95d33298b2ece0b63",
    "semantic_title": "#howyoutagtweets: learning user hashtagging preferences via personalized topic attention",
    "citation_count": 12,
    "authors": [
      "Yuji Zhang",
      "Yubo Zhang",
      "Chunpu Xu",
      "Jing Li",
      "Ziyan Jiang",
      "Baolin Peng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.617": {
    "title": "Learning Neural Templates for Recommender Dialogue System",
    "volume": "main",
    "abstract": "The task of Conversational Recommendation System (CRS), i.e., recommender dialog system, aims to recommend precise items to users through natural language interactions. Though recent end-to-end neural models have shown promising progress on this task, two key challenges still remain. First, the recommended items cannot be always incorporated into the generated response precisely and appropriately. Second, only the items mentioned in the training corpus have a chance to be recommended in the conversation. To tackle these challenges, we introduce a novel framework called NTRD for recommender dialogue system that can decouple the dialogue generation from the item recommendation. NTRD has two key components, i.e., response template generator and item selector. The former adopts an encoder-decoder model to generate a response template with slot locations tied to target items, while the latter fills in slot locations with the proper items using a sufficient attention mechanism. Our approach combines the strengths of both classical slot filling approaches (that are generally controllable) and modern neural NLG approaches (that are generally more natural and accurate). Extensive experiments on the benchmark ReDial show our approach significantly outperforms the previous state-of-the-art methods. Besides, our approach has the unique advantage to produce novel items that do not appear in the training set of dialogue corpus. The code is available at https://github.com/jokieleung/NTRD",
    "checked": true,
    "id": "82ae5061485e1c2cba0330608fccd218f2afd8f4",
    "semantic_title": "learning neural templates for recommender dialogue system",
    "citation_count": 54,
    "authors": [
      "Zujie Liang",
      "Huang Hu",
      "Can Xu",
      "Jian Miao",
      "Yingying He",
      "Yining Chen",
      "Xiubo Geng",
      "Fan Liang",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.618": {
    "title": "Proxy Indicators for the Quality of Open-domain Dialogues",
    "volume": "main",
    "abstract": "The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues' quality. As a consequence, performing such evaluations at scale is usually expensive. This work investigates using a deep-learning model trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as different perspectives on judging the quality of conversation, thus reducing the need for additional training data or responses that serve as quality references. Due to this nature, the method can infer various quality metrics and can derive a component-based overall score. We achieve statistically significant correlation coefficients of up to 0.7",
    "checked": true,
    "id": "df39e25d31e76427e5a6e91246951b1049b639ab",
    "semantic_title": "proxy indicators for the quality of open-domain dialogues",
    "citation_count": 1,
    "authors": [
      "Rostislav Nedelchev",
      "Jens Lehmann",
      "Ricardo Usbeck"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.619": {
    "title": "Q2: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering",
    "volume": "main",
    "abstract": "Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted Q2, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements",
    "checked": true,
    "id": "36f141fc5bc6813073736cf886e264606d9403bf",
    "semantic_title": "q^{2}: evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering",
    "citation_count": 143,
    "authors": [
      "Or Honovich",
      "Leshem Choshen",
      "Roee Aharoni",
      "Ella Neeman",
      "Idan Szpektor",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.620": {
    "title": "Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking",
    "volume": "main",
    "abstract": "Dialogue State Tracking is central to multi-domain task-oriented dialogue systems, responsible for extracting information from user utterances. We present a novel hybrid architecture that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal, sequential prediction of slot values. The model architecture captures inter-slot relationships and dependencies across domains that otherwise can be lost in sequential prediction. We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level. We further report detailed analyses to demonstrate the effectiveness of graph models in DST by showing that the proposed graph modules capture inter-slot dependencies and improve the predictions of values that are common to multiple domains",
    "checked": true,
    "id": "5150e2febe2cfafe0ac8b557cc1ff117aac9b93d",
    "semantic_title": "knowledge-aware graph-enhanced gpt-2 for dialogue state tracking",
    "citation_count": 43,
    "authors": [
      "Weizhe Lin",
      "Bo-Hsiang Tseng",
      "Bill Byrne"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.621": {
    "title": "A Collaborative Multi-agent Reinforcement Learning Framework for Dialog Action Decomposition",
    "volume": "main",
    "abstract": "Most reinforcement learning methods for dialog policy learning train a centralized agent that selects a predefined joint action concatenating domain name, intent type, and slot name. The centralized dialog agent suffers from a great many user-agent interaction requirements due to the large action space. Besides, designing the concatenated actions is laborious to engineers and maybe struggled with edge cases. To solve these problems, we model the dialog policy learning problem with a novel multi-agent framework, in which each part of the action is led by a different agent. The framework reduces labor costs for action templates and decreases the size of the action space for each agent. Furthermore, we relieve the non-stationary problem caused by the changing dynamics of the environment as evolving of agents' policies by introducing a joint optimization process that makes agents can exchange their policy information. Concurrently, an independent experience replay buffer mechanism is integrated to reduce the dependence between gradients of samples to improve training efficiency. The effectiveness of the proposed framework is demonstrated in a multi-domain environment with both user simulator evaluation and human evaluation",
    "checked": true,
    "id": "1aaa49865e3f3bd07b20c641add4a10b7d296c22",
    "semantic_title": "a collaborative multi-agent reinforcement learning framework for dialog action decomposition",
    "citation_count": 14,
    "authors": [
      "Huimin Wang",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.622": {
    "title": "Zero-Shot Dialogue State Tracking via Cross-Task Transfer",
    "volume": "main",
    "abstract": "Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained baseline on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains",
    "checked": true,
    "id": "64f3a18921f7f3a384dca073cd6d2476b9af47f2",
    "semantic_title": "zero-shot dialogue state tracking via cross-task transfer",
    "citation_count": 74,
    "authors": [
      "Zhaojiang Lin",
      "Bing Liu",
      "Andrea Madotto",
      "Seungwhan Moon",
      "Zhenpeng Zhou",
      "Paul Crook",
      "Zhiguang Wang",
      "Zhou Yu",
      "Eunjoon Cho",
      "Rajen Subba",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.623": {
    "title": "Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance",
    "volume": "main",
    "abstract": "The ability to identify and resolve uncertainty is crucial for the robustness of a dialogue system. Indeed, this has been confirmed empirically on systems that utilise Bayesian approaches to dialogue belief tracking. However, such systems consider only confidence estimates and have difficulty scaling to more complex settings. Neural dialogue systems, on the other hand, rarely take uncertainties into account. They are therefore overconfident in their decisions and less robust. Moreover, the performance of the tracking task is often evaluated in isolation, without consideration of its effect on the downstream policy optimisation. We propose the use of different uncertainty measures in neural belief tracking. The effects of these measures on the downstream task of policy optimisation are evaluated by adding selected measures of uncertainty to the feature space of the policy and training policies through interaction with a user simulator. Both human and simulated user results show that incorporating these measures leads to improvements both of the performance and of the robustness of the downstream dialogue policy. This highlights the importance of developing neural dialogue belief trackers that take uncertainty into account",
    "checked": true,
    "id": "d150bce3bc29c506f690b4131ac37dd9051e31ba",
    "semantic_title": "uncertainty measures in neural belief tracking and the effects on dialogue policy performance",
    "citation_count": 10,
    "authors": [
      "Carel van Niekerk",
      "Andrey Malinin",
      "Christian Geishauser",
      "Michael Heck",
      "Hsien-chin Lin",
      "Nurul Lubis",
      "Shutong Feng",
      "Milica Gasic"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.624": {
    "title": "Dynamic Forecasting of Conversation Derailment",
    "volume": "main",
    "abstract": "Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work in this space is limited, and we extend it in several ways. We apply a pretrained language encoder to the task, which outperforms earlier approaches. We further experiment with shifting the training paradigm for the task from a static to a dynamic one to increase the forecast horizon. This approach shows mixed results: in a high-quality data setting, a longer average forecast horizon can be achieved at the cost of a small drop in F1; in a low-quality data setting, however, dynamic training propagates the noise and is highly detrimental to performance",
    "checked": true,
    "id": "f38d20c500bd3b985df4902c071c0b5c81d11b14",
    "semantic_title": "dynamic forecasting of conversation derailment",
    "citation_count": 16,
    "authors": [
      "Yova Kementchedjhieva",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.625": {
    "title": "A Semantic Filter Based on Relations for Knowledge Graph Completion",
    "volume": "main",
    "abstract": "Knowledge graph embedding, representing entities and relations in the knowledge graphs with high-dimensional vectors, has made significant progress in link prediction. More researchers have explored the representational capabilities of models in recent years. That is, they investigate better representational models to fit symmetry/antisymmetry and combination relationships. The current embedding models are more inclined to utilize the identical vector for the same entity in various triples to measure the matching performance. The observation that measuring the rationality of specific triples means comparing the matching degree of the specific attributes associated with the relations is well-known. Inspired by this fact, this paper designs Semantic Filter Based on Relations(SFBR) to extract the required attributes of the entities. Then the rationality of triples is compared under these extracted attributes through the traditional embedding models. The semantic filter module can be added to most geometric and tensor decomposition models with minimal additional memory. experiments on the benchmark datasets show that the semantic filter based on relations can suppress the impact of other attribute dimensions and improve link prediction performance. The tensor decomposition models with SFBR have achieved state-of-the-art",
    "checked": true,
    "id": "a9b15384b71c97a7d3e893d6a5f73ec9a3ba2c37",
    "semantic_title": "a semantic filter based on relations for knowledge graph completion",
    "citation_count": 9,
    "authors": [
      "Zongwei Liang",
      "Junan Yang",
      "Hui Liu",
      "Keju Huang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.626": {
    "title": "AdapterDrop: On the Efficiency of Adapters in Transformers",
    "volume": "main",
    "abstract": "Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely",
    "checked": true,
    "id": "bdeec55f95fd6b73e3e4635459b14c7248543efb",
    "semantic_title": "adapterdrop: on the efficiency of adapters in transformers",
    "citation_count": 268,
    "authors": [
      "Andreas Rücklé",
      "Gregor Geigle",
      "Max Glockner",
      "Tilman Beck",
      "Jonas Pfeiffer",
      "Nils Reimers",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.627": {
    "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization",
    "volume": "main",
    "abstract": "Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges – namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme – per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at https://github.com/qualcomm-ai-research/transformer-quantization",
    "checked": true,
    "id": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
    "semantic_title": "understanding and overcoming the challenges of efficient transformer quantization",
    "citation_count": 150,
    "authors": [
      "Yelysei Bondarenko",
      "Markus Nagel",
      "Tijmen Blankevoort"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.628": {
    "title": "CAPE: Context-Aware Private Embeddings for Private Language Learning",
    "volume": "main",
    "abstract": "Neural language models have contributed to state-of-the-art results in a number of downstream applications including sentiment analysis, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding personally identifiable information learned from language and context cues that may lead to privacy leaks. To ameliorate this issue, we propose Context-Aware Private Embeddings (CAPE), a novel approach which combines differential privacy and adversarial learning to preserve privacy during training of embeddings. Specifically, CAPE firstly applies calibrated noise through differential privacy to maintain the privacy of text representations by preserving the encoded semantic links while obscuring sensitive information. Next, CAPE employs an adversarial training regime that obscures identified private variables. Experimental results demonstrate that our proposed approach is more effective in reducing private information leakage than either single intervention, with approximately a 3% reduction in attacker performance compared to the best-performing current method",
    "checked": true,
    "id": "1e67bdbdfc65890578f9e8fa2e3ffb1274dfad6a",
    "semantic_title": "cape: context-aware private embeddings for private language learning",
    "citation_count": 27,
    "authors": [
      "Richard Plant",
      "Dimitra Gkatzia",
      "Valerio Giuffrida"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.629": {
    "title": "Text Detoxification using Large Pre-trained Neural Models",
    "volume": "main",
    "abstract": "We present two novel unsupervised methods for eliminating toxicity in text. Our first method combines two recent ideas: (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing paraphraser guided by style-trained language models to keep the text content and remove toxicity. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the method more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our models with a number of methods for style transfer. The models are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both methods we suggest yield new SOTA results",
    "checked": true,
    "id": "e6213b75d1d43009c1fc014af2c0f4bc705bc6cf",
    "semantic_title": "text detoxification using large pre-trained neural models",
    "citation_count": 77,
    "authors": [
      "David Dale",
      "Anton Voronov",
      "Daryna Dementieva",
      "Varvara Logacheva",
      "Olga Kozlova",
      "Nikita Semenov",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.630": {
    "title": "Document-Level Text Simplification: Dataset, Criteria and Baseline",
    "volume": "main",
    "abstract": "Text simplification is a valuable technique. However, current research is limited to sentence simplification. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on Wikipedia dumps, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the dataset is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the baseline models",
    "checked": true,
    "id": "1f3ac86b4fefa3f1d6713d061318bdf92a456d2e",
    "semantic_title": "document-level text simplification: dataset, criteria and baseline",
    "citation_count": 46,
    "authors": [
      "Renliang Sun",
      "Hanqi Jin",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.631": {
    "title": "A Bag of Tricks for Dialogue Summarization",
    "volume": "main",
    "abstract": "Dialogue summarization comes with its own peculiar challenges as opposed to news or scientific articles summarization. In this work, we explore four different challenges of the task: handling and differentiating parts of the dialogue belonging to multiple speakers, negation understanding, reasoning about the situation, and informal language understanding. Using a pretrained sequence-to-sequence language model, we explore speaker name substitution, negation scope highlighting, multi-task learning with relevant tasks, and pretraining on in-domain data. Our experiments show that our proposed techniques indeed improve summarization performance, outperforming strong baselines",
    "checked": true,
    "id": "5a07cbdf8eddd887ab73996e3ca11ed425a54919",
    "semantic_title": "a bag of tricks for dialogue summarization",
    "citation_count": 28,
    "authors": [
      "Muhammad Khalifa",
      "Miguel Ballesteros",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.632": {
    "title": "Paraphrasing Compound Nominalizations",
    "volume": "main",
    "abstract": "A nominalization uses a deverbal noun to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret nominalizations by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of unsupervised methods, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model",
    "checked": true,
    "id": "a07ce5ec464a16f27981c3b624ec1bdf0f8e7bec",
    "semantic_title": "paraphrasing compound nominalizations",
    "citation_count": 4,
    "authors": [
      "John Lee",
      "Ho Hung Lim",
      "Carol Webster"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.633": {
    "title": "Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation",
    "volume": "main",
    "abstract": "QuestEval is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to Data-to-Text tasks is not straightforward, as it requires multimodal Question Generation and Answering systems on the considered tasks, which are seldom available. To this purpose, we propose a method to build synthetic multimodal corpora enabling to train multimodal components for a data-QuestEval metric. The resulting metric is reference-less and multimodal; it obtains state-of-the-art correlations with human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's code and models available for reproducibility purpose, as part of the QuestEval project",
    "checked": true,
    "id": "9c2145dc180fd769205dbd1e936ae057b0061a84",
    "semantic_title": "data-questeval: a referenceless metric for data-to-text semantic evaluation",
    "citation_count": 32,
    "authors": [
      "Clement Rebuffel",
      "Thomas Scialom",
      "Laure Soulier",
      "Benjamin Piwowarski",
      "Sylvain Lamprier",
      "Jacopo Staiano",
      "Geoffrey Scoutheeten",
      "Patrick Gallinari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.634": {
    "title": "Low-Rank Subspaces for Unsupervised Entity Linking",
    "volume": "main",
    "abstract": "Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, Eigenthemes, that relies solely on the availability of entity names and a referent knowledge base. Eigenthemes exploits the fact that the entities that are truly mentioned in a document (the \"gold entities\") tend to form a semantically dense subset of the set of all candidate entities in the document. Geometrically speaking, when representing entities as vectors via some given embedding, the gold entities tend to lie in a low-rank subspace of the full embedding space. Eigenthemes identifies this subspace using the singular value decomposition and scores candidate entities according to their proximity to the subspace. On the empirical front, we introduce multiple strong baselines that compare favorably to (and sometimes even outperform) the existing state of the art. Extensive experiments on benchmark datasets from a variety of real-world domains showcase the effectiveness of our approach",
    "checked": true,
    "id": "95e3ca1a340a180dc6ed8123d3bba098e5e37eb4",
    "semantic_title": "low-rank subspaces for unsupervised entity linking",
    "citation_count": 10,
    "authors": [
      "Akhil Arora",
      "Alberto Garcia-Duran",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.635": {
    "title": "TDEER: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations",
    "volume": "main",
    "abstract": "Joint extraction of entities and relations from unstructured texts to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding relation. However, it is still challenging to handle this task efficiently, especially for the overlapping triple problem. To address such a problem, this paper proposes a novel efficient entities and relations extraction model called TDEER, which stands for Translating Decoding Schema for Joint Extraction of Entities and Relations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as subject + relation → objects. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance model robustness, we introduce negative samples to alleviate error accumulation at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful baselines. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at https://github.com/4AI/TDEER",
    "checked": true,
    "id": "b292a7b33f939083879c28846f9ff2fb4fe9c79d",
    "semantic_title": "tdeer: an efficient translating decoding schema for joint extraction of entities and relations",
    "citation_count": 59,
    "authors": [
      "Xianming Li",
      "Xiaotian Luo",
      "Chenghao Dong",
      "Daichuan Yang",
      "Beidi Luan",
      "Zhen He"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.636": {
    "title": "Extracting Event Temporal Relations via Hyperbolic Geometry",
    "volume": "main",
    "abstract": "Detecting events and their evolution through time is a crucial task in natural language understanding. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the Euclidean space and train a classifier to detect temporal relations between event pairs. However, embeddings in the Euclidean space cannot capture richer asymmetric relations such as event temporal relations. We thus propose to embed events into hyperbolic spaces, which are intrinsically oriented at modeling hierarchical structures. We introduce two approaches to encode events and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer event relations through simple geometrical operations. In the second one, we devise an end-to-end architecture composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different geometrical space, resulting in state-of-the-art performance on several standard metrics. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces",
    "checked": true,
    "id": "8c4e9f559367fbbf731e97f66cad5559373863b0",
    "semantic_title": "extracting event temporal relations via hyperbolic geometry",
    "citation_count": 24,
    "authors": [
      "Xingwei Tan",
      "Gabriele Pergola",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.637": {
    "title": "Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention",
    "volume": "main",
    "abstract": "Event detection has long been troubled by the trigger curse: overfitting the trigger will harm the generalization ability while underfitting it will hurt the detection performance. This problem is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem in few-shot event detection (FSED) from a causal view. By formulating FSED with a structural causal model (SCM), we found that the trigger is a confounder of the context and the result, which makes previous FSED methods much easier to overfit triggers. To resolve this problem, we propose to intervene on the context via backdoor adjustment during training. Experiments show that our method significantly improves the FSED on both ACE05 and MAVEN datasets",
    "checked": true,
    "id": "df65efe264627303cbcbe89508d703fe8f9c716a",
    "semantic_title": "honey or poison? solving the trigger curse in few-shot event detection via causal intervention",
    "citation_count": 27,
    "authors": [
      "Jiawei Chen",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.638": {
    "title": "Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction",
    "volume": "main",
    "abstract": "Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we perform an exhaustive and large-scale empirical comparison of both statistical and graph-based term weighting methods in the context of keyword extraction. Our analysis reveals some interesting findings such as the advantages of the less-known lexical specificity with respect to tf-idf, or the qualitative differences between statistical and graph-based methods. Finally, based on our findings we discuss and devise some suggestions for practitioners. Source code to reproduce our experimental results, including a keyword extraction library, are available in the following repository: https://github.com/asahi417/kex",
    "checked": true,
    "id": "fc679882be74a279cb548c0cd6c0c4df02bad1e3",
    "semantic_title": "back to the basics: a quantitative analysis of statistical and graph-based term weighting schemes for keyword extraction",
    "citation_count": 15,
    "authors": [
      "Asahi Ushio",
      "Federico Liberatore",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.639": {
    "title": "Time-dependent Entity Embedding is not All You Need: A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework",
    "volume": "main",
    "abstract": "Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The models usually contain two parts, a temporal embedding layer and a score function derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different score functions and training strategies, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we systematically study six temporal embedding approaches and empirically quantify their performance across a wide range of configurations with about 3000 experiments and 13159 GPU hours. We classify the temporal embeddings into two classes: (1) timestamp embeddings and (2) time-dependent entity embeddings. Despite the common belief that the latter is more expressive, an extensive experimental study shows that timestamp embeddings can achieve on-par or even better performance with significantly fewer parameters. Moreover, we find that when trained appropriately, the relative performance differences between various temporal embeddings often shrink and sometimes even reverse when compared to prior results. For example, TTransE (CITATION), one of the first temporal KG models, can outperform more recent architectures on ICEWS datasets. To foster further research, we provide the first unified open-source framework for temporal KG completion models with full composability, where temporal embeddings, score functions, loss functions, regularizers, and the explicit modeling of reciprocal relations can be combined arbitrarily",
    "checked": true,
    "id": "f0e29bcea02510d992bd4b2ac4f2b63a8ea37705",
    "semantic_title": "time-dependent entity embedding is not all you need: a re-evaluation of temporal knowledge graph completion models under a unified framework",
    "citation_count": 23,
    "authors": [
      "Zhen Han",
      "Gengyuan Zhang",
      "Yunpu Ma",
      "Volker Tresp"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.640": {
    "title": "Matching-oriented Embedding Quantization For Ad-hoc Retrieval",
    "volume": "main",
    "abstract": "Product quantization (PQ) is a widely used technique for ad-hoc retrieval. Recent studies propose supervised PQ, where the embedding and quantization models can be jointly trained with supervised learning. However, there is a lack of appropriate formulation of the joint training objective; thus, the improvements over previous non-supervised baselines are limited in reality. In this work, we propose the Matching-oriented Product Quantization (MoPQ), where a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the minimization of MCL, we are able to maximize the matching probability of query and ground-truth key, which contributes to the optimal retrieval accuracy. Given that the exact computation of MCL is intractable due to the demand of vast contrastive samples, we further propose the Differentiable Cross-device Sampling (DCS), which significantly augments the contrastive samples for precise approximation of MCL. We conduct extensive experimental studies on four real-world datasets, whose results verify the effectiveness of MoPQ. The code is available at https://github.com/microsoft/MoPQ",
    "checked": true,
    "id": "9e9f25efb20fdbcf3e3f484719a9b70455a6bad1",
    "semantic_title": "matching-oriented embedding quantization for ad-hoc retrieval",
    "citation_count": 9,
    "authors": [
      "Shitao Xiao",
      "Zheng Liu",
      "Yingxia Shao",
      "Defu Lian",
      "Xing Xie"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.641": {
    "title": "Efficient Mind-Map Generation via Sequence-to-Graph and Reinforced Graph Refinement",
    "volume": "main",
    "abstract": "A mind-map is a diagram that represents the central concept and key ideas in a hierarchical way. Converting plain text into a mind-map will reveal its key semantic structure and be easier to understand. Given a document, the existing automatic mind-map generation method extracts the relationships of every sentence pair to generate the directed semantic graph for this document. The computation complexity increases exponentially with the length of the document. Moreover, it is difficult to capture the overall semantics. To deal with the above challenges, we propose an efficient mind-map generation network that converts a document into a graph via sequence-to-graph. To guarantee a meaningful mind-map, we design a graph refinement module to adjust the relation graph in a reinforcement learning manner. Extensive experimental results demonstrate that the proposed approach is more effective and efficient than the existing methods. The inference time is reduced by thousands of times compared with the existing methods. The case studies verify that the generated mind-maps better reveal the underlying semantic structures of the document",
    "checked": true,
    "id": "71a0b9724a46fea44d18d513a815a7a71a86e82d",
    "semantic_title": "efficient mind-map generation via sequence-to-graph and reinforced graph refinement",
    "citation_count": 4,
    "authors": [
      "Mengting Hu",
      "Honglei Guo",
      "Shiwan Zhao",
      "Hang Gao",
      "Zhong Su"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.642": {
    "title": "Deep Attention Diffusion Graph Neural Networks for Text Classification",
    "volume": "main",
    "abstract": "Text classification is a fundamental task with broad applications in natural language processing. Recently, graph neural networks (GNNs) have attracted much attention due to their powerful representation ability. However, most existing methods for text classification based on GNNs consider only one-hop neighborhoods and low-frequency information within texts, which cannot fully utilize the rich context information of documents. Moreover, these models suffer from over-smoothing issues if many graph layers are stacked. In this paper, a Deep Attention Diffusion Graph Neural Network (DADGNN) model is proposed to learn text representations, bridging the chasm of interaction difficulties between a word and its distant neighbors. Experimental results on various standard benchmark datasets demonstrate the superior performance of the present approach",
    "checked": true,
    "id": "7c6210a7b2dcc2f65a43d29422acc51380083f71",
    "semantic_title": "deep attention diffusion graph neural networks for text classification",
    "citation_count": 56,
    "authors": [
      "Yonghao Liu",
      "Renchu Guan",
      "Fausto Giunchiglia",
      "Yanchun Liang",
      "Xiaoyue Feng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.643": {
    "title": "Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution",
    "volume": "main",
    "abstract": "Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multi-label text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from PubMed with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used loss functions. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in natural language processing. Source code is available at https://github.com/blessu/BalancedLossNLP",
    "checked": true,
    "id": "2fe1c8221dd90ad8655d4b518f9c10420cc6ed55",
    "semantic_title": "balancing methods for multi-label text classification with long-tailed class distribution",
    "citation_count": 82,
    "authors": [
      "Yi Huang",
      "Buse Giledereli",
      "Abdullatif Köksal",
      "Arzucan Özgür",
      "Elif Ozkirimli"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.644": {
    "title": "Bayesian Topic Regression for Causal Inference",
    "volume": "main",
    "abstract": "Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the Frisch-Waugh-Lovell theorem. Our paper makes two main contributions. First, we provide a regression framework that allows causal inference in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks",
    "checked": true,
    "id": "dd907add7ca274cfa708d28da88bea85d9cc5f89",
    "semantic_title": "bayesian topic regression for causal inference",
    "citation_count": 10,
    "authors": [
      "Maximilian Ahrens",
      "Julian Ashwin",
      "Jan-Peter Calliess",
      "Vu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.645": {
    "title": "Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience",
    "volume": "main",
    "abstract": "Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks",
    "checked": true,
    "id": "311e48e1c4a0dcd65a6699376ffc85a24a333a56",
    "semantic_title": "enjoy the salience: towards better transformer-based faithful explanations with word salience",
    "citation_count": 20,
    "authors": [
      "George Chrysostomou",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.646": {
    "title": "What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models",
    "volume": "main",
    "abstract": "The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit emergent behaviour, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This emergent behaviour suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and generalization",
    "checked": true,
    "id": "2e2ea29356e006fdbedb7592caf5090260f81f1e",
    "semantic_title": "what's in your head? emergent behaviour in multi-task transformer models",
    "citation_count": 11,
    "authors": [
      "Mor Geva",
      "Uri Katz",
      "Aviv Ben-Arie",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.647": {
    "title": "Don't Search for a Search Method — Simple Heuristics Suffice for Adversarial Text Attacks",
    "volume": "main",
    "abstract": "Recently more attention has been given to adversarial attacks on neural networks for natural language processing (NLP). A central research topic has been the investigation of search algorithms and search constraints, accompanied by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple heuristics exploiting nearest neighbors without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks",
    "checked": true,
    "id": "0b7c3a530d126370fe2fed62974091f36731ec45",
    "semantic_title": "don't search for a search method — simple heuristics suffice for adversarial text attacks",
    "citation_count": 5,
    "authors": [
      "Nathaniel Berger",
      "Stefan Riezler",
      "Sebastian Ebert",
      "Artem Sokolov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.648": {
    "title": "Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods",
    "volume": "main",
    "abstract": "Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model's predictions on test instances. We use these influential triples as adversarial deletions. We further propose a heuristic method to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62% over the baselines",
    "checked": true,
    "id": "41c4b45a27222e9e6f0a0da677d8033c29b85026",
    "semantic_title": "adversarial attacks on knowledge graph embeddings via instance attribution methods",
    "citation_count": 21,
    "authors": [
      "Peru Bhardwaj",
      "John Kelleher",
      "Luca Costabello",
      "Declan O’Sullivan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.649": {
    "title": "Locke's Holiday: Belief Bias in Machine Reading",
    "volume": "main",
    "abstract": "I highlight a simple failure mode of state-of-the-art machine reading systems: when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer What did Elizabeth want? correctly in the context of ‘My kingdom for a cough drop, cried Queen Elizabeth.' Biased by co-occurrence statistics in the training data of pretrained language models, systems predict my kingdom, rather than a cough drop. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of machine reading systems on Auto-Locke show the pervasiveness of belief bias in machine reading",
    "checked": true,
    "id": "a1f4d7ee567023028fe4f1ead1b0bb6a494d7456",
    "semantic_title": "locke's holiday: belief bias in machine reading",
    "citation_count": 5,
    "authors": [
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.650": {
    "title": "Sequence Length is a Domain: Length-based Overfitting in Transformer Models",
    "volume": "main",
    "abstract": "Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence",
    "checked": true,
    "id": "64a0a4f357be12aaf30cc6e4964d1c3a9d927aac",
    "semantic_title": "sequence length is a domain: length-based overfitting in transformer models",
    "citation_count": 58,
    "authors": [
      "Dusan Varis",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.651": {
    "title": "Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification",
    "volume": "main",
    "abstract": "Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient",
    "checked": true,
    "id": "cc9d336749119a2053ac3bb2c5a9d0bdb4d1e1dc",
    "semantic_title": "contrasting human- and machine-generated word-level adversarial examples for text classification",
    "citation_count": 7,
    "authors": [
      "Maximilian Mozes",
      "Max Bartolo",
      "Pontus Stenetorp",
      "Bennett Kleinberg",
      "Lewis Griffin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.652": {
    "title": "Is Information Density Uniform in Task-Oriented Dialogues?",
    "volume": "main",
    "abstract": "The Uniform Information Density principle states that speakers plan their utterances to reduce fluctuations in the density of the information transmitted. In this paper, we test whether, and within which contextual units this principle holds in task-oriented dialogues. We show that there is evidence supporting the principle in written dialogues where participants play a cooperative reference game as well as in spoken dialogues involving instruction giving and following. Our study underlines the importance of identifying the relevant contextual components, showing that information content increases particularly within topically and referentially related contextual units",
    "checked": true,
    "id": "ce3d778838cfa51a8ce7744a68dffaf20fd5763f",
    "semantic_title": "is information density uniform in task-oriented dialogues?",
    "citation_count": 19,
    "authors": [
      "Mario Giulianelli",
      "Arabella Sinclair",
      "Raquel Fernández"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.653": {
    "title": "On Homophony and Rényi Entropy",
    "volume": "main",
    "abstract": "Homophony's widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time, e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language's homophony: the sample Rényi entropy. Then, we use this quantification to revisit Trott and Bergen's claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony—a much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's findings",
    "checked": true,
    "id": "b0f82e0b52335e13297f5a1f98723d4a3684e9c4",
    "semantic_title": "on homophony and rényi entropy",
    "citation_count": 2,
    "authors": [
      "Tiago Pimentel",
      "Clara Meister",
      "Simone Teufel",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.654": {
    "title": "Synthetic Textual Features for the Large-Scale Detection of Basic-level Categories in English and Mandarin",
    "volume": "main",
    "abstract": "Basic-level categories (BLC) are an important psycholinguistic concept introduced by Rosch et al. (1976); they are defined as the most inclusive categories for which a concrete mental image of the category as a whole can be formed, and also as those categories which are acquired early in life. Rosch's original algorithm for detecting BLC (called cue-validity) is based on the availability of semantic features such as \"has tail\" for \"cat\", and has remained untested at large. An at-scale algorithm for the automatic determination of BLC exists, but it operates without Rosch-style semantic features, and is thus unable to verify Rosch's hypothesis. We present the first method for the detection of BLC at scale that makes use of Rosch-style semantic features. For both English and Mandarin, we test three methods of generating such features for any synset within Wordnet (WN): extraction of textual features from Wikipedia pages, Distributional Memory (DM) and BART. The best of our methods outperforms the current SoA in BLC detection, with an accuracy of English BLC detection of 75.0%, and of Mandarin BLC detection 80.7% on a test set. When applied to all of WordNet, our model predicts that 1,118 synsets in English Wordnet (1.4%) are BLC, far fewer than existing methods, and with a precision improvement of over 200% over these. As well as confirming the usefulness of Rosch's cue validity algorithm, we also developed and evaluated our own new indicator for BLC, which models the fact that BLC features tend to be BLC themselves",
    "checked": true,
    "id": "37cc3accccf7586de5013c3888361466ba9baa89",
    "semantic_title": "synthetic textual features for the large-scale detection of basic-level categories in english and mandarin",
    "citation_count": 0,
    "authors": [
      "Yiwen Chen",
      "Simone Teufel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.655": {
    "title": "TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting",
    "volume": "main",
    "abstract": "Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult that faces two main challenges: (1) how to effectively model the time information to handle future timestamps? (2) how to make inductive inference to handle previously unseen entities that emerge over time? To address these challenges, we propose the first reinforcement learning method for forecasting. Specifically, the agent travels on historical knowledge graph snapshots to search for the answer. Our method defines a relative time encoding function to capture the timespan information, and we design a novel time-shaped reward based on Dirichlet distribution to guide the model learning. Furthermore, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for this link prediction task at future timestamps. Extensive experiments on four benchmark datasets demonstrate substantial performance improvement meanwhile with higher explainability, less calculation, and fewer parameters when compared with existing state-of-the-art methods",
    "checked": true,
    "id": "4c3c152fa3942b8cd93031424b0b33f59ba1896e",
    "semantic_title": "timetraveler: reinforcement learning for temporal knowledge graph forecasting",
    "citation_count": 163,
    "authors": [
      "Haohai Sun",
      "Jialun Zhong",
      "Yunpu Ma",
      "Zhen Han",
      "Kun He"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.656": {
    "title": "Code-switched inspired losses for spoken dialog representations",
    "volume": "main",
    "abstract": "Spoken dialogue systems need to be able to handle both multiple languages and multilinguality inside a conversation (e.g in case of code-switching). In this work, we introduce new pretraining losses tailored to learn generic multilingual spoken dialogue representations. The goal of these losses is to expose the model to code-switched language. In order to scale up training, we automatically build a pretraining corpus composed of multilingual conversations in five different languages (French, Italian, English, German and Spanish) from OpenSubtitles, a huge multilingual corpus composed of 24.3G tokens. We test the generic representations on MIAM, a new benchmark composed of five dialogue act corpora on the same aforementioned languages as well as on two novel multilingual tasks (i.e multilingual mask utterance retrieval and multilingual inconsistency identification). Our experiments show that our new losses achieve a better performance in both monolingual and multilingual settings",
    "checked": true,
    "id": "657a8a8c83339dff13892b26bb989f97b2acb182",
    "semantic_title": "code-switched inspired losses for spoken dialog representations",
    "citation_count": 12,
    "authors": [
      "Pierre Colombo",
      "Emile Chapuis",
      "Matthieu Labeau",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.657": {
    "title": "BiQUE: Biquaternionic Embeddings of Knowledge Graphs",
    "volume": "main",
    "abstract": "Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge graphs (KGs). Existing KGE models rely on geometric operations to model relational patterns. Euclidean (circular) rotation is useful for modeling patterns such as symmetry, but cannot represent hierarchical semantics. In contrast, hyperbolic models are effective at modeling hierarchical relations, but do not perform as well on patterns on which circular rotation excels. It is crucial for KGE models to unify multiple geometric transformations so as to fully cover the multifarious relations in KGs. To do so, we propose BiQUE, a novel model that employs biquaternions to integrate multiple geometric transformations, viz., scaling, translation, Euclidean rotation, and hyperbolic rotation. BiQUE makes the best trade-offs among geometric operators during training, picking the best one (or their best combination) for each relation. Experiments on five datasets show BiQUE's effectiveness",
    "checked": true,
    "id": "190f71c6f835f3040cbe2be7f15b05b117dced9d",
    "semantic_title": "bique: biquaternionic embeddings of knowledge graphs",
    "citation_count": 33,
    "authors": [
      "Jia Guo",
      "Stanley Kok"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.658": {
    "title": "Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs",
    "volume": "main",
    "abstract": "There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea of neural ordinary differential equations (ODEs) to multi-relational graph convolutional networks. The proposed model preserves the continuous nature of dynamic multi-relational graph data and encodes both temporal and structural information into continuous-time dynamic embeddings. In addition, a novel graph transition layer is applied to capture the transitions on the dynamic graph, i.e., edge formation and dissolution. We perform extensive experiments on five benchmark datasets for temporal KG reasoning, showing our model's superior performance on the future link forecasting task",
    "checked": true,
    "id": "a937a182c37ed75fc0806289516c1fa0c5019454",
    "semantic_title": "learning neural ordinary equations for forecasting future links on temporal knowledge graphs",
    "citation_count": 122,
    "authors": [
      "Zhen Han",
      "Zifeng Ding",
      "Yunpu Ma",
      "Yujia Gu",
      "Volker Tresp"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.659": {
    "title": "RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models",
    "volume": "main",
    "abstract": "Backdoor attacks, which maliciously control a well-trained model's outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at https://github.com/lancopku/RAP",
    "checked": true,
    "id": "ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8",
    "semantic_title": "rap: robustness-aware perturbations for defending against backdoor attacks on nlp models",
    "citation_count": 113,
    "authors": [
      "Wenkai Yang",
      "Yankai Lin",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.660": {
    "title": "FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input Representations",
    "volume": "main",
    "abstract": "Combining several embeddings typically improves performance in downstream tasks as different embeddings encode different information. It has been shown that even models using embeddings from transformers still benefit from the inclusion of standard word embeddings. However, the combination of embeddings of different types and dimensions is challenging. As an alternative to attention-based meta-embeddings, we propose feature-based adversarial meta-embeddings (FAME) with an attention function that is guided by features reflecting word-specific properties, such as shape and frequency, and show that this is beneficial to handle subword-based embeddings. In addition, FAME uses adversarial training to optimize the mappings of differently-sized embeddings to the same space. We demonstrate that FAME works effectively across languages and domains for sequence labeling and sentence classification, in particular in low-resource settings. FAME sets the new state of the art for POS tagging in 27 languages, various NER settings and question classification in different domains",
    "checked": true,
    "id": "df59a7bdb67413b07f1a11a687a9942d02b926d0",
    "semantic_title": "fame: feature-based adversarial meta-embeddings for robust input representations",
    "citation_count": 7,
    "authors": [
      "Lukas Lange",
      "Heike Adel",
      "Jannik Strötgen",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.661": {
    "title": "A Strong Baseline for Query Efficient Attacks in a Black Box Setting",
    "volume": "main",
    "abstract": "Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a consistent search space while comparing different search methods. In this paper, we propose a query efficient attack strategy to generate plausible adversarial examples on text classification and entailment tasks. Our attack jointly leverages attention mechanism and locality sensitive hashing (LSH) to reduce the query count. We demonstrate the efficacy of our approach by comparing our attack with four baselines across three different search spaces. Further, we benchmark our results across the same search space used in prior attacks. In comparison to attacks proposed, on an average, we are able to reduce the query count by 75% across all datasets and target models. We also demonstrate that our attack achieves a higher success rate when compared to prior attacks in a limited query setting",
    "checked": true,
    "id": "b958872bbe8d6cb37ebad96173399403482bf340",
    "semantic_title": "a strong baseline for query efficient attacks in a black box setting",
    "citation_count": 33,
    "authors": [
      "Rishabh Maheshwary",
      "Saket Maheshwary",
      "Vikram Pudi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.662": {
    "title": "Machine Translation Decoding beyond Beam Search",
    "volume": "main",
    "abstract": "Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area",
    "checked": true,
    "id": "e8cc5b6204970a88cd1b2df491aa10c4333e083e",
    "semantic_title": "machine translation decoding beyond beam search",
    "citation_count": 71,
    "authors": [
      "Rémi Leblond",
      "Jean-Baptiste Alayrac",
      "Laurent Sifre",
      "Miruna Pislar",
      "Lespiau Jean-Baptiste",
      "Ioannis Antonoglou",
      "Karen Simonyan",
      "Oriol Vinyals"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.663": {
    "title": "Document Graph for Neural Machine Translation",
    "volume": "main",
    "abstract": "Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English–French, Chinese-English, WMT English–German and Opensubtitle English–Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena",
    "checked": true,
    "id": "37f4cecf9cf83cb6de2ffaecc864226eff1f9918",
    "semantic_title": "document graph for neural machine translation",
    "citation_count": 26,
    "authors": [
      "Mingzhou Xu",
      "Liangyou Li",
      "Derek F. Wong",
      "Qun Liu",
      "Lidia S. Chao"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.664": {
    "title": "An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results. In this paper, we investigate the benefits of an explicit alignment to language labels in Transformer-based MNMT models in the zero-shot context, by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label. We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs",
    "checked": true,
    "id": "229ee685a5915897795b9f66fc7b2f884551dff6",
    "semantic_title": "an empirical investigation of word alignment supervision for zero-shot multilingual neural machine translation",
    "citation_count": 9,
    "authors": [
      "Alessandro Raganato",
      "Raúl Vázquez",
      "Mathias Creutz",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.665": {
    "title": "Graph Algorithms for Multiparallel Word Alignment",
    "volume": "main",
    "abstract": "With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts, and can be used in the decoding of machine translation systems. At the same time, massively multilingual processing is becoming an important NLP scenario, and pretrained language and machine translation models that are truly multilingual are proposed. However, most alignment algorithms rely on bitexts only and do not leverage the fact that many parallel corpora are multiparallel. In this work, we exploit the multiparallelity of corpora by representing an initial set of bilingual alignments as a graph and then predicting additional edges in the graph. We present two graph algorithms for edge prediction: one inspired by recommender systems and one based on network link prediction. Our experimental results show absolute improvements in F1 of up to 28% over the baseline bilingual word aligner in different datasets",
    "checked": true,
    "id": "cb0974e233a7195eb2db4e628e8929cbbaf89088",
    "semantic_title": "graph algorithms for multiparallel word alignment",
    "citation_count": 6,
    "authors": [
      "Ayyoob Imani",
      "Masoud Jalili Sabet",
      "Lutfi Kerem Senel",
      "Philipp Dufter",
      "François Yvon",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.666": {
    "title": "Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation",
    "volume": "main",
    "abstract": "Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original translation system. We find that we can improve over the performance trade-off offered by Elastic Weight Consolidation with a relatively simple data mixing strategy. At comparable performance on the new domains, catastrophic forgetting is mitigated significantly on strong WMT baselines. Combining both approaches improves the Pareto frontier on this task",
    "checked": true,
    "id": "354baa59362da14e2d6f50d5ebaa76f672d0f7af",
    "semantic_title": "improving the quality trade-off for neural machine translation multi-domain adaptation",
    "citation_count": 9,
    "authors": [
      "Eva Hasler",
      "Tobias Domhan",
      "Jonay Trenous",
      "Ke Tran",
      "Bill Byrne",
      "Felix Hieber"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.667": {
    "title": "Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT",
    "volume": "main",
    "abstract": "Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection",
    "checked": true,
    "id": "5f918b7c01c31364c30aa2c57ec6df1e3b6f5f18",
    "semantic_title": "language modeling, lexical translation, reordering: the training process of nmt through the lens of classical smt",
    "citation_count": 23,
    "authors": [
      "Elena Voita",
      "Rico Sennrich",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.668": {
    "title": "Effective Fine-Tuning Methods for Cross-lingual Adaptation",
    "volume": "main",
    "abstract": "Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as a complementary to multilingual language modeling using the unlabeled data in the target language. We also propose an adaption method based on contrastive learning to better capture the semantic relationship in the parallel data, when a few translation pairs are available. To show our method's effectiveness, we conduct extensive experiments on cross-lingual inference and review classification tasks across various languages. We report significant gains compared to directly fine-tuning multilingual pre-trained models and other semi-supervised alternatives",
    "checked": true,
    "id": "e01124ff4a3ba76fcac040597395b816cc074bc6",
    "semantic_title": "effective fine-tuning methods for cross-lingual adaptation",
    "citation_count": 11,
    "authors": [
      "Tao Yu",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.669": {
    "title": "Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach",
    "volume": "main",
    "abstract": "In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA approach in which we generate new sentence pairs with transformations, such as reversing the order of the target sentence, which produce unfluent target sentences. During training, these augmented sentences are used as auxiliary tasks in a multi-task framework with the aim of providing new contexts where the target prefix is not informative enough to predict the next word. This strengthens the encoder and forces the decoder to pay more attention to the source representations of the encoder. Experiments carried out on six low-resource translation tasks show consistent improvements over the baseline and over DA methods aiming at extending the support of the empirical data distribution. The systems trained with our approach rely more on the source tokens, are more robust against domain shift and suffer less hallucinations",
    "checked": true,
    "id": "d539b8ce8e94025200468a49099802491396a9d4",
    "semantic_title": "rethinking data augmentation for low-resource neural machine translation: a multi-task learning approach",
    "citation_count": 27,
    "authors": [
      "Víctor M. Sánchez-Cartagena",
      "Miquel Esplà-Gomis",
      "Juan Antonio Pérez-Ortiz",
      "Felipe Sánchez-Martínez"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.670": {
    "title": "Wino-X: Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution",
    "volume": "main",
    "abstract": "Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to English, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesirable biases and unable to detect disambiguating information. We quantify biases using established statistical methods and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages",
    "checked": true,
    "id": "f5ba443a4fc17e1b3bb8fcfedcbace6f3dd6dd68",
    "semantic_title": "wino-x: multilingual winograd schemas for commonsense reasoning and coreference resolution",
    "citation_count": 25,
    "authors": [
      "Denis Emelin",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.671": {
    "title": "One Source, Two Targets: Challenges and Rewards of Dual Decoding",
    "volume": "main",
    "abstract": "Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four applications. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations",
    "checked": true,
    "id": "be331172867850eab74435f2b0919a69b5609f1d",
    "semantic_title": "one source, two targets: challenges and rewards of dual decoding",
    "citation_count": 6,
    "authors": [
      "Jitao Xu",
      "François Yvon"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.672": {
    "title": "Discrete and Soft Prompting for Multilingual Models",
    "volume": "main",
    "abstract": "It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline (33.33%). In contrast, discrete and soft prompting outperform finetuning, achieving 36.43% and 38.79%. We also demonstrate good performance of prompting with training data in multiple languages other than English",
    "checked": true,
    "id": "769f8b6d73fcb5b5013ae2c1c48b94c801c88ba3",
    "semantic_title": "discrete and soft prompting for multilingual models",
    "citation_count": 73,
    "authors": [
      "Mengjie Zhao",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.673": {
    "title": "Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models",
    "volume": "main",
    "abstract": "Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly used evaluation benchmark, also known as Multi30K, where the translations of image captions were prepared without actually showing the images to human translators. In this paper, we present a qualitative study that examines the role of datasets in stimulating the leverage of visual modality and we propose methods to highlight the importance of visual signals in the datasets which demonstrate improvements in reliance of models on the source images. Our findings suggest the research on effective MMT architectures is currently impaired by the lack of suitable datasets and careful consideration must be taken in creation of future MMT datasets, for which we also provide useful insights",
    "checked": true,
    "id": "8e988bf81b506edc2fd8be71db4831ccf80f3db9",
    "semantic_title": "vision matters when it should: sanity checking multimodal machine translation models",
    "citation_count": 34,
    "authors": [
      "Jiaoda Li",
      "Duygu Ataman",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.674": {
    "title": "Efficient Inference for Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with several \"light decoder\" architectures in two 20-language multi-parallel settings: small-scale on TED Talks and large-scale on ParaCrawl. Our experiments demonstrate that combining a shallow decoder with vocabulary filtering leads to almost 2 times faster inference with no loss in translation quality. We validate our findings with BLEU and chrF (on 380 language pairs), robustness evaluation and human evaluation",
    "checked": true,
    "id": "31bfcbb1cbd74e8f38d97911f987108bd9517c6b",
    "semantic_title": "efficient inference for multilingual neural machine translation",
    "citation_count": 12,
    "authors": [
      "Alexandre Berard",
      "Dain Lee",
      "Stephane Clinchant",
      "Kweonwoo Jung",
      "Vassilina Nikoulina"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.675": {
    "title": "Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages",
    "volume": "main",
    "abstract": "We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications, compared to models fine-tuned on individual languages. A first of its kind detailed study is presented to track performance change as languages are added to a base language in a graded and greedy (in the sense of best boost of performance) manner; which reveals that careful selection of subset of related languages can significantly improve performance than utilizing all related languages. The Indo-Aryan (IA) language family is chosen for the study, the exact languages being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script barrier is crossed by simple rule-based transliteration of the text of all languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource languages, such as Oriya and Punjabi, are found to be the largest beneficiaries of multilingual fine-tuning. Textual Entailment, Entity Classification, Section Title Prediction, tasks of IndicGLUE and POS tagging form our test bed. Compared to monolingual fine tuning we get relative performance improvement of up to 150% in the downstream tasks. The surprise take-away is that for any language there is a particular combination of other languages which yields the best performance, and any additional language is in fact detrimental",
    "checked": true,
    "id": "719cafe39b732589aecf81f9d0eafcd84bdc3e99",
    "semantic_title": "role of language relatedness in multilingual fine-tuning of language models: a case study in indo-aryan languages",
    "citation_count": 28,
    "authors": [
      "Tejas Dhamecha",
      "Rudra Murthy",
      "Samarth Bharadwaj",
      "Karthik Sankaranarayanan",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.676": {
    "title": "Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification",
    "volume": "main",
    "abstract": "Traditional hand-crafted linguistically-informed features have often been used for distinguishing between translated and original non-translated texts. By contrast, to date, neural architectures without manual feature engineering have been less explored for this task. In this work, we (i) compare the traditional feature-engineering-based approach to the feature-learning-based one and (ii) analyse the neural architectures in order to investigate how well the hand-crafted features explain the variance in the neural models' predictions. We use pre-trained neural word embeddings, as well as several end-to-end neural architectures in both monolingual and multilingual settings and compare them to feature-engineering-based SVM classifiers. We show that (i) neural architectures outperform other approaches by more than 20 accuracy points, with the BERT-based model performing the best in both the monolingual and multilingual settings; (ii) while many individual hand-crafted translationese features correlate with neural model predictions, feature importance analysis shows that the most important features for neural and classical architectures differ; and (iii) our multilingual experiments provide empirical evidence for translationese universals across languages",
    "checked": true,
    "id": "271027f9051b4a66e897634a9a8e10d13b93c97b",
    "semantic_title": "comparing feature-engineering and feature-learning approaches for multilingual translationese classification",
    "citation_count": 17,
    "authors": [
      "Daria Pylypenko",
      "Kwabena Amponsah-Kaakyire",
      "Koel Dutta Chowdhury",
      "Josef van Genabith",
      "Cristina España-Bonet"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.677": {
    "title": "Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation",
    "volume": "main",
    "abstract": "Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for autoregressive NMT models, there is still no consensus on its underlying cause. In this work, we analyze errors that cause major quality degradation with large beams in NMT and Automatic Speech Recognition (ASR). We show that a factor that strongly contributes to the quality degradation with large beams is dataset length-bias - NMT datasets are strongly biased towards short sentences. To mitigate this issue, we propose a new data augmentation technique – Multi-Sentence Resampling (MSR). This technique extends the training examples by concatenating several sentences from the original dataset to make a long training example. We demonstrate that MSR significantly reduces degradation with growing beam size and improves final translation quality on the IWSTL15 En-Vi, IWSTL17 En-Fr, and WMT14 En-De datasets",
    "checked": true,
    "id": "0f79d7c74ccb483c79cb28dfdc8811646efe6aa9",
    "semantic_title": "multi-sentence resampling: a simple approach to alleviate dataset length bias and beam-search degradation",
    "citation_count": 4,
    "authors": [
      "Ivan Provilkov",
      "Andrey Malinin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.678": {
    "title": "Cross-Policy Compliance Detection via Question Answering",
    "volume": "main",
    "abstract": "Policy compliance detection is the task of ensuring that a scenario conforms to a policy (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of textual entailment, which results in poor accuracy due to the complexity of the policies. In this paper we propose to address policy compliance detection via decomposing it into question answering, where questions check whether the conditions stated in the policy apply to the scenario, and an expression tree combines the answers to obtain the label. Despite the initial upfront annotation cost, we demonstrate that this approach results in better accuracy, especially in the cross-policy setup where the policies during testing are unseen in training. In addition, it allows us to use existing question answering models pre-trained on existing large datasets. Finally, it explicitly identifies the information missing from a scenario in case policy compliance cannot be determined. We conduct our experiments using a recent dataset consisting of government policies, which we augment with expert annotations and find that the cost of annotating question answering decomposition is largely offset by improved inter-annotator agreement and speed",
    "checked": true,
    "id": "2fd9ace0ff2e9ff40213f0524cf6ada9b18d56bb",
    "semantic_title": "cross-policy compliance detection via question answering",
    "citation_count": 9,
    "authors": [
      "Marzieh Saeidi",
      "Majid Yazdani",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.679": {
    "title": "Meta-LMTC: Meta-Learning for Large-Scale Multi-Label Text Classification",
    "volume": "main",
    "abstract": "Large-scale multi-label text classification (LMTC) tasks often face long-tailed label distributions, where many labels have few or even no training instances. Although current methods can exploit prior knowledge to handle these few/zero-shot labels, they neglect the meta-knowledge contained in the dataset that can guide models to learn with few samples. In this paper, for the first time, this problem is addressed from a meta-learning perspective. However, the simple extension of meta-learning approaches to multi-label classification is sub-optimal for LMTC tasks due to long-tailed label distribution and coexisting of few- and zero-shot scenarios. We propose a meta-learning approach named META-LMTC. Specifically, it constructs more faithful and more diverse tasks according to well-designed sampling strategies and directly incorporates the objective of adapting to new low-resource tasks into the meta-learning phase. Extensive experiments show that META-LMTC achieves state-of-the-art performance against strong baselines and can still enhance powerful BERTlike models",
    "checked": true,
    "id": "1a465177a95a9529e02884996dc0a9c2fbaea3a7",
    "semantic_title": "meta-lmtc: meta-learning for large-scale multi-label text classification",
    "citation_count": 13,
    "authors": [
      "Ran Wang",
      "Xi’ao Su",
      "Siyu Long",
      "Xinyu Dai",
      "Shujian Huang",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.680": {
    "title": "Unsupervised Multi-View Post-OCR Error Correction With Language Models",
    "volume": "main",
    "abstract": "We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for error correction is too risky. We evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view. We also show the importance of domain adaptation for post-OCR correction on out-of-domain documents",
    "checked": true,
    "id": "a5edb776606696127a27223315af48c01f62797c",
    "semantic_title": "unsupervised multi-view post-ocr error correction with language models",
    "citation_count": 13,
    "authors": [
      "Harsh Gupta",
      "Luciano Del Corro",
      "Samuel Broscheit",
      "Johannes Hoffart",
      "Eliot Brenner"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.681": {
    "title": "Parallel Refinements for Lexically Constrained Text Generation with BART",
    "volume": "main",
    "abstract": "Lexically constrained text generation aims to control the generated text by incorporating certain pre-specified keywords into the output. Previous work injects lexical constraints into the output by controlling the decoding process or refining the candidate output iteratively, which tends to generate generic or ungrammatical sentences, and has high computational complexity. To address these challenges, we proposed Constrained BART (CBART) for lexically constrained text generation. CBART leverages the pre-trained model, BART and transfers part of the generation burden from the decoder to the encoder by decomposing this task into two sub-tasks, thereby improving the sentence quality. Concretely, we extended BART by adding a token-level classifier over the encoder, aiming at instructing the decoder where to replace and insert. Guided by the encoder, the decoder refines multiple tokens of the input in one step by inserting tokens before specific positions and re-predicting tokens at a low confidence level. To further reduce the inference latency, the decoder predicts all tokens in parallel. Experiment results on One-Billion-Word and Yelp show that CBART can generate plausible text with high quality and diversity while largely accelerating inference",
    "checked": true,
    "id": "69fe3902299026f3596e88e70bf651a659308d60",
    "semantic_title": "parallel refinements for lexically constrained text generation with bart",
    "citation_count": 44,
    "authors": [
      "Xingwei He"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.682": {
    "title": "BERT-Beta: A Proactive Probabilistic Approach to Text Moderation",
    "volume": "main",
    "abstract": "Text moderation for user generated content, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and interpretation benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the linear model offers useful insights beyond this work",
    "checked": true,
    "id": "c8b2780824c37140eeada3c451caf2d0a8b77a84",
    "semantic_title": "bert-beta: a proactive probabilistic approach to text moderation",
    "citation_count": 7,
    "authors": [
      "Fei Tan",
      "Yifan Hu",
      "Kevin Yen",
      "Changwei Hu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.683": {
    "title": "STaCK: Sentence Ordering with Temporal Commonsense Knowledge",
    "volume": "main",
    "abstract": "Sentence order prediction is the task of finding the correct order of sentences in a randomly ordered document. Correctly ordering the sentences requires an understanding of coherence with respect to the chronological sequence of events described in the text. Document-level contextual understanding and commonsense knowledge centered around these events are often essential in uncovering this coherence and predicting the exact chronological order. In this paper, we introduce STaCK — a framework based on graph neural networks and temporal commonsense knowledge to model global information and predict the relative order of sentences. Our graph network accumulates temporal evidence using knowledge of ‘past' and ‘future' and formulates sentence ordering as a constrained edge classification problem. We report results on five different datasets, and empirically show that the proposed method is naturally suitable for order prediction. The implementation of this work is available at: https://github.com/declare-lab/sentence-ordering",
    "checked": true,
    "id": "6d8ee0bdad3b20458b16e56d8592d0c898f70ffd",
    "semantic_title": "stack: sentence ordering with temporal commonsense knowledge",
    "citation_count": 11,
    "authors": [
      "Deepanway Ghosal",
      "Navonil Majumder",
      "Rada Mihalcea",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.684": {
    "title": "Preventing Author Profiling through Zero-Shot Multilingual Back-Translation",
    "volume": "main",
    "abstract": "Documents as short as a single sentence may inadvertently reveal sensitive information about their authors, including e.g. their gender or ethnicity. Style transfer is an effective way of transforming texts in order to remove any information that enables author profiling. However, for a number of current state-of-the-art approaches the improved privacy is accompanied by an undesirable drop in the down-stream utility of the transformed data. In this paper, we propose a simple, zero-shot way to effectively lower the risk of author profiling through multilingual back-translation using off-the-shelf translation models. We compare our models with five representative text style transfer models on three datasets across different domains. Results from both an automatic and a human evaluation show that our approach achieves the best overall performance while requiring no training data. We are able to lower the adversarial prediction of gender and race by up to 22% while retaining 95% of the original utility on downstream tasks",
    "checked": true,
    "id": "1a92c78f09c55498d2377b4d6ccebafc36b38a4c",
    "semantic_title": "preventing author profiling through zero-shot multilingual back-translation",
    "citation_count": 5,
    "authors": [
      "David Ifeoluwa Adelani",
      "Miaoran Zhang",
      "Xiaoyu Shen",
      "Ali Davody",
      "Thomas Kleinbauer",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.685": {
    "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
    "volume": "main",
    "abstract": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5",
    "checked": true,
    "id": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
    "semantic_title": "codet5: identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
    "citation_count": 1673,
    "authors": [
      "Yue Wang",
      "Weishi Wang",
      "Shafiq Joty",
      "Steven C.H. Hoi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.686": {
    "title": "Detect and Classify – Joint Span Detection and Classification for Health Outcomes",
    "volume": "main",
    "abstract": "A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and classification is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results",
    "checked": true,
    "id": "1f35da501bec24a0dc70398160df349adbfea820",
    "semantic_title": "detect and classify – joint span detection and classification for health outcomes",
    "citation_count": 12,
    "authors": [
      "Micheal Abaho",
      "Danushka Bollegala",
      "Paula Williamson",
      "Susanna Dodd"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.687": {
    "title": "Multi-Class Grammatical Error Detection for Correction: A Tale of Two Systems",
    "volume": "main",
    "abstract": "In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for English. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class detection using different error type tagsets derived from the ERRANT framework. Output from this detection system is used as auxiliary input to fine-tune a novel encoder-decoder GEC model, and we subsequently re-rank the N-best GEC output to find the hypothesis that most agrees with the GED output. Results show that fine-tuning the GEC system using 4-class GED produces the best model, but re-ranking using 55-class GED leads to the best performance overall. This suggests that different multi-class GED systems benefit GEC in different ways. Ultimately, our system outperforms all other previous work that combines GED and GEC, and achieves a new single-model NMT-based state of the art on the BEA-test benchmark",
    "checked": true,
    "id": "e5807eda58dcc2277b23de7edd06ed7f2b41edca",
    "semantic_title": "multi-class grammatical error detection for correction: a tale of two systems",
    "citation_count": 37,
    "authors": [
      "Zheng Yuan",
      "Shiva Taslimipoor",
      "Christopher Davis",
      "Christopher Bryant"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.688": {
    "title": "Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models",
    "volume": "main",
    "abstract": "Can we get existing language models and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the Winograd Schema Challenge by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the language model utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks",
    "checked": true,
    "id": "383a734d473033378b0cbc83a3556050214c2109",
    "semantic_title": "towards zero-shot commonsense reasoning with self-supervised refinement of language models",
    "citation_count": 9,
    "authors": [
      "Tassilo Klein",
      "Moin Nabi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.689": {
    "title": "To Share or not to Share: Predicting Sets of Sources for Model Transfer Learning",
    "volume": "main",
    "abstract": "In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity — as suggested in prior work — may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how many sources should be exploited. For this, we study the effects of model transfer on sequence labeling across various domains and tasks and show that our methods based on model similarity and support vector machines are able to predict promising sources, resulting in performance increases of up to 24 F1 points",
    "checked": true,
    "id": "29be3eec992d4ce79ea732af510573fb4f436d52",
    "semantic_title": "to share or not to share: predicting sets of sources for model transfer learning",
    "citation_count": 12,
    "authors": [
      "Lukas Lange",
      "Jannik Strötgen",
      "Heike Adel",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.690": {
    "title": "Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case",
    "volume": "main",
    "abstract": "Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our methodology in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from electronic health records. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After fine-tuning with as little as 20% of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the phenotypes annotated by our model as features",
    "checked": true,
    "id": "3654c1fdfca1b49301412d6de83fc3f6e76da686",
    "semantic_title": "self-supervised detection of contextual synonyms in a multi-class setting: phenotype annotation use case",
    "citation_count": 14,
    "authors": [
      "Jingqing Zhang",
      "Luis Bolanos Trujillo",
      "Tong Li",
      "Ashwani Tanwar",
      "Guilherme Freire",
      "Xian Yang",
      "Julia Ive",
      "Vibhor Gupta",
      "Yike Guo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.691": {
    "title": "ClauseRec: A Clause Recommendation Framework for AI-aided Contract Authoring",
    "volume": "main",
    "abstract": "Contracts are a common type of legal document that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such documents, and even lesser in generating them. These contracts are made up of clauses, and the unique nature of these clauses calls for specific methods to understand and generate such documents. In this paper, we introduce the task of clause recommendation, as a first step to aid and accelerate the authoring of contract documents. We propose a two-staged pipeline to first predict if a specific clause type is relevant to be added in a contract, and then recommend the top clauses for the given type based on the contract context. We pre-train BERT on an existing library of clauses with two additional tasks and use it for our prediction and recommendation. We experiment with classification methods and similarity-based heuristics for clause relevance prediction, and generation-based methods for clause recommendation, and evaluate the results from various methods on several clause types. We provide analyses on the results, and further outline the limitations and future directions of this line of research",
    "checked": true,
    "id": "0e9f3dd7a70ea2986dcf78377b5be084fa6ffd60",
    "semantic_title": "clauserec: a clause recommendation framework for ai-aided contract authoring",
    "citation_count": 11,
    "authors": [
      "Vinay Aggarwal",
      "Aparna Garimella",
      "Balaji Vasan Srinivasan",
      "Anandhavelu N",
      "Rajiv Jain"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.692": {
    "title": "Finnish Dialect Identification: The Effect of Audio and Text",
    "volume": "main",
    "abstract": "Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is received by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo",
    "checked": true,
    "id": "efab151109289a721ea5728deacc14fa9f91ce5f",
    "semantic_title": "finnish dialect identification: the effect of audio and text",
    "citation_count": 15,
    "authors": [
      "Mika Hämäläinen",
      "Khalid Alnajjar",
      "Niko Partanen",
      "Jack Rueter"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.693": {
    "title": "English Machine Reading Comprehension Datasets: A Survey",
    "volume": "main",
    "abstract": "This paper surveys 60 English Machine Reading Comprehension datasets, with a view to providing a convenient resource for other researchers interested in this problem. We categorize the datasets according to their question and answer form and compare them across various dimensions including size, vocabulary, data source, method of creation, human performance level, and first question word. Our analysis reveals that Wikipedia is by far the most common data source and that there is a relative lack of why, when, and where questions across datasets",
    "checked": true,
    "id": "94bde15e6636168a388f33d161fc3d7f947a1add",
    "semantic_title": "english machine reading comprehension datasets: a survey",
    "citation_count": 49,
    "authors": [
      "Daria Dzendzik",
      "Jennifer Foster",
      "Carl Vogel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.694": {
    "title": "Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection",
    "volume": "main",
    "abstract": "End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only weak supervision, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a model that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements between two sets of entities. We find that introducing intersection improves performance over a baseline model on two datasets, WebQuestionsSP (69.6% to 73.3% Hits@1) and ComplexWebQuestions (39.8% to 48.7% Hits@1), and in particular, improves performance on questions with multiple entities by over 14% on WebQuestionsSP and by 19% on ComplexWebQuestions",
    "checked": true,
    "id": "f77d63d7014204bd8bb678708880a561ffe0fe42",
    "semantic_title": "expanding end-to-end question answering on differentiable knowledge graphs with intersection",
    "citation_count": 21,
    "authors": [
      "Priyanka Sen",
      "Armin Oliya",
      "Amir Saffari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.695": {
    "title": "Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs",
    "volume": "main",
    "abstract": "We tackle the problem of weakly-supervised conversational Question Answering over large Knowledge Graphs using a neural semantic parsing approach. We introduce a new Logical Form (LF) grammar that can model a wide range of queries on the graph while remaining sufficiently simple to generate supervision data efficiently. Our Transformer-based model takes a JSON-like structure as input, allowing us to easily incorporate both Knowledge Graph and conversational contexts. This structured input is transformed to lists of embeddings and then fed to standard attention layers. We validate our approach, both in terms of grammar coverage and LF execution accuracy, on two publicly available datasets, CSQA and ConvQuestions, both grounded in Wikidata. On CSQA, our approach increases the coverage from 80% to 96.2%, and the LF execution accuracy from 70.6% to 75.6%, with respect to previous state-of-the-art results. On ConvQuestions, we achieve competitive results with respect to the state-of-the-art",
    "checked": true,
    "id": "3d75654553cb1d7d7cffab105a5bf7932818cf85",
    "semantic_title": "structured context and high-coverage grammar for conversational question answering over knowledge graphs",
    "citation_count": 21,
    "authors": [
      "Pierre Marion",
      "Pawel Nowak",
      "Francesco Piccinno"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.696": {
    "title": "Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation",
    "volume": "main",
    "abstract": "Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of adversarial attacks. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make question answering models more robust to human adversaries. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or re-labels them to improve quality. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation and show that our models are considerably more robust to new human-written adversarial examples: crowdworkers can fool our model only 8.8% of the time on average, compared to 17.6% for a model trained without synthetic data",
    "checked": true,
    "id": "d8094a2fa4f3c7a3bf641d12f314a085d166f8a8",
    "semantic_title": "improving question answering model robustness with synthetic adversarial data generation",
    "citation_count": 106,
    "authors": [
      "Max Bartolo",
      "Tristan Thrush",
      "Robin Jia",
      "Sebastian Riedel",
      "Pontus Stenetorp",
      "Douwe Kiela"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.697": {
    "title": "BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief",
    "volume": "main",
    "abstract": "Although pretrained language models (PTLMs) contain significant amounts of world knowledge, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the model actually \"believes\" about the world, making it susceptible to inconsistent behavior and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs – a BeliefBank – that records but then may modify the raw PTLM answers. We describe two mechanisms to improve belief consistency in the overall system. First, a reasoning component – a weighted MaxSAT solver – revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the accuracy and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining",
    "checked": true,
    "id": "ecad7a322ede6de0013b46dc64429eed4c43e8af",
    "semantic_title": "beliefbank: adding memory to a pre-trained language model for a systematic notion of belief",
    "citation_count": 65,
    "authors": [
      "Nora Kassner",
      "Oyvind Tafjord",
      "Hinrich Schütze",
      "Peter Clark"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.698": {
    "title": "MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset",
    "volume": "main",
    "abstract": "Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40% to 55% on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems",
    "checked": true,
    "id": "31acfba3a19f780a3239925ff12a7a4047d6a705",
    "semantic_title": "mlec-qa: a chinese multi-choice biomedical question answering dataset",
    "citation_count": 34,
    "authors": [
      "Jing Li",
      "Shangping Zhong",
      "Kaizhi Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.699": {
    "title": "IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation",
    "volume": "main",
    "abstract": "Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resource—yet widely spoken—languages of Indonesia: Indonesian, Javanese, and Sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks—despite using only one-fifth the parameters of a larger multilingual model, mBART-large (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, localized languages to achieve more efficient learning and faster inference at very low-resource languages like Javanese and Sundanese",
    "checked": true,
    "id": "fe0b1f6194b490f6bbc41c716a58901c1049ccd8",
    "semantic_title": "indonlg: benchmark and resources for evaluating indonesian natural language generation",
    "citation_count": 104,
    "authors": [
      "Samuel Cahyawijaya",
      "Genta Indra Winata",
      "Bryan Wilie",
      "Karissa Vincentio",
      "Xiaohong Li",
      "Adhiguna Kuncoro",
      "Sebastian Ruder",
      "Zhi Yuan Lim",
      "Syafri Bahar",
      "Masayu Khodra",
      "Ayu Purwarianti",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.700": {
    "title": "Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability",
    "volume": "main",
    "abstract": "Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little work has been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics, including path recall, local interpretability, and global interpretability for evaluation, and design an approximate strategy to calculate these metrics using the interpretability scores of rules. We manually annotate all possible rules and establish a benchmark. In experiments, we verify the effectiveness of our benchmark. Besides, we run nine representative baselines on our benchmark, and the experimental results show that the interpretability of current multi-hop reasoning models is less satisfactory and is 51.7% lower than the upper bound given by our benchmark. Moreover, the rule-based models outperform the multi-hop reasoning models in terms of performance and interpretability, which points to a direction for future research, i.e., how to better incorporate rule information into the multi-hop reasoning model. We will publish our codes and datasets upon acceptance",
    "checked": true,
    "id": "3d3b1300c7cd6a820a6d08605248f875a3ad20b9",
    "semantic_title": "is multi-hop reasoning really explainable? towards benchmarking reasoning interpretability",
    "citation_count": 19,
    "authors": [
      "Xin Lv",
      "Yixin Cao",
      "Lei Hou",
      "Juanzi Li",
      "Zhiyuan Liu",
      "Yichi Zhang",
      "Zelin Dai"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.701": {
    "title": "Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors",
    "volume": "main",
    "abstract": "Evaluation metrics are a key ingredient for progress of text generation systems. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than BLEU or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including semantics, syntax, morphology, and lexical overlap. We show that the different metrics capture all aspects to some degree, but that they are all substantially sensitive to lexical overlap, just like BLEU and ROUGE. This exposes limitations of these novelly proposed metrics, which we also highlight in an adversarial test scenario",
    "checked": true,
    "id": "e06ef772c028de1ba093270a62084588e7c4deaa",
    "semantic_title": "global explainability of bert-based evaluation metrics by disentangling along linguistic factors",
    "citation_count": 27,
    "authors": [
      "Marvin Kaster",
      "Wei Zhao",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.702": {
    "title": "Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization",
    "volume": "main",
    "abstract": "Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing domain knowledge that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of domain knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding domain knowledge that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such domain knowledge, even if the domain knowledge appears in the training set, and the model provides the correct predictions for related training samples",
    "checked": true,
    "id": "0b3863c21a7fb5ac61a447611cba0ec9ce1ab4a4",
    "semantic_title": "exploring underexplored limitations of cross-domain text-to-sql generalization",
    "citation_count": 93,
    "authors": [
      "Yujian Gan",
      "Xinyun Chen",
      "Matthew Purver"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.703": {
    "title": "What happens if you treat ordinal ratings as interval data? Human evaluations in NLP are even more under-powered than you think",
    "volume": "main",
    "abstract": "Previous work has shown that human evaluations in NLP are notoriously under-powered. Here, we argue that there are two common factors which make this problem even worse: NLP studies usually (a) treat ordinal data as interval data and (b) operate under high variance settings while the differences they are hoping to detect are often subtle. We demonstrate through simulation that ordinal mixed effects models are better able to detect small differences between models, especially in high variance settings common in evaluations of generated texts. We release tools for researchers to conduct their own power analysis and test their assumptions. We also make recommendations for improving statistical power",
    "checked": true,
    "id": "39391ebdc03485b7cc85852ddc999b0da6324da2",
    "semantic_title": "what happens if you treat ordinal ratings as interval data? human evaluations in nlp are even more under-powered than you think",
    "citation_count": 16,
    "authors": [
      "David M. Howcroft",
      "Verena Rieser"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.704": {
    "title": "NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives",
    "volume": "main",
    "abstract": "Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on data generated by the rule-based approach, obtains word error rates (WER) below 0.18% on synthetic, in-domain and out-domain test sets",
    "checked": true,
    "id": "95ec171964b16e7c14739b1ab039df2cca6abb73",
    "semantic_title": "neutral rewriter: a rule-based and neural approach to automatic rewriting into gender neutral alternatives",
    "citation_count": 57,
    "authors": [
      "Eva Vanmassenhove",
      "Chris Emmery",
      "Dimitar Shterionov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.705": {
    "title": "Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset",
    "volume": "main",
    "abstract": "Reasoning over commonsense knowledge bases (CSKB) whose elements are in the form of free-text is an important yet hard task in NLP. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen assertions from external resources. In this task, CSKBs are grounded to a large-scale eventuality (activity, state, and event) graph to discriminate whether novel triples from the eventuality graph are plausible or not. However, existing evaluations on the population task are either not accurate (automatic evaluation with randomly sampled negative examples) or of small scale (human annotation). In this paper, we benchmark the CSKB population task with a new large-scale dataset by first aligning four popular CSKBs, and then presenting a high-quality human-annotated evaluation set to probe neural models' commonsense reasoning ability. We also propose a novel inductive commonsense reasoning model that reasons over graphs. Experimental results show that generalizing commonsense reasoning on unseen assertions is inherently a hard task. Models achieving high accuracy during training perform poorly on the evaluation set, with a large gap between human performance. We will make the data publicly available for future contributions. Codes and data are available at https://github.com/HKUST-KnowComp/CSKB-Population",
    "checked": true,
    "id": "e13dd6d87a34f1fb965ee6e281e49ba3a212e075",
    "semantic_title": "benchmarking commonsense knowledge base population with an effective evaluation dataset",
    "citation_count": 32,
    "authors": [
      "Tianqing Fang",
      "Weiqi Wang",
      "Sehyun Choi",
      "Shibo Hao",
      "Hongming Zhang",
      "Yangqiu Song",
      "Bin He"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.706": {
    "title": "Enhancing the Context Representation in Similarity-based Word Sense Disambiguation",
    "volume": "main",
    "abstract": "In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting its global context. In this paper, we investigate the contribution of both word-level and sense-level global context of an ambiguous word for disambiguation. Experiments have shown that the Context-Oriented Embedding (COE) can enhance a similarity-based system's performance on WSD by relatively large margins, achieving state-of-the-art on all-words WSD benchmarks in knowledge-based category",
    "checked": true,
    "id": "afada23e35b72e462c8bdabf63b8d08d80c48a77",
    "semantic_title": "enhancing the context representation in similarity-based word sense disambiguation",
    "citation_count": 6,
    "authors": [
      "Ming Wang",
      "Jianzhang Zhang",
      "Yinglin Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.707": {
    "title": "Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing",
    "volume": "main",
    "abstract": "Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement",
    "checked": true,
    "id": "98bdace6ab1f0797323943c51436a8ec53a67ac1",
    "semantic_title": "data augmentation with hierarchical sql-to-question generation for cross-domain text-to-sql parsing",
    "citation_count": 36,
    "authors": [
      "Kun Wu",
      "Lijie Wang",
      "Zhenghua Li",
      "Ao Zhang",
      "Xinyan Xiao",
      "Hua Wu",
      "Min Zhang",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.708": {
    "title": "SPARQLing Database Queries from Intermediate Question Decompositions",
    "volume": "main",
    "abstract": "To translate natural language questions into executable database queries, most approaches rely on a fully annotated training set. Annotating a large dataset with queries is difficult as it requires query-language expertise. We reduce this burden using grounded in databases intermediate question representations. These representations are simpler to collect and were originally crowdsourced within the Break dataset (Wolfson et al., 2020). Our pipeline consists of two parts: a neural semantic parser that converts natural language questions into the intermediate representations and a non-trainable transpiler to the SPARQL query language (a standard language for accessing knowledge graphs and semantic web). We chose SPARQL because its queries are structurally closer to our intermediate representations (compared to SQL). We observe that the execution accuracy of queries constructed by our model on the challenging Spider dataset is comparable with the state-of-the-art text-to-SQL methods trained with annotated SQL queries. Our code and data are publicly available (https://github.com/yandex-research/sparqling-queries)",
    "checked": true,
    "id": "bd7907735793f122c400d3afc4e73187dc57f376",
    "semantic_title": "sparqling database queries from intermediate question decompositions",
    "citation_count": 14,
    "authors": [
      "Irina Saparina",
      "Anton Osokin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.709": {
    "title": "Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs",
    "volume": "main",
    "abstract": "Entity alignment aims to identify equivalent entity pairs between different knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that contain time information created the need for reasoning over time in such TKGs. Existing embedding-based entity alignment approaches disregard time information that commonly exists in many large-scale KGs, leaving much room for improvement. In this paper, we focus on the task of aligning entity pairs between TKGs and propose a novel Time-aware Entity Alignment approach based on Graph Neural Networks (TEA-GNN). We embed entities, relations and timestamps of different KGs into a vector space and use GNNs to learn entity representations. To incorporate both relation and time information into the GNN structure of our model, we use a self-attention mechanism which assigns different weights to different nodes with orthogonal transformation matrices computed from embeddings of the relevant relations and timestamps in a neighborhood. Experimental results on multiple real-world TKG datasets show that our method significantly outperforms the state-of-the-art methods due to the inclusion of time information",
    "checked": false,
    "id": "556946d5ef6d87016907585a361574a339c8a1f7",
    "semantic_title": "collective multi-type entity alignment between knowledge graphs",
    "citation_count": 53,
    "authors": [
      "Chengjin Xu",
      "Fenglong Su",
      "Jens Lehmann"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.710": {
    "title": "Cross-Domain Label-Adaptive Stance Detection",
    "volume": "main",
    "abstract": "Stance detection concerns the classification of a writer's viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the model performance",
    "checked": true,
    "id": "ba9f284f6cda1a64a6079e3d931898ba880d961d",
    "semantic_title": "cross-domain label-adaptive stance detection",
    "citation_count": 75,
    "authors": [
      "Momchil Hardalov",
      "Arnav Arora",
      "Preslav Nakov",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.711": {
    "title": "Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification",
    "volume": "main",
    "abstract": "Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous methods, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a framework named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for data augmentation. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best policy, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines",
    "checked": true,
    "id": "23d21bb495061564d2ce4540ea312959ab5e10f0",
    "semantic_title": "text autoaugment: learning compositional augmentation policy for text classification",
    "citation_count": 35,
    "authors": [
      "Shuhuai Ren",
      "Jinchao Zhang",
      "Lei Li",
      "Xu Sun",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.712": {
    "title": "Distilling Relation Embeddings from Pretrained Language Models",
    "volume": "main",
    "abstract": "Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository: https://github.com/asahi417/relbert",
    "checked": true,
    "id": "e113570afc29562bfde48f89fbb5efa624610573",
    "semantic_title": "distilling relation embeddings from pretrained language models",
    "citation_count": 22,
    "authors": [
      "Asahi Ushio",
      "Jose Camacho-Collados",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.713": {
    "title": "Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning",
    "volume": "main",
    "abstract": "Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics",
    "checked": true,
    "id": "fccce60283729934467877f0730317c3e9fcc61e",
    "semantic_title": "avoiding inference heuristics in few-shot prompt-based finetuning",
    "citation_count": 36,
    "authors": [
      "Prasetya Utama",
      "Nafise Sadat Moosavi",
      "Victor Sanh",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.714": {
    "title": "A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing",
    "volume": "main",
    "abstract": "Abstract Meaning Representations (AMR) are a broad-coverage semantic formalism which represents sentence meaning as a directed acyclic graph. To train most AMR parsers, one needs to segment the graph into subgraphs and align each such subgraph to a word in a sentence; this is normally done at preprocessing, relying on hand-crafted rules. In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over the structured latent variables is infeasible, we use the variational autoencoding framework. To ensure end-to-end differentiable optimization, we introduce a differentiable relaxation of the segmentation and alignment problems. We observe that inducing segmentation yields substantial gains over using a ‘greedy' segmentation heuristic. The performance of our method also approaches that of a model that relies on the segmentation rules of Lyu and Titov (2018), which were hand-crafted to handle individual AMR constructions",
    "checked": true,
    "id": "856208bf9f17320dddb4bdd7127f08cae7d922e3",
    "semantic_title": "a differentiable relaxation of graph segmentation and alignment for amr parsing",
    "citation_count": 11,
    "authors": [
      "Chunchuan Lyu",
      "Shay B. Cohen",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.715": {
    "title": "Integrating Personalized PageRank into Neural Word Sense Disambiguation",
    "volume": "main",
    "abstract": "Neural Word Sense Disambiguation (WSD) has recently been shown to benefit from the incorporation of pre-existing knowledge, such as that coming from the WordNet graph. However, state-of-the-art approaches have been successful in exploiting only the local structure of the graph, with only close neighbors of a given synset influencing the prediction. In this work, we improve a classification model by recomputing logits as a function of both the vanilla independently produced logits and the global WordNet graph. We achieve this by incorporating an online neural approximated PageRank, which enables us to refine edge weights as well. This method exploits the global graph structure while keeping space requirements linear in the number of edges. We obtain strong improvements, matching the current state of the art. Code is available at https://github.com/SapienzaNLP/neural-pagerank-wsd",
    "checked": true,
    "id": "ad51d18fe2b91da618c0f1938e41e1f4ca02d65b",
    "semantic_title": "integrating personalized pagerank into neural word sense disambiguation",
    "citation_count": 8,
    "authors": [
      "Ahmed El Sheikh",
      "Michele Bevilacqua",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.716": {
    "title": "Cross-lingual Sentence Embedding using Multi-Task Learning",
    "volume": "main",
    "abstract": "Multilingual sentence embeddings capture rich semantic information not only for measuring similarity between texts but also for catering to a broad range of downstream cross-lingual NLP tasks. State-of-the-art multilingual sentence embedding models require large parallel corpora to learn efficiently, which confines the scope of these models. In this paper, we propose a novel sentence embedding framework based on an unsupervised loss function for generating effective multilingual sentence embeddings, eliminating the need for parallel corpora. We capture semantic similarity and relatedness between sentences using a multi-task loss function for training a dual encoder model mapping different languages onto the same vector space. We demonstrate the efficacy of an unsupervised as well as a weakly supervised variant of our framework on STS, BUCC and Tatoeba benchmark tasks. The proposed unsupervised sentence embedding framework outperforms even supervised state-of-the-art methods for certain under-resourced languages on the Tatoeba dataset and on a monolingual benchmark. Further, we show enhanced zero-shot learning capabilities for more than 30 languages, with the model being trained on only 13 languages. Our model can be extended to a wide range of languages from any language family, as it overcomes the requirement of parallel corpora for training",
    "checked": true,
    "id": "8a097088a201180847d2cb4b2e3f911016f8faf9",
    "semantic_title": "cross-lingual sentence embedding using multi-task learning",
    "citation_count": 21,
    "authors": [
      "Koustava Goswami",
      "Sourav Dutta",
      "Haytham Assem",
      "Theodorus Fransen",
      "John P. McCrae"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.717": {
    "title": "NB-MLM: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis",
    "volume": "main",
    "abstract": "While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient adaptation that focuses on predicting words with large weights of the Naive Bayes classifier trained for the task at hand, which are likely more relevant than the most frequent words. The proposed method provides faster adaptation and better final performance for sentiment analysis compared to the standard approach",
    "checked": true,
    "id": "1af0e8006b8453ef5b60fdaf6b9f7b5a2b6b4be9",
    "semantic_title": "nb-mlm: efficient domain adaptation of masked language models for sentiment analysis",
    "citation_count": 14,
    "authors": [
      "Nikolay Arefyev",
      "Dmitrii Kharchev",
      "Artem Shelmanov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.718": {
    "title": "Revisiting Self-training for Few-shot Learning of Language Model",
    "volume": "main",
    "abstract": "As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the same pseudo label when fine-tuned with the strongly augmented version. This simple approach is shown to outperform other state-of-the-art supervised and semi-supervised counterparts on six sentence classification and six sentence-pair classification benchmarking tasks. In addition, SFLM only relies on a few in-domain unlabeled data. We conduct a comprehensive analysis to demonstrate the robustness of our proposed approach under various settings, including augmentation techniques, model scale, and few-shot knowledge transfer across tasks",
    "checked": true,
    "id": "528dc51f9c2bb4c771e886427b16cfc40cfa2ecc",
    "semantic_title": "revisiting self-training for few-shot learning of language model",
    "citation_count": 42,
    "authors": [
      "Yiming Chen",
      "Yan Zhang",
      "Chen Zhang",
      "Grandee Lee",
      "Ran Cheng",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.719": {
    "title": "Bridging Perception, Memory, and Inference through Semantic Relations",
    "volume": "main",
    "abstract": "There is a growing consensus that surface form alone does not enable models to learn meaning and gain language understanding. This warrants an interest in hybrid systems that combine the strengths of neural and symbolic methods. We favour triadic systems consisting of neural networks, knowledge bases, and inference engines. The network provides perception, that is, the interface between the system and its environment. The knowledge base provides explicit memory and thus immediate access to established facts. Finally, inference capabilities are provided by the inference engine which reflects on the perception, supported by memory, to reason and discover new facts. In this work, we probe six popular language models for semantic relations and outline a future line of research to study how the constituent subsystems can be jointly realised and integrated",
    "checked": true,
    "id": "ba68be6b5627271b6962b390ab4e29a8e9766c87",
    "semantic_title": "bridging perception, memory, and inference through semantic relations",
    "citation_count": 0,
    "authors": [
      "Johanna Björklund",
      "Adam Dahlgren Lindström",
      "Frank Drewes"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.720": {
    "title": "Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion",
    "volume": "main",
    "abstract": "Effective unimodal representation and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements, which incurs a bottleneck of performance improvement. In this paper, Unimodal and Crossmodal Refinement Network (UCRN) is proposed to enhance both unimodal and crossmodal representations. Specifically, to improve unimodal representations, a unimodal refinement module is designed to refine modality-specific learning via iteratively updating the distribution with transformer-based attention layers. Self-quality improvement layers are followed to generate the desired weighted representations progressively. Subsequently, those unimodal representations are projected into a common latent space, regularized by a multimodal Jensen-Shannon divergence loss for better crossmodal refinement. Lastly, a crossmodal refinement module is employed to integrate all information. By hierarchical explorations on unimodal, bimodal, and trimodal interactions, UCRN is highly robust against missing modality and noisy data. Experimental results on MOSI and MOSEI datasets illustrated that the proposed UCRN outperforms recent state-of-the-art techniques and its robustness is highly preferred in real multimodal sequence fusion scenarios. Codes will be shared publicly",
    "checked": true,
    "id": "59a80f6db564693535ef23a8b0f9034023842f6e",
    "semantic_title": "unimodal and crossmodal refinement network for multimodal sequence fusion",
    "citation_count": 4,
    "authors": [
      "Xiaobao Guo",
      "Adams Kong",
      "Huan Zhou",
      "Xianfeng Wang",
      "Min Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.721": {
    "title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews",
    "volume": "main",
    "abstract": "Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO – a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis verifies the reliability of these annotations, and explores the characteristics of the collected data. Benchmark results using five contemporary TSA systems show there is ample room for improvement on this challenging new dataset. YASO is available at https://github.com/IBM/yaso-tsa",
    "checked": true,
    "id": "2325771df127aefe04abf72d1116a9ee98c149e9",
    "semantic_title": "yaso: a targeted sentiment analysis evaluation dataset for open-domain reviews",
    "citation_count": 18,
    "authors": [
      "Matan Orbach",
      "Orith Toledo-Ronen",
      "Artem Spector",
      "Ranit Aharonov",
      "Yoav Katz",
      "Noam Slonim"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.722": {
    "title": "An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction",
    "volume": "main",
    "abstract": "Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new subtask of target-oriented sentiment analysis that aims to extract opinion words for a given aspect in text. Current state-of-the-art methods leverage position embeddings to capture the relative position of a word to the target. However, the performance of these methods depends on the ability to incorporate this information into word representations. In this paper, we explore a variety of text encoders based on pretrained word embeddings or language models that leverage part-of-speech and position embeddings, aiming to examine the actual contribution of each component in TOWE. We also adapt a graph convolutional network (GCN) to enhance word representations by incorporating syntactic information. Our experimental results demonstrate that BiLSTM-based models can effectively encode position information into word representations while using a GCN only achieves marginal gains. Interestingly, our simple methods outperform several state-of-the-art complex neural structures",
    "checked": true,
    "id": "6e40431b649d4333eecd1ea549d2e550a7e6a619",
    "semantic_title": "an empirical study on leveraging position embeddings for target-oriented opinion words extraction",
    "citation_count": 18,
    "authors": [
      "Samuel Mensah",
      "Kai Sun",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.723": {
    "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
    "volume": "main",
    "abstract": "In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows from input to the fusion results. In this work, we propose a framework named MultiModal InfoMax (MMIM), which hierarchically maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality) and between multimodal fusion result and unimodal input in order to maintain task-related information through multimodal fusion. The framework is jointly trained with the main task (MSA) to improve the performance of the downstream MSA task. To address the intractable issue of MI bounds, we further formulate a set of computationally simple parametric and non-parametric methods to approximate their truth value. Experimental results on the two widely used datasets demonstrate the efficacy of our approach",
    "checked": true,
    "id": "077d10a856d12855eea13fe0960a2df006bba257",
    "semantic_title": "improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
    "citation_count": 333,
    "authors": [
      "Wei Han",
      "Hui Chen",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.724": {
    "title": "BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based Sentiment Classification",
    "volume": "main",
    "abstract": "Graph-based Aspect-based Sentiment Classification (ABSC) approaches have yielded state-of-the-art results, expecially when equipped with contextual word embedding from pre-training language models (PLMs). However, they ignore sequential features of the context and have not yet made the best of PLMs. In this paper, we propose a novel model, BERT4GCN, which integrates the grammatical sequential features from the PLM of BERT, and the syntactic knowledge from dependency graphs. BERT4GCN utilizes outputs from intermediate layers of BERT and positional information between words to augment GCN (Graph Convolutional Network) to better encode the dependency graphs for the downstream classification. Experimental results demonstrate that the proposed BERT4GCN outperforms all state-of-the-art baselines, justifying that augmenting GCN with the grammatical features from intermediate layers of BERT can significantly empower ABSC models",
    "checked": true,
    "id": "c50d15706163ed4bcb0d6a2dfefeec953b86d4fb",
    "semantic_title": "bert4gcn: using bert intermediate layers to augment gcn for aspect-based sentiment classification",
    "citation_count": 78,
    "authors": [
      "Zeguan Xiao",
      "Jiarun Wu",
      "Qingliang Chen",
      "Congjian Deng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.725": {
    "title": "Does Social Pressure Drive Persuasion in Online Fora?",
    "volume": "main",
    "abstract": "Online forums such as ChangeMyView have been explored to research aspects of persuasion and argumentative quality in language. While previous research has focused on arguments between a view-holder and a persuader, we explore the premise that apart from the merits of arguments, persuasion is influenced by the ambient social community. We hypothesize that comments from the rest of the community can either affirm the original view or implicitly exert pressure to change it. We develop a structured model to capture the ambient community's sentiment towards the discussion and its effect on persuasion. Our experiments show that social features themselves are significantly predictive of persuasion (even without looking at the actual content of discussion), with performance comparable to some earlier approaches that use content features. Combining community and content features leads to overall performance of 78.5% on the persuasion prediction task. Our analyses suggest that the effect of social pressure is comparable to the difference between persuasive and non-persuasive language strategies in driving persuasion and that social pressure might be a causal factor for persuasion",
    "checked": true,
    "id": "a74d8265bedadb09f10a4bdf2dcd0eb8d6c8964e",
    "semantic_title": "does social pressure drive persuasion in online fora?",
    "citation_count": 3,
    "authors": [
      "Ayush Jain",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.726": {
    "title": "Aspect Sentiment Quad Prediction as Paraphrase Generation",
    "volume": "main",
    "abstract": "Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment elements, instead of predicting the four elements in one shot. In this work, we introduce the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all sentiment elements in quads for a given opinionated sentence, which can reveal a more comprehensive and complete aspect-level sentiment structure. We further propose a novel Paraphrase modeling paradigm to cast the ASQP task to a paraphrase generation process. On one hand, the generation formulation allows solving ASQP in an end-to-end manner, alleviating the potential error propagation in the pipeline solution. On the other hand, the semantics of the sentiment elements can be fully exploited by learning to generate them in the natural language form. Extensive experiments on benchmark datasets show the superiority of our proposed method and the capacity of cross-task transfer with the proposed unified Paraphrase modeling framework",
    "checked": true,
    "id": "85241d5942966f6b5af19f3cf80f7156dbcddf5f",
    "semantic_title": "aspect sentiment quad prediction as paraphrase generation",
    "citation_count": 195,
    "authors": [
      "Wenxuan Zhang",
      "Yang Deng",
      "Xin Li",
      "Yifei Yuan",
      "Lidong Bing",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.727": {
    "title": "Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching",
    "volume": "main",
    "abstract": "Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised cross-lingual transfer for the ABSA task, where only labeled data in the source language is available and we aim at transferring its knowledge to the target language having no labeled data. To this end, we propose an alignment-free label projection method to obtain high-quality pseudo-labeled data of the target language with the help of the translation system, which could preserve more accurate task-specific knowledge in the target language. For better utilizing the source and translated data, as well as enhancing the cross-lingual alignment, we design an aspect code-switching mechanism to augment the training data with code-switched bilingual sentences. To further investigate the importance of language-specific knowledge in solving the ABSA problem, we distill the above model on the unlabeled target language data which improves the performance to the same level of the supervised method",
    "checked": true,
    "id": "8dd7cac75655e22039f9b86799cb88d83e7bae49",
    "semantic_title": "cross-lingual aspect-based sentiment analysis with aspect term code-switching",
    "citation_count": 55,
    "authors": [
      "Wenxuan Zhang",
      "Ruidan He",
      "Haiyun Peng",
      "Lidong Bing",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.728": {
    "title": "Towards Label-Agnostic Emotion Embeddings",
    "volume": "main",
    "abstract": "Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) natural languages and text genres (e.g., product reviews, tweets, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, natural languages, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired interoperability without penalizing prediction quality. Code and data are archived under DOI 10.5281/zenodo.5466068",
    "checked": true,
    "id": "8979467532ec8e03187f6bc922f4bbde04565500",
    "semantic_title": "towards label-agnostic emotion embeddings",
    "citation_count": 10,
    "authors": [
      "Sven Buechel",
      "Luise Modersohn",
      "Udo Hahn"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.729": {
    "title": "Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer",
    "volume": "main",
    "abstract": "Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting the over-transfer and under-transfer problems. We attribute these problems to the conflicting driving forces of the style conversion goal and content preservation goal. In this paper, we propose a collaborative learning framework for unsupervised text style transfer using a pair of bidirectional decoders, one decoding from left to right while the other decoding from right to left. In our collaborative learning mechanism, each decoder is regularized by knowledge from its peer which has a different knowledge acquisition process. The difference is guaranteed by their opposite decoding directions and a distinguishability constraint. As a result, mutual knowledge distillation drives both decoders to a better optimum and alleviates the over-transfer and under-transfer problems. Experimental results on two benchmark datasets show that our framework achieves strong empirical results on both style compatibility and content preservation",
    "checked": true,
    "id": "0e62b77d7bef8a80cdbfb3b2ad5142c4eca69315",
    "semantic_title": "collaborative learning of bidirectional decoders for unsupervised text style transfer",
    "citation_count": 7,
    "authors": [
      "Yun Ma",
      "Yangbin Chen",
      "Xudong Mao",
      "Qing Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.730": {
    "title": "Exploring Non-Autoregressive Text Style Transfer",
    "volume": "main",
    "abstract": "In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the AR model, this NAR model sacrifices its transfer performance due to the lack of conditional dependence between output tokens. To this end, we investigate three techniques, i.e., knowledge distillation, contrastive learning, and iterative decoding, for performance enhancement. Experimental results on two benchmark datasets suggest that, although the base NAR model is generally inferior to AR decoding, their performance gap can be clearly narrowed when empowering NAR decoding with knowledge distillation, contrastive learning, and iterative decoding",
    "checked": true,
    "id": "4a2709c974a62184755bfd06e86220016624ba78",
    "semantic_title": "exploring non-autoregressive text style transfer",
    "citation_count": 7,
    "authors": [
      "Yun Ma",
      "Qing Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.731": {
    "title": "PASTE: A Tagging-Free Decoding Framework Using Pointer Networks for Aspect Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) deals with extracting opinion triplets, consisting of an opinion target or aspect, its associated sentiment, and the corresponding opinion term/span explaining the rationale behind the sentiment. Existing research efforts are majorly tagging-based. Among the methods taking a sequence tagging approach, some fail to capture the strong interdependence between the three opinion factors, whereas others fall short of identifying triplets with overlapping aspect/opinion spans. A recent grid tagging approach on the other hand fails to capture the span-level semantics while predicting the sentiment between an aspect-opinion pair. Different from these, we present a tagging-free solution for the task, while addressing the limitations of the existing works. We adapt an encoder-decoder architecture with a Pointer Network-based decoding framework that generates an entire opinion triplet at each time step thereby making our solution end-to-end. Interactions between the aspects and opinions are effectively captured by the decoder by considering their entire detected spans while predicting their connecting sentiment. Extensive experiments on several benchmark datasets establish the better efficacy of our proposed approach, especially in recall, and in predicting multiple and aspect/opinion-overlapped triplets from the same review sentence. We report our results both with and without BERT and also demonstrate the utility of domain-specific BERT post-training for the task",
    "checked": true,
    "id": "ac7e6d1c9234d6246e680cb97be44ff9df6d677f",
    "semantic_title": "paste: a tagging-free decoding framework using pointer networks for aspect sentiment triplet extraction",
    "citation_count": 58,
    "authors": [
      "Rajdeep Mukherjee",
      "Tapas Nayak",
      "Yash Butala",
      "Sourangshu Bhattacharya",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.732": {
    "title": "Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos",
    "volume": "main",
    "abstract": "We address the problem of temporal sentence localization in videos (TSLV). Traditional methods follow a top-down framework which localizes the target segment with pre-defined segment proposals. Although they have achieved decent performance, the proposals are handcrafted and redundant. Recently, bottom-up framework attracts increasing attention due to its superior efficiency. It directly predicts the probabilities for each frame as a boundary. However, the performance of bottom-up model is inferior to the top-down counterpart as it fails to exploit the segment-level interaction. In this paper, we propose an Adaptive Proposal Generation Network (APGN) to maintain the segment-level interaction while speeding up the efficiency. Specifically, we first perform a foreground-background classification upon the video and regress on the foreground frames to adaptively generate proposals. In this way, the handcrafted proposal design is discarded and the redundant proposals are decreased. Then, a proposal consolidation module is further developed to enhance the semantics of the generated proposals. Finally, we locate the target moments with these generated proposals following the top-down framework. Extensive experiments show that our proposed APGN significantly outperforms previous state-of-the-art methods on three challenging benchmarks",
    "checked": true,
    "id": "63fc6886020dfc09e5f55d886c528ef1ddb32ac5",
    "semantic_title": "adaptive proposal generation network for temporal sentence localization in videos",
    "citation_count": 55,
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Jianfeng Dong",
      "Pan Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.733": {
    "title": "Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding",
    "volume": "main",
    "abstract": "A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we propose an Iterative Alignment Network (IA-Net) for TSG task, which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. Specifically, during the iterative reasoning process, we pad multi-modal features with learnable parameters to alleviate the nowhere-to-attend problem of non-matched frame-word pairs, and enhance the basic co-attention mechanism in a parallel manner. To further calibrate the misaligned attention caused by each reasoning step, we also devise a calibration module following each attention module to refine the alignment knowledge. With such iterative alignment scheme, our IA-Net can robustly capture the fine-grained relations between vision and language domains step-by-step for progressively reasoning the temporal boundaries. Extensive experiments conducted on three challenging benchmarks demonstrate that our proposed model performs better than the state-of-the-arts",
    "checked": true,
    "id": "f897e208e86600244813a9fcc929a69daf923cb8",
    "semantic_title": "progressively guide to attend: an iterative alignment framework for temporal sentence grounding",
    "citation_count": 46,
    "authors": [
      "Daizong Liu",
      "Xiaoye Qu",
      "Pan Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.734": {
    "title": "Language Models are Few-Shot Butlers",
    "volume": "main",
    "abstract": "Pretrained language models demonstrate strong performance in most NLP tasks when fine-tuned on small task-specific datasets. Hence, these autoregressive models constitute ideal agents to operate in text-based environments where language understanding and generative capabilities are essential. Nonetheless, collecting expert demonstrations in such environments is a time-consuming endeavour. We introduce a two-stage procedure to learn from a small set of demonstrations and further improve by interacting with an environment. We show that language models fine-tuned with only 1.2% of the expert demonstrations and a simple reinforcement learning algorithm achieve a 51% absolute improvement in success rate over existing methods in the ALFWorld environment",
    "checked": true,
    "id": "6263f69b0154fb090fc76f792c1a4aa182e24294",
    "semantic_title": "language models are few-shot butlers",
    "citation_count": 34,
    "authors": [
      "Vincent Micheli",
      "Francois Fleuret"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.735": {
    "title": "Rˆ3Net:Relation-embedded Representation Reconstruction Network for Change Captioning",
    "volume": "main",
    "abstract": "Change captioning is to use a natural language sentence to describe the fine-grained disagreement between two similar images. Viewpoint change is the most typical distractor in this task, because it changes the scale and location of the objects and overwhelms the representation of real change. In this paper, we propose a Relation-embedded Representation Reconstruction Network (Rˆ3Net) to explicitly distinguish the real change from the large amount of clutter and irrelevant changes. Specifically, a relation-embedded module is first devised to explore potential changed objects in the large amount of clutter. Then, based on the semantic similarities of corresponding locations in the two images, a representation reconstruction module (RRM) is designed to learn the reconstruction representation and further model the difference representation. Besides, we introduce a syntactic skeleton predictor (SSP) to enhance the semantic interaction between change localization and caption generation. Extensive experiments show that the proposed method achieves the state-of-the-art results on two public datasets",
    "checked": true,
    "id": "f8b9d8d187413d758d7849e96288838b5c1b75cf",
    "semantic_title": "rˆ3net:relation-embedded representation reconstruction network for change captioning",
    "citation_count": 25,
    "authors": [
      "Yunbin Tu",
      "Liang Li",
      "Chenggang Yan",
      "Shengxiang Gao",
      "Zhengtao Yu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.736": {
    "title": "Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy",
    "volume": "main",
    "abstract": "Generating goal-oriented questions in Visual Dialogue tasks is a challenging and longstanding problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model's conjecture about the referent. We take the GuessWhat?! game as a case-study. We show that dialogues generated by Confirm-it are more natural and effective than beam search decoding without re-ranking",
    "checked": true,
    "id": "2a523a83ee40a60d071ec4cddc8b64d90950119b",
    "semantic_title": "looking for confirmations: an effective and human-like visual dialogue strategy",
    "citation_count": 11,
    "authors": [
      "Alberto Testoni",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.737": {
    "title": "A Unified Speaker Adaptation Approach for ASR",
    "volume": "main",
    "abstract": "Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.58% relative WER reduction, and surpasses the finetuning method by up to relative 2.54%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53% with only a few epochs of training",
    "checked": true,
    "id": "76babaf07b884d7be6327d7cee0f9419df66159d",
    "semantic_title": "a unified speaker adaptation approach for asr",
    "citation_count": 9,
    "authors": [
      "Yingzhu Zhao",
      "Chongjia Ni",
      "Cheung-Chi Leung",
      "Shafiq Joty",
      "Eng Siong Chng",
      "Bin Ma"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.738": {
    "title": "Caption Enriched Samples for Improving Hateful Memes Detection",
    "volume": "main",
    "abstract": "The recently introduced hateful meme challenge demonstrates the difficulty of determining whether a meme is hateful or not. Specifically, both unimodal language models and multimodal vision-language models cannot reach the human level of performance. Motivated by the need to model the contrast between the image content and the overlayed text, we suggest applying an off-the-shelf image captioning tool in order to capture the first. We demonstrate that the incorporation of such automatic captions during fine-tuning improves the results for various unimodal and multimodal models. Moreover, in the unimodal case, continuing the pre-training of language models on augmented and original caption pairs, is highly beneficial to the classification accuracy",
    "checked": true,
    "id": "67b870fce2d18a92651e3cc9397e6f8d1b9ddee3",
    "semantic_title": "caption enriched samples for improving hateful memes detection",
    "citation_count": 24,
    "authors": [
      "Efrat Blaier",
      "Itzik Malkiel",
      "Lior Wolf"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.739": {
    "title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems",
    "volume": "main",
    "abstract": "Transformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer's encoder-decoder attention mechanism. The cost of this attention becomes more significant in inference or training approaches that require model-generated histories. First, we examine the complexity of the encoder-decoder attention. We demonstrate empirically that there is a sparse sentence structure in document summarization that can be exploited by constraining the attention mechanism to a subset of input sentences, whilst maintaining system performance. Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention. Experiments are carried out on abstractive summarization tasks, including CNN/DailyMail, XSum, Spotify Podcast, and arXiv",
    "checked": true,
    "id": "e4dd6650497ce07d78ea4f57ea1434f4a3e3f771",
    "semantic_title": "sparsity and sentence structure in encoder-decoder attention of summarization systems",
    "citation_count": 5,
    "authors": [
      "Potsawee Manakul",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.740": {
    "title": "BARThez: a Skilled Pretrained French Sequence-to-Sequence Model",
    "volume": "main",
    "abstract": "Inductive transfer learning has taken the entire NLP field by storm, with models such as BERT and BART setting new state of the art on countless NLU tasks. However, most of the available models and research have been conducted for English. In this work, we introduce BARThez, the first large-scale pretrained seq2seq model for French. Being based on BART, BARThez is particularly well-suited for generative tasks. We evaluate BARThez on five discriminative tasks from the FLUE benchmark and two generative tasks from a novel summarization dataset, OrangeSum, that we created for this research. We show BARThez to be very competitive with state-of-the-art BERT-based French language models such as CamemBERT and FlauBERT. We also continue the pretraining of a multilingual BART on BARThez' corpus, and show our resulting model, mBARThez, to significantly boost BARThez' generative performance",
    "checked": true,
    "id": "192534be208e99174ee83fccabf8457bb8bfb9c5",
    "semantic_title": "barthez: a skilled pretrained french sequence-to-sequence model",
    "citation_count": 65,
    "authors": [
      "Moussa Kamal Eddine",
      "Antoine Tixier",
      "Michalis Vazirgiannis"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.741": {
    "title": "ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization",
    "volume": "main",
    "abstract": "Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the semantic similarity between generated sentences and the original document. We propose ARMAN, a Transformer-based encoder-decoder model pre-trained with three novel objectives to address this issue. In ARMAN, salient sentences from a document are selected according to a modified semantic score to be masked and form a pseudo summary. To summarize more accurately and similar to human writing patterns, we applied modified sentence reordering. We evaluated our proposed models on six downstream Persian summarization tasks. Experimental results show that our proposed model achieves state-of-the-art performance on all six summarization tasks measured by ROUGE and BERTScore. Our models also outperform prior works in textual entailment, question paraphrasing, and multiple choice question answering. Finally, we established a human evaluation and show that using the semantic score significantly improves summarization results",
    "checked": true,
    "id": "3e4c671a59939f2893787ad89b61245cc42081e5",
    "semantic_title": "arman: pre-training with semantically selecting and reordering of sentences for persian abstractive summarization",
    "citation_count": 6,
    "authors": [
      "Alireza Salemi",
      "Emad Kebriaei",
      "Ghazal Neisi Minaei",
      "Azadeh Shakery"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.742": {
    "title": "Models and Datasets for Cross-Lingual Summarisation",
    "volume": "main",
    "abstract": "We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology for its creation can be applied to several other languages. We derive cross-lingual document-summary instances from Wikipedia by combining lead paragraphs and articles' bodies from language aligned Wikipedia titles. We analyse the proposed cross-lingual summarisation task with automatic metrics and validate it with a human study. To illustrate the utility of our dataset we report experiments with multi-lingual pre-trained models in supervised, zero- and few-shot, and out-of-domain scenarios",
    "checked": true,
    "id": "b51bebb08a619945970610912377356f98ebe941",
    "semantic_title": "models and datasets for cross-lingual summarisation",
    "citation_count": 49,
    "authors": [
      "Laura Perez-Beltrachini",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.743": {
    "title": "Learning Opinion Summarizers by Selecting Informative Reviews",
    "volume": "main",
    "abstract": "Opinion summarization has been traditionally approached with unsupervised, weakly-supervised and few-shot learning techniques. In this work, we collect a large dataset of summaries paired with user reviews for over 31,000 products, enabling supervised training. However, the number of reviews per product is large (320 on average), making summarization – and especially training a summarizer – impractical. Moreover, the content of many reviews is not reflected in the human-written summaries, and, thus, the summarizer trained on random review subsets hallucinates. In order to deal with both of these challenges, we formulate the task as jointly learning to select informative subsets of reviews and summarizing the opinions expressed in these subsets. The choice of the review subset is treated as a latent variable, predicted by a small and simple selector. The subset is then fed into a more powerful summarizer. For joint training, we use amortized variational inference and policy gradient methods. Our experiments demonstrate the importance of selecting informative reviews resulting in improved quality of summaries and reduced hallucinations",
    "checked": true,
    "id": "fb822f7f01e87cd1f031cae5b351e73ab1568190",
    "semantic_title": "learning opinion summarizers by selecting informative reviews",
    "citation_count": 31,
    "authors": [
      "Arthur Bražinskas",
      "Mirella Lapata",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.744": {
    "title": "Enriching and Controlling Global Semantics for Text Summarization",
    "volume": "main",
    "abstract": "Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the summarization model. In addition, to avoid the overwhelming effect of global semantics on contextualized representation, we introduce a mechanism to control the amount of global semantics supplied to the text generation module. Our method outperforms state-of-the-art summarization models on five common text summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and PubMed",
    "checked": true,
    "id": "c6a712f98be7f0a9957c373fafa4b2fcfe4d661b",
    "semantic_title": "enriching and controlling global semantics for text summarization",
    "citation_count": 34,
    "authors": [
      "Thong Nguyen",
      "Anh Tuan Luu",
      "Truc Lu",
      "Tho Quan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.745": {
    "title": "Revisiting Tri-training of Dependency Parsers",
    "volume": "main",
    "abstract": "We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as semi-supervised learning can be expected to have the most impact here. Based on treebank size and available ELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and Vietnamese. Furthermore, we include English in a simulated low-resource setting. We find that pretrained word embeddings make more effective use of unlabelled data than tri-training but that the two approaches can be successfully combined",
    "checked": true,
    "id": "4eebec592d7cb93a33f19ae9d56b30c82b2eb3d5",
    "semantic_title": "revisiting tri-training of dependency parsers",
    "citation_count": 2,
    "authors": [
      "Joachim Wagner",
      "Jennifer Foster"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.746": {
    "title": "Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling",
    "volume": "main",
    "abstract": "Zero-shot cross-domain slot filling alleviates the data dependence in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective knowledge transfer to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our model achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task",
    "checked": true,
    "id": "a07b7d9c4dbaaf547692e758b583e67d103dc854",
    "semantic_title": "bridge to target domain by prototypical contrastive learning and label confusion: re-explore zero-shot learning for slot filling",
    "citation_count": 24,
    "authors": [
      "Liwen Wang",
      "Xuefeng Li",
      "Jiachi Liu",
      "Keqing He",
      "Yuanmeng Yan",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.747": {
    "title": "Neuralizing Regular Expressions for Slot Filling",
    "volume": "main",
    "abstract": "Neural models and symbolic rules such as regular expressions have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting regular expressions into neural networks. Specifically, we first convert regular expressions into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via sequence labeling. Experimental results show that our model has superior zero-shot and few-shot performance and stays competitive when there are sufficient training data",
    "checked": true,
    "id": "152c9c4e5404c55c7cbec6ca05ee15e2e4dd066c",
    "semantic_title": "neuralizing regular expressions for slot filling",
    "citation_count": 7,
    "authors": [
      "Chengyue Jiang",
      "Zijian Jin",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.748": {
    "title": "Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP",
    "volume": "main",
    "abstract": "The principle of independent causal mechanisms (ICM) states that generative processes of real world data consist of independent modules which do not influence or inform each other. While this idea has led to fruitful developments in the field of causal inference, it is not widely-known in the NLP community. In this work, we argue that the causal direction of the data collection process bears nontrivial implications that can explain a number of published NLP findings, such as differences in semi-supervised learning (SSL) and domain adaptation (DA) performance across different settings. We categorize common NLP tasks according to their causal direction and empirically assay the validity of the ICM principle for text data using minimum description length. We conduct an extensive meta-analysis of over 100 published SSL and 30 DA studies, and find that the results are consistent with our expectations based on causal insights. This work presents the first attempt to analyze the ICM principle in NLP, and provides constructive suggestions for future modeling choices",
    "checked": true,
    "id": "abdcbc579a9b63d19bda86569e220a54e4dad1ba",
    "semantic_title": "causal direction of data collection matters: implications of causal and anticausal learning for nlp",
    "citation_count": 31,
    "authors": [
      "Zhijing Jin",
      "Julius von Kügelgen",
      "Jingwei Ni",
      "Tejas Vaidhya",
      "Ayush Kaushal",
      "Mrinmaya Sachan",
      "Bernhard Schoelkopf"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.749": {
    "title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning",
    "volume": "main",
    "abstract": "Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins",
    "checked": true,
    "id": "f45261b7b53043c316f45f613cb735907b93fb5a",
    "semantic_title": "raise a child in large language model: towards effective and generalizable fine-tuning",
    "citation_count": 184,
    "authors": [
      "Runxin Xu",
      "Fuli Luo",
      "Zhiyuan Zhang",
      "Chuanqi Tan",
      "Baobao Chang",
      "Songfang Huang",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.750": {
    "title": "Knowledge Graph Representation Learning using Ordinary Differential Equations",
    "volume": "main",
    "abstract": "Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a knowledge graph into a geometric space. The capability of KGEs in preserving graph characteristics including structural aspects and semantics, highly depends on the design of their score function, as well as the inherited abilities from the underlying geometry. Many KGEs use the Euclidean geometry which renders them incapable of preserving complex structures and consequently causes wrong inferences by the models. To address this problem, we propose a neuro differential KGE that embeds nodes of a KG on the trajectories of Ordinary Differential Equations (ODEs). To this end, we represent each relation (edge) in a KG as a vector field on several manifolds. We specifically parameterize ODEs by a neural network to represent complex manifolds and complex vector fields on the manifolds. Therefore, the underlying embedding space is capable to assume the shape of various geometric forms to encode heterogeneous subgraphs. Experiments on synthetic and benchmark datasets using state-of-the-art KGE models justify the ODE trajectories as a means to enable structure preservation and consequently avoiding wrong inferences",
    "checked": true,
    "id": "c7d5669401132f08f16e8f174862468bc33bf943",
    "semantic_title": "knowledge graph representation learning using ordinary differential equations",
    "citation_count": 12,
    "authors": [
      "Mojtaba Nayyeri",
      "Chengjin Xu",
      "Franca Hoffmann",
      "Mirza Mohtashim Alam",
      "Jens Lehmann",
      "Sahar Vahdati"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.751": {
    "title": "KnowMAN: Weakly Supervised Multinomial Adversarial Networks",
    "volume": "main",
    "abstract": "The absence of labeled data for training neural models is often addressed by leveraging knowledge about the specific task, resulting in heuristic but noisy labels. The knowledge is captured in labeling functions, which detect certain regularities or patterns in the training samples and annotate corresponding labels for training. This process of weakly supervised training may result in an over-reliance on the signals captured by the labeling functions and hinder models to exploit other signals or to generalize well. We propose KnowMAN, an adversarial scheme that enables to control influence of signals associated with specific labeling functions. KnowMAN forces the network to learn representations that are invariant to those signals and to pick up other signals that are more generally associated with an output label. KnowMAN strongly improves results compared to direct weakly supervised learning with a pre-trained transformer language model and a feature-based baseline",
    "checked": true,
    "id": "248521c176882cae6dae3e00e44a6d33052d2389",
    "semantic_title": "knowman: weakly supervised multinomial adversarial networks",
    "citation_count": 2,
    "authors": [
      "Luisa März",
      "Ehsaneddin Asgari",
      "Fabienne Braune",
      "Franziska Zimmermann",
      "Benjamin Roth"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.752": {
    "title": "ONION: A Simple and Effective Defense Against Textual Backdoor Attacks",
    "volume": "main",
    "abstract": "Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on multiple popular models. Nevertheless, there are few studies on defending against textual backdoor attacks. In this paper, we propose a simple and effective textual backdoor defense named ONION, which is based on outlier word detection and, to the best of our knowledge, is the first method that can handle all the textual backdoor attack situations. Experiments demonstrate the effectiveness of our model in defending BiLSTM and BERT against five different backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/ONION",
    "checked": true,
    "id": "3a1f8829e641b46f661775f64a7f27b933a46103",
    "semantic_title": "onion: a simple and effective defense against textual backdoor attacks",
    "citation_count": 273,
    "authors": [
      "Fanchao Qi",
      "Yangyi Chen",
      "Mukai Li",
      "Yuan Yao",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.753": {
    "title": "Value-aware Approximate Attention",
    "volume": "main",
    "abstract": "Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. However, all approximations thus far have ignored the contribution of the *value vectors* to the quality of approximation. In this work, we argue that research efforts should be directed towards approximating the true output of the attention sub-layer, which includes the value vectors. We propose a value-aware objective, and show theoretically and empirically that an optimal approximation of a value-aware objective substantially outperforms an optimal approximation that ignores values, in the context of language modeling. Moreover, we show that the choice of kernel function for computing attention similarity can substantially affect the quality of sparse approximations, where kernel functions that are less skewed are more affected by the value vectors",
    "checked": true,
    "id": "a4bd6a22dcdc740a9ff20af48fe2d828f0190b17",
    "semantic_title": "value-aware approximate attention",
    "citation_count": 5,
    "authors": [
      "Ankit Gupta",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.754": {
    "title": "Contrastive Domain Adaptation for Question Answering using Limited Text Corpora",
    "volume": "main",
    "abstract": "Question generation has recently shown impressive results in customizing question answering (QA) systems to new domains. These approaches circumvent the need for manually annotated training data from the new domain and, instead, generate synthetic question-answer pairs that are used for training. However, existing methods for question generation rely on large amounts of synthetically generated datasets and costly computational resources, which render these techniques widely inaccessible when the text corpora is of limited size. This is problematic as many niche domains rely on small text corpora, which naturally restricts the amount of synthetic data that can be generated. In this paper, we propose a novel framework for domain adaptation called contrastive domain adaptation for QA (CAQA). Specifically, CAQA combines techniques from question generation and domain-invariant learning to answer out-of-domain questions in settings with limited text corpora. Here, we train a QA system on both source data and generated data from the target domain with a contrastive adaptation loss that is incorporated in the training objective. By combining techniques from question generation and domain-invariant learning, our model achieved considerable improvements compared to state-of-the-art baselines",
    "checked": true,
    "id": "5fbe287249c71eb080962ad0c180b15edb72f75f",
    "semantic_title": "contrastive domain adaptation for question answering using limited text corpora",
    "citation_count": 38,
    "authors": [
      "Zhenrui Yue",
      "Bernhard Kratzwald",
      "Stefan Feuerriegel"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.755": {
    "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases",
    "volume": "main",
    "abstract": "It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions — a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11% on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations",
    "checked": true,
    "id": "a07a94168608322600fd3cab54df1410b96852b6",
    "semantic_title": "case-based reasoning for natural language queries over knowledge bases",
    "citation_count": 164,
    "authors": [
      "Rajarshi Das",
      "Manzil Zaheer",
      "Dung Thai",
      "Ameya Godbole",
      "Ethan Perez",
      "Jay Yoon Lee",
      "Lizhen Tan",
      "Lazaros Polymenakos",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.756": {
    "title": "Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation",
    "volume": "main",
    "abstract": "Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the more common setting, where only question–answer pairs are available. This paper investigates whether models can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR",
    "checked": true,
    "id": "50ec3d960ac458573a1e4a1556420c5e96d58609",
    "semantic_title": "distantly-supervised dense retrieval enables open-domain question answering without evidence annotation",
    "citation_count": 10,
    "authors": [
      "Chen Zhao",
      "Chenyan Xiong",
      "Jordan Boyd-Graber",
      "Hal Daumé III"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.757": {
    "title": "What's in a Name? Answer Equivalence For Open-Domain Question Answering",
    "volume": "main",
    "abstract": "A flaw in QA evaluation is that annotations often only provide one gold answer. Thus, model predictions semantically equivalent to the answer but superficially different are considered incorrect. This work explores mining alias entities from knowledge bases and using them as additional gold answers (i.e., equivalent answers). We incorporate answers for two settings: evaluation with additional answers and model training with equivalent answers. We analyse three QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion increases the exact match score on all datasets for evaluation, while incorporating it helps model training over real-world datasets. We ensure the additional answers are valid through a human post hoc evaluation",
    "checked": true,
    "id": "898033364fc216f9ed3fe3b1a6bf62c07f47420a",
    "semantic_title": "what's in a name? answer equivalence for open-domain question answering",
    "citation_count": 35,
    "authors": [
      "Chenglei Si",
      "Chen Zhao",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.758": {
    "title": "Evaluation Paradigms in Question Answering",
    "volume": "main",
    "abstract": "Question answering (QA) primarily descends from two branches of research: (1) Alan Turing's investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon's comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these paradigms. Despite substantial overlap, subtle but significant distinctions exert an outsize influence on research. While one evaluation paradigm values creating more intelligent QA systems, the other paradigm values building QA systems that appeal to users. By better understanding the epistemic heritage of QA, researchers, academia, and industry can more effectively accelerate QA research",
    "checked": true,
    "id": "45793225c6593db60e9efd95bce1d70bf4844198",
    "semantic_title": "evaluation paradigms in question answering",
    "citation_count": 12,
    "authors": [
      "Pedro Rodriguez",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.759": {
    "title": "Numerical reasoning in machine reading comprehension tasks: are we there yet?",
    "volume": "main",
    "abstract": "Numerical reasoning based machine reading comprehension is a task that involves reading comprehension along with using arithmetic operations such as addition, subtraction, sorting and counting. The DROP benchmark (Dua et al., 2019) is a recent dataset that has inspired the design of NLP models aimed at solving this task. The current standings of these models in the DROP leaderboard, over standard metrics, suggests that the models have achieved near-human performance. However, does this mean that these models have learned to reason? In this paper, we present a controlled study on some of the top-performing model architectures for the task of numerical reasoning. Our observations suggest that the standard metrics are incapable of measuring progress towards such tasks",
    "checked": true,
    "id": "5d9ac727b4a4e1044b3ba464df3be66eb8568127",
    "semantic_title": "numerical reasoning in machine reading comprehension tasks: are we there yet?",
    "citation_count": 13,
    "authors": [
      "Hadeel Al-Negheimish",
      "Pranava Madhyastha",
      "Alessandra Russo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.760": {
    "title": "Set Generation Networks for End-to-End Knowledge Base Population",
    "volume": "main",
    "abstract": "The task of knowledge base population (KBP) aims to discover facts about entities from texts and expand a knowledge base with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the networks, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets",
    "checked": true,
    "id": "ede9363165c0f4b0b6501ab2696e7059d460756e",
    "semantic_title": "set generation networks for end-to-end knowledge base population",
    "citation_count": 14,
    "authors": [
      "Dianbo Sui",
      "Chenhao Wang",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao",
      "Wei Bi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.761": {
    "title": "Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction",
    "volume": "main",
    "abstract": "Distantly supervised relation extraction (RE) automatically aligns unstructured text with relation instances in a knowledge base (KB). Due to the incompleteness of current KBs, sentences implying certain relations may be annotated as N/A instances, which causes the so-called false negative (FN) problem. Current RE methods usually overlook this problem, inducing improper biases in both training and testing procedures. To address this issue, we propose a two-stage approach. First, it finds out possible FN samples by heuristically leveraging the memory mechanism of deep neural networks. Then, it aligns those unlabeled data with the training data into a unified feature space by adversarial training to assign pseudo labels and further utilize the information contained in them. Experiments on two wildly-used benchmark datasets demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "51c023f10658d77aac185cee9e1c9da4c5256993",
    "semantic_title": "knowing false negatives: an adversarial training method for distantly supervised relation extraction",
    "citation_count": 19,
    "authors": [
      "Kailong Hao",
      "Botao Yu",
      "Wei Hu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.762": {
    "title": "Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion",
    "volume": "main",
    "abstract": "Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for bootstrapping which jointly models the bootstrapping process and the boundary learning process in a GAN framework. Specifically, the expansion boundaries of different bootstrapping iterations are learned via different discriminator networks; the bootstrapping network is the generator to generate new positive entities, and the discriminator networks identify the expansion boundaries by trying to distinguish the generated entities from known positive entities. By iteratively performing the above adversarial learning, the generator and the discriminators can reinforce each other and be progressively refined along the whole bootstrapping process. Experiments show that BootstrapGAN achieves the new state-of-the-art entity set expansion performance",
    "checked": true,
    "id": "5f98175e514f58d39d14ad16a7959b6470a85451",
    "semantic_title": "progressive adversarial learning for bootstrapping: a case study on entity set expansion",
    "citation_count": 5,
    "authors": [
      "Lingyong Yan",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.763": {
    "title": "Uncovering Main Causalities for Long-tailed Information Extraction",
    "volume": "main",
    "abstract": "Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional likelihood models. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference. Specifically, 1) we first introduce a unified structural causal model (SCM) for various IE tasks, describing the relationships among variables; 2) with our SCM, we then generate counterfactuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues",
    "checked": true,
    "id": "46125cd229711e6b3b39d521f11c39f911fe1b79",
    "semantic_title": "uncovering main causalities for long-tailed information extraction",
    "citation_count": 47,
    "authors": [
      "Guoshun Nan",
      "Jiaqi Zeng",
      "Rui Qiao",
      "Zhijiang Guo",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.764": {
    "title": "Maximal Clique Based Non-Autoregressive Open Information Extraction",
    "volume": "main",
    "abstract": "Open Information Extraction (OpenIE) aims to discover textual facts from a given sentence. In essence, the facts contained in plain text are unordered. However, the popular OpenIE systems usually output facts sequentially in the way of predicting the next fact conditioned on the previous decoded ones, which enforce an unnecessary order on the facts and involve the error accumulation between autoregressive steps. To break this bottleneck, we propose MacroIE, a novel non-autoregressive framework for OpenIE. MacroIE firstly constructs a fact graph based on the table filling scheme, in which each node denotes a fact element, and an edge links two nodes that belong to the same fact. Then OpenIE can be reformulated as a non-parametric process of finding maximal cliques from the graph. It directly outputs the final set of facts in one go, thus getting rid of the burden of predicting fact order, as well as the error propagation between facts. Experiments conducted on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods, beats the previous systems by as much as 5.7 absolute gain in F1 score",
    "checked": true,
    "id": "95b5b6140c8ba8e3575bf9dd4b085e0562075cde",
    "semantic_title": "maximal clique based non-autoregressive open information extraction",
    "citation_count": 13,
    "authors": [
      "Bowen Yu",
      "Yucheng Wang",
      "Tingwen Liu",
      "Hongsong Zhu",
      "Limin Sun",
      "Bin Wang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.765": {
    "title": "A Relation-Oriented Clustering Method for Open Relation Extraction",
    "volume": "main",
    "abstract": "The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters cannot explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the model to learn to cluster relational data, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our method reduces the error rate by 29.2% and 15.7%, on two datasets respectively, compared with current SOTA methods",
    "checked": true,
    "id": "af1561f3c02753672d433d339b2bce100fa9e5af",
    "semantic_title": "a relation-oriented clustering method for open relation extraction",
    "citation_count": 34,
    "authors": [
      "Jun Zhao",
      "Tao Gui",
      "Qi Zhang",
      "Yaqian Zhou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.766": {
    "title": "Exploring Methods for Generating Feedback Comments for Writing Learning",
    "volume": "main",
    "abstract": "The task of generating explanatory notes for language learners is known as feedback comment generation. Although various generation techniques are available, little is known about which methods are appropriate for this task. Nagata (2019) demonstrates the effectiveness of neural-retrieval-based methods in generating feedback comments for preposition use. Retrieval-based methods have limitations in that they can only output feedback comments existing in a given training data. Furthermore, feedback comments can be made on other grammatical and writing items than preposition use, which is still unaddressed. To shed light on these points, we investigate a wider range of methods for generating many feedback comments in this study. Our close analysis of the type of task leads us to investigate three different architectures for comment generation: (i) a neural-retrieval-based method as a baseline, (ii) a pointer-generator-based generation method as a neural seq2seq method, (iii) a retrieve-and-edit method, a hybrid of (i) and (ii). Intuitively, the pointer-generator should outperform neural-retrieval, and retrieve-and-edit should perform best. However, in our experiments, this expectation is completely overturned. We closely analyze the results to reveal the major causes of these counter-intuitive results and report on our findings from the experiments",
    "checked": true,
    "id": "e91c6ff2ee95525113622f9834cf0521fa619484",
    "semantic_title": "exploring methods for generating feedback comments for writing learning",
    "citation_count": 24,
    "authors": [
      "Kazuaki Hanawa",
      "Ryo Nagata",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.767": {
    "title": "A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis",
    "volume": "main",
    "abstract": "Chatbot is increasingly thriving in different domains, however, because of unexpected discourse complexity and training data sparseness, its potential distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff (MHCH), predicting chatbot failure and enabling human-algorithm collaboration to enhance chatbot quality, has attracted increasing attention from industry and academia. In this study, we propose a novel model, Role-Selected Sharing Network (RSSN), which integrates both dialogue satisfaction estimation and handoff prediction in one multi-task learning framework. Unlike prior efforts in dialog mining, by utilizing local user satisfaction as a bridge, global satisfaction detector and handoff predictor can effectively exchange critical information. Specifically, we decouple the relation and interaction between the two tasks by the role information after the shared encoder. Extensive experiments on two public datasets demonstrate the effectiveness of our model",
    "checked": true,
    "id": "42bb41493332abb170abf326bcc9740f1432a211",
    "semantic_title": "a role-selected sharing network for joint machine-human chatting handoff and service satisfaction analysis",
    "citation_count": 7,
    "authors": [
      "Jiawei Liu",
      "Kaisong Song",
      "Yangyang Kang",
      "Guoxiu He",
      "Zhuoren Jiang",
      "Changlong Sun",
      "Wei Lu",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.768": {
    "title": "Meta Distant Transfer Learning for Pre-trained Language Models",
    "volume": "main",
    "abstract": "With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task",
    "checked": true,
    "id": "8c33ffd4b3bc0e0af6d85c1b71f3cecf7cde7160",
    "semantic_title": "meta distant transfer learning for pre-trained language models",
    "citation_count": 7,
    "authors": [
      "Chengyu Wang",
      "Haojie Pan",
      "Minghui Qiu",
      "Jun Huang",
      "Fei Yang",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.769": {
    "title": "UniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference",
    "volume": "main",
    "abstract": "Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical rules for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use probabilistic model to approximate the exact logical inference (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting logical rules to be definite Horn rules, which can fully exploit the knowledge in logical rules and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both efficiency and effectiveness",
    "checked": true,
    "id": "e5c75925d01938b60bc0bba65b0161e74107c5d6",
    "semantic_title": "uniker: a unified framework for combining embedding and definite horn rule reasoning for knowledge graph inference",
    "citation_count": 30,
    "authors": [
      "Kewei Cheng",
      "Ziqing Yang",
      "Ming Zhang",
      "Yizhou Sun"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.770": {
    "title": "Wasserstein Selective Transfer Learning for Cross-domain Text Mining",
    "volume": "main",
    "abstract": "Transfer learning (TL) seeks to improve the learning of a data-scarce target domain by using information from source domains. However, the source and target domains usually have different data distributions, which may lead to negative transfer. To alleviate this issue, we propose a Wasserstein Selective Transfer Learning (WSTL) method. Specifically, the proposed method considers a reinforced selector to select helpful data for transfer learning. We further use a Wasserstein-based discriminator to maximize the empirical distance between the selected source data and target data. The TL module is then trained to minimize the estimated Wasserstein distance in an adversarial manner and provides domain invariant features for the reinforced selector. We adopt an evaluation metric based on the performance of the TL module as delayed reward and a Wasserstein-based metric as immediate rewards to guide the reinforced selector learning. Compared with the competing TL approaches, the proposed method selects data samples that are closer to the target domain. It also provides better state features and reward signals that lead to better performance with faster convergence. Extensive experiments on three real-world text mining tasks demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "335ac7ae2885d8c54df37a0f221a0a79dfd07341",
    "semantic_title": "wasserstein selective transfer learning for cross-domain text mining",
    "citation_count": 6,
    "authors": [
      "Lingyun Feng",
      "Minghui Qiu",
      "Yaliang Li",
      "Haitao Zheng",
      "Ying Shen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.771": {
    "title": "Jointly Learning to Repair Code and Generate Commit Message",
    "volume": "main",
    "abstract": "We propose a novel task of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for software development. However, existing work usually performs the two tasks independently. We construct a multilingual triple dataset including buggy code, fixed code, and commit messages for this novel task. We first introduce a cascaded method with two models, one is to generate the fixed code first, and the other generates the commit message based on the fixed and original codes. We enhance the cascaded method with different training approaches, including the teacher-student method, the multi-task method, and the back-translation method. To deal with the error propagation problem of the cascaded method, we also propose a joint model that can both repair the program code and generate the commit message in a unified framework. Massive experiments on our constructed buggy-fixed-commit dataset reflect the challenge of this task and that the enhanced cascaded model and the proposed joint model significantly outperform baselines in both quality of code and commit messages",
    "checked": true,
    "id": "85a12c858f871afae2f4669fbe44d2e5bd9691aa",
    "semantic_title": "jointly learning to repair code and generate commit message",
    "citation_count": 4,
    "authors": [
      "Jiaqi Bai",
      "Long Zhou",
      "Ambrosio Blanco",
      "Shujie Liu",
      "Furu Wei",
      "Ming Zhou",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.772": {
    "title": "Inflate and Shrink:Enriching and Reducing Interactions for Fast Text-Image Retrieval",
    "volume": "main",
    "abstract": "By exploiting the cross-modal attention, cross-BERT methods have achieved state-of-the-art accuracy in cross-modal retrieval. Nevertheless, the heavy text-image interactions in the cross-BERT model are prohibitively slow for large-scale retrieval. Late-interaction methods trade off retrieval accuracy and efficiency by exploiting cross-modal interaction only in the late stage, attaining a satisfactory retrieval speed. In this work, we propose an inflating and shrinking approach to further boost the efficiency and accuracy of late-interaction methods. The inflating operation plugs several codes in the input of the encoder to exploit the text-image interactions more thoroughly for higher retrieval accuracy. Then the shrinking operation gradually reduces the text-image interactions through knowledge distilling for higher efficiency. Through an inflating operation followed by a shrinking operation, both efficiency and accuracy of a late-interaction model are boosted. Systematic experiments on public benchmarks demonstrate the effectiveness of our inflating and shrinking approach",
    "checked": true,
    "id": "29841237c5b7e65c06bef48f313bc9e57b4f8f7f",
    "semantic_title": "inflate and shrink:enriching and reducing interactions for fast text-image retrieval",
    "citation_count": 16,
    "authors": [
      "Haoliang Liu",
      "Tan Yu",
      "Ping Li"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.773": {
    "title": "On Pursuit of Designing Multi-modal Transformer for Video Grounding",
    "volume": "main",
    "abstract": "Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottom-up model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two encoders for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the decoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three benchmarks have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and metrics, with several times faster inference speed",
    "checked": true,
    "id": "e924ce5fe6da93ce6ada4e001449da8f1e18b779",
    "semantic_title": "on pursuit of designing multi-modal transformer for video grounding",
    "citation_count": 81,
    "authors": [
      "Meng Cao",
      "Long Chen",
      "Mike Zheng Shou",
      "Can Zhang",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.774": {
    "title": "COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images",
    "volume": "main",
    "abstract": "While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with real images. To create COVR, we use real images annotated with scene graphs, and propose an almost fully automatic procedure for generating question-answer pairs along with a set of context images. COVR focuses on questions that require complex reasoning, including higher-order operations such as quantification and aggregation. Due to the automatic generation process, COVR facilitates the creation of compositional splits, where models at test time need to generalize to new concepts and compositions in a zero- or few-shot setting. We construct compositional splits using COVR and demonstrate a myriad of cases where state-of-the-art pre-trained language-and-vision models struggle to compositionally generalize",
    "checked": true,
    "id": "06fbeaf4d16639f177973a06cd7c4f78cb5e38ed",
    "semantic_title": "covr: a test-bed for visually grounded compositional generalization with real images",
    "citation_count": 29,
    "authors": [
      "Ben Bogin",
      "Shivanshu Gupta",
      "Matt Gardner",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.775": {
    "title": "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers",
    "volume": "main",
    "abstract": "Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal",
    "checked": true,
    "id": "de2f6a8921f9bf984fdc2b46964a03d3b83e2fcd",
    "semantic_title": "vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers",
    "citation_count": 82,
    "authors": [
      "Stella Frank",
      "Emanuele Bugliarello",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.776": {
    "title": "HypMix: Hyperbolic Interpolative Data Augmentation",
    "volume": "main",
    "abstract": "Interpolation-based regularisation methods for data augmentation have proven to be effective for various tasks and modalities. These methods involve performing mathematical operations over the raw input samples or their latent states representations - vectors that often possess complex hierarchical geometries. However, these operations are performed in the Euclidean space, simplifying these representations, which may lead to distorted and noisy interpolations. We propose HypMix, a novel model-, data-, and modality-agnostic interpolative data augmentation technique operating in the hyperbolic space, which captures the complex geometry of input and hidden state hierarchies better than its contemporaries. We evaluate HypMix on benchmark and low resource datasets across speech, text, and vision modalities, showing that HypMix consistently outperforms state-of-the-art data augmentation techniques. In addition, we demonstrate the use of HypMix in semi-supervised settings. We further probe into the adversarial robustness and qualitative inferences we draw from HypMix that elucidate the efficacy of the Riemannian hyperbolic manifolds for interpolation-based data augmentation",
    "checked": true,
    "id": "7be52a6c61dcac71a3aecb16cfb810ac201f97a3",
    "semantic_title": "hypmix: hyperbolic interpolative data augmentation",
    "citation_count": 12,
    "authors": [
      "Ramit Sawhney",
      "Megh Thakkar",
      "Shivam Agarwal",
      "Di Jin",
      "Diyi Yang",
      "Lucie Flek"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.777": {
    "title": "Integrating Deep Event-Level and Script-Level Information for Script Event Prediction",
    "volume": "main",
    "abstract": "Scripts are structured sequences of events together with the participants, which are extracted from the texts. Script event prediction aims to predict the subsequent event given the historical events in the script. Two kinds of information facilitate this task, namely, the event-level information and the script-level information. At the event level, existing studies view an event as a verb with its participants, while neglecting other useful properties, such as the state of the participants. At the script level, most existing studies only consider a single event sequence corresponding to one common protagonist. In this paper, we propose a Transformer-based model, called MCPredictor, which integrates deep event-level and script-level information for script event prediction. At the event level, MCPredictor utilizes the rich information in the text to obtain more comprehensive event semantic representations. At the script-level, it considers multiple event sequences corresponding to different participants of the subsequent event. The experimental results on the widely-used New York Times corpus demonstrate the effectiveness and superiority of the proposed model",
    "checked": true,
    "id": "93ffd6441f7c8719a1a3cdd633b7e891c052bda0",
    "semantic_title": "integrating deep event-level and script-level information for script event prediction",
    "citation_count": 23,
    "authors": [
      "Long Bai",
      "Saiping Guan",
      "Jiafeng Guo",
      "Zixuan Li",
      "Xiaolong Jin",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.778": {
    "title": "QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions",
    "volume": "main",
    "abstract": "Multi-text applications, such as multi-document summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our setting exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our dataset. Analyses show that our new task is semantically challenging, capturing content overlap beyond lexical similarity and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks",
    "checked": true,
    "id": "27fb56d75af31c827566e3b3bd24278a42c0424b",
    "semantic_title": "qa-align: representing cross-text content overlap by aligning question-answer propositions",
    "citation_count": 18,
    "authors": [
      "Daniela Brook Weiss",
      "Paul Roit",
      "Ayal Klein",
      "Ori Ernst",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.779": {
    "title": "PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
    "volume": "main",
    "abstract": "Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions",
    "checked": true,
    "id": "5fbcfccd3736969d95ed660d8e6962c86b7a9113",
    "semantic_title": "picard: parsing incrementally for constrained auto-regressive decoding from language models",
    "citation_count": 385,
    "authors": [
      "Torsten Scholak",
      "Nathan Schucher",
      "Dzmitry Bahdanau"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.780": {
    "title": "Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings",
    "volume": "main",
    "abstract": "Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on unsupervised approaches that require not-paired input sentences. We instead propose a language-independent approach to build large datasets of pairs of informal texts weakly similar, without manual human effort, exploiting Twitter's intrinsic powerful signals of relatedness: replies and quotes of tweets. We use the collected pairs to train a Transformer model with triplet-like structures, and we test the generated embeddings on Twitter NLP similarity tasks (PIT and TURL) and STSb. We also introduce four new sentence ranking evaluation benchmarks of informal texts, carefully extracted from the initial collections of tweets, proving not only that our best model learns classical Semantic Textual Similarity, but also excels on tasks where pairs of sentences are not exact paraphrases. Ablation studies reveal how increasing the corpus size influences positively the results, even at 2M samples, suggesting that bigger collections of Tweets still do not contain redundant information about semantic similarities. Code available at https://github.com/marco-digio/Twitter4SSE",
    "checked": true,
    "id": "e50c73f3b7b4556f282ae1d9b41c18fad1979883",
    "semantic_title": "exploiting twitter as source of large corpora of weakly similar pairs for semantic sentence embeddings",
    "citation_count": 10,
    "authors": [
      "Marco Di Giovanni",
      "Marco Brambilla"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.781": {
    "title": "Guilt by Association: Emotion Intensities in Lexical Representations",
    "volume": "main",
    "abstract": "What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data",
    "checked": true,
    "id": "de2dfb9fef91ca3136fca0f065acee9f77083673",
    "semantic_title": "guilt by association: emotion intensities in lexical representations",
    "citation_count": 2,
    "authors": [
      "Shahab Raji",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.782": {
    "title": "Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender",
    "volume": "main",
    "abstract": "Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable—alternate words used to express the same concept—in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word \"partner\" and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change",
    "checked": true,
    "id": "82927568101c73077ec0cb157855ab263637ee59",
    "semantic_title": "using sociolinguistic variables to reveal changing attitudes towards sexuality and gender",
    "citation_count": 7,
    "authors": [
      "Sky CH-Wang",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.783": {
    "title": "Identifying Morality Frames in Political Tweets using Relational Learning",
    "volume": "main",
    "abstract": "Extracting moral sentiment from text is a vital component in understanding public opinion, social movements, and policy decisions. The Moral Foundation Theory identifies five moral foundations, each associated with a positive and negative polarity. However, moral sentiment is often motivated by its targets, which can correspond to individuals or collective entities. In this paper, we introduce morality frames, a representation framework for organizing moral attitudes directed at different entities, and come up with a novel and high-quality annotated dataset of tweets written by US politicians. Then, we propose a relational learning model to predict moral attitudes towards entities and moral foundations jointly. We do qualitative and quantitative evaluations, showing that moral sentiment towards entities differs highly across political ideologies",
    "checked": true,
    "id": "542caba91507b66faf2a9479d61cf3d05be70e0b",
    "semantic_title": "identifying morality frames in political tweets using relational learning",
    "citation_count": 41,
    "authors": [
      "Shamik Roy",
      "Maria Leonor Pacheco",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.784": {
    "title": "Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications",
    "volume": "main",
    "abstract": "Certainty and uncertainty are fundamental to science communication. Hedges have widely been used as proxies for uncertainty. However, certainty is a complex construct, with authors expressing not only the degree but the type and aspects of uncertainty in order to give the reader a certain impression of what is known. Here, we introduce a new study of certainty that models both the level and the aspects of certainty in scientific findings. Using a new dataset of 2167 annotated scientific findings, we demonstrate that hedges alone account for only a partial explanation of certainty. We show that both the overall certainty and individual aspects can be predicted with pre-trained language models, providing a more complete picture of the author's intended communication. Downstream analyses on 431K scientific findings from news and scientific abstracts demonstrate that modeling sentence-level and aspect-level certainty is meaningful for areas like science communication. Both the model and datasets used in this paper are released at https://blablablab.si.umich.edu/projects/certainty/",
    "checked": true,
    "id": "5a5169364b072740d43a796a71f80e05186a8786",
    "semantic_title": "measuring sentence-level and aspect-level (un)certainty in science communications",
    "citation_count": 30,
    "authors": [
      "Jiaxin Pei",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.785": {
    "title": "Assessing the Reliability of Word Embedding Gender Bias Measures",
    "volume": "main",
    "abstract": "Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures' reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures",
    "checked": true,
    "id": "d7aa0383dd5ed751bda53bbe5caacf68d6325956",
    "semantic_title": "assessing the reliability of word embedding gender bias measures",
    "citation_count": 21,
    "authors": [
      "Yupei Du",
      "Qixiang Fang",
      "Dong Nguyen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.786": {
    "title": "Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks",
    "volume": "main",
    "abstract": "Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this study, to substantially reinforces the interaction of user opinions while alleviating the negative impact imposed by irrelevant posts, we first represent the conversation thread as an undirected interaction graph. We then present a Claim-guided Hierarchical Graph Attention Network for rumor classification, which enhances the representation learning for responsive posts considering the entire social contexts and attends over the posts that can semantically infer the target claim. Extensive experiments on three Twitter datasets demonstrate that our rumor detection method achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages",
    "checked": true,
    "id": "a3452276ada37727d0008dad8ca7c27bbbee6984",
    "semantic_title": "rumor detection on twitter with claim-guided hierarchical graph attention networks",
    "citation_count": 65,
    "authors": [
      "Hongzhan Lin",
      "Jing Ma",
      "Mingfei Cheng",
      "Zhiwei Yang",
      "Liangliang Chen",
      "Guang Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.787": {
    "title": "Learning Bill Similarity with Annotated and Augmented Corpora of Bills",
    "volume": "main",
    "abstract": "Bill writing is a critical element of representative democracy. However, it is often overlooked that most legislative bills are derived, or even directly copied, from other bills. Despite the significance of bill-to-bill linkages for understanding the legislative process, existing approaches fail to address semantic similarities across bills, let alone reordering or paraphrasing which are prevalent in legal document writing. In this paper, we overcome these limitations by proposing a 5-class classification task that closely reflects the nature of the bill generation process. In doing so, we construct a human-labeled dataset of 4,721 bill-to-bill relationships at the subsection-level and release this annotated dataset to the research community. To augment the dataset, we generate synthetic data with varying degrees of similarity, mimicking the complex bill writing process. We use BERT variants and apply multi-stage training, sequentially fine-tuning our models with synthetic and human-labeled datasets. We find that the predictive performance significantly improves when training with both human-labeled and synthetic data. Finally, we apply our trained model to infer section- and bill-level similarities. Our analysis shows that the proposed methodology successfully captures the similarities across legal documents at various levels of aggregation",
    "checked": true,
    "id": "e1404561cd98ffa11e743d6c3cc30f431a29a068",
    "semantic_title": "learning bill similarity with annotated and augmented corpora of bills",
    "citation_count": 6,
    "authors": [
      "Jiseon Kim",
      "Elden Griggs",
      "In Song Kim",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.788": {
    "title": "SWEAT: Scoring Polarization of Topics across Different Corpora",
    "volume": "main",
    "abstract": "Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the usefulness of the introduced measure",
    "checked": true,
    "id": "d5cb26451f96c9d24a58fe9021f134a863a28d23",
    "semantic_title": "sweat: scoring polarization of topics across different corpora",
    "citation_count": 6,
    "authors": [
      "Federico Bianchi",
      "Marco Marelli",
      "Paolo Nicoli",
      "Matteo Palmonari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.789": {
    "title": "So You Think You're Funny?\": Rating the Humour Quotient in Standup Comedy",
    "volume": "main",
    "abstract": "Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour-annotated dataset (~40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience's laughter. The normalized duration (laughter duration divided by the clip duration) of laughter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by comparing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that provides a ‘funniness' score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our ‘Open Mic' dataset is released for further research along with the code",
    "checked": true,
    "id": "1158a9a9fe1f557c19f3450097cbb76ef5fe6aee",
    "semantic_title": "so you think you're funny?\": rating the humour quotient in standup comedy",
    "citation_count": 15,
    "authors": [
      "Anirudh Mittal",
      "Pranav Jeevan P",
      "Prerak Gandhi",
      "Diptesh Kanojia",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.790": {
    "title": "Was it \"stated\" or was it \"claimed\"?: How linguistic bias affects generative language models",
    "volume": "main",
    "abstract": "People use language in subtle and nuanced ways to convey their beliefs. For instance, saying claimed instead of said casts doubt on the truthfulness of the underlying proposition, thus representing the author's opinion on the matter. Several works have identified such linguistic classes of words that occur frequently in natural language text and are bias-inducing by virtue of their framing effects. In this paper, we test whether generative language models (including GPT-2 (CITATION) are sensitive to these linguistic framing effects. In particular, we test whether prompts that contain linguistic markers of author bias (e.g., hedges, implicatives, subjective intensifiers, assertives) influence the distribution of the generated text. Although these framing effects are subtle and stylistic, we find evidence that they lead to measurable style and topic differences in the generated text, leading to language that is, on average, more polarised and more skewed towards controversial entities and events",
    "checked": true,
    "id": "d8d5ab10cb30034296b1bf6acd15a89b7be13a25",
    "semantic_title": "was it \"stated\" or was it \"claimed\"?: how linguistic bias affects generative language models",
    "citation_count": 17,
    "authors": [
      "Roma Patel",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.791": {
    "title": "PAUSE: Positive and Annealed Unlabeled Sentence Embedding",
    "volume": "main",
    "abstract": "Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of natural language processing (NLP) applications. The majority of these techniques are either supervised or unsupervised. Compared to the unsupervised methods, the supervised ones make less assumptions about optimization objectives and usually achieve better results. However, the training requires a large amount of labeled sentence pairs, which is not available in many industrial scenarios. To that end, we propose a generic and end-to-end approach – PAUSE (Positive and Annealed Unlabeled Sentence Embedding), capable of learning high-quality sentence embeddings from a partially labeled dataset. We experimentally show that PAUSE achieves, and sometimes surpasses, state-of-the-art results using only a small fraction of labeled sentence pairs on various benchmark tasks. When applied to a real industrial use case where labeled samples are scarce, PAUSE encourages us to extend our dataset without the burden of extensive manual annotation work",
    "checked": true,
    "id": "c79182423e7bdddbb4574927be2dfa0ba6e8276f",
    "semantic_title": "pause: positive and annealed unlabeled sentence embedding",
    "citation_count": 5,
    "authors": [
      "Lele Cao",
      "Emil Larsson",
      "Vilhelm von Ehrenheim",
      "Dhiana Deva Cavalcanti Rocha",
      "Anna Martin",
      "Sonja Horn"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.792": {
    "title": "A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders",
    "volume": "main",
    "abstract": "Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector representations can be manipulated to indirectly steer such properties. For efficient learning, we investigate the use of a geometric mapping in embedding space to transform linguistic properties, without any tuning of the pre-trained sentence encoder or decoder. We validate our approach on three linguistic properties using a pre-trained multilingual autoencoder and analyze the results in both monolingual and cross-lingual settings",
    "checked": true,
    "id": "60484ee2ed2248cffabb4696fa1e4e0df9cc0a64",
    "semantic_title": "a simple geometric method for cross-lingual linguistic transformations with pre-trained autoencoders",
    "citation_count": 1,
    "authors": [
      "Maarten De Raedt",
      "Fréderic Godin",
      "Pieter Buteneers",
      "Chris Develder",
      "Thomas Demeester"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.793": {
    "title": "An Information-Theoretic Characterization of Morphological Fusion",
    "volume": "main",
    "abstract": "Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that languages have characteristic levels of fusion; rather, the degree of fusion varies across part-of-speech within languages",
    "checked": true,
    "id": "fe5ee3ca5a9cdbf77c1bfc44088bfc10dc675f2b",
    "semantic_title": "an information-theoretic characterization of morphological fusion",
    "citation_count": 10,
    "authors": [
      "Neil Rathi",
      "Michael Hahn",
      "Richard Futrell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.794": {
    "title": "The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning",
    "volume": "main",
    "abstract": "Natural languages display a trade-off among different strategies to convey syntactic structure, such as word order or inflection. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field: (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during learning, instead of developing a more efficient or systematic language",
    "checked": true,
    "id": "0a1e6d53fce228e6e8104dc1b1ccaf4ecb410d96",
    "semantic_title": "the effect of efficient messaging and input variability on neural-agent iterated language learning",
    "citation_count": 8,
    "authors": [
      "Yuchen Lian",
      "Arianna Bisazza",
      "Tessa Verhoef"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.795": {
    "title": "On Classifying whether Two Texts are on the Same Side of an Argument",
    "volume": "main",
    "abstract": "To ease the difficulty of argument stance classification, the task of same side stance classification (S3C) has been proposed. In contrast to actual stance classification, which requires a substantial amount of domain knowledge to identify whether an argument is in favor or against a certain issue, it is argued that, for S3C, only argument similarity within stances needs to be learned to successfully solve the task. We evaluate several transformer-based approaches on the dataset of the recent S3C shared task, followed by an in-depth evaluation and error analysis of our model and the task's hypothesis. We show that, although we achieve state-of-the-art results, our model fails to generalize both within as well as across topics and domains when adjusting the sampling strategy of the training and test set to a more adversarial scenario. Our evaluation shows that current state-of-the-art approaches cannot determine same side stance by considering only domain-independent linguistic similarity features, but appear to require domain knowledge and semantic inference, too",
    "checked": true,
    "id": "983ff7ddb1b7c35be0b3929d20391216e6dde191",
    "semantic_title": "on classifying whether two texts are on the same side of an argument",
    "citation_count": 13,
    "authors": [
      "Erik Körner",
      "Gregor Wiedemann",
      "Ahmad Dawar Hakimi",
      "Gerhard Heyer",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.796": {
    "title": "Chinese Opinion Role Labeling with Corpus Translation: A Pivot Study",
    "volume": "main",
    "abstract": "Opinion Role Labeling (ORL), aiming to identify the key roles of opinion, has received increasing interest. Unlike most of the previous works focusing on the English language, in this paper, we present the first work of Chinese ORL. We construct a Chinese dataset by manually translating and projecting annotations from a standard English MPQA dataset. Then, we investigate the effectiveness of cross-lingual transfer methods, including model transfer and corpus translation. We exploit multilingual BERT with Contextual Parameter Generator and Adapter methods to examine the potentials of unsupervised cross-lingual learning and our experiments and analyses for both bilingual and multilingual transfers establish a foundation for the future research of this task",
    "checked": true,
    "id": "22552a35a73730887e803a45fb4da73c6a6932dd",
    "semantic_title": "chinese opinion role labeling with corpus translation: a pivot study",
    "citation_count": 6,
    "authors": [
      "Ranran Zhen",
      "Rui Wang",
      "Guohong Fu",
      "Chengguo Lv",
      "Meishan Zhang"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.797": {
    "title": "MassiveSumm: a very large-scale, very multilingual, news summarisation dataset",
    "volume": "main",
    "abstract": "Current research in automatic summarisation is unapologetically anglo-centered–a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with digitalisation, archiving, and social media advertising of newswire articles, recent work has shown how, with careful methodology application, large-scale datasets can now be simply gathered instead of written. In this paper, we present a large-scale multilingual summarisation dataset containing articles in 92 languages, spread across 28.8 million articles, in more than 35 writing scripts. This is both the largest, most inclusive, existing automatic summarisation dataset, as well as one of the largest, most inclusive, ever published datasets for any NLP task. We present the first investigation on the efficacy of resource building from news platforms in the low-resource language setting. Finally, we provide some first insight on how low-resource language settings impact state-of-the-art automatic summarisation system performance",
    "checked": true,
    "id": "3b1de514ff3955034605a4a5d7121ecff694bc4a",
    "semantic_title": "massivesumm: a very large-scale, very multilingual, news summarisation dataset",
    "citation_count": 47,
    "authors": [
      "Daniel Varab",
      "Natalie Schluter"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.798": {
    "title": "AUTOSUMM: Automatic Model Creation for Text Summarization",
    "volume": "main",
    "abstract": "Recent efforts to develop deep learning models for text generation tasks such as extractive and abstractive summarization have resulted in state-of-the-art performances on various datasets. However, obtaining the best model configuration for a given dataset requires an extensive knowledge of deep learning specifics like model architecture, tuning parameters etc., and is often extremely challenging for a non-expert. In this paper, we propose methods to automatically create deep learning models for the tasks of extractive and abstractive text summarization. Based on the recent advances in Automated Machine Learning and the success of large language models such as BERT and GPT-2 in encoding knowledge, we use a combination of Neural Architecture Search (NAS) and Knowledge Distillation (KD) techniques to perform model search and compression using the vast knowledge provided by these language models to develop smaller, customized models for any given dataset. We present extensive empirical results to illustrate the effectiveness of our model creation methods in terms of inference time and model size, while achieving near state-of-the-art performances in terms of accuracy across a range of datasets",
    "checked": true,
    "id": "0141258554d52b562f202746a756280ef189143c",
    "semantic_title": "autosumm: automatic model creation for text summarization",
    "citation_count": 3,
    "authors": [
      "Sharmila Reddy Nangi",
      "Atharv Tyagi",
      "Jay Mundra",
      "Sagnik Mukherjee",
      "Raj Snehal",
      "Niyati Chhaya",
      "Aparna Garimella"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.799": {
    "title": "Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output",
    "volume": "main",
    "abstract": "Compared to fully manual translation, post-editing (PE) machine translation (MT) output can save time and reduce errors. Automatic word-level quality estimation (QE) aims to predict the correctness of words in MT output and holds great promise to aid PE by flagging problematic output. Quality of QE is crucial, as incorrect QE might lead to translators missing errors or wasting time on already correct MT output. Achieving accurate automatic word-level QE is very hard, and it is currently not known (i) at what quality threshold QE is actually beginning to be useful for human PE, and (ii), how to best present word-level QE information to translators. In particular, should word-level QE visualization indicate uncertainty of the QE model or not? In this paper, we address both research questions with real and simulated word-level QE, visualizations, and user studies, where time, subjective ratings, and quality of the final translations are assessed. Results show that current word-level QE models are not yet good enough to support PE. Instead, quality levels of > 80% F1 are required. For helpful quality levels, a visualization reflecting the uncertainty of the QE model is preferred. Our analysis further shows that speed gains achieved through QE are not merely a result of blindly trusting the QE system, but that the quality of the final translations also improves. The threshold results from the paper establish a quality goal for future word-level QE research",
    "checked": true,
    "id": "833fe195669c4fcb89f1d5d11c268757df76501d",
    "semantic_title": "investigating the helpfulness of word-level quality estimation for post-editing machine translation output",
    "citation_count": 4,
    "authors": [
      "Raksha Shenoy",
      "Nico Herbig",
      "Antonio Krüger",
      "Josef van Genabith"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.800": {
    "title": "UNKs Everywhere: Adapting Multilingual Language Models to New Scripts",
    "volume": "main",
    "abstract": "Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model's embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT's and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model",
    "checked": true,
    "id": "13bcfb944779165983aaef22cec8a3bbd3e98e62",
    "semantic_title": "unks everywhere: adapting multilingual language models to new scripts",
    "citation_count": 131,
    "authors": [
      "Jonas Pfeiffer",
      "Ivan Vulić",
      "Iryna Gurevych",
      "Sebastian Ruder"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.801": {
    "title": "Neural Machine Translation Quality and Post-Editing Performance",
    "volume": "main",
    "abstract": "We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies. Through an experimental study involving over 30 professional translators for English -> Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality",
    "checked": true,
    "id": "9eca44a87882c506fce846cec66b0b389a598bef",
    "semantic_title": "neural machine translation quality and post-editing performance",
    "citation_count": 20,
    "authors": [
      "Vilém Zouhar",
      "Martin Popel",
      "Ondřej Bojar",
      "Aleš Tamchyna"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.802": {
    "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation",
    "volume": "main",
    "abstract": "Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models",
    "checked": true,
    "id": "2b9762e91305986ac8a2d624d0a69521304405f3",
    "semantic_title": "xtreme-r: towards more challenging and nuanced multilingual evaluation",
    "citation_count": 170,
    "authors": [
      "Sebastian Ruder",
      "Noah Constant",
      "Jan Botha",
      "Aditya Siddhant",
      "Orhan Firat",
      "Jinlan Fu",
      "Pengfei Liu",
      "Junjie Hu",
      "Dan Garrette",
      "Graham Neubig",
      "Melvin Johnson"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.803": {
    "title": "Contrastive Conditioning for Assessing Disambiguation in MT: A Case Study of Distilled Bias",
    "volume": "main",
    "abstract": "Lexical disambiguation is a major challenge for machine translation systems, especially if some senses of a word are trained less often than others. Identifying patterns of overgeneralization requires evaluation methods that are both reliable and scalable. We propose contrastive conditioning as a reference-free black-box method for detecting disambiguation errors. Specifically, we score the quality of a translation by conditioning on variants of the source that provide contrastive disambiguation cues. After validating our method, we apply it in a case study to perform a targeted evaluation of sequence-level knowledge distillation. By probing word sense disambiguation and translation of gendered occupation names, we show that distillation-trained models tend to overgeneralize more than other models with a comparable BLEU score. Contrastive conditioning thus highlights a side effect of distillation that is not fully captured by standard evaluation metrics. Code and data to reproduce our findings are publicly available",
    "checked": true,
    "id": "1fe457167f898eb6edb9ada7ccd4249e19237ba0",
    "semantic_title": "contrastive conditioning for assessing disambiguation in mt: a case study of distilled bias",
    "citation_count": 21,
    "authors": [
      "Jannis Vamvas",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.804": {
    "title": "Measuring Association Between Labels and Free-Text Rationales",
    "volume": "main",
    "abstract": "In interpretable NLP, we require faithful rationales that reflect the model's decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on \"reasoning\" tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales",
    "checked": true,
    "id": "343e06bae852f74a98573e798b501f6003bcb1c0",
    "semantic_title": "measuring association between labels and free-text rationales",
    "citation_count": 179,
    "authors": [
      "Sarah Wiegreffe",
      "Ana Marasović",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.805": {
    "title": "Discretized Integrated Gradients for Explaining Language Models",
    "volume": "main",
    "abstract": "As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model's output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the gradients computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research",
    "checked": true,
    "id": "5710567c482376c3c7c559062e884492090f7aca",
    "semantic_title": "discretized integrated gradients for explaining language models",
    "citation_count": 54,
    "authors": [
      "Soumya Sanyal",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.806": {
    "title": "Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords",
    "volume": "main",
    "abstract": "We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized \"pseudoword\" vector as a stand-in for a static embedding in the input layer, and then performing masked prediction of a word in the sentence, we are able to investigate the geometry of the BERT-space in a controlled manner around individual instances. Using our method on a set of carefully constructed sentences targeting highly ambiguous English words, we find substantial regularity in the contextualized space, with regions that correspond to distinct word senses; but between these regions there are occasionally \"sense voids\"—regions that do not correspond to any intelligible sense",
    "checked": true,
    "id": "cf8a7ba69339baeb0f62d697c322965cff2b38c5",
    "semantic_title": "putting words in bert's mouth: navigating contextualized vector spaces with pseudowords",
    "citation_count": 13,
    "authors": [
      "Taelin Karidi",
      "Yichu Zhou",
      "Nathan Schneider",
      "Omri Abend",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.807": {
    "title": "Rationales for Sequential Predictions",
    "volume": "main",
    "abstract": "Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the sequential objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales",
    "checked": true,
    "id": "1c709eef701d933af1383c790c13209f06806b60",
    "semantic_title": "rationales for sequential predictions",
    "citation_count": 33,
    "authors": [
      "Keyon Vafa",
      "Yuntian Deng",
      "David Blei",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.808": {
    "title": "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging",
    "volume": "main",
    "abstract": "Influence functions approximate the \"influences\" of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FastIF, a set of simple modifications to influence functions that significantly improves their run-time. We use k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good candidate data points, identify the configurations that best balance the speed-quality trade-off in estimating the inverse Hessian-vector product, and introduce a fast parallel variant. Our proposed method achieves about 80X speedup while being highly correlated with the original influence values. With the availability of the fast influence functions, we demonstrate their usefulness in four applications. First, we examine whether influential data-points can \"explain\" test time behavior using the framework of simulatability. Second, we visualize the influence interactions between training and test data-points. Third, we show that we can correct model errors by additional fine-tuning on certain influential data-points, improving the accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we experiment with a similar setup but fine-tuning on datapoints not seen during training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI datasets respectively. Overall, our fast influence functions can be efficiently applied to large models and datasets, and our experiments demonstrate the potential of influence functions in model interpretation and correcting model errors",
    "checked": true,
    "id": "18fb344c9bfd019014996e57c465aa279b7e0151",
    "semantic_title": "fastif: scalable influence functions for efficient model interpretation and debugging",
    "citation_count": 109,
    "authors": [
      "Han Guo",
      "Nazneen Rajani",
      "Peter Hase",
      "Mohit Bansal",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.809": {
    "title": "Studying word order through iterative shuffling",
    "volume": "main",
    "abstract": "As neural language models approach human performance on NLP benchmark tasks, their advances are widely seen as evidence of an increasingly complex understanding of syntax. This view rests upon a hypothesis that has not yet been empirically tested: that word order encodes meaning essential to performing these tasks. We refute this hypothesis in many cases: in the GLUE suite and in various genres of English text, the words in a sentence or phrase can rarely be permuted to form a phrase carrying substantially different information. Our surprising result relies on inference by iterative shuffling (IBIS), a novel, efficient procedure that finds the ordering of a bag of words having the highest likelihood under a fixed language model. IBIS can use any black-box model without additional training and is superior to existing word ordering algorithms. Coalescing our findings, we discuss how shuffling inference procedures such as IBIS can benefit language modeling and constrained generation",
    "checked": true,
    "id": "0412441ad2559c44075ca991b2b4ca533ac03dc5",
    "semantic_title": "studying word order through iterative shuffling",
    "citation_count": 14,
    "authors": [
      "Nikolay Malkin",
      "Sameera Lanka",
      "Pranav Goel",
      "Nebojsa Jojic"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.810": {
    "title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training",
    "volume": "main",
    "abstract": "We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantly-supervised NER models by significant margins",
    "checked": true,
    "id": "e199f191e54e662ea6a10a39d524ad767b5445cf",
    "semantic_title": "distantly-supervised named entity recognition with noise-robust learning and language model augmented self-training",
    "citation_count": 71,
    "authors": [
      "Yu Meng",
      "Yunyi Zhang",
      "Jiaxin Huang",
      "Xuan Wang",
      "Yu Zhang",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.811": {
    "title": "Open Knowledge Graphs Canonicalization using Variational Autoencoders",
    "volume": "main",
    "abstract": "Noun phrases and Relation phrases in open knowledge graphs are not canonicalized, leading to an explosion of redundant and ambiguous subject-relation-object triples. Existing approaches to solve this problem take a two-step approach. First, they generate embedding representations for both noun and relation phrases, then a clustering algorithm is used to group them using the embeddings as features. In this work, we propose Canonicalizing Using Variational AutoEncoders and Side Information (CUVA), a joint model to learn both embeddings and cluster assignments in an end-to-end approach, which leads to a better vector representation for the noun and relation phrases. Our evaluation over multiple benchmarks shows that CUVA outperforms the existing state-of-the-art approaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate entity canonicalization systems",
    "checked": true,
    "id": "73b1d409b71db3e9de8208254dae4a446a374c2f",
    "semantic_title": "open knowledge graphs canonicalization using variational autoencoders",
    "citation_count": 15,
    "authors": [
      "Sarthak Dash",
      "Gaetano Rossiello",
      "Nandana Mihindukulasooriya",
      "Sugato Bagchi",
      "Alfio Gliozzo"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.812": {
    "title": "HittER: Hierarchical Transformers for Knowledge Graph Embeddings",
    "volume": "main",
    "abstract": "This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entity-relation composition and Relational contextualization based on a source entity's neighborhood. Our proposed model consists of two different Transformer blocks: the bottom block extracts features of each entity-relation pair in the local neighborhood of the source entity and the top block aggregates the relational information from outputs of the bottom block. We further design a masked entity prediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new state-of-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets",
    "checked": true,
    "id": "7e7499b47fe57033768f26ef98a3b644688eb2a2",
    "semantic_title": "hitter: hierarchical transformers for knowledge graph embeddings",
    "citation_count": 109,
    "authors": [
      "Sanxing Chen",
      "Xiaodong Liu",
      "Jianfeng Gao",
      "Jian Jiao",
      "Ruofei Zhang",
      "Yangfeng Ji"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.813": {
    "title": "Few-Shot Named Entity Recognition: An Empirical Baseline Study",
    "volume": "main",
    "abstract": "This paper presents an empirical study to efficiently build named entity recognition (NER) systems when a small amount of in-domain labeled data is available. Based upon recent Transformer-based self-supervised pre-trained language models (PLMs), we investigate three orthogonal schemes to improve model generalization ability in few-shot settings: (1) meta-learning to construct prototypes for different entity types, (2) task-specific supervised pre-training on noisy web data to extract entity-related representations and (3) self-training to leverage unlabeled in-domain data. On 10 public NER datasets, we perform extensive empirical comparisons over the proposed schemes and their combinations with various proportions of labeled data, our experiments show that (i)in the few-shot learning setting, the proposed NER schemes significantly improve or outperform the commonly used baseline, a PLM-based linear classifier fine-tuned using domain labels. (ii) We create new state-of-the-art results on both few-shot and training-free settings compared with existing methods",
    "checked": true,
    "id": "84aec29de31b56b3324c00667dfac62850f8dadf",
    "semantic_title": "few-shot named entity recognition: an empirical baseline study",
    "citation_count": 94,
    "authors": [
      "Jiaxin Huang",
      "Chunyuan Li",
      "Krishan Subudhi",
      "Damien Jose",
      "Shobana Balakrishnan",
      "Weizhu Chen",
      "Baolin Peng",
      "Jianfeng Gao",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.814": {
    "title": "XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment",
    "volume": "main",
    "abstract": "Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as machine translation and cross-lingual wikification. While knowledge bases contain a large number of entities in high-resource languages such as English and French, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community",
    "checked": true,
    "id": "50a22023ee96e364174a70452e2528686971f457",
    "semantic_title": "xlent: mining a large cross-lingual entity dataset with lexical-semantic-phonetic word alignment",
    "citation_count": 18,
    "authors": [
      "Ahmed El-Kishky",
      "Adithya Renduchintala",
      "James Cross",
      "Francisco Guzmán",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.815": {
    "title": "Utilizing Relative Event Time to Enhance Event-Event Temporal Relation Extraction",
    "volume": "main",
    "abstract": "Event time is one of the most important features for event-event temporal relation extraction. However, explicit event time information in text is sparse. For example, only about 20% of event mentions in TimeBank-Dense have event-time links. In this paper, we propose a joint model for event-event temporal relation classification and an auxiliary task, relative event time prediction, which predicts the event time as real numbers. We adopt the Stack-Propagation framework to incorporate predicted relative event time for temporal relation classification and keep the differentiability. Our experiments on MATRES dataset show that our model can significantly improve the RoBERTa-based baseline and achieve state-of-the-art performance",
    "checked": true,
    "id": "9055d0c286811b1a4f68804805cf440f2d687931",
    "semantic_title": "utilizing relative event time to enhance event-event temporal relation extraction",
    "citation_count": 32,
    "authors": [
      "Haoyang Wen",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.816": {
    "title": "Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction",
    "volume": "main",
    "abstract": "State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such heuristics include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another heuristic: the mere retention of training relation triples. In this paper we propose two experiments confirming that retention of known facts is a key factor of performance on standard benchmarks. Furthermore, one experiment suggests that a pipeline model able to use intermediate type representations is less prone to over-rely on retention",
    "checked": true,
    "id": "e2d3bec24639ef2aaaf74aedf6b761a56d03a224",
    "semantic_title": "separating retention from extraction in the evaluation of end-to-end relation extraction",
    "citation_count": 6,
    "authors": [
      "Bruno Taillé",
      "Vincent Guigue",
      "Geoffrey Scoutheeten",
      "Patrick Gallinari"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.817": {
    "title": "Automatic Text Evaluation through the Lens of Wasserstein Barycenters",
    "volume": "main",
    "abstract": "A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, i.e., Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, it provides theoretical grounds to our metric and offers an alternative to available solutions (e.g., MoverScore and BertScore). Numerical evaluation is performed on four different tasks: machine translation, summarization, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for text summarization",
    "checked": true,
    "id": "9d7e7291ef34d88931cf306be7cced8d98e27d70",
    "semantic_title": "automatic text evaluation through the lens of wasserstein barycenters",
    "citation_count": 41,
    "authors": [
      "Pierre Colombo",
      "Guillaume Staerman",
      "Chloé Clavel",
      "Pablo Piantanida"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.818": {
    "title": "Visually Grounded Reasoning across Languages and Cultures",
    "volume": "main",
    "abstract": "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems",
    "checked": true,
    "id": "eea16dfc29f0521dd547e67a84af4ff95a9c5529",
    "semantic_title": "visually grounded reasoning across languages and cultures",
    "citation_count": 178,
    "authors": [
      "Fangyu Liu",
      "Emanuele Bugliarello",
      "Edoardo Maria Ponti",
      "Siva Reddy",
      "Nigel Collier",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.819": {
    "title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
    "volume": "main",
    "abstract": "The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning. To support this claim, we first show that the current evaluation method of WS is sub-optimal and propose a modification that uses twin sentences for evaluation. We also propose two new baselines that indicate the existence of artifacts in WS benchmarks. We then develop a method for evaluating WS-like sentences in a zero-shot setting to account for the commonsense reasoning abilities acquired during the pretraining and observe that popular language models perform randomly in this setting when using our more strict evaluation. We conclude that the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required commonsense reasoning skills and knowledge",
    "checked": true,
    "id": "2ef4be35f8424ea768aa2e1b44392b3eddbc780b",
    "semantic_title": "back to square one: artifact detection, training and commonsense disentanglement in the winograd schema",
    "citation_count": 44,
    "authors": [
      "Yanai Elazar",
      "Hongming Zhang",
      "Yoav Goldberg",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.820": {
    "title": "Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing",
    "volume": "main",
    "abstract": "Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior probability bias of the entity distribution towards more frequently occurring entities. It was shown that the performance of the EL systems on such datasets is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in accuracy between more and less common entities for all of the EL systems under evaluation, demonstrating the effect of prior probability bias and entity overshadowing",
    "checked": true,
    "id": "46a2b7182b40562de53c9be1f0107aa5b59c0b46",
    "semantic_title": "robustness evaluation of entity disambiguation using prior probes: the case of entity overshadowing",
    "citation_count": 15,
    "authors": [
      "Vera Provatorova",
      "Samarth Bhargav",
      "Svitlana Vakulenko",
      "Evangelos Kanoulas"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.821": {
    "title": "IndoNLI: A Natural Language Inference Dataset for Indonesian",
    "volume": "main",
    "abstract": "We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We adapt the data collection protocol for MNLI and collect ~18K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning. Experiment results show that XLM-R outperforms other pre-trained models in our data. The best performance on the expert-annotated data is still far below human performance (13.4% accuracy gap), suggesting that this test set is especially challenging. Furthermore, our analysis shows that our expert-annotated data is more diverse and contains fewer annotation artifacts than the crowd-annotated data. We hope this dataset can help accelerate progress in Indonesian NLP research",
    "checked": true,
    "id": "35772d76f90bdf74b9d49ec8e476fe27679022af",
    "semantic_title": "indonli: a natural language inference dataset for indonesian",
    "citation_count": 31,
    "authors": [
      "Rahmad Mahendra",
      "Alham Fikri Aji",
      "Samuel Louvan",
      "Fahrurrozi Rahman",
      "Clara Vania"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.822": {
    "title": "Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement",
    "volume": "main",
    "abstract": "Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce the need for annotated data, less attention has been paid to the quality of these data. Following a trend that has emerged recently, we focus on the level of agreement among annotators while selecting data to create offensive language datasets, a task involving a high level of subjectivity. Our study comprises the creation of three novel datasets of English tweets covering different topics and having five crowd-sourced judgments each. We also present an extensive set of experiments showing that selecting training and test data according to different levels of annotators' agreement has a strong effect on classifiers performance and robustness. Our findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. We show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and we advocate for a higher presence of ambiguous cases in future datasets, in order to train more robust systems and better account for the different points of view expressed online",
    "checked": true,
    "id": "f800278bceb610c7619b3fe4cc194321e0eeb6bf",
    "semantic_title": "agreeing to disagree: annotating offensive language datasets with annotators' disagreement",
    "citation_count": 97,
    "authors": [
      "Elisa Leonardelli",
      "Stefano Menini",
      "Alessio Palmero Aprosio",
      "Marco Guerini",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.823": {
    "title": "A Root of a Problem: Optimizing Single-Root Dependency Parsing",
    "volume": "main",
    "abstract": "We describe two approaches to single-root dependency parsing that yield significant speed ups in such parsing. One approach has been previously used in dependency parsers in practice, but remains undocumented in the parsing literature, and is considered a heuristic. We show that this approach actually finds the optimal dependency tree. The second approach relies on simple reweighting of the inference graph being input to the dependency parser and has an optimal running time. Here, we again show that this approach is fully correct and identifies the highest-scoring parse tree. Our experiments demonstrate a manyfold speed up compared to a previous graph-based state-of-the-art parser without any loss in accuracy or optimality",
    "checked": true,
    "id": "f85820cfe2b5172c3e8c2a00f1c120fef162bb1d",
    "semantic_title": "a root of a problem: optimizing single-root dependency parsing",
    "citation_count": 6,
    "authors": [
      "Miloš Stanojević",
      "Shay B. Cohen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.824": {
    "title": "Efficient Sampling of Dependency Structure",
    "volume": "main",
    "abstract": "Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a graph subject to the root constraint. Wilson (1996('s sampling algorithm has a running time of O(H) where H is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm has a running time of O(Nˆ3), which is often greater than the mean hitting time of a directed graph. Additionally, we build upon Colbourn's algorithm and present a novel extension that can sample K trees without replacement in O(K Nˆ3 + Kˆ2 N) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph",
    "checked": true,
    "id": "2c11cb80ff697f81d2e67ee87fb4a0064ef48954",
    "semantic_title": "efficient sampling of dependency structure",
    "citation_count": 0,
    "authors": [
      "Ran Zmigrod",
      "Tim Vieira",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.825": {
    "title": "Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering",
    "volume": "main",
    "abstract": "Discontinuous constituent parsers have always lagged behind continuous approaches in terms of accuracy and speed, as the presence of constituents with discontinuous yield introduces extra complexity to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer Network capable of accurately generating the continuous token arrangement for a given input sentence and define a bijective function to recover the original order. Experiments on the main benchmarks with two continuous parsers prove that our approach is on par in accuracy with purely discontinuous state-of-the-art algorithms, but considerably faster",
    "checked": true,
    "id": "dbc169f84f10b436a120b88a071339672e5ac996",
    "semantic_title": "reducing discontinuous to continuous parsing with pointer network reordering",
    "citation_count": 11,
    "authors": [
      "Daniel Fernández-González",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.826": {
    "title": "A New Representation for Span-based CCG Parsing",
    "volume": "main",
    "abstract": "This paper proposes a new representation for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and prevents the span-based parsing models from violating the schemata. Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers",
    "checked": true,
    "id": "3712be27993ee340920659e422a4b98581e2e094",
    "semantic_title": "a new representation for span-based ccg parsing",
    "citation_count": 2,
    "authors": [
      "Yoshihide Kato",
      "Shigeki Matsubara"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.827": {
    "title": "What to Pre-Train on? Efficient Intermediate Task Selection",
    "volume": "main",
    "abstract": "Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to experiment with all combinations to find the best transfer setting. In this work, we provide a comprehensive comparison of different methods for efficiently identifying beneficial tasks for intermediate transfer learning. We focus on parameter and computationally efficient adapter settings, highlight different data-availability scenarios, and provide expense estimates for each method. We experiment with a diverse set of 42 intermediate and 11 target English classification, multiple choice, question answering, and sequence tagging tasks. Our results demonstrate that efficient embedding based methods, which rely solely on the respective datasets, outperform computational expensive few-shot fine-tuning approaches. Our best methods achieve an average Regret@3 of 1% across all target tasks, demonstrating that we are able to efficiently identify the best datasets for intermediate training",
    "checked": true,
    "id": "7b99c51d562e33309a46601c846abbe72a65c6a4",
    "semantic_title": "what to pre-train on? efficient intermediate task selection",
    "citation_count": 98,
    "authors": [
      "Clifton Poth",
      "Jonas Pfeiffer",
      "Andreas Rücklé",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.828": {
    "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
    "volume": "main",
    "abstract": "A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks",
    "checked": true,
    "id": "bb363c8c5bc1c473f0801c647c88d0c071792858",
    "semantic_title": "permuteformer: efficient relative position encoding for long sequences",
    "citation_count": 21,
    "authors": [
      "Peng Chen"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.829": {
    "title": "Block Pruning For Faster Transformers",
    "volume": "main",
    "abstract": "Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying model, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74% smaller BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models in speed and pruned models in size",
    "checked": true,
    "id": "01b1293ddea9bcd6df1185b0b934503de01d6561",
    "semantic_title": "block pruning for faster transformers",
    "citation_count": 221,
    "authors": [
      "François Lagunas",
      "Ella Charlaix",
      "Victor Sanh",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.830": {
    "title": "Finetuning Pretrained Transformers into RNNs",
    "volume": "main",
    "abstract": "Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process",
    "checked": true,
    "id": "054e307c1edf4b28137ffcbce980fe81f0647d20",
    "semantic_title": "finetuning pretrained transformers into rnns",
    "citation_count": 64,
    "authors": [
      "Jungo Kasai",
      "Hao Peng",
      "Yizhe Zhang",
      "Dani Yogatama",
      "Gabriel Ilharco",
      "Nikolaos Pappas",
      "Yi Mao",
      "Weizhu Chen",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.831": {
    "title": "How to Train BERT with an Academic Budget",
    "volume": "main",
    "abstract": "While large language models a la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERT-base on GLUE tasks at a fraction of the original pretraining cost",
    "checked": true,
    "id": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9",
    "semantic_title": "how to train bert with an academic budget",
    "citation_count": 117,
    "authors": [
      "Peter Izsak",
      "Moshe Berchansky",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.832": {
    "title": "Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression",
    "volume": "main",
    "abstract": "Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowledge distillation and progressive module replacing with loyalty and robustness. By combining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness",
    "checked": true,
    "id": "5b05e150ef225b8004b4be42b7325a4de51d495d",
    "semantic_title": "beyond preserved accuracy: evaluating loyalty and robustness of bert compression",
    "citation_count": 42,
    "authors": [
      "Canwen Xu",
      "Wangchunshu Zhou",
      "Tao Ge",
      "Ke Xu",
      "Julian McAuley",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.833": {
    "title": "IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization",
    "volume": "main",
    "abstract": "We present IndoBERTweet, the first large-scale pretrained model for Indonesian Twitter that is trained by extending a monolingually-trained Indonesian BERT model with additive domain-specific vocabulary. We focus in particular on efficient model adaptation under vocabulary mismatch, and benchmark different ways of initializing the BERT embedding layer for new word types. We find that initializing with the average BERT subword embedding makes pretraining five times faster, and is more effective than proposed methods for vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based datasets",
    "checked": true,
    "id": "59c0c6b62e33850cda08663d4c9ecabcf5d21596",
    "semantic_title": "indobertweet: a pretrained language model for indonesian twitter with effective domain-specific vocabulary initialization",
    "citation_count": 83,
    "authors": [
      "Fajri Koto",
      "Jey Han Lau",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.834": {
    "title": "Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features",
    "volume": "main",
    "abstract": "We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we explore suitable transformers and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular datasets in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99%, a 20.3% increase from the previous SOTA",
    "checked": true,
    "id": "e3e27f04498dcbdf2b01d9fa241a1a03dfe1e12d",
    "semantic_title": "pushing on text readability assessment: a transformer meets handcrafted linguistic features",
    "citation_count": 79,
    "authors": [
      "Bruce W. Lee",
      "Yoo Sung Jang",
      "Jason Lee"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.835": {
    "title": "Types of Out-of-Distribution Texts and How to Detect Them",
    "volume": "main",
    "abstract": "Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of the distribution shifts of OOD examples and how to best detect them. We categorize these examples as exhibiting a background shift or semantic shift, and find that the two major approaches to OOD detection, calibration and density estimation (language modeling for text), have distinct behavior on these types of OOD data. Across 14 pairs of in-distribution and OOD English natural language understanding datasets, we find that density estimation methods consistently beat calibration methods in background shift settings and perform worse in semantic shift settings. In addition, we find that both methods generally fail to detect examples from challenge data, indicating that these examples constitute a different type of OOD data. Overall, while the categorization we apply explains many of the differences between the two methods, our results call for a more explicit definition of OOD to create better benchmarks and build detectors that can target the type of OOD data expected at test time",
    "checked": true,
    "id": "8331f4363d65235a8344e6a0c9b21fa3ab4c1d5e",
    "semantic_title": "types of out-of-distribution texts and how to detect them",
    "citation_count": 99,
    "authors": [
      "Udit Arora",
      "William Huang",
      "He He"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.836": {
    "title": "Self-training with Few-shot Rationalization",
    "volume": "main",
    "abstract": "While pre-trained language models have obtained state-of-the-art performance for several natural language understanding tasks, they are quite opaque in terms of their decision-making process. While some recent works focus on rationalizing neural predictions by highlighting salient concepts in the text as justifications or rationales, they rely on thousands of labeled training examples for both task labels as well as annotated rationales for every instance. Such extensive large-scale annotations are infeasible to obtain for many tasks. To this end, we develop a multi-task teacher-student framework based on self-training pre-trained language models with limited task-specific labels and rationales and judicious sample selection to learn from informative pseudo-labeled examples. We study several characteristics of what constitutes a good rationale and demonstrate that the neural model performance can be significantly improved by making it aware of its rationalized predictions, particularly in low-resource settings. Extensive experiments in several benchmark datasets demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "bebb3a214c9c0eab2099f1b7f5824bbf73726ec7",
    "semantic_title": "self-training with few-shot rationalization",
    "citation_count": 24,
    "authors": [
      "Meghana Moorthy Bhat",
      "Alessandro Sordoni",
      "Subhabrata Mukherjee"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.837": {
    "title": "MTAdam: Automatic Balancing of Multiple Training Loss Terms",
    "volume": "main",
    "abstract": "When training neural models, it is common to combine multiple loss terms. The balancing of these terms requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the loss terms can change as training progresses, e.g., for adversarial terms. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the Multi-Term Adam (MTAdam) computes the derivative of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the gradients across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match or improve conventional training with the prescribed hyperparameters of each method",
    "checked": true,
    "id": "a076ba807c7561e850684452e25771436b05025b",
    "semantic_title": "mtadam: automatic balancing of multiple training loss terms",
    "citation_count": 22,
    "authors": [
      "Itzik Malkiel",
      "Lior Wolf"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.838": {
    "title": "Softmax Tree: An Accurate, Fast Classifier When the Number of Classes Is Large",
    "volume": "main",
    "abstract": "Classification problems having thousands or more classes naturally occur in NLP, for example language models or document classification. A softmax or one-vs-all classifier naturally handles many classes, but it is very slow at inference time, because every class score must be calculated to find the top class. We propose the \"softmax tree\", consisting of a binary tree having sparse hyperplanes at the decision nodes (which make hard, not soft, decisions) and small softmax classifiers at the leaves. This is much faster at inference because the input instance follows a single path to a leaf (whose length is logarithmic on the number of leaves) and the softmax classifier at each leaf operates on a small subset of the classes. Although learning accurate tree-based models has proven difficult in the past, we are able to overcome this by using a variation of a recent algorithm, tree alternating optimization (TAO). Compared to a softmax and other classifiers, the resulting softmax trees are both more accurate in prediction and faster in inference, as shown in NLP problems having from one thousand to one hundred thousand classes",
    "checked": true,
    "id": "d953ecd4343516e2f437756419547612ac6e284e",
    "semantic_title": "softmax tree: an accurate, fast classifier when the number of classes is large",
    "citation_count": 5,
    "authors": [
      "Arman Zharmagambetov",
      "Magzhan Gabidolla",
      "Miguel A. Carreira-Perpinan"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.839": {
    "title": "Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning",
    "volume": "main",
    "abstract": "Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of noise and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutually-beneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods",
    "checked": true,
    "id": "20af1061f27bb44dab8e09aeb80c07efc5e23ac7",
    "semantic_title": "improving distantly-supervised named entity recognition with self-collaborative denoising learning",
    "citation_count": 22,
    "authors": [
      "Xinghua Zhang",
      "Bowen Yu",
      "Tingwen Liu",
      "Zhenyu Zhang",
      "Jiawei Sheng",
      "Xue Mengge",
      "Hongbo Xu"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.840": {
    "title": "Multivalent Entailment Graphs for Question Answering",
    "volume": "main",
    "abstract": "Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies, like DEFEAT(Biden, Trump) entails WIN(Biden); (2) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3) we demonstrate the capabilities of these graphs on a novel question answering task. We show that directional entailment is more helpful for inference than non-directional similarity on questions of fine-grained semantics. We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence",
    "checked": true,
    "id": "77276bcacaab2c0340e3a8544670d9c5cef207ab",
    "semantic_title": "multivalent entailment graphs for question answering",
    "citation_count": 14,
    "authors": [
      "Nick McKenna",
      "Liane Guillou",
      "Mohammad Javad Hosseini",
      "Sander Bijl de Vroe",
      "Mark Johnson",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.841": {
    "title": "Is Everything in Order? A Simple Way to Order Sentences",
    "volume": "main",
    "abstract": "The task of organizing a shuffled set of sentences into a coherent text has been used to evaluate a machine's understanding of causal and temporal relations. We formulate the sentence ordering task as a conditional text-to-marker generation problem. We present Reorder-BART (Re-BART) that leverages a pre-trained Transformer-based model to identify a coherent order for a given set of shuffled sentences. The model takes a set of shuffled sentences with sentence-specific markers as input and generates a sequence of position markers of the sentences in the ordered text. Re-BART achieves the state-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and Kendall's tau. We perform evaluations in a zero-shot setting, showcasing that our model is able to generalize well across other datasets. We additionally perform several experiments to understand the functioning and limitations of our framework",
    "checked": true,
    "id": "e8267569b7a1af840a3e0f6b58590549104d95fe",
    "semantic_title": "is everything in order? a simple way to order sentences",
    "citation_count": 25,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Faeze Brahman",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.842": {
    "title": "VeeAlign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment",
    "volume": "main",
    "abstract": "Ontology Alignment is an important research problem applied to various fields such as data integration, data transfer, data preparation, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a concept which, in turn, is used to discover alignments. By doing this, not only is our approach able to exploit both syntactic and semantic information encoded in ontologies, it is also, by design, flexible and scalable to different domains with minimal effort. We evaluate our model on four different datasets from different domains and languages, and establish its superiority through these results as well as detailed ablation studies. The code and datasets used are available at https://github.com/Remorax/VeeAlign",
    "checked": true,
    "id": "ff2880301a8433083d274f6fccbff702c4f899ee",
    "semantic_title": "veealign: multifaceted context representation using dual attention for ontology alignment",
    "citation_count": 17,
    "authors": [
      "Vivek Iyer",
      "Arvind Agarwal",
      "Harshit Kumar"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.843": {
    "title": "Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization",
    "volume": "main",
    "abstract": "Modern semantic parsers suffer from two principal limitations. First, training requires expensive collection of utterance-program pairs. Second, semantic parsers fail to generalize at test time to new compositions/structures that have not been observed during training. Recent research has shown that automatic generation of synthetic utterance-program pairs can alleviate the first problem, but its potential for the second has thus far been under-explored. In this work, we investigate automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing. Given a small training set of annotated examples and an \"infinite\" pool of synthetic examples, we select a subset of synthetic examples that are structurally-diverse and use them to improve compositional generalization. We evaluate our approach on a new split of the schema2QA dataset, and show that it leads to dramatic improvements in compositional generalization as well as moderate improvements in the traditional i.i.d setup. Moreover, structurally-diverse sampling achieves these improvements with as few as 5K examples, compared to 1M examples when sampling uniformly at random – a 200x improvement in data efficiency",
    "checked": true,
    "id": "676fa805bd715591f99bb17e36d673a6a14e92fe",
    "semantic_title": "finding needles in a haystack: sampling structurally-diverse training sets from synthetic data for compositional generalization",
    "citation_count": 31,
    "authors": [
      "Inbar Oren",
      "Jonathan Herzig",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.844": {
    "title": "GeneSis: A Generative Approach to Substitutes in Context",
    "volume": "main",
    "abstract": "The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as language models. Furthermore, lexical substitution is usually evaluated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, substitutes. To assess these issues, we proposed GeneSis (Generating Substitutes in contexts), the first generative approach to lexical substitution. Thanks to a seq2seq model, we generate substitutes for a word according to the context it appears in, attaining state-of-the-art results on different benchmarks. Moreover, our approach allows silver data to be produced for further improving the performances of lexical substitution systems. Along with an extensive analysis of GeneSis results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets, and the code to reproduce the experiments at https://github.com/SapienzaNLP/genesis",
    "checked": true,
    "id": "31a619a914a6859e98fdb1b714851f7c229f8483",
    "semantic_title": "genesis: a generative approach to substitutes in context",
    "citation_count": 14,
    "authors": [
      "Caterina Lacerra",
      "Rocco Tripodi",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.845": {
    "title": "Semi-Supervised Exaggeration Detection of Health Science Press Releases",
    "volume": "main",
    "abstract": "Public trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of exaggeration detection in science communication. While there are an abundance of scientific papers and popular media articles written about them, very rarely do the articles include a direct link to the original paper, making data collection challenging, and necessitating the need for few-shot learning. We address this by curating a set of labeled press release/abstract pairs from existing expert annotated studies on exaggeration in press releases of scientific papers suitable for benchmarking the performance of machine learning models on the task. Using limited data from this and previous studies on exaggeration detection in science, we introduce MT-PET, a multi-task version of Pattern Exploiting Training (PET), which leverages knowledge from complementary cloze-style QA tasks to improve few-shot learning. We demonstrate that MT-PET outperforms PET and supervised learning both when data is limited, as well as when there is an abundance of data for the main task",
    "checked": true,
    "id": "0e96b7f20eff4144851869fed0b7924db91e3010",
    "semantic_title": "semi-supervised exaggeration detection of health science press releases",
    "citation_count": 13,
    "authors": [
      "Dustin Wright",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.846": {
    "title": "Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration",
    "volume": "main",
    "abstract": "Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT",
    "checked": true,
    "id": "46ed42e4318e1363a0ec3dde195422cdfecf2017",
    "semantic_title": "phrase-bert: improved phrase embeddings from bert with an application to corpus exploration",
    "citation_count": 67,
    "authors": [
      "Shufan Wang",
      "Laure Thompson",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.emnlp-main.847": {
    "title": "Detecting Contact-Induced Semantic Shifts: What Can Embedding-Based Methods Do in Practice?",
    "volume": "main",
    "abstract": "This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in Quebec English. We contrast synchronic data from different regions in order to identify the meanings that are specific to Quebec and potentially related to language contact. Type-level embeddings are used to detect new semantic shifts, and token-level embeddings to isolate regionally specific occurrences. We introduce a new 80-item test set and conduct both quantitative and qualitative evaluations. We demonstrate that diachronic word embedding methods can be applied to contact-induced semantic shifts observed in synchrony, obtaining results comparable to the state of the art on similar tasks in diachrony. However, we show that encouraging evaluation results do not translate to practical value in detecting new semantic shifts. Finally, our application of token-level embeddings accelerates manual data exploration and provides an efficient way of scaling up sociolinguistic analyses",
    "checked": true,
    "id": "d5d381aa5eac463d1089a4ff06e27faccc8d00c1",
    "semantic_title": "detecting contact-induced semantic shifts: what can embedding-based methods do in practice?",
    "citation_count": 4,
    "authors": [
      "Filip Miletic",
      "Anne Przewozny-Desriaux",
      "Ludovic Tanguy"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.1": {
    "title": "K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce",
    "volume": "findings",
    "abstract": "Existing pre-trained language models (PLMs) have demonstrated the effectiveness of self-supervised learning for a broad range of natural language processing (NLP) tasks. However, most of them are not explicitly aware of domain-specific knowledge, which is essential for downstream tasks in many domains, such as tasks in e-commerce scenarios. In this paper, we propose K-PLUG, a knowledge-injected pre-trained language model based on the encoder-decoder transformer that can be transferred to both natural language understanding and generation tasks. Specifically, we propose five knowledge-aware self-supervised pre-training objectives to formulate the learning of domain-specific knowledge, including e-commerce domain-specific knowledge-bases, aspects of product entities, categories of product entities, and unique selling propositions of product entities. We verify our method in a diverse range of e-commerce scenarios that require domain-specific knowledge, including product knowledge base completion, abstractive product summarization, and multi-turn dialogue. K-PLUG significantly outperforms baselines across the board, which demonstrates that the proposed method effectively learns a diverse set of domain-specific knowledge for both language understanding and generation tasks. Our code is available",
    "checked": true,
    "id": "49a77a36a0a60d29aa7838ac49a055a69658b195",
    "semantic_title": "k-plug: knowledge-injected pre-trained language model for natural language understanding and generation in e-commerce",
    "citation_count": 24,
    "authors": [
      "Song Xu",
      "Haoran Li",
      "Peng Yuan",
      "Yujia Wang",
      "Youzheng Wu",
      "Xiaodong He",
      "Ying Liu",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.2": {
    "title": "Extracting Topics with Simultaneous Word Co-occurrence and Semantic Correlation Graphs: Neural Topic Modeling for Short Texts",
    "volume": "findings",
    "abstract": "Short text nowadays has become a more fashionable form of text data, e.g., Twitter posts, news titles, and product reviews. Extracting semantic topics from short texts plays a significant role in a wide spectrum of NLP applications, and neural topic modeling is now a major tool to achieve it. Motivated by learning more coherent and semantic topics, in this paper we develop a novel neural topic model named Dual Word Graph Topic Model (DWGTM), which extracts topics from simultaneous word co-occurrence and semantic correlation graphs. To be specific, we learn word features from the global word co-occurrence graph, so as to ingest rich word co-occurrence information; we then generate text features with word features, and feed them into an encoder network to get topic proportions per-text; finally, we reconstruct texts and word co-occurrence graph with topical distributions and word features, respectively. Besides, to capture semantics of words, we also apply word features to reconstruct a word semantic correlation graph computed by pre-trained word embeddings. Upon those ideas, we formulate DWGTM in an auto-encoding paradigm and efficiently train it with the spirit of neural variational inference. Empirical results validate that DWGTM can generate more semantically coherent topics than baseline topic models",
    "checked": true,
    "id": "668531be7bcd10219fab456bcc9ee282c942ccaf",
    "semantic_title": "extracting topics with simultaneous word co-occurrence and semantic correlation graphs: neural topic modeling for short texts",
    "citation_count": 16,
    "authors": [
      "Yiming Wang",
      "Ximing Li",
      "Xiaotang Zhou",
      "Jihong Ouyang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.3": {
    "title": "Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering",
    "volume": "findings",
    "abstract": "Spoken question answering (SQA) requires fine-grained understanding of both spoken documents and questions for the optimal answer prediction. In this paper, we propose novel training schemes for spoken question answering with a self-supervised training stage and a contrastive representation learning stage. In the self-supervised stage, we propose three auxiliary self-supervised tasks, including utterance restoration, utterance insertion, and question discrimination, and jointly train the model to capture consistency and coherence among speech documents without any additional data or annotations. We then propose to learn noise-invariant utterance representations in a contrastive objective by adopting multiple augmentation strategies, including span deletion and span substitution. Besides, we design a Temporal-Alignment attention to semantically align the speech-text clues in the learned common space and benefit the SQA tasks. By this means, the training schemes can more effectively guide the generation model to predict more proper answers. Experimental results show that our model achieves state-of-the-art results on three SQA benchmarks. Our code will be publicly available after publication",
    "checked": true,
    "id": "7790fe39f795ad2979e83289148bd69aba72290a",
    "semantic_title": "self-supervised contrastive cross-modality representation learning for spoken question answering",
    "citation_count": 63,
    "authors": [
      "Chenyu You",
      "Nuo Chen",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.4": {
    "title": "Language Clustering for Multilingual Named Entity Recognition",
    "volume": "findings",
    "abstract": "Recent work in multilingual natural language processing has shown progress in various tasks such as natural language inference and joint multilingual translation. Despite success in learning across many languages, challenges arise where multilingual training regimes often boost performance on some languages at the expense of others. For multilingual named entity recognition (NER) we propose a simple technique that groups similar languages together by using embeddings from a pre-trained masked language model, and automatically discovering language clusters in this embedding space. Specifically, we fine-tune an XLM-Roberta model on a language identification task, and use embeddings from this model for clustering. We conduct experiments on 15 diverse languages in the WikiAnn dataset and show our technique largely outperforms three baselines: (1) training a multilingual model jointly on all available languages, (2) training one monolingual model per language, and (3) grouping languages by linguistic family. We also conduct analyses showing meaningful multilingual transfer for low-resource languages (Swahili and Yoruba), despite being automatically grouped with other seemingly disparate languages",
    "checked": true,
    "id": "0f86e85f2891b5e2dc17a432879b013c925dfc99",
    "semantic_title": "language clustering for multilingual named entity recognition",
    "citation_count": 11,
    "authors": [
      "Kyle Shaffer"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.5": {
    "title": "Neural News Recommendation with Collaborative News Encoding and Structural User Encoding",
    "volume": "findings",
    "abstract": "Automatic news recommendation has gained much attention from the academic community and industry. Recent studies reveal that the key to this task lies within the effective representation learning of both news and users. Existing works typically encode news title and content separately while neglecting their semantic interaction, which is inadequate for news text comprehension. Besides, previous models encode user browsing history without leveraging the structural correlation of user browsed news to reflect user interests explicitly. In this work, we propose a news recommendation framework consisting of collaborative news encoding (CNE) and structural user encoding (SUE) to enhance news and user representation learning. CNE equipped with bidirectional LSTMs encodes news title and content collaboratively with cross-selection and cross-attention modules to learn semantic-interactive news representations. SUE utilizes graph convolutional networks to extract cluster-structural features of user history, followed by intra-cluster and inter-cluster attention modules to learn hierarchical user interest representations. Experiment results on the MIND dataset validate the effectiveness of our model to improve the performance of news recommendation",
    "checked": true,
    "id": "dc9b49b89d9fe0e8163f0dac9819316a3bfec78a",
    "semantic_title": "neural news recommendation with collaborative news encoding and structural user encoding",
    "citation_count": 21,
    "authors": [
      "Zhiming Mao",
      "Xingshan Zeng",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.6": {
    "title": "Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question-Answering Data",
    "volume": "findings",
    "abstract": "Despite considerable progress, most machine reading comprehension (MRC) tasks still lack sufficient training data to fully exploit powerful deep neural network models with millions of parameters, and it is laborious, expensive, and time-consuming to create large-scale, high-quality MRC data through crowdsourcing. This paper focuses on generating more training data for MRC tasks by leveraging existing question-answering (QA) data. We first collect a large-scale multi-subject multiple-choice QA dataset for Chinese, ExamQA. We next use incomplete, yet relevant snippets returned by a web search engine as the context for each QA instance to convert it into a weakly-labeled MRC instance. To better use the weakly-labeled data to improve a target MRC task, we evaluate and compare several methods and further propose a self-teaching paradigm. Experimental results show that, upon state-of-the-art MRC baselines, we can obtain +5.1% in accuracy on a multiple-choice Chinese MRC dataset, Cˆ3, and +3.8% in exact match on an extractive Chinese MRC dataset, CMRC 2018, demonstrating the usefulness of the generated QA-based weakly-labeled data for different types of MRC tasks as well as the effectiveness of self-teaching. ExamQA will be available at https://dataset.org/examqa/",
    "checked": true,
    "id": "964da4ed9ac61b12bc2a7adc1e94e3964cc63861",
    "semantic_title": "self-teaching machines to read and comprehend with large-scale multi-subject question answering data",
    "citation_count": 7,
    "authors": [
      "Dian Yu",
      "Kai Sun",
      "Dong Yu",
      "Claire Cardie"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.7": {
    "title": "A Web Scale Entity Extraction System",
    "volume": "findings",
    "abstract": "Understanding the semantic meaning of content on the web through the lens of entities and concepts has many practical advantages. However, when building large-scale entity extraction systems, practitioners are facing unique challenges involving finding the best ways to leverage the scale and variety of data available on internet platforms. We present learnings from our efforts in building an entity extraction system for multiple document types at large scale using multi-modal Transformers. We empirically demonstrate the effectiveness of multi-lingual, multi-task and cross-document type learning. We also discuss the label collection schemes that help to minimize the amount of noise in the collected data",
    "checked": true,
    "id": "a86a5a68047e115aa3808aedc17acf18f4795ad7",
    "semantic_title": "a web scale entity extraction system",
    "citation_count": 0,
    "authors": [
      "Xuanting Cai",
      "Quanbin Ma",
      "Jianyu Liu",
      "Pan Li",
      "Qi Zeng",
      "Zhengkan Yang",
      "Pushkar Tripathi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.8": {
    "title": "Joint Multimedia Event Extraction from Video and Article",
    "volume": "findings",
    "abstract": "Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not handle video or solely target video while ignoring other modalities. In contrast, we propose the first approach to jointly extract events from both video and text articles. We introduce the new task of Video MultiMedia Event Extraction and propose two novel components to build the first system towards this task. First, we propose the first self-supervised cross-modal event coreference model that can determine coreference between video events and text events without any manually annotated pairs. Second, we introduce the first cross-modal transformer architecture, which extracts structured event information from both videos and text documents. We also construct and will publicly release a new benchmark of video-article pairs, consisting of 860 video-article pairs with extensive annotations for evaluating methods on this task. Our experimental results demonstrate the effectiveness of our proposed method on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score gain on multimodal event coreference resolution and multimedia event extraction",
    "checked": true,
    "id": "66c5a67baf4f1c3302dcff05febfc19c16e7f457",
    "semantic_title": "joint multimedia event extraction from video and article",
    "citation_count": 20,
    "authors": [
      "Brian Chen",
      "Xudong Lin",
      "Christopher Thomas",
      "Manling Li",
      "Shoya Yoshida",
      "Lovish Chum",
      "Heng Ji",
      "Shih-Fu Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.9": {
    "title": "Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding",
    "volume": "findings",
    "abstract": "Temporal language grounding (TLG) aims to localize a video segment in an untrimmed video based on a natural language description. To alleviate the expensive cost of manual annotations for temporal boundary labels,we are dedicated to the weakly supervised setting, where only video-level descriptions are provided for training. Most of the existing weakly supervised methods generate a candidate segment set and learn cross-modal alignment through a MIL-based framework. However, the temporal structure of the video as well as the complicated semantics in the sentence are lost during the learning. In this work, we propose a novel candidate-free framework: Fine-grained Semantic Alignment Network (FSAN), for weakly supervised TLG. Instead of view the sentence and candidate moments as a whole, FSAN learns token-by-clip cross-modal semantic alignment by an iterative cross-modal interaction module, generates a fine-grained cross-modal semantic alignment map, and performs grounding directly on top of the map. Extensive experiments are conducted on two widely-used benchmarks: ActivityNet-Captions, and DiDeMo, where our FSAN achieves state-of-the-art performance",
    "checked": false,
    "id": "37937c6bfe0bf4a02bf3f8ef248017da2293b2dd",
    "semantic_title": "weakly supervised temporal adjacent network for language grounding",
    "citation_count": 67,
    "authors": [
      "Yuechen Wang",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.10": {
    "title": "Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation",
    "volume": "findings",
    "abstract": "Despite significant progress has been achieved in text summarization, factual inconsistency in generated summaries still severely limits its practical applications. Among the key factors to ensure factual consistency, a reliable automatic evaluation metric is the first and the most crucial one. However, existing metrics either neglect the intrinsic cause of the factual inconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation with human judgments or increasing the inconvenience of usage in practice. In light of these challenges, we propose a novel metric to evaluate the factual consistency in text summarization via counterfactual estimation, which formulates the causal relationship among the source document, the generated summary, and the language prior. We remove the effect of language prior, which can cause factual inconsistency, from the total causal effect on the generated summary, and provides a simple yet effective way to evaluate consistency without relying on other auxiliary tasks. We conduct a series of experiments on three public abstractive text summarization datasets, and demonstrate the advantages of the proposed metric in both improving the correlation with human judgments and the convenience of usage. The source code is available at https://github.com/xieyxclack/factual_coco",
    "checked": true,
    "id": "76e37d6e05059f97b3b99db6877f5879810ce817",
    "semantic_title": "factual consistency evaluation for text summarization via counterfactual estimation",
    "citation_count": 54,
    "authors": [
      "Yuexiang Xie",
      "Fei Sun",
      "Yang Deng",
      "Yaliang Li",
      "Bolin Ding"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.11": {
    "title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification",
    "volume": "findings",
    "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves substantial improvement in performance on image-caption retrieval w.r.t. similar methods. Second, we show that retrieval-augmented multi-modal transformers using the trained alignment model improve results on VQA over strong baselines. We further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices",
    "checked": true,
    "id": "b3c914c8fc3eb3620ed1406289ae4f1ca2c617b0",
    "semantic_title": "cross-modal retrieval augmentation for multi-modal classification",
    "citation_count": 28,
    "authors": [
      "Shir Gur",
      "Natalia Neverova",
      "Chris Stauffer",
      "Ser-Nam Lim",
      "Douwe Kiela",
      "Austin Reiter"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.12": {
    "title": "HiTRANS: A Hierarchical Transformer Network for Nested Named Entity Recognition",
    "volume": "findings",
    "abstract": "Nested Named Entity Recognition (NNER) has been extensively studied, aiming to identify all nested entities from potential spans (i.e., one or more continuous tokens). However, recent studies for NNER either focus on tedious tagging schemas or utilize complex structures, which fail to learn effective span representations from the input sentence with highly nested entities. Intuitively, explicit span representations will contribute to NNER due to the rich context information they contain. In this study, we propose a Hierarchical Transformer (HiTRANS) network for the NNER task, which decomposes the input sentence into multi-grained spans and enhances the representation learning in a hierarchical manner. Specifically, we first utilize a two-phase module to generate span representations by aggregating context information based on a bottom-up and top-down transformer network. Then a label prediction layer is designed to recognize nested entities hierarchically, which naturally explores semantic dependencies among different spans. Experiments on GENIA, ACE-2004, ACE-2005 and NNE datasets demonstrate that our proposed method achieves much better performance than the state-of-the-art approaches",
    "checked": true,
    "id": "81bcb704d1990682e2a31542f1ce2d588bcba05a",
    "semantic_title": "hitrans: a hierarchical transformer network for nested named entity recognition",
    "citation_count": 8,
    "authors": [
      "Zhiwei Yang",
      "Jing Ma",
      "Hechang Chen",
      "Yunke Zhang",
      "Yi Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.13": {
    "title": "Improving Embedding-based Large-scale Retrieval via Label Enhancement",
    "volume": "findings",
    "abstract": "Current embedding-based large-scale retrieval models are trained with 0-1 hard label that indicates whether a query is relevant to a document, ignoring rich information of the relevance degree. This paper proposes to improve embedding-based retrieval from the perspective of better characterizing the query-document relevance degree by introducing label enhancement (LE) for the first time. To generate label distribution in the retrieval scenario, we design a novel and effective supervised LE method that incorporates prior knowledge from dynamic term weighting methods into contextual embeddings. Our method significantly outperforms four competitive existing retrieval models and its counterparts equipped with two alternative LE techniques by training models with the generated label distribution as auxiliary supervision information. The superiority can be easily observed on English and Chinese large-scale retrieval tasks under both standard and cold-start settings",
    "checked": true,
    "id": "ef59decf7809ec601f40b9c821e32c9324458c1f",
    "semantic_title": "improving embedding-based large-scale retrieval via label enhancement",
    "citation_count": 4,
    "authors": [
      "Peiyang Liu",
      "Xi Wang",
      "Sen Wang",
      "Wei Ye",
      "Xiangyu Xi",
      "Shikun Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.14": {
    "title": "Improving Privacy Guarantee and Efficiency of Latent Dirichlet Allocation Model Training Under Differential Privacy",
    "volume": "findings",
    "abstract": "Latent Dirichlet allocation (LDA), a widely used topic model, is often employed as a fundamental tool for text analysis in various applications. However, the training process of the LDA model typically requires massive text corpus data. On one hand, such massive data may expose private information in the training data, thereby incurring significant privacy concerns. On the other hand, the efficiency of the LDA model training may be impacted, since LDA training often needs to handle these massive text corpus data. To address the privacy issues in LDA model training, some recent works have combined LDA training algorithms that are based on collapsed Gibbs sampling (CGS) with differential privacy. Nevertheless, these works usually have a high accumulative privacy budget due to vast iterations in CGS. Moreover, these works always have low efficiency due to handling massive text corpus data. To improve the privacy guarantee and efficiency, we combine a subsampling method with CGS and propose a novel LDA training algorithm with differential privacy, SUB-LDA. We find that subsampling in CGS naturally improves efficiency while amplifying privacy. We propose a novel metric, the efficiency–privacy function, to evaluate improvements of the privacy guarantee and efficiency. Based on a conventional subsampling method, we propose an adaptive subsampling method to improve the model's utility produced by SUB-LDA when the subsampling ratio is small. We provide a comprehensive analysis of SUB-LDA, and the experiment results validate its efficiency and privacy guarantee improvements",
    "checked": true,
    "id": "69cd55b9185c0073468394685d6c8b70302b3068",
    "semantic_title": "improving privacy guarantee and efficiency of latent dirichlet allocation model training under differential privacy",
    "citation_count": 3,
    "authors": [
      "Tao Huang",
      "Hong Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.15": {
    "title": "Generating Mammography Reports from Multi-view Mammograms with BERT",
    "volume": "findings",
    "abstract": "Writing mammography reports can be error-prone and time-consuming for radiologists. In this paper we propose a method to generate mammography reports given four images, corresponding to the four views used in screening mammography. To the best of our knowledge our work represents the first attempt to generate the mammography report using deep-learning. We propose an encoder-decoder model that includes an EfficientNet-based encoder and a Transformer-based decoder. We demonstrate that the Transformer-based attention mechanism can combine visual and semantic information to localize salient regions on the input mammograms and generate a visually interpretable report. The conducted experiments, including an evaluation by a certified radiologist, show the effectiveness of the proposed method",
    "checked": true,
    "id": "acbc78a40b1ca3fae23c23d11347219a45b3a4f3",
    "semantic_title": "generating mammography reports from multi-view mammograms with bert",
    "citation_count": 0,
    "authors": [
      "Alexander Yalunin",
      "Elena Sokolova",
      "Ilya Burenko",
      "Alexander Ponomarchuk",
      "Olga Puchkova",
      "Dmitriy Umerenkov"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.16": {
    "title": "Euphemistic Phrase Detection by Masked Language Model",
    "volume": "findings",
    "abstract": "It is a well-known approach for fringe groups and organizations to use euphemisms—ordinary-sounding and innocent-looking words with a secret meaning—to conceal what they are discussing. For instance, drug dealers often use \"pot\" for marijuana and \"avocado\" for heroin. From a social media content moderation perspective, though recent advances in NLP have enabled the automatic detection of such single-word euphemisms, no existing work is capable of automatically detecting multi-word euphemisms, such as \"blue dream\" (marijuana) and \"black tar\" (heroin). Our paper tackles the problem of euphemistic phrase detection without human effort for the first time, as far as we are aware. We first perform phrase mining on a raw text corpus (e.g., social media posts) to extract quality phrases. Then, we utilize word embedding similarities to select a set of euphemistic phrase candidates. Finally, we rank those candidates by a masked language model—SpanBERT. Compared to strong baselines, we report 20-50% higher detection accuracies using our algorithm for detecting euphemistic phrases",
    "checked": true,
    "id": "8bcb8dd3fadb35320fad382abda725e49454be6f",
    "semantic_title": "euphemistic phrase detection by masked language model",
    "citation_count": 26,
    "authors": [
      "Wanzheng Zhu",
      "Suma Bhat"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.17": {
    "title": "Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable",
    "volume": "findings",
    "abstract": "Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine's reasoning process. We propose Relation Extractor-Reader and Comparator (RERC), a three-stage framework based on complex question decomposition. The Relation Extractor decomposes the complex question, and then the Reader answers the sub-questions in turn, and finally the Comparator performs numerical comparison and summarizes all to get the final answer, where the entire process itself constitutes a complete reasoning evidence path. In the 2WikiMultiHopQA dataset, our RERC model has achieved the state-of-the-art performance, with a winning joint F1 score of 53.58 on the leaderboard. All indicators of our RERC are close to human performance, with only 1.95 behind the human level in F1 score of support fact. At the same time, the evidence path provided by our RERC framework has excellent readability and faithfulness",
    "checked": true,
    "id": "3edb127305862504a8f3b3f6b6ed4cd951062cc3",
    "semantic_title": "decomposing complex questions makes multi-hop qa easier and more interpretable",
    "citation_count": 32,
    "authors": [
      "Ruiliu Fu",
      "Han Wang",
      "Xuejun Zhang",
      "Jun Zhou",
      "Yonghong Yan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.18": {
    "title": "Segmenting Natural Language Sentences via Lexical Unit Analysis",
    "volume": "findings",
    "abstract": "The span-based model enjoys great popularity in recent works of sequence segmentation. However, each of these methods suffers from its own defects, such as invalid predictions. In this work, we introduce a unified span-based model, lexical unit analysis (LUA), that addresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we embed every span by using the representations from a pretraining language model. Secondly, we define a score for every segmentation candidate and apply dynamic programming (DP) to extract the candidate with the maximum score. We have conducted extensive experiments on 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-of-the-art performances on 6 of them. We have achieved even better results through incorporating label correlations",
    "checked": true,
    "id": "41caa95a3b4569851f970de93c391d1bcd2f279f",
    "semantic_title": "segmenting natural language sentences via lexical unit analysis",
    "citation_count": 5,
    "authors": [
      "Yangming Li",
      "Lemao Liu",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.19": {
    "title": "Dense Hierarchical Retrieval for Open-domain Question Answering",
    "volume": "findings",
    "abstract": "Dense neural text retrieval has achieved promising results on open-domain Question Answering (QA), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short passages that usually contain local, partial and sometimes biased context, and highly depend on the splitting process. As a consequence, it may yield inaccurate and misleading hidden representations, thus deteriorating the final retrieval result. In this work, we propose Dense Hierarchical Retrieval (DHR), a hierarchical framework which can generate accurate dense representations of passages by utilizing both macroscopic semantics in the document and microscopic semantics specific to each passage. Specifically, a document-level retriever first identifies relevant documents, among which relevant passages are then retrieved by a passage-level retriever. The ranking of the retrieved passages will be further calibrated by examining the document-level relevance. In addition, hierarchical title structure and two negative sampling strategies (i.e., In-Doc and In-Sec negatives) are investigated. We apply DHR to large-scale open-domain QA datasets. DHR significantly outperforms the original dense passage retriever, and helps an end-to-end QA system outperform the strong baselines on multiple open-domain QA benchmarks",
    "checked": true,
    "id": "a729503f528d5f0be0f897aa1841e1ff8ffcb313",
    "semantic_title": "dense hierarchical retrieval for open-domain question answering",
    "citation_count": 35,
    "authors": [
      "Ye Liu",
      "Kazuma Hashimoto",
      "Yingbo Zhou",
      "Semih Yavuz",
      "Caiming Xiong",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.20": {
    "title": "Visually Grounded Concept Composition",
    "volume": "findings",
    "abstract": "We investigate ways to compose complex concepts in texts from primitive ones while grounding them in images. We propose Concept and Relation Graph (CRG), which builds on top of constituency analysis and consists of recursively combined concepts with predicate functions. Meanwhile, we propose a concept composition neural network called Composer to leverage the CRG for visually grounded concept learning. Specifically, we learn the grounding of both primitive and all composed concepts by aligning them to images and show that learning to compose leads to more robust grounding results, measured in text-to-image matching accuracy. Notably, our model can model grounded concepts forming at both the finer-grained sentence level and the coarser-grained intermediate level (or word-level). Composer leads to pronounced improvement in matching accuracy when the evaluation data has significant compound divergence from the training data",
    "checked": true,
    "id": "cdb2a71f42c88be6d5c5e4fecb8288a9a315f094",
    "semantic_title": "visually grounded concept composition",
    "citation_count": 5,
    "authors": [
      "Bowen Zhang",
      "Hexiang Hu",
      "Linlu Qiu",
      "Peter Shaw",
      "Fei Sha"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.21": {
    "title": "Compositional Networks Enable Systematic Generalization for Grounded Language Understanding",
    "volume": "findings",
    "abstract": "Humans are remarkably flexible when understanding new sentences that include combinations of concepts they have never encountered before. Recent work has shown that while deep networks can mimic some human language abilities when presented with novel sentences, systematic variation uncovers the limitations in the language-understanding abilities of networks. We demonstrate that these limitations can be overcome by addressing the generalization challenges in the gSCAN dataset, which explicitly measures how well an agent is able to interpret novel linguistic commands grounded in vision, e.g., novel pairings of adjectives and nouns. The key principle we employ is compositionality: that the compositional structure of networks should reflect the compositional structure of the problem domain they address, while allowing other parameters to be learned end-to-end. We build a general-purpose mechanism that enables agents to generalize their language understanding to compositional domains. Crucially, our network has the same state-of-the-art performance as prior work while generalizing its knowledge when prior work does not. Our network also provides a level of interpretability that enables users to inspect what each part of networks learns. Robust grounded language understanding without dramatic failures and without corner cases is critical to building safe and fair robots; we demonstrate the significant role that compositionality can play in achieving that goal",
    "checked": true,
    "id": "3268a9371aad1bc8ba1458338ab184ef000a9525",
    "semantic_title": "compositional networks enable systematic generalization for grounded language understanding",
    "citation_count": 22,
    "authors": [
      "Yen-Ling Kuo",
      "Boris Katz",
      "Andrei Barbu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.22": {
    "title": "An Unsupervised Method for Building Sentence Simplification Corpora in Multiple Languages",
    "volume": "findings",
    "abstract": "The availability of parallel sentence simplification (SS) is scarce for neural SS modelings. We propose an unsupervised method to build SS corpora from large-scale bilingual translation corpora, alleviating the need for SS supervised corpora. Our method is motivated by the following two findings: neural machine translation model usually tends to generate more high-frequency tokens and the difference of text complexity levels exists between the source and target language of a translation corpus. By taking the pair of the source sentences of translation corpus and the translations of their references in a bridge language, we can construct large-scale pseudo parallel SS data. Then, we keep these sentence pairs with a higher complexity difference as SS sentence pairs. The building SS corpora with an unsupervised approach can satisfy the expectations that the aligned sentences preserve the same meanings and have difference in text complexity levels. Experimental results show that SS methods trained by our corpora achieve the state-of-the-art results and significantly outperform the results on English benchmark WikiLarge",
    "checked": true,
    "id": "a8a62b318756279a3b31fabe54bf8e18e0d5241a",
    "semantic_title": "an unsupervised method for building sentence simplification corpora in multiple languages",
    "citation_count": 19,
    "authors": [
      "Xinyu Lu",
      "Jipeng Qiang",
      "Yun Li",
      "Yunhao Yuan",
      "Yi Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.23": {
    "title": "WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach",
    "volume": "findings",
    "abstract": "Producing the embedding of a sentence in anunsupervised way is valuable to natural language matching and retrieval problems in practice. In this work, we conduct a thorough examination of pretrained model based unsupervised sentence embeddings. We study on fourpretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have three main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both topand bottom layers is better than only using toplayers. Lastly, an easy whitening-based vector normalization strategy with less than 10 linesof code consistently boosts the performance. The whole project including codes and data is publicly available at https://github.com/Jun-jie-Huang/WhiteningBERT",
    "checked": true,
    "id": "d185d5a0918e9bb25412bd85398f8e28033df2a6",
    "semantic_title": "whiteningbert: an easy unsupervised sentence embedding approach",
    "citation_count": 102,
    "authors": [
      "Junjie Huang",
      "Duyu Tang",
      "Wanjun Zhong",
      "Shuai Lu",
      "Linjun Shou",
      "Ming Gong",
      "Daxin Jiang",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.24": {
    "title": "TWEETSUMM - A Dialog Summarization Dataset for Customer Service",
    "volume": "findings",
    "abstract": "In a typical customer service chat scenario, customers contact a support center to ask for help or raise complaints, and human agents try to solve the issues. In most cases, at the end of the conversation, agents are asked to write a short summary emphasizing the problem and the proposed solution, usually for the benefit of other agents that may have to deal with the same customer or issue. The goal of the present article is advancing the automation of this task. We introduce the first large scale, high quality, customer care dialog summarization dataset with close to 6500 human annotated summaries. The data is based on real-world customer support dialogs and includes both extractive and abstractive summaries. We also introduce a new unsupervised, extractive summarization method specific to dialogs",
    "checked": true,
    "id": "cc84cf176321b2008e36ca628fa8c97a813f0118",
    "semantic_title": "tweetsumm - a dialog summarization dataset for customer service",
    "citation_count": 42,
    "authors": [
      "Guy Feigenblat",
      "Chulaka Gunasekara",
      "Benjamin Sznajder",
      "Sachindra Joshi",
      "David Konopnicki",
      "Ranit Aharonov"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.25": {
    "title": "Discourse-Based Sentence Splitting",
    "volume": "findings",
    "abstract": "Sentence splitting involves the segmentation of a sentence into two or more shorter sentences. It is a key component of sentence simplification, has been shown to help human comprehension and is a useful preprocessing step for NLP tasks such as summarisation and relation extraction. While several methods and datasets have been proposed for developing sentence splitting models, little attention has been paid to how sentence splitting interacts with discourse structure. In this work, we focus on cases where the input text contains a discourse connective, which we refer to as discourse-based sentence splitting. We create synthetic and organic datasets for discourse-based splitting and explore different ways of combining these datasets using different model architectures. We show that pipeline models which use discourse structure to mediate sentence splitting outperform end-to-end models in learning the various ways of expressing a discourse relation but generate text that is less grammatical; that large scale synthetic data provides a better basis for learning than smaller scale organic data; and that training on discourse-focused, rather than on general sentence splitting data provides a better basis for discourse splitting",
    "checked": true,
    "id": "5ae137ef20f457a8cb33db138ab9f554dddc794e",
    "semantic_title": "discourse-based sentence splitting",
    "citation_count": 6,
    "authors": [
      "Liam Cripwell",
      "Joël Legrand",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.26": {
    "title": "Multi-Task Dense Retrieval via Model Uncertainty Fusion for Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "Multi-task dense retrieval models can be used to retrieve documents from a common corpus (e.g., Wikipedia) for different open-domain question-answering (QA) tasks. However, Karpukhin et al. (2020) shows that jointly learning different QA tasks with one dense model is not always beneficial due to corpus inconsistency. For example, SQuAD only focuses on a small set of Wikipedia articles while datasets like NQ and Trivia cover more entries, and joint training on their union can cause performance degradation. To solve this problem, we propose to train individual dense passage retrievers (DPR) for different tasks and aggregate their predictions during test time, where we use uncertainty estimation as weights to indicate how probable a specific query belongs to each expert's expertise. Our method reaches state-of-the-art performance on 5 benchmark QA datasets, with up to 10% improvement in top-100 accuracy compared to a joint-training multi-task DPR on SQuAD. We also show that our method handles corpus inconsistency better than the joint-training DPR on a mixed subset of different QA datasets. Code and data are available at https://github.com/alexlimh/DPR_MUF",
    "checked": true,
    "id": "e47d338ca879a184c1977ffa8623d2a225b0a319",
    "semantic_title": "multi-task dense retrieval via model uncertainty fusion for open-domain question answering",
    "citation_count": 12,
    "authors": [
      "Minghan Li",
      "Ming Li",
      "Kun Xiong",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.27": {
    "title": "Mining the Cause of Political Decision-Making from Social Media: A Case Study of COVID-19 Policies across the US States",
    "volume": "findings",
    "abstract": "Mining the causes of political decision-making is an active research area in the field of political science. In the past, most studies have focused on long-term policies that are collected over several decades of time, and have primarily relied on surveys as the main source of predictors. However, the recent COVID-19 pandemic has given rise to a new political phenomenon, where political decision-making consists of frequent short-term decisions, all on the same controlled topic—the pandemic. In this paper, we focus on the question of how public opinion influences policy decisions, while controlling for confounders such as COVID-19 case increases or unemployment rates. Using a dataset consisting of Twitter data from the 50 US states, we classify the sentiments toward governors of each state, and conduct controlled studies and comparisons. Based on the compiled samples of sentiments, policies, and confounders, we conduct causal inference to discover trends in political decision-making across different states",
    "checked": true,
    "id": "0266eac00547e099ccc654907ab5e7b9335d4293",
    "semantic_title": "mining the cause of political decision-making from social media: a case study of covid-19 policies across the us states",
    "citation_count": 13,
    "authors": [
      "Zhijing Jin",
      "Zeyu Peng",
      "Tejas Vaidhya",
      "Bernhard Schoelkopf",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.28": {
    "title": "Self-Attention Graph Residual Convolutional Networks for Event Detection with dependency relations",
    "volume": "findings",
    "abstract": "Event detection (ED) task aims to classify events by identifying key event trigger words embedded in a piece of text. Previous research have proved the validity of fusing syntactic dependency relations into Graph Convolutional Networks(GCN). While existing GCN-based methods explore latent node-to-node dependency relations according to a stationary adjacency tensor, an attention-based dynamic tensor, which can pay much attention to the key node like event trigger or its neighboring nodes, has not been developed. Simultaneously, suffering from the phenomenon of graph information vanishing caused by the symmetric adjacency tensor, existing GCN models can not achieve higher overall performance. In this paper, we propose a novel model Self-Attention Graph Residual Convolution Networks (SA-GRCN) to mine node-to-node latent dependency relations via self-attention mechanism and introduce Graph Residual Network (GResNet) to solve graph information vanishing problem. Specifically, a self-attention module is constructed to generate an attention tensor, representing the dependency attention scores of all words in the sentence. Furthermore, a graph residual term is added to the baseline SA-GCN to construct a GResNet. Considering the syntactically connection of the network input, we initialize the raw adjacency tensor without processed by the self-attention module as the residual term. We conduct experiments on the ACE2005 dataset and the results show significant improvement over competitive baseline methods",
    "checked": true,
    "id": "2f62dacb10fe5c88e8f3f218b849dd3d6b691f6f",
    "semantic_title": "self-attention graph residual convolutional networks for event detection with dependency relations",
    "citation_count": 26,
    "authors": [
      "Anan Liu",
      "Ning Xu",
      "Haozhe Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.29": {
    "title": "Mixup Decoding for Diverse Machine Translation",
    "volume": "findings",
    "abstract": "Diverse machine translation aims at generating various target language translations for a given source language sentence. To leverage the linear relationship in the sentence latent space introduced by the mixup training, we propose a novel method, MixDiversity, to generate different translations for the input sentence by linearly interpolating it with different sentence pairs sampled from the training corpus during decoding. To further improve the faithfulness and diversity of the translations, we propose two simple but effective approaches to select diverse sentence pairs in the training corpus and adjust the interpolation weight for each pair correspondingly. Moreover, by controlling the interpolation weight, our method can achieve the trade-off between faithfulness and diversity without any additional training, which is required in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14 en-de, and WMT'17 zh-en are conducted to show that our method substantially outperforms all previous diverse machine translation methods",
    "checked": true,
    "id": "8953b9df5249b62f2705590912713d8f42e29ee4",
    "semantic_title": "mixup decoding for diverse machine translation",
    "citation_count": 14,
    "authors": [
      "Jicheng Li",
      "Pengzhi Gao",
      "Xuanfu Wu",
      "Yang Feng",
      "Zhongjun He",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.30": {
    "title": "An Alignment-Agnostic Model for Chinese Text Error Correction",
    "volume": "findings",
    "abstract": "This paper investigates how to correct Chinese text errors with types of mistaken, missing and redundant characters, which are common for Chinese native speakers. Most existing models based on detect-correct framework can correct mistaken characters, but cannot handle missing or redundant characters due to inconsistency between model inputs and outputs. Although Seq2Seq-based or sequence tagging methods provide solutions to the three error types and achieved relatively good results in English context, they do not perform well in Chinese context according to our experiments. In our work, we propose a novel alignment-agnostic detect-correct framework that can handle both text aligned and non-aligned situations and can serve as a cold start model when no annotation data are provided. Experimental results on three datasets demonstrate that our method is effective and achieves a better performance than most recent published models",
    "checked": true,
    "id": "8ffd2a2ec996bbec9db3743bd7cb3ad2a83bdb96",
    "semantic_title": "an alignment-agnostic model for chinese text error correction",
    "citation_count": 4,
    "authors": [
      "Liying Zheng",
      "Yue Deng",
      "Weishun Song",
      "Liang Xu",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.31": {
    "title": "Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer",
    "volume": "findings",
    "abstract": "Visual dialog is a task of answering a sequence of questions grounded in an image using the previous dialog history as context. In this paper, we study how to address two fundamental challenges for this task: (1) reasoning over underlying semantic structures among dialog rounds and (2) identifying several appropriate answers to the given question. To address these challenges, we propose a Sparse Graph Learning (SGL) method to formulate visual dialog as a graph structure learning task. SGL infers inherently sparse dialog structures by incorporating binary and score edges and leveraging a new structural loss function. Next, we introduce a Knowledge Transfer (KT) method that extracts the answer predictions from the teacher model and uses them as pseudo labels. We propose KT to remedy the shortcomings of single ground-truth labels, which severely limit the ability of a model to obtain multiple reasonable answers. As a result, our proposed model significantly improves reasoning capability compared to baseline methods and outperforms the state-of-the-art approaches on the VisDial v1.0 dataset. The source code is available at https://github.com/gicheonkang/SGLKT-VisDial",
    "checked": true,
    "id": "71d6f8b6d9ca397826ac4fe788333281702d6ca8",
    "semantic_title": "reasoning visual dialog with sparse graph learning and knowledge transfer",
    "citation_count": 10,
    "authors": [
      "Gi-Cheon Kang",
      "Junseok Park",
      "Hwaran Lee",
      "Byoung-Tak Zhang",
      "Jin-Hwa Kim"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.32": {
    "title": "Exploring Sentence Community for Document-Level Event Extraction",
    "volume": "findings",
    "abstract": "Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore, our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset",
    "checked": true,
    "id": "e68195d8a592c55444bb2304e5a8fc4d6485f18d",
    "semantic_title": "exploring sentence community for document-level event extraction",
    "citation_count": 35,
    "authors": [
      "Yusheng Huang",
      "Weijia Jia"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.33": {
    "title": "A Model of Cross-Lingual Knowledge-Grounded Response Generation for Open-Domain Dialogue Systems",
    "volume": "findings",
    "abstract": "Research on open-domain dialogue systems that allow free topics is challenging in the field of natural language processing (NLP). The performance of the dialogue system has been improved recently by the method utilizing dialogue-related knowledge; however, non-English dialogue systems suffer from reproducing the performance of English dialogue systems because securing knowledge in the same language with the dialogue system is relatively difficult. Through experiments with a Korean dialogue system, this paper proves that the performance of a non-English dialogue system can be improved by utilizing English knowledge, highlighting the system uses cross-lingual knowledge. For the experiments, we 1) constructed a Korean version of the Wizard of Wikipedia dataset, 2) built Korean-English T5 (KE-T5), a language model pre-trained with Korean and English corpus, and 3) developed a knowledge-grounded Korean dialogue model based on KE-T5. We observed the performance improvement in the open-domain Korean dialogue model even only English knowledge was given. The experimental results showed that the knowledge inherent in cross-lingual language models can be helpful for generating responses in open dialogue systems",
    "checked": true,
    "id": "3f4e9c2ed833c335d001254c4550140f59a324ab",
    "semantic_title": "a model of cross-lingual knowledge-grounded response generation for open-domain dialogue systems",
    "citation_count": 18,
    "authors": [
      "San Kim",
      "Jin Yea Jang",
      "Minyoung Jung",
      "Saim Shin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.34": {
    "title": "WHOSe Heritage: Classification of UNESCO World Heritage Statements of \"Outstanding Universal Value\" with Soft Labels",
    "volume": "findings",
    "abstract": "The UNESCO World Heritage List (WHL) includes the exceptionally valuable cultural and natural heritage to be preserved for mankind. Evaluating and justifying the Outstanding Universal Value (OUV) is essential for each site inscribed in the WHL, and yet a complex task, even for experts, since the selection criteria of OUV are not mutually exclusive. Furthermore, manual annotation of heritage values and attributes from multi-source textual data, which is currently dominant in heritage studies, is knowledge-demanding and time-consuming, impeding systematic analysis of such authoritative documents in terms of their implications on heritage management. This study applies state-of-the-art NLP models to build a classifier on a new dataset containing Statements of OUV, seeking an explainable and scalable automation tool to facilitate the nomination, evaluation, research, and monitoring processes of World Heritage sites. Label smoothing is innovatively adapted to improve the model performance by adding prior inter-class relationship knowledge to generate soft labels. The study shows that the best models fine-tuned from BERT and ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation on the model prediction shows that the models are sufficiently generalizable. The study is promising to be further developed and applied in heritage research and practice",
    "checked": true,
    "id": "fe331ec5f4db2adf5ebbb9ffd4391c8f501e546b",
    "semantic_title": "whose heritage: classification of unesco world heritage statements of \"outstanding universal value\" with soft labels",
    "citation_count": 6,
    "authors": [
      "Nan Bai",
      "Renqian Luo",
      "Pirouz Nourian",
      "Ana Pereira Roders"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.35": {
    "title": "P-INT: A Path-based Interaction Model for Few-shot Knowledge Graph Completion",
    "volume": "findings",
    "abstract": "Few-shot knowledge graph completion is to infer the unknown facts (i.e., query head-tail entity pairs) of a given relation with only a few observed reference entity pairs. Its general process is to first encode the implicit relation of an entity pair and then match the relation of a query entity pair with the relations of the reference entity pairs. Most existing methods have thus far encoded an entity pair and matched entity pairs by using the direct neighbors of concerned entities. In this paper, we propose the P-INT model for effective few-shot knowledge graph completion. First, P-INT infers and leverages the paths that can expressively encode the relation of two entities. Second, to capture the fine grained matches, P-INT calculates the interactions of paths instead of mix- ing them for each entity pair. Extensive experimental results demonstrate that P-INT out- performs the state-of-the-art baselines by 11.2– 14.2% in terms of Hits@1. Our codes and datasets are online now",
    "checked": true,
    "id": "ba4b55be0fc4f8fd7fa0787c9ab5e67edf40a5b4",
    "semantic_title": "p-int: a path-based interaction model for few-shot knowledge graph completion",
    "citation_count": 30,
    "authors": [
      "Jingwen Xu",
      "Jing Zhang",
      "Xirui Ke",
      "Yuxiao Dong",
      "Hong Chen",
      "Cuiping Li",
      "Yongbin Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.36": {
    "title": "Cartography Active Learning",
    "volume": "findings",
    "abstract": "We propose Cartography Active Learning (CAL), a novel Active Learning (AL) algorithm that exploits the behavior of the model on individual instances during training as a proxy to find the most informative instances for labeling. CAL is inspired by data maps, which were recently proposed to derive insights into dataset quality (Swayamdipta et al., 2020). We compare our method on popular text classification tasks to commonly used AL strategies, which instead rely on post-training behavior. We demonstrate that CAL is competitive to other common AL methods, showing that training dynamics derived from small seed data can be successfully used for AL. We provide insights into our new AL method by analyzing batch-level statistics utilizing the data maps. Our results further show that CAL results in a more data-efficient learning strategy, achieving comparable or better results with considerably less training data",
    "checked": true,
    "id": "0a881b7e252a2e9d39074e553350a7e3a1e79ca7",
    "semantic_title": "cartography active learning",
    "citation_count": 38,
    "authors": [
      "Mike Zhang",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.37": {
    "title": "Beyond Reptile: Meta-Learned Dot-Product Maximization between Gradients for Improved Single-Task Regularization",
    "volume": "findings",
    "abstract": "Meta-learning algorithms such as MAML, Reptile, and FOMAML have led to improved performance of several neural models. The primary difference between standard gradient descent and these meta-learning approaches is that they contain as a small component the gradient for maximizing dot-product between gradients of batches, leading to improved generalization. Previous work has shown that aligned gradients are related to generalization, and have also used the Reptile algorithm in a single-task setting to improve generalization. Inspired by these approaches for a single task setting, this paper proposes to use the finite differences first-order algorithm to calculate this gradient from dot-product of gradients, allowing explicit control on the weightage of this component relative to standard gradients. We use this gradient as a regularization technique, leading to more aligned gradients between different batches. By using the finite differences approximation, our approach does not suffer from O(nˆ2) memory usage of naively calculating the Hessian and can be easily applied to large models with large batch sizes. Our approach achieves state-of-the-art performance on the Gigaword dataset, and shows performance improvements on several datasets such as SQuAD-v2.0, Quasar-T, NewsQA and all the SuperGLUE datasets, with a range of models such as BERT, RoBERTa and ELECTRA. Our method also outperforms previous approaches of Reptile and FOMAML when used as a regularization technique, in both single and multi-task settings. Our method is model agnostic, and introduces no extra trainable weights",
    "checked": true,
    "id": "1f81257f8394db2e5a9d1679dc009a709a30d69b",
    "semantic_title": "beyond reptile: meta-learned dot-product maximization between gradients for improved single-task regularization",
    "citation_count": 14,
    "authors": [
      "Akhil Kedia",
      "Sai Chetan Chinthakindi",
      "Wonho Ryu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.38": {
    "title": "GooAQ: Open Question Answering with Diverse Answer Types",
    "volume": "findings",
    "abstract": "While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to our collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections. We benchmark T5 models on GooAQ and observe that: (a) in line with recent work, LM's strong performance on GooAQ's short-answer questions heavily benefit from annotated data; however, (b) their quality in generating coherent and accurate responses for questions requiring long responses (such as ‘how' and ‘why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. We release GooAQ to facilitate further research on improving QA with diverse response types",
    "checked": true,
    "id": "0a36cbfb05e60758d49aa10685460ed9afd96977",
    "semantic_title": "gooaq: open question answering with diverse answer types",
    "citation_count": 53,
    "authors": [
      "Daniel Khashabi",
      "Amos Ng",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Hannaneh Hajishirzi",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.39": {
    "title": "Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions",
    "volume": "findings",
    "abstract": "This work proposes an extensive analysis of the Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder attention mechanism, we prove that attention weights systematically make alignment errors by relying mainly on uninformative tokens from the source sequence. However, we observe that NMT models assign attention to these tokens to regulate the contribution in the prediction of the two contexts, the source and the prefix of the target sequence. We provide evidence about the influence of wrong alignments on the model behavior, demonstrating that the encoder-decoder attention mechanism is well suited as an interpretability method for NMT. Finally, based on our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights",
    "checked": true,
    "id": "4fbd8505973851271d6e4f72a2068a855e21b2df",
    "semantic_title": "attention weights in transformer nmt fail aligning words between sequences but largely explain model predictions",
    "citation_count": 13,
    "authors": [
      "Javier Ferrando",
      "Marta R. Costa-jussà"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.40": {
    "title": "BFClass: A Backdoor-free Text Classification Framework",
    "volume": "findings",
    "abstract": "Backdoor attack introduces artificial vulnerabilities into the model by poisoning a subset of the training data via injecting triggers and modifying labels. Various trigger design strategies have been explored to attack text classifiers, however, defending such attacks remains an open problem. In this work, we propose BFClass, a novel efficient backdoor-free training framework for text classification. The backbone of BFClass is a pre-trained discriminator that predicts whether each token in the corrupted input was replaced by a masked language model. To identify triggers, we utilize this discriminator to locate the most suspicious token from each training sample and then distill a concise set by considering their association strengths with particular labels. To recognize the poisoned subset, we examine the training samples with these identified triggers as the most suspicious token, and check if removing the trigger will change the poisoned model's prediction. Extensive experiments demonstrate that BFClass can identify all the triggers, remove 95% poisoned training samples with very limited false alarms, and achieve almost the same performance as the models trained on the benign training data",
    "checked": true,
    "id": "eaf2a661ec6e1ef4fe97f45c9e1da5e04613dc34",
    "semantic_title": "bfclass: a backdoor-free text classification framework",
    "citation_count": 28,
    "authors": [
      "Zichao Li",
      "Dheeraj Mekala",
      "Chengyu Dong",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.41": {
    "title": "Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models",
    "volume": "findings",
    "abstract": "As it has been unveiled that pre-trained language models (PLMs) are to some extent capable of recognizing syntactic concepts in natural language, much effort has been made to develop a method for extracting complete (binary) parses from PLMs without training separate parsers. We improve upon this paradigm by proposing a novel chart-based method and an effective top-K ensemble technique. Moreover, we demonstrate that we can broaden the scope of application of the approach into multilingual settings. Specifically, we show that by applying our method on multilingual PLMs, it becomes possible to induce non-trivial parses for sentences from nine languages in an integrated and language-agnostic manner, attaining performance superior or comparable to that of unsupervised PCFGs. We also verify that our approach is robust to cross-lingual transfer. Finally, we provide analyses on the inner workings of our method. For instance, we discover universal attention heads which are consistently sensitive to syntactic information irrespective of the input language",
    "checked": true,
    "id": "ec1ce50b7a65c6ac6cd04e57b7e5521e209b9d8b",
    "semantic_title": "multilingual chart-based constituency parse extraction from pre-trained language models",
    "citation_count": 6,
    "authors": [
      "Taeuk Kim",
      "Bowen Li",
      "Sang-goo Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.42": {
    "title": "Hyperbolic Geometry is Not Necessary: Lightweight Euclidean-Based Models for Low-Dimensional Knowledge Graph Embeddings",
    "volume": "findings",
    "abstract": "Recent knowledge graph embedding (KGE) models based on hyperbolic geometry have shown great potential in a low-dimensional embedding space. However, the necessity of hyperbolic space in KGE is still questionable, because the calculation based on hyperbolic geometry is much more complicated than Euclidean operations. In this paper, based on the state-of-the-art hyperbolic-based model RotH, we develop two lightweight Euclidean-based models, called RotL and Rot2L. The RotL model simplifies the hyperbolic operations while keeping the flexible normalization effect. Utilizing a novel two-layer stacked transformation and based on RotL, the Rot2L model obtains an improved representation capability, yet costs fewer parameters and calculations than RotH. The experiments on link prediction show that Rot2L achieves the state-of-the-art performance on two widely-used datasets in low-dimensional knowledge graph embeddings. Furthermore, RotL achieves similar performance as RotH but only requires half of the training time",
    "checked": true,
    "id": "479e1ba1618d35c893aa04dad3d6b6e27eda17cb",
    "semantic_title": "hyperbolic geometry is not necessary: lightweight euclidean-based models for low-dimensional knowledge graph embeddings",
    "citation_count": 19,
    "authors": [
      "Kai Wang",
      "Yu Liu",
      "Dan Lin",
      "Michael Sheng"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.43": {
    "title": "CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade",
    "volume": "findings",
    "abstract": "Dynamic early exiting aims to accelerate the inference of pre-trained language models (PLMs) by emitting predictions in internal layers without passing through the entire model. In this paper, we empirically analyze the working mechanism of dynamic early exiting and find that it faces a performance bottleneck under high speed-up ratios. On one hand, the PLMs' representations in shallow layers lack high-level semantic information and thus are not sufficient for accurate predictions. On the other hand, the exiting decisions made by internal classifiers are unreliable, leading to wrongly emitted early predictions. We instead propose a new framework for accelerating the inference of PLMs, CascadeBERT, which dynamically selects proper-sized and complete models in a cascading manner, providing comprehensive representations for predictions. We further devise a difficulty-aware objective, encouraging the model to output the class probability that reflects the real difficulty of each instance for a more reliable cascading mechanism. Experimental results show that CascadeBERT can achieve an overall 15% improvement under 4x speed-up compared with existing dynamic early exiting methods on six classification tasks, yielding more calibrated and accurate predictions",
    "checked": true,
    "id": "14c26c4f14c9cad4d805583eb36b7ba52120423e",
    "semantic_title": "cascadebert: accelerating inference of pre-trained language models via calibrated complete models cascade",
    "citation_count": 52,
    "authors": [
      "Lei Li",
      "Yankai Lin",
      "Deli Chen",
      "Shuhuai Ren",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.44": {
    "title": "Semi-supervised Relation Extraction via Incremental Meta Self-Training",
    "volume": "findings",
    "abstract": "To alleviate human efforts from obtaining large-scale annotations, Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in addition to learning from limited samples. Existing self-training methods suffer from the gradual drift problem, where noisy pseudo labels on unlabeled data are incorporated during training. To alleviate the noise in pseudo labels, we propose a method called MetaSRE, where a Relation Label Generation Network generates accurate quality assessment on pseudo labels by (meta) learning from the successful and failed attempts on Relation Classification Network as an additional meta-objective. To reduce the influence of noisy pseudo labels, MetaSRE adopts a pseudo label selection and exploitation scheme which assesses pseudo label quality on unlabeled samples and only exploits high-quality pseudo labels in a self-training fashion to incrementally augment labeled samples for both robustness and accuracy. Experimental results on two public datasets demonstrate the effectiveness of the proposed approach",
    "checked": true,
    "id": "dfb87c3d93ad1bf8e3bdf2ab641068301cac23bb",
    "semantic_title": "semi-supervised relation extraction via incremental meta self-training",
    "citation_count": 78,
    "authors": [
      "Xuming Hu",
      "Chenwei Zhang",
      "Fukun Ma",
      "Chenyao Liu",
      "Lijie Wen",
      "Philip S. Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.45": {
    "title": "Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning",
    "volume": "findings",
    "abstract": "Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a classical task for capturing the central idea from a given document. Based on Seq2Seq models, the previous reinforcement learning framework on KG tasks utilizes the evaluation metrics to further improve the well-trained neural models. However, these KG evaluation metrics such as F1@5 and F1@M are only aware of the exact correctness of predictions on phrase-level and ignore the semantic similarities between similar predictions and targets, which inhibits the model from learning deep linguistic patterns. In response to this problem, we propose a new fine-grained evaluation metric to improve the RL framework, which considers different granularities: token-level F1 score, edit distance, duplication, and prediction quantities. On the whole, the new framework includes two reward functions: the fine-grained evaluation score and the vanilla F1 score. This framework helps the model identifying some partial match phrases which can be further optimized as the exact match ones. Experiments on KG benchmarks show that our proposed training framework outperforms the previous RL training frameworks among all evaluation scores. In addition, our method can effectively ease the synonym problem and generate a higher quality prediction. The source code is available at https://github.com/xuyige/FGRL4KG",
    "checked": true,
    "id": "b3e25ca21cc1da92728da3bfba7217343ea672b7",
    "semantic_title": "keyphrase generation with fine-grained evaluation-guided reinforcement learning",
    "citation_count": 19,
    "authors": [
      "Yichao Luo",
      "Yige Xu",
      "Jiacheng Ye",
      "Xipeng Qiu",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.46": {
    "title": "Improving Knowledge Graph Embedding Using Affine Transformations of Entities Corresponding to Each Relation",
    "volume": "findings",
    "abstract": "To find a suitable embedding for a knowledge graph remains a big challenge nowadays. By using previous knowledge graph embedding methods, every entity in a knowledge graph is usually represented as a k-dimensional vector. As we know, an affine transformation can be expressed in the form of a matrix multiplication followed by a translation vector. In this paper, we firstly utilize a set of affine transformations related to each relation to operate on entity vectors, and then these transformed vectors are used for performing embedding with previous methods. The main advantage of using affine transformations is their good geometry properties with interpretability. Our experimental results demonstrate that the proposed intuitive design with affine transformations provides a statistically significant increase in performance with adding a few extra processing steps or adding a limited number of additional variables. Taking TransE as an example, we employ the scale transformation (the special case of an affine transformation), and only introduce k additional variables for each relation. Surprisingly, it even outperforms RotatE to some extent on various data sets. We also introduce affine transformations into RotatE, Distmult and ComplEx, respectively, and each one outperforms its original method",
    "checked": true,
    "id": "5d9f61b1aadcc097a69e4a05b569add6d145a9c2",
    "semantic_title": "improving knowledge graph embedding using affine transformations of entities corresponding to each relation",
    "citation_count": 11,
    "authors": [
      "Jinfa Yang",
      "Yongjie Shi",
      "Xin Tong",
      "Robin Wang",
      "Taiyan Chen",
      "Xianghua Ying"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.47": {
    "title": "Using Question Answering Rewards to Improve Abstractive Summarization",
    "volume": "findings",
    "abstract": "Neural abstractive summarization models have drastically improved in the recent years. However, the summaries generated by these models generally suffer from issues such as: not capturing the critical facts in source documents, and containing facts that are inconsistent with the source documents. In this work, we present a general framework to train abstractive summarization models to alleviate such issues. We first train a sequence-to-sequence model to summarize documents, and then further train this model in a Reinforcement Learning setting with question-answering based rewards. We evaluate the summaries generated by the this framework using multiple automatic measures and human judgements. The experimental results show that the question-answering rewards can be used as a general framework to improve neural abstractive summarization. Particularly, the results from human evaluations show that the summaries generated by our approach is preferred over 30% of the time over the summaries generated by general abstractive summarization models",
    "checked": true,
    "id": "0179a1904058c3741cb253281786bdba519f345b",
    "semantic_title": "using question answering rewards to improve abstractive summarization",
    "citation_count": 24,
    "authors": [
      "Chulaka Gunasekara",
      "Guy Feigenblat",
      "Benjamin Sznajder",
      "Ranit Aharonov",
      "Sachindra Joshi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.48": {
    "title": "Effect Generation Based on Causal Reasoning",
    "volume": "findings",
    "abstract": "Causal reasoning aims to predict the future scenarios that may be caused by the observed actions. However, existing causal reasoning methods deal with causalities on the word level. In this paper, we propose a novel event-level causal reasoning method and demonstrate its use in the task of effect generation. In particular, we structuralize the observed cause-effect event pairs into an event causality network, which describes causality dependencies. Given an input cause sentence, a causal subgraph is retrieved from the event causality network and is encoded with the graph attention mechanism, in order to support better reasoning of the potential effects. The most probable effect event is then selected from the causal subgraph and is used as guidance to generate an effect sentence. Experiments show that our method generates more reasonable effect sentences than various well-designed competitors",
    "checked": true,
    "id": "85f05fbeea9ea9dfdd8c1d8aa1f71cb011854957",
    "semantic_title": "effect generation based on causal reasoning",
    "citation_count": 0,
    "authors": [
      "Feiteng Mu",
      "Wenjie Li",
      "Zhipeng Xie"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.49": {
    "title": "Distilling Word Meaning in Context from Pre-trained Language Models",
    "volume": "findings",
    "abstract": "In this study, we propose a self-supervised learning method that distils representations of word meaning in context from a pre-trained masked language model. Word representations are the basis for context-aware lexical semantics and unsupervised semantic textual similarity (STS) estimation. A previous study transforms contextualised representations employing static word embeddings to weaken excessive effects of contextual information. In contrast, the proposed method derives representations of word meaning in context while preserving useful context information intact. Specifically, our method learns to combine outputs of different hidden layers using self-attention through self-supervised learning with an automatically generated training corpus. To evaluate the performance of the proposed approach, we performed comparative experiments using a range of benchmark tasks. The results confirm that our representations exhibited a competitive performance compared to that of the state-of-the-art method transforming contextualised representations for the context-aware lexical semantic tasks and outperformed it for STS estimation",
    "checked": true,
    "id": "b0aeda9fb9e5175e1a055fe9c3c5246ddab4a251",
    "semantic_title": "distilling word meaning in context from pre-trained language models",
    "citation_count": 1,
    "authors": [
      "Yuki Arase",
      "Tomoyuki Kajiwara"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.50": {
    "title": "Unseen Entity Handling in Complex Question Answering over Knowledge Base via Language Generation",
    "volume": "findings",
    "abstract": "Complex question answering over knowledge base remains as a challenging task because it involves reasoning over multiple pieces of information, including intermediate entities/relations and other constraints. Previous methods simplify the SPARQL query of a question into such forms as a list or a graph, missing such constraints as \"filter\" and \"order_by\", and present models specialized for generating those simplified forms from a given question. We instead introduce a novel approach that directly generates an executable SPARQL query without simplification, addressing the issue of generating unseen entities. We adapt large scale pre-trained encoder-decoder models and show that our method significantly outperforms the previous methods and also that our method has higher interpretability and computational efficiency than the previous methods",
    "checked": true,
    "id": "d500c90cfd1f50f7951ac5b0abe57b978410fe80",
    "semantic_title": "unseen entity handling in complex question answering over knowledge base via language generation",
    "citation_count": 18,
    "authors": [
      "Xin Huang",
      "Jung-Jae Kim",
      "Bowei Zou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.51": {
    "title": "Bidirectional Hierarchical Attention Networks based on Document-level Context for Emotion Cause Extraction",
    "volume": "findings",
    "abstract": "Emotion cause extraction (ECE) aims to extract the causes behind the certain emotion in text. Some works related to the ECE task have been published and attracted lots of attention in recent years. However, these methods neglect two major issues: 1) pay few attentions to the effect of document-level context information on ECE, and 2) lack of sufficient exploration for how to effectively use the annotated emotion clause. For the first issue, we propose a bidirectional hierarchical attention network (BHA) corresponding to the specified candidate cause clause to capture the document-level context in a structured and dynamic manner. For the second issue, we design an emotional filtering module (EF) for each layer of the graph attention network, which calculates a gate score based on the emotion clause to filter the irrelevant information. Combining the BHA and EF, the EF-BHA can dynamically aggregate the contextual information from two directions and filters irrelevant information. The experimental results demonstrate that EF-BHA achieves the competitive performances on two public datasets in different languages (Chinese and English). Moreover, we quantify the effect of context on emotion cause extraction and provide the visualization of the interactions between candidate cause clauses and contexts",
    "checked": true,
    "id": "7e8ee2b792922690f3bbb428a6667b5fd7a2a077",
    "semantic_title": "bidirectional hierarchical attention networks based on document-level context for emotion cause extraction",
    "citation_count": 20,
    "authors": [
      "Guimin Hu",
      "Guangming Lu",
      "Yi Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.52": {
    "title": "Distantly Supervised Relation Extraction in Federated Settings",
    "volume": "findings",
    "abstract": "In relation extraction, distant supervision is widely used to automatically label a large-scale training dataset by aligning a knowledge base with unstructured text. Most existing studies in this field have assumed there is a great deal of centralized unstructured text. However, in practice, texts are usually distributed on different platforms and cannot be centralized due to privacy restrictions. Therefore, it is worthwhile to investigate distant supervision in the federated learning paradigm, which decouples the training of the model from the need for direct access to raw texts. However, overcoming label noise of distant supervision becomes more difficult in federated settings, because texts containing the same entity pair scatter around different platforms. In this paper, we propose a federated denoising framework to suppress label noise in federated settings. The key of this framework is a multiple instance learning based denoising method that is able to select reliable sentences via cross-platform collaboration. Various experiments on New York Times dataset and miRNA gene regulation relation dataset demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "fb4227db25d1d6fcfab30dc21a4a37fa138d9274",
    "semantic_title": "distantly supervised relation extraction in federated settings",
    "citation_count": 10,
    "authors": [
      "Dianbo Sui",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.53": {
    "title": "Casting the Same Sentiment Classification Problem",
    "volume": "findings",
    "abstract": "We introduce and study a problem variant of sentiment analysis, namely the \"same sentiment classification problem\", where, given a pair of texts, the task is to determine if they have the same sentiment, disregarding the actual sentiment polarity. Among other things, our goal is to enable a more topic-agnostic sentiment classification. We study the problem using the Yelp business review dataset, demonstrating how sentiment data needs to be prepared for this task, and then carry out sequence pair classification using the BERT language model. In a series of experiments, we achieve an accuracy above 83% for category subsets across topics, and 89% on average",
    "checked": true,
    "id": "057f6b7aa41fa52561bbe058fa1331e2a73efe99",
    "semantic_title": "casting the same sentiment classification problem",
    "citation_count": 1,
    "authors": [
      "Erik Körner",
      "Ahmad Dawar Hakimi",
      "Gerhard Heyer",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.54": {
    "title": "Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing",
    "volume": "findings",
    "abstract": "While neural networks are ubiquitous in state-of-the-art semantic parsers, it has been shown that most standard models suffer from dramatic performance losses when faced with compositionally out-of-distribution (OOD) data. Recently several methods have been proposed to improve compositional generalization in semantic parsing. In this work we instead focus on the problem of detecting compositionally OOD examples with neural semantic parsers, which, to the best of our knowledge, has not been investigated before. We investigate several strong yet simple methods for OOD detection based on predictive uncertainty. The experimental results demonstrate that these techniques perform well on the standard SCAN and CFQ datasets. Moreover, we show that OOD detection can be further improved by using a heterogeneous ensemble",
    "checked": true,
    "id": "0b1470014bdbaa80ba63da0491d9db6c7d4febcc",
    "semantic_title": "detecting compositionally out-of-distribution examples in semantic parsing",
    "citation_count": 9,
    "authors": [
      "Denis Lukovnikov",
      "Sina Daubener",
      "Asja Fischer"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.55": {
    "title": "Saliency-based Multi-View Mixed Language Training for Zero-shot Cross-lingual Classification",
    "volume": "findings",
    "abstract": "Recent multilingual pre-trained models, like XLM-RoBERTa (XLM-R), have been demonstrated effective in many cross-lingual tasks. However, there are still gaps between the contextualized representations of similar words in different languages. To solve this problem, we propose a novel framework named Multi-View Mixed Language Training (MVMLT), which leverages code-switched data with multi-view learning to fine-tune XLM-R. MVMLT uses gradient-based saliency to extract keywords which are the most relevant to downstream tasks and replaces them with the corresponding words in the target language dynamically. Furthermore, MVMLT utilizes multi-view learning to encourage contextualized embeddings to align into a more refined language-invariant space. Extensive experiments with four languages show that our model achieves state-of-the-art results on zero-shot cross-lingual sentiment classification and dialogue state tracking tasks, demonstrating the effectiveness of our proposed model",
    "checked": true,
    "id": "f96e4732cd4eb66b0dc0d6e912b0c894d6bf0e35",
    "semantic_title": "saliency-based multi-view mixed language training for zero-shot cross-lingual classification",
    "citation_count": 7,
    "authors": [
      "Siyu Lai",
      "Hui Huang",
      "Dong Jing",
      "Yufeng Chen",
      "Jinan Xu",
      "Jian Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.56": {
    "title": "Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society",
    "volume": "findings",
    "abstract": "With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and (iii) covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual vs. multilingual, and single task vs. multitask settings",
    "checked": true,
    "id": "1d8d75c51c6f3208af8f2308e9621473c8e518a6",
    "semantic_title": "fighting the covid-19 infodemic: modeling the perspective of journalists, fact-checkers, social media platforms, policy makers, and the society",
    "citation_count": 158,
    "authors": [
      "Firoj Alam",
      "Shaden Shaar",
      "Fahim Dalvi",
      "Hassan Sajjad",
      "Alex Nikolov",
      "Hamdy Mubarak",
      "Giovanni Da San Martino",
      "Ahmed Abdelali",
      "Nadir Durrani",
      "Kareem Darwish",
      "Abdulaziz Al-Homaid",
      "Wajdi Zaghouani",
      "Tommaso Caselli",
      "Gijs Danoe",
      "Friso Stolk",
      "Britt Bruntink",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.57": {
    "title": "FANATIC: FAst Noise-Aware TopIc Clustering",
    "volume": "findings",
    "abstract": "Extracting salient topics from a collection of documents can be a challenging task when a) the amount of data is large, b) the number of topics is not known a priori, and/or c) \"topic noise\" is present. We define \"topic noise\" as the collection of documents that are irrelevant to any coherent topic and should be filtered out. By design, most clustering algorithms (e.g. k-means, hierarchical clustering) assign all input documents to one of the available clusters, guaranteeing any topic noise to propagate into the result. To address these challenges, we present a novel algorithm, FANATIC, that efficiently distinguishes documents from genuine topics and those that are topic noise. We also introduce a new Reddit dataset to showcase FANATIC as it contains short, noisy data that is difficult to cluster using most clustering algorithms. We find that FANATIC clusters 500k Reddit titles (of which 20% are topic noise) in 2 minutes and achieves an AMI score of 0.59, in contrast with hdbscan (McInnes et al., 2017), a popular algorithm suited for this type of task, which requires over 7 hours and achieves an AMI of 0.03. Finally, we test FANATIC against a Twitter dataset and find again that it outperforms the other algorithms with an AMI score of 0.60. We make our code and data publicly available",
    "checked": true,
    "id": "8c6fbc2791f9a886d19694d00ebfc0ee5d0c3fbe",
    "semantic_title": "fanatic: fast noise-aware topic clustering",
    "citation_count": 2,
    "authors": [
      "Ari Silburt",
      "Anja Subasic",
      "Evan Thompson",
      "Carmeline Dsilva",
      "Tarec Fares"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.58": {
    "title": "Stream-level Latency Evaluation for Simultaneous Machine Translation",
    "volume": "findings",
    "abstract": "Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task",
    "checked": true,
    "id": "3de6daeb811b2565a1c657c1cbbd3a5517c0ec6e",
    "semantic_title": "stream-level latency evaluation for simultaneous machine translation",
    "citation_count": 14,
    "authors": [
      "Javier Iranzo-Sánchez",
      "Jorge Civera Saiz",
      "Alfons Juan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.59": {
    "title": "TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning",
    "volume": "findings",
    "abstract": "Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains",
    "checked": true,
    "id": "b6647280615f667dd7418bfb9b13d828a22c1cfe",
    "semantic_title": "tsdae: using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning",
    "citation_count": 184,
    "authors": [
      "Kexin Wang",
      "Nils Reimers",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.60": {
    "title": "How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?",
    "volume": "findings",
    "abstract": "Data-driven subword segmentation has become the default strategy for open-vocabulary machine translation and other NLP tasks, but may not be sufficiently generic for optimal learning of non-concatenative morphology. We design a test suite to evaluate segmentation strategies on different types of morphological phenomena in a controlled, semi-synthetic setting. In our experiments, we compare how well machine translation models trained on subword- and character-level can translate these morphological phenomena. We find that learning to analyse and generate morphologically complex surface representations is still challenging, especially for non-concatenative morphological phenomena like reduplication or vowel harmony and for rare word stems. Based on our results, we recommend that novel text representation strategies be tested on a range of typologically diverse languages to minimise the risk of adopting a strategy that inadvertently disadvantages certain languages",
    "checked": true,
    "id": "2ffbe6040369a82d5a003c2bb835e221c9d2f896",
    "semantic_title": "how suitable are subword segmentation strategies for translating non-concatenative morphology?",
    "citation_count": 13,
    "authors": [
      "Chantal Amrhein",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.61": {
    "title": "Rethinking Why Intermediate-Task Fine-Tuning Works",
    "volume": "findings",
    "abstract": "Supplementary Training on Intermediate Labeled-data Tasks (STILT) is a widely applied technique, which first fine-tunes the pretrained language models on an intermediate task before on the target task of interest. While STILT is able to further improve the performance of pretrained language models, it is still unclear why and when it works. Previous research shows that those intermediate tasks involving complex inference, such as commonsense reasoning, work especially well for RoBERTa-large. In this paper, we discover that the improvement from an intermediate task could be orthogonal to it containing reasoning or other complex skills — a simple real-fake discrimination task synthesized by GPT2 can benefit diverse target tasks. We conduct extensive experiments to study the impact of different factors on STILT. These findings suggest rethinking the role of intermediate fine-tuning in the STILT pipeline",
    "checked": true,
    "id": "e6f94081276a7a5e6aef34a080cb3d3a4b1b9c20",
    "semantic_title": "rethinking why intermediate-task fine-tuning works",
    "citation_count": 30,
    "authors": [
      "Ting-Yun Chang",
      "Chi-Jen Lu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.62": {
    "title": "Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning",
    "volume": "findings",
    "abstract": "The ability to continuously expand knowledge over time and utilize it to rapidly generalize to new tasks is a key feature of human linguistic intelligence. Existing models that pursue rapid generalization to new tasks (e.g., few-shot learning methods), however, are mostly trained in a single shot on fixed datasets, unable to dynamically expand their knowledge; while continual learning algorithms are not specifically designed for rapid generalization. We present a new learning setup, Continual Learning of Few-Shot Learners (CLIF), to address challenges of both learning settings in a unified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks arriving sequentially, accumulating knowledge for improved generalization to new tasks, while also retaining performance on the tasks learned earlier. We examine how the generalization ability is affected in the continual learning setup, evaluate a number of continual learning algorithms, and propose a novel regularized adapter generation approach. We find that catastrophic forgetting affects generalization ability to a lesser degree than performance on seen tasks; while continual learning algorithms can still bring considerable benefit to the generalization ability",
    "checked": true,
    "id": "b3f98f5baf3b7239ce0da45147bda4c4154fac9c",
    "semantic_title": "learn continually, generalize rapidly: lifelong knowledge accumulation for few-shot learning",
    "citation_count": 42,
    "authors": [
      "Xisen Jin",
      "Bill Yuchen Lin",
      "Mohammad Rostami",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.63": {
    "title": "Efficient Test Time Adapter Ensembling for Low-resource Language Varieties",
    "volume": "findings",
    "abstract": "Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires training a separate language adapter for every language one wishes to support, which can be impractical for languages with limited data. An intuitive solution is to use a related language adapter for the new language variety, but we observe that this solution can lead to sub-optimal performance. In this paper, we aim to improve the robustness of language adapters to uncovered languages without training new adapters. We find that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters. Building upon this observation, we propose Entropy Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. Experiments on three diverse groups of language varieties show that our method leads to significant improvements on both named entity recognition and part-of-speech tagging across all languages",
    "checked": true,
    "id": "7b5b15279e5a52439614f886b79fa33f4b88bfb2",
    "semantic_title": "efficient test time adapter ensembling for low-resource language varieties",
    "citation_count": 35,
    "authors": [
      "Xinyi Wang",
      "Yulia Tsvetkov",
      "Sebastian Ruder",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.64": {
    "title": "An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces",
    "volume": "findings",
    "abstract": "Much recent work in bilingual lexicon induction (BLI) views word embeddings as vectors in Euclidean space. As such, BLI is typically solved by finding a linear transformation that maps embeddings to a common space. Alternatively, word embeddings may be understood as nodes in a weighted graph. This framing allows us to examine a node's graph neighborhood without assuming a linear transform, and exploits new techniques from the graph matching optimization literature. These contrasting approaches have not been compared in BLI so far. In this work, we study the behavior of Euclidean versus graph-based approaches to BLI under differing data conditions and show that they complement each other when combined. We release our code at https://github.com/kellymarchisio/euc-v-graph-bli",
    "checked": true,
    "id": "0a5fc6d1735dd2761fc31fad5a3b40a4fa06546b",
    "semantic_title": "an analysis of euclidean vs. graph-based framing for bilingual lexicon induction from word embedding spaces",
    "citation_count": 4,
    "authors": [
      "Kelly Marchisio",
      "Youngser Park",
      "Ali Saad-Eldin",
      "Anton Alyakin",
      "Kevin Duh",
      "Carey Priebe",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.65": {
    "title": "How to Select One Among All ? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding",
    "volume": "findings",
    "abstract": "Knowledge Distillation (KD) is a model compression algorithm that helps transfer the knowledge in a large neural network into a smaller one. Even though KD has shown promise on a wide range of Natural Language Processing (NLP) applications, little is understood about how one KD algorithm compares to another and whether these approaches can be complimentary to each other. In this work, we evaluate various KD algorithms on in-domain, out-of-domain and adversarial testing. We propose a framework to assess adversarial robustness of multiple KD algorithms. Moreover, we introduce a new KD algorithm, Combined-KD, which takes advantage of two promising approaches (better training scheme and more efficient data augmentation). Our extensive experimental results show that Combined-KD achieves state-of-the-art results on the GLUE benchmark, out-of-domain generalization, and adversarial robustness compared to competitive methods",
    "checked": true,
    "id": "8d1b4088849c998cdadd653b67d38c4886663959",
    "semantic_title": "how to select one among all ? an empirical study towards the robustness of knowledge distillation in natural language understanding",
    "citation_count": 9,
    "authors": [
      "Tianda Li",
      "Ahmad Rashid",
      "Aref Jafari",
      "Pranav Sharma",
      "Ali Ghodsi",
      "Mehdi Rezagholizadeh"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.66": {
    "title": "Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction",
    "volume": "findings",
    "abstract": "Compliments and concerns in reviews are valuable for understanding users' shopping interests and their opinions with respect to specific aspects of certain items. Existing review-based recommenders favor large and complex language encoders that can only learn latent and uninterpretable text representations. They lack explicit user-attention and item-property modeling, which however could provide valuable information beyond the ability to recommend items. Therefore, we propose a tightly coupled two-stage approach, including an Aspect-Sentiment Pair Extractor (ASPE) and an Attention-Property-aware Rating Estimator (APRE). Unsupervised ASPE mines Aspect-Sentiment pairs (AS-pairs) and APRE predicts ratings using AS-pairs as concrete aspect-level evidences. Extensive experiments on seven real-world Amazon Review Datasets demonstrate that ASPE can effectively extract AS-pairs which enable APRE to deliver superior accuracy over the leading baselines",
    "checked": true,
    "id": "a06c851dc4ec6850b561f0f0c48834fb43211a4f",
    "semantic_title": "recommend for a reason: unlocking the power of unsupervised aspect-sentiment co-extraction",
    "citation_count": 5,
    "authors": [
      "Zeyu Li",
      "Wei Cheng",
      "Reema Kshetramade",
      "John Houser",
      "Haifeng Chen",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.67": {
    "title": "Learning Hard Retrieval Decoder Attention for Transformers",
    "volume": "findings",
    "abstract": "The Transformer translation model is based on the multi-head attention mechanism, which can be parallelized easily. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. In this paper, we present an approach to learning a hard retrieval attention where an attention head only attends to one token in the sentence rather than all tokens. The matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can thus be replaced by a simple and efficient retrieval operation. We show that our hard retrieval attention mechanism is 1.43 times faster in decoding, while preserving translation quality on a wide range of machine translation tasks when used in the decoder self- and cross-attention networks",
    "checked": true,
    "id": "6f3797d3e88b5ba957ffef70621f74ebd2f4df9c",
    "semantic_title": "learning hard retrieval decoder attention for transformers",
    "citation_count": 1,
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.68": {
    "title": "Recall and Learn: A Memory-augmented Solver for Math Word Problems",
    "volume": "findings",
    "abstract": "In this article, we tackle the math word problem, namely, automatically answering a mathematical problem according to its textual description. Although recent methods have demonstrated their promising results, most of these methods are based on template-based generation scheme which results in limited generalization capability. To this end, we propose a novel human-like analogical learning method in a recall and learn manner. Our proposed framework is composed of modules of memory, representation, analogy, and reasoning, which are designed to make a new exercise by referring to the exercises learned in the past. Specifically, given a math word problem, the model first retrieves similar questions by a memory module and then encodes the unsolved problem and each retrieved question using a representation module. Moreover, to solve the problem in a way of analogy, an analogy module and a reasoning module with a copy mechanism are proposed to model the interrelationship between the problem and each retrieved question. Extensive experiments on two well-known datasets show the superiority of our proposed algorithm as compared to other state-of-the-art competitors from both overall performance comparison and micro-scope studies",
    "checked": true,
    "id": "81c241c2cb8e86a5574797424bc281e323cd831b",
    "semantic_title": "recall and learn: a memory-augmented solver for math word problems",
    "citation_count": 31,
    "authors": [
      "Shifeng Huang",
      "Jiawei Wang",
      "Jiao Xu",
      "Da Cao",
      "Ming Yang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.69": {
    "title": "An Uncertainty-Aware Encoder for Aspect Detection",
    "volume": "findings",
    "abstract": "Aspect detection is a fundamental task in opinion mining. Previous works use seed words either as priors of topic models, as anchors to guide the learning of aspects, or as features of aspect classifiers. This paper presents a novel weakly-supervised method to exploit seed words for aspect detection based on an encoder architecture. The encoder maps segments and aspects into a low-dimensional embedding space. The goal is approximating similarity between segments and aspects in the embedding space and their ground-truth similarity generated from seed words. An objective function is proposed to capture the uncertainty of ground-truth similarity. Our method outperforms previous works on several benchmarks in various domains",
    "checked": true,
    "id": "c819d19bff000ff1adf6ae194503d1f2975b3077",
    "semantic_title": "an uncertainty-aware encoder for aspect detection",
    "citation_count": 4,
    "authors": [
      "Thi-Nhung Nguyen",
      "Kiem-Hieu Nguyen",
      "Young-In Song",
      "Tuan-Dung Cao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.70": {
    "title": "Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations",
    "volume": "findings",
    "abstract": "Current approaches to empathetic response generation focus on learning a model to predict an emotion label and generate a response based on this label and have achieved promising results. However, the emotion cause, an essential factor for empathetic responding, is ignored. The emotion cause is a stimulus for human emotions. Recognizing the emotion cause is helpful to better understand human emotions so as to generate more empathetic responses. To this end, we propose a novel framework that improves empathetic response generation by recognizing emotion cause in conversations. Specifically, an emotion reasoner is designed to predict a context emotion label and a sequence of emotion cause-oriented labels, which indicate whether the word is related to the emotion cause. Then we devise both hard and soft gated attention mechanisms to incorporate the emotion cause into response generation. Experiments show that incorporating emotion cause information improves the performance of the model on both emotion recognition and response generation",
    "checked": true,
    "id": "4a70987a92f9593ace8f3670918eb0ce9e8a475e",
    "semantic_title": "improving empathetic response generation by recognizing emotion cause in conversations",
    "citation_count": 94,
    "authors": [
      "Jun Gao",
      "Yuhan Liu",
      "Haolin Deng",
      "Wei Wang",
      "Yu Cao",
      "Jiachen Du",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.71": {
    "title": "Probing Across Time: What Does RoBERTa Know and When?",
    "volume": "findings",
    "abstract": "Models of language trained on very large corpora have been demonstrated useful for natural language processing. As fixed artifacts, they have become the object of intense study, with many researchers \"probing\" the extent to which they acquire and readily demonstrate linguistic abstractions, factual and commonsense knowledge, and reasoning abilities. Recent work applied several probes to intermediate training stages to observe the developmental process of a large-scale model (Chiang et al., 2020). Following this effort, we systematically answer a question: for various types of knowledge a language model learns, when during (pre)training are they acquired? Using RoBERTa as a case study, we find: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster",
    "checked": true,
    "id": "0672f88d5dc762002b515ca4a0a9f101017fea35",
    "semantic_title": "probing across time: what does roberta know and when?",
    "citation_count": 84,
    "authors": [
      "Zeyu Liu",
      "Yizhong Wang",
      "Jungo Kasai",
      "Hannaneh Hajishirzi",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.72": {
    "title": "Knowledge-Guided Paraphrase Identification",
    "volume": "findings",
    "abstract": "Paraphrase identification (PI), a fundamental task in natural language processing, is to identify whether two sentences express the same or similar meaning, which is a binary classification problem. Recently, BERT-like pre-trained language models have been a popular choice for the frameworks of various PI models, but almost all existing methods consider general domain text. When these approaches are applied to a specific domain, existing models cannot make accurate predictions due to the lack of professional knowledge. In light of this challenge, we propose a novel framework, namely , which can leverage the external unstructured Wikipedia knowledge to accurately identify paraphrases. We propose to mine outline knowledge of concepts related to given sentences from Wikipedia via BM25 model. After retrieving related outline knowledge, makes predictions based on both the semantic information of two sentences and the outline knowledge. Besides, we propose a gating mechanism to aggregate the semantic information-based prediction and the knowledge-based prediction. Extensive experiments are conducted on two public datasets: PARADE (a computer science domain dataset) and clinicalSTS2019 (a biomedical domain dataset). The results show that the proposed outperforms state-of-the-art methods",
    "checked": true,
    "id": "8aa69a6f0cbdc2ed4f09f59c790a9a0eaa775d24",
    "semantic_title": "knowledge-guided paraphrase identification",
    "citation_count": 10,
    "authors": [
      "Haoyu Wang",
      "Fenglong Ma",
      "Yaqing Wang",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.73": {
    "title": "R2-D2: A Modular Baseline for Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank twice, reaD twice). The pipeline is composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the final prediction from all system's components. We demonstrate its strength across three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA, surpassing state-of-the-art on the first two. Our analysis demonstrates that: (i) combining extractive and generative reader yields absolute improvements up to 5 exact match and it is at least twice as effective as the posterior averaging ensemble of the same models with different parameters, (ii) the extractive reader with fewer parameters can match the performance of the generative reader on extractive QA datasets",
    "checked": true,
    "id": "d82c779e316aebf0a6b08904dff9cd26ba57219b",
    "semantic_title": "r2-d2: a modular baseline for open-domain question answering",
    "citation_count": 47,
    "authors": [
      "Martin Fajcik",
      "Martin Docekal",
      "Karel Ondrej",
      "Pavel Smrz"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.74": {
    "title": "What Does Your Smile Mean? Jointly Detecting Multi-Modal Sarcasm and Sentiment Using Quantum Probability",
    "volume": "findings",
    "abstract": "Sarcasm and sentiment embody intrinsic uncertainty of human cognition, making joint detection of multi-modal sarcasm and sentiment a challenging task. In view of the advantages of quantum probability (QP) in modeling such uncertainty, this paper explores the potential of QP as a mathematical framework and proposes a QP driven multi-task (QPM) learning framework. The QPM framework involves a complex-valued multi-modal representation encoder, a quantum-like fusion subnetwork and a quantum measurement mechanism. Each multi-modal (e.g., textual, visual) utterance is first encoded as a quantum superposition of a set of basis terms using a complex-valued representation. Then, the quantum-like fusion subnetwork leverages quantum state composition and quantum interference to model the contextual interaction between adjacent utterances and the correlations across modalities respectively. Finally, quantum incompatible measurements are performed on the multi-modal representation of each utterance to yield the probabilistic outcomes of sarcasm and sentiment recognition. The experimental results show that our model achieves a state-of-the-art performance",
    "checked": true,
    "id": "b41d4363c097c68f345d421097d506d2ac5a7d53",
    "semantic_title": "what does your smile mean? jointly detecting multi-modal sarcasm and sentiment using quantum probability",
    "citation_count": 43,
    "authors": [
      "Yaochen Liu",
      "Yazhou Zhang",
      "Qiuchi Li",
      "Benyou Wang",
      "Dawei Song"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.75": {
    "title": "Discovering Representation Sprachbund For Multilingual Pre-Training",
    "volume": "findings",
    "abstract": "Multilingual pre-trained models have demonstrated their effectiveness in many multilingual NLP tasks and enabled zero-shot or few-shot transfer from high-resource languages to low-resource ones. However, due to significant typological differences and contradictions between some languages, such models usually perform poorly on many languages and cross-lingual settings, which shows the difficulty of learning a single model to handle massive diverse languages well at the same time. To alleviate this issue, we present a new multilingual pre-training pipeline. We propose to generate language representation from multilingual pre-trained model and conduct linguistic analysis to show that language representation similarity reflects linguistic similarity from multiple perspectives, including language family, geographical sprachbund, lexicostatistics, and syntax. Then we cluster all the target languages into multiple groups and name each group as a representation sprachbund. Thus, languages in the same representation sprachbund are supposed to boost each other in both pre-training and fine-tuning as they share rich linguistic similarity. We pre-train one multilingual model for each representation sprachbund. Experiments are conducted on cross-lingual benchmarks and significant improvements are achieved compared to strong baselines",
    "checked": true,
    "id": "32fc867c053c3733a9d173a2a342a6561a259f04",
    "semantic_title": "discovering representation sprachbund for multilingual pre-training",
    "citation_count": 9,
    "authors": [
      "Yimin Fan",
      "Yaobo Liang",
      "Alexandre Muzio",
      "Hany Hassan",
      "Houqiang Li",
      "Ming Zhou",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.76": {
    "title": "Plan-then-Generate: Controlled Data-to-Text Generation via Planning",
    "volume": "findings",
    "abstract": "Recent developments in neural networks have led to the advance in data-to-text generation. However, the lack of ability of neural models to control the structure of generated output can be limiting in certain real-world applications. In this study, we propose a novel Plan-then-Generate (PlanGen) framework to improve the controllability of neural data-to-text models. Extensive experiments and analyses are conducted on two benchmark datasets, ToTTo and WebNLG. The results show that our model is able to control both the intra-sentence and inter-sentence structure of the generated output. Furthermore, empirical comparisons against previous state-of-the-art methods show that our model improves the generation quality as well as the output diversity as judged by human and automatic evaluations",
    "checked": true,
    "id": "946f28b30c0aede54eed7787c717dfd2e2c59bdd",
    "semantic_title": "plan-then-generate: controlled data-to-text generation via planning",
    "citation_count": 81,
    "authors": [
      "Yixuan Su",
      "David Vandyke",
      "Sihui Wang",
      "Yimai Fang",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.77": {
    "title": "Few-Shot Table-to-Text Generation with Prototype Memory",
    "volume": "findings",
    "abstract": "Neural table-to-text generation models have achieved remarkable progress on an array of tasks. However, due to the data-hungry nature of neural models, their performances strongly rely on large-scale training examples, limiting their applicability in real-world applications. To address this, we propose a new framework: Prototype-to-Generate (P2G), for table-to-text generation under the few-shot scenario. The proposed framework utilizes the retrieved prototypes, which are jointly selected by an IR system and a novel prototype selector to help the model bridging the structural gap between tables and texts. Experimental results on three benchmark datasets with three state-of-the-art models demonstrate that the proposed framework significantly improves the model performance across various evaluation metrics",
    "checked": true,
    "id": "0cf07903ea51d147e924ff1732ecd609ae9b35a0",
    "semantic_title": "few-shot table-to-text generation with prototype memory",
    "citation_count": 32,
    "authors": [
      "Yixuan Su",
      "Zaiqiao Meng",
      "Simon Baker",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.78": {
    "title": "Leveraging Word-Formation Knowledge for Chinese Word Sense Disambiguation",
    "volume": "findings",
    "abstract": "In parataxis languages like Chinese, word meanings are constructed using specific word-formations, which can help to disambiguate word senses. However, such knowledge is rarely explored in previous word sense disambiguation (WSD) methods. In this paper, we propose to leverage word-formation knowledge to enhance Chinese WSD. We first construct a large-scale Chinese lexical sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into sense disambiguation. To further enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines",
    "checked": true,
    "id": "270fb38e4eabe0a74c5dffd472ae6740b4da9887",
    "semantic_title": "leveraging word-formation knowledge for chinese word sense disambiguation",
    "citation_count": 9,
    "authors": [
      "Hua Zheng",
      "Lei Li",
      "Damai Dai",
      "Deli Chen",
      "Tianyu Liu",
      "Xu Sun",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.79": {
    "title": "Exploiting Curriculum Learning in Unsupervised Neural Machine Translation",
    "volume": "findings",
    "abstract": "Back-translation (BT) has become one of the de facto components in unsupervised neural machine translation (UNMT), and it explicitly makes UNMT have translation ability. However, all the pseudo bi-texts generated by BT are treated equally as clean data during optimization without considering the quality diversity, leading to slow convergence and limited translation performance. To address this problem, we propose a curriculum learning method to gradually utilize pseudo bi-texts based on their quality from multiple granularities. Specifically, we first apply crosslingual word embedding to calculate the potential translation difficulty (quality) for the monolingual sentences. Then, the sentences are fed into UNMT from easy to hard batch by batch. Furthermore, considering the quality of sentences/tokens in a particular batch are also diverse, we further adopt the model itself to calculate the fine-grained quality scores, which are served as learning factors to balance the contributions of different parts when computing loss and encourage the UNMT model to focus on pseudo data with higher quality. Experimental results on WMT 14 En-Fr, WMT 14 En-De, WMT 16 En-Ro, and LDC En-Zh translation tasks demonstrate that the proposed method achieves consistent improvements with faster convergence speed",
    "checked": true,
    "id": "7fb364a70a185c7e32bdd8844b7d789b09958464",
    "semantic_title": "exploiting curriculum learning in unsupervised neural machine translation",
    "citation_count": 8,
    "authors": [
      "Jinliang Lu",
      "Jiajun Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.80": {
    "title": "Robust Fragment-Based Framework for Cross-lingual Sentence Retrieval",
    "volume": "findings",
    "abstract": "Cross-lingual Sentence Retrieval (CLSR) aims at retrieving parallel sentence pairs that are translations of each other from a multilingual set of comparable documents. The retrieved parallel sentence pairs can be used in other downstream NLP tasks such as machine translation and cross-lingual word sense disambiguation. We propose a CLSR framework called Robust Fragment-level Representation (RFR) CLSR framework to address Out-of-Domain (OOD) CLSR problems. In particular, we improve the sentence retrieval robustness by representing each sentence as a collection of fragments. In this way, we change the retrieval granularity from the sentence to the fragment level. We performed CLSR experiments based on three OOD datasets, four language pairs, and three base well-known sentence encoders: m-USE, LASER, and LaBSE. Experimental results show that RFR significantly improves the base encoders' performance for more than 85% of the cases",
    "checked": true,
    "id": "2f3adb22e44748cb46d4e8bf528129cd21936cce",
    "semantic_title": "robust fragment-based framework for cross-lingual sentence retrieval",
    "citation_count": 4,
    "authors": [
      "Nattapol Trijakwanich",
      "Peerat Limkonchotiwat",
      "Raheem Sarwar",
      "Wannaphong Phatthiyaphaibun",
      "Ekapol Chuangsuwanich",
      "Sarana Nutanong"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.81": {
    "title": "Towards Improving Adversarial Training of NLP Models",
    "volume": "findings",
    "abstract": "Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models' performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla adversarial training process for NLP models, which we name Attacking to Training (A2T). The core part of A2T is a new and cheaper word substitution attack optimized for vanilla adversarial training. We use A2T to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI datasets. Our results empirically show that it is possible to train robust NLP models using a much cheaper adversary. We demonstrate that vanilla adversarial training with A2T can improve an NLP model's robustness to the attack it was originally trained with and also defend the model against other types of word substitution attacks. Furthermore, we show that A2T can improve NLP models' standard accuracy, cross-domain generalization, and interpretability",
    "checked": true,
    "id": "fa7b8acd47631bada5b66049824bfd335ac6bf8f",
    "semantic_title": "towards improving adversarial training of nlp models",
    "citation_count": 125,
    "authors": [
      "Jin Yong Yoo",
      "Yanjun Qi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.82": {
    "title": "To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence",
    "volume": "findings",
    "abstract": "Framing has significant but subtle effects on public opinion and policy. We propose an NLP framework to measure entity-centric frames. We use it to understand media coverage on police violence in the United States in a new Police Violence Frames Corpus of 82k news articles spanning 7k police killings. Our work uncovers more than a dozen framing devices and reveals significant differences in the way liberal and conservative news sources frame both the issue of police violence and the entities involved. Conservative sources emphasize when the victim is armed or attacking an officer and are more likely to mention the victim's criminal record. Liberal sources focus more on the underlying systemic injustice, highlighting the victim's race and that they were unarmed. We discover temporary spikes in these injustice frames near high-profile shooting events, and finally, we show protest volume correlates with and precedes media framing decisions",
    "checked": true,
    "id": "a0872d551163d86e788b8fa609e9847a18cc693c",
    "semantic_title": "to protect and to serve? analyzing entity-centric framing of police violence",
    "citation_count": 16,
    "authors": [
      "Caleb Ziems",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.83": {
    "title": "Calibrate your listeners! Robust communication-based training for pragmatic speakers",
    "volume": "findings",
    "abstract": "To be good conversational partners, natural language processing (NLP) systems should be trained to produce contextually useful utterances. Prior work has investigated training NLP systems with communication-based objectives, where a neural listener stands in as a communication partner. However, these systems commonly suffer from semantic drift where the learned language diverges radically from natural language. We propose a method that uses a population of neural listeners to regularize speaker training. We first show that language drift originates from the poor uncertainty calibration of a neural listener, which makes high-certainty predictions on novel sentences. We explore ensemble- and dropout-based populations of listeners and find that the former results in better uncertainty quantification. We evaluate both population-based objectives on reference games, and show that the ensemble method with better calibration enables the speaker to generate pragmatic utterances while scaling to a large vocabulary and generalizing to new games and listeners",
    "checked": true,
    "id": "13524c776d9f143ea98625bca8b291dd5d9bc71c",
    "semantic_title": "calibrate your listeners! robust communication-based training for pragmatic speakers",
    "citation_count": 7,
    "authors": [
      "Rose Wang",
      "Julia White",
      "Jesse Mu",
      "Noah Goodman"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.84": {
    "title": "When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions",
    "volume": "findings",
    "abstract": "Scenario-based question answering (SQA) requires retrieving and reading paragraphs from a large corpus to answer a question which is contextualized by a long scenario description. Since a scenario contains both keyphrases for retrieval and much noise, retrieval for SQA is extremely difficult. Moreover, it can hardly be supervised due to the lack of relevance labels of paragraphs for SQA. To meet the challenge, in this paper we propose a joint retriever-reader model called JEEVES where the retriever is implicitly supervised only using QA labels via a novel word weighting mechanism. JEEVES significantly outperforms a variety of strong baselines on multiple-choice questions in three SQA datasets",
    "checked": true,
    "id": "c53b77645811a78b25d6b41feff37170811f7b88",
    "semantic_title": "when retriever-reader meets scenario-based multiple-choice questions",
    "citation_count": 4,
    "authors": [
      "ZiXian Huang",
      "Ao Wu",
      "Yulin Shen",
      "Gong Cheng",
      "Yuzhong Qu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.85": {
    "title": "Structured abbreviation expansion in context",
    "volume": "findings",
    "abstract": "Ad hoc abbreviations are commonly found in informal communication channels that favor shorter messages. We consider the task of reversing these abbreviations in context to recover normalized, expanded versions of abbreviated messages. The problem is related to, but distinct from, spelling correction, as ad hoc abbreviations are intentional and can involve more substantial differences from the original words. Ad hoc abbreviations are also productively generated on-the-fly, so they cannot be resolved solely by dictionary lookup. We generate a large, open-source data set of ad hoc abbreviations. This data is used to study abbreviation strategies and to develop two strong baselines for abbreviation expansion",
    "checked": true,
    "id": "87ac6f724dc740a982a5442f90a7dbc3828d7f2d",
    "semantic_title": "structured abbreviation expansion in context",
    "citation_count": 10,
    "authors": [
      "Kyle Gorman",
      "Christo Kirov",
      "Brian Roark",
      "Richard Sproat"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.86": {
    "title": "Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding",
    "volume": "findings",
    "abstract": "Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the major semi-supervised approaches to improve natural language understanding (NLU) tasks with massive amount of unlabeled data. However, it's unclear whether they learn similar representations or they can be effectively combined. In this paper, we show that TAPT and ST can be complementary with simple TFS protocol by following TAPT -> Finetuning -> Self-training (TFS) process. Experimental results show that TFS protocol can effectively utilize unlabeled data to achieve strong combined gains consistently across six datasets covering sentiment classification, paraphrase identification, natural language inference, named entity recognition and dialogue slot classification. We investigate various semi-supervised settings and consistently show that gains from TAPT and ST can be strongly additive by following TFS procedure. We hope that TFS could serve as an important semi-supervised baseline for future NLP studies",
    "checked": true,
    "id": "364621e7e864d3399ee90a7b2a058474e698e6de",
    "semantic_title": "task-adaptive pre-training and self-training are complementary for natural language understanding",
    "citation_count": 12,
    "authors": [
      "Shiyang Li",
      "Semih Yavuz",
      "Wenhu Chen",
      "Xifeng Yan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.87": {
    "title": "CNNBiF: CNN-based Bigram Features for Named Entity Recognition",
    "volume": "findings",
    "abstract": "Transformer models fine-tuned with a sequence labeling objective have become the dominant choice for named entity recognition tasks. However, a self-attention mechanism with unconstrained length can fail to fully capture local dependencies, particularly when training data is limited. In this paper, we propose a novel joint training objective which better captures the semantics of words corresponding to the same entity. By augmenting the training objective with a group-consistency loss component we enhance our ability to capture local dependencies while still enjoying the advantages of the unconstrained self-attention mechanism. On the CoNLL2003 dataset, our method achieves a test F1 of 93.98 with a single transformer model. More importantly our fine-tuned CoNLL2003 model displays significant gains in generalization to out of domain datasets: on the OntoNotes subset we achieve an F1 of 72.67 which is 0.49 points absolute better than the baseline, and on the WNUT16 set an F1 of 68.22 which is a gain of 0.48 points. Furthermore, on the WNUT17 dataset we achieve an F1 of 55.85, yielding a 2.92 point absolute improvement",
    "checked": true,
    "id": "266e16487ff6a4db3738629db76e77d8cb413817",
    "semantic_title": "cnnbif: cnn-based bigram features for named entity recognition",
    "citation_count": 6,
    "authors": [
      "Chul Sung",
      "Vaibhava Goel",
      "Etienne Marcheret",
      "Steven Rennie",
      "David Nahamoo"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.88": {
    "title": "Compositional Generalization via Semantic Tagging",
    "volume": "findings",
    "abstract": "Although neural sequence-to-sequence models have been successfully applied to semantic parsing, they fail at compositional generalization, i.e., they are unable to systematically generalize to unseen compositions of seen components. Motivated by traditional semantic parsing where compositionality is explicitly accounted for by symbolic grammars, we propose a new decoding framework that preserves the expressivity and generality of sequence-to-sequence models while featuring lexicon-style alignments and disentangled information processing. Specifically, we decompose decoding into two phases where an input utterance is first tagged with semantic symbols representing the meaning of individual words, and then a sequence-to-sequence model is used to predict the final meaning representation conditioning on the utterance and the predicted tag sequence. Experimental results on three semantic parsing datasets show that the proposed approach consistently improves compositional generalization across model architectures, domains, and semantic formalisms",
    "checked": true,
    "id": "cc0fc60e9b9d036acf53899c259574c8ce2aedfc",
    "semantic_title": "compositional generalization via semantic tagging",
    "citation_count": 35,
    "authors": [
      "Hao Zheng",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.89": {
    "title": "Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering",
    "volume": "findings",
    "abstract": "Paraphrase generation is an important task in natural language processing. Previous works focus on sentence-level paraphrase generation, while ignoring document-level paraphrase generation, which is a more challenging and valuable task. In this paper, we explore the task of document-level paraphrase generation for the first time and focus on the inter-sentence diversity by considering sentence rewriting and reordering. We propose CoRPG (Coherence Relationship guided Paraphrase Generation), which leverages graph GRU to encode the coherence relationship graph and get the coherence-aware representation for each sentence, which can be used for re-arranging the multiple (possibly modified) input sentences. We create a pseudo document-level paraphrase dataset for training CoRPG. Automatic evaluation results show CoRPG outperforms several strong baseline models on the BERTScore and diversity scores. Human evaluation also shows our model can generate document paraphrase with more diversity and semantic preservation",
    "checked": true,
    "id": "9b1263b047b13276f03030670da8175677102c74",
    "semantic_title": "towards document-level paraphrase generation with sentence rewriting and reordering",
    "citation_count": 13,
    "authors": [
      "Zhe Lin",
      "Yitao Cai",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.90": {
    "title": "Exploring Decomposition for Table-based Fact Verification",
    "volume": "findings",
    "abstract": "Fact verification based on structured data is challenging as it requires models to understand both natural language and symbolic operations performed over tables. Although pre-trained language models have demonstrated a strong capability in verifying simple statements, they struggle with complex statements that involve multiple operations. In this paper, we improve fact verification by decomposing complex statements into simpler subproblems. Leveraging the programs synthesized by a weakly supervised semantic parser, we propose a program-guided approach to constructing a pseudo dataset for decomposition model training. The subproblems, together with their predicted answers, serve as the intermediate evidence to enhance our fact verification model. Experiments show that our proposed approach achieves the new state-of-the-art performance, an 82.7% accuracy, on the TabFact benchmark",
    "checked": true,
    "id": "52babc837d175f0323d75b4f13cae829070daeba",
    "semantic_title": "exploring decomposition for table-based fact verification",
    "citation_count": 23,
    "authors": [
      "Xiaoyu Yang",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.91": {
    "title": "Diversity and Consistency: Exploring Visual Question-Answer Pair Generation",
    "volume": "findings",
    "abstract": "Although showing promising values to downstream applications, generating question and answer together is under-explored. In this paper, we introduce a novel task that targets question-answer pair generation from visual images. It requires not only generating diverse question-answer pairs but also keeping the consistency of them. We study different generation paradigms for this task and propose three models: the pipeline model, the joint model, and the sequential model. We integrate variational inference into these models to achieve diversity and consistency. We also propose region representation scaling and attention alignment to improve the consistency further. We finally devise an evaluator as a quantitative metric for consistency. We validate our approach on two benchmarks, VQA2.0 and Visual-7w, by automatically and manually evaluating diversity and consistency. Experimental results show the effectiveness of our models: they can generate diverse or consistent pairs. Moreover, this task can be used to improve visual question generation and visual question answering",
    "checked": true,
    "id": "4f0dec967abc9f24468ac30b112a56c5045ce58c",
    "semantic_title": "diversity and consistency: exploring visual question-answer pair generation",
    "citation_count": 6,
    "authors": [
      "Sen Yang",
      "Qingyu Zhou",
      "Dawei Feng",
      "Yang Liu",
      "Chao Li",
      "Yunbo Cao",
      "Dongsheng Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.92": {
    "title": "Entity-level Cross-modal Learning Improves Multi-modal Machine Translation",
    "volume": "findings",
    "abstract": "Multi-modal machine translation (MMT) aims at improving translation performance by incorporating visual information. Most of the studies leverage the visual information through integrating the global image features as auxiliary input or decoding by attending to relevant local regions of the image. However, this kind of usage of visual information makes it difficult to figure out how the visual modality helps and why it works. Inspired by the findings of (CITATION) that entities are most informative in the image, we propose an explicit entity-level cross-modal learning approach that aims to augment the entity representation. Specifically, the approach is framed as a reconstruction task that reconstructs the original textural input from multi-modal input in which entities are replaced with visual features. Then, a multi-task framework is employed to combine the translation task and the reconstruction task to make full use of cross-modal entity representation learning. The extensive experiments demonstrate that our approach can achieve comparable or even better performance than state-of-the-art models. Furthermore, our in-depth analysis shows how visual information improves translation",
    "checked": true,
    "id": "7dcc6a44da042bbf87c3c958e10b2f6ea19347df",
    "semantic_title": "entity-level cross-modal learning improves multi-modal machine translation",
    "citation_count": 9,
    "authors": [
      "Xin Huang",
      "Jiajun Zhang",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.93": {
    "title": "Learning to Ground Visual Objects for Visual Dialog",
    "volume": "findings",
    "abstract": "Visual dialog is challenging since it needs to answer a series of coherent questions based on understanding the visual environment. How to ground related visual objects is one of the key problems. Previous studies utilize the question and history to attend to the image and achieve satisfactory performance, while these methods are not sufficient to locate related visual objects without any guidance. The inappropriate grounding of visual objects prohibits the performance of visual dialog models. In this paper, we propose a novel approach to Learn to Ground visual objects for visual dialog, which employs a novel visual objects grounding mechanism where both prior and posterior distributions over visual objects are used to facilitate visual objects grounding. Specifically, a posterior distribution over visual objects is inferred from both context (history and questions) and answers, and it ensures the appropriate grounding of visual objects during the training process. Meanwhile, a prior distribution, which is inferred from context only, is used to approximate the posterior distribution so that appropriate visual objects can be grounding even without answers during the inference process. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our approach improves the previous strong models in both generative and discriminative settings by a significant margin",
    "checked": true,
    "id": "5a98dd487fdf22bcb8787d514a48081b2e1a907f",
    "semantic_title": "learning to ground visual objects for visual dialog",
    "citation_count": 18,
    "authors": [
      "Feilong Chen",
      "Xiuyi Chen",
      "Can Xu",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.94": {
    "title": "KERS: A Knowledge-Enhanced Framework for Recommendation Dialog Systems with Multiple Subgoals",
    "volume": "findings",
    "abstract": "Recommendation dialogs require the system to build a social bond with users to gain trust and develop affinity in order to increase the chance of a successful recommendation. It is beneficial to divide up, such conversations with multiple subgoals (such as social chat, question answering, recommendation, etc.), so that the system can retrieve appropriate knowledge with better accuracy under different subgoals. In this paper, we propose a unified framework for common knowledge-based multi-subgoal dialog: knowledge-enhanced multi-subgoal driven recommender system (KERS). We first predict a sequence of subgoals and use them to guide the dialog model to select knowledge from a sub-set of existing knowledge graph. We then propose three new mechanisms to filter noisy knowledge and to enhance the inclusion of cleaned knowledge in the dialog response generation process. Experiments show that our method obtains state-of-the-art results on DuRecDial dataset in both automatic and human evaluation",
    "checked": true,
    "id": "9fd654a651bf3de96f0249510d2cd41927e65791",
    "semantic_title": "kers: a knowledge-enhanced framework for recommendation dialog systems with multiple subgoals",
    "citation_count": 32,
    "authors": [
      "Jun Zhang",
      "Yan Yang",
      "Chencai Chen",
      "Liang He",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.95": {
    "title": "Less Is More: Domain Adaptation with Lottery Ticket for Reading Comprehension",
    "volume": "findings",
    "abstract": "In this paper, we propose a simple few-shot domain adaptation paradigm for reading comprehension. We first identify the lottery subnetwork structure within the Transformer-based source domain model via gradual magnitude pruning. Then, we only fine-tune the lottery subnetwork, a small fraction of the whole parameters, on the annotated target domain data for adaptation. To obtain more adaptable subnetworks, we introduce self-attention attribution to weigh parameters, beyond simply pruning the smallest magnitude parameters, which can be seen as combining structured pruning and unstructured magnitude pruning softly. Experimental results show that our method outperforms the full model fine-tuning adaptation on four out of five domains when only a small amount of annotated data available for adaptation. Moreover, introducing self-attention attribution reserves more parameters for important attention heads in the lottery subnetwork and improves the target domain model performance. Our further analyses reveal that, besides exploiting fewer parameters, the choice of subnetworks is critical to the effectiveness",
    "checked": true,
    "id": "900be4c03e6103b9f94e8dfb2f5b3cf5ad830b6a",
    "semantic_title": "less is more: domain adaptation with lottery ticket for reading comprehension",
    "citation_count": 8,
    "authors": [
      "Haichao Zhu",
      "Zekun Wang",
      "Heng Zhang",
      "Ming Liu",
      "Sendong Zhao",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.96": {
    "title": "Effectiveness of Pre-training for Few-shot Intent Classification",
    "volume": "findings",
    "abstract": "This paper investigates the effectiveness of pre-training for few-shot intent classification. While existing paradigms commonly further pre-train language models such as BERT on a vast amount of unlabeled corpus, we find it highly effective and efficient to simply fine-tune BERT with a small set of labeled utterances from public datasets. Specifically, fine-tuning BERT with roughly 1,000 labeled data yields a pre-trained model – IntentBERT, which can easily surpass the performance of existing pre-trained models for few-shot intent classification on novel domains with very different semantics. The high effectiveness of IntentBERT confirms the feasibility and practicality of few-shot intent detection, and its high generalization ability across different domains suggests that intent classification tasks may share a similar underlying structure, which can be efficiently learned from a small set of labeled data. The source code can be found at https://github.com/hdzhang-code/IntentBERT",
    "checked": true,
    "id": "c80436a9d7ec5d832e4875c89829294fc25841fd",
    "semantic_title": "effectiveness of pre-training for few-shot intent classification",
    "citation_count": 45,
    "authors": [
      "Haode Zhang",
      "Yuwei Zhang",
      "Li-Ming Zhan",
      "Jiaxin Chen",
      "Guangyuan Shi",
      "Albert Y.S. Lam",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.97": {
    "title": "Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment",
    "volume": "findings",
    "abstract": "With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text summarization. However, the semantic structure and style of meeting transcripts are quite different from that of articles. In this work, we propose a hierarchical transformer encoder-decoder network with multi-task pre-training. Specifically, we mask key sentences at the word-level encoder and generate them at the decoder. Besides, we randomly mask some of the role alignments in the input text and force the model to recover the original role tags to complete the alignments. In addition, we introduce a topic segmentation mechanism to further improve the quality of the generated summaries. The experimental results show that our model is superior to the previous methods in meeting summary datasets AMI and ICSI",
    "checked": true,
    "id": "9dc59836186df055528f02455221bbceb61f6e41",
    "semantic_title": "improving abstractive dialogue summarization with hierarchical pretraining and topic segment",
    "citation_count": 16,
    "authors": [
      "MengNan Qi",
      "Hao Liu",
      "YuZhuo Fu",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.98": {
    "title": "Learning to Answer Psychological Questionnaire for Personality Detection",
    "volume": "findings",
    "abstract": "Existing text-based personality detection research mostly relies on data-driven approaches to implicitly capture personality cues in online posts, lacking the guidance of psychological knowledge. Psychological questionnaire, which contains a series of dedicated questions highly related to personality traits, plays a critical role in self-report personality assessment. We argue that the posts created by a user contain critical contents that could help answer the questions in a questionnaire, resulting in an assessment of his personality by linking the texts and the questionnaire. To this end, we propose a new model named Psychological Questionnaire enhanced Network (PQ-Net) to guide personality detection by tracking critical information in texts with a questionnaire. Specifically, PQ-Net contains two streams: a context stream to encode each piece of text into a contextual text representation, and a questionnaire stream to capture relevant information in the contextual text representation to generate potential answer representations for a questionnaire. The potential answer representations are used to enhance the contextual text representation and to benefit personality prediction. Experimental results on two datasets demonstrate the superiority of PQ-Net in capturing useful cues from the posts for personality detection",
    "checked": true,
    "id": "3d54742bdd05917d9ed6cd5392fe4ff88edb4d36",
    "semantic_title": "learning to answer psychological questionnaire for personality detection",
    "citation_count": 18,
    "authors": [
      "Feifan Yang",
      "Tao Yang",
      "Xiaojun Quan",
      "Qinliang Su"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.99": {
    "title": "Exploiting Reasoning Chains for Multi-hop Science Question Answering",
    "volume": "findings",
    "abstract": "We propose a novel Chain Guided Retriever-reader (CGR) framework to model the reasoning chain for multi-hop Science Question Answering. Our framework is capable of performing explainable reasoning without the need of any corpus-specific annotations, such as the ground-truth reasoning chain, or human-annotated entity mentions. Specifically, we first generate reasoning chains from a semantic graph constructed by Abstract Meaning Representation of retrieved evidence facts. A Chain-aware loss, concerning both local and global chain information, is also designed to enable the generated chains to serve as distant supervision signals for training the retriever, where reinforcement learning is also adopted to maximize the utility of the reasoning chains. Our framework allows the retriever to capture step-by-step clues of the entire reasoning process, which is not only shown to be effective on two challenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge, but also favors explainability",
    "checked": true,
    "id": "368ffb89988eef9898440569e5378d2cb18cddd2",
    "semantic_title": "exploiting reasoning chains for multi-hop science question answering",
    "citation_count": 22,
    "authors": [
      "Weiwen Xu",
      "Yang Deng",
      "Huihui Zhang",
      "Deng Cai",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.100": {
    "title": "Winnowing Knowledge for Multi-choice Question Answering",
    "volume": "findings",
    "abstract": "We tackle multi-choice question answering. Acquiring related commonsense knowledge to the question and options facilitates the recognition of the correct answer. However, the current reasoning models suffer from the noises in the retrieved knowledge. In this paper, we propose a novel encoding method which is able to conduct interception and soft filtering. This contributes to the harvesting and absorption of representative information with less interference from noises. We experiment on CommonsenseQA. Experimental results illustrate that our method yields substantial and consistent improvements compared to the strong Bert, RoBERTa and Albert-based baselines",
    "checked": true,
    "id": "58a7934ed2aa05c1c48436680806ab35cfda1bd2",
    "semantic_title": "winnowing knowledge for multi-choice question answering",
    "citation_count": 7,
    "authors": [
      "Yeqiu Li",
      "Bowei Zou",
      "Zhifeng Li",
      "Ai Ti Aw",
      "Yu Hong",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.101": {
    "title": "Neural Media Bias Detection Using Distant Supervision With BABE - Bias Annotations By Experts",
    "volume": "findings",
    "abstract": "Media coverage has a substantial effect on the public perception of events. Nevertheless, media outlets are often biased. One way to bias news articles is by altering the word choice. The automatic identification of bias by word choice is challenging, primarily due to the lack of a gold standard data set and high context dependencies. This paper presents BABE, a robust and diverse data set created by trained experts, for media bias research. We also analyze why expert labeling is essential within this domain. Our data set offers better annotation quality and higher inter-annotator agreement than existing work. It consists of 3,700 sentences balanced among topics and outlets, containing media bias labels on the word and sentence level. Based on our data, we also introduce a way to detect bias-inducing sentences in news articles automatically. Our best performing BERT-based model is pre-trained on a larger corpus consisting of distant labels. Fine-tuning and evaluating the model on our proposed supervised data set, we achieve a macro F1-score of 0.804, outperforming existing methods",
    "checked": true,
    "id": "e36caccdeb4d85842e46363b0938ef0235a8db6c",
    "semantic_title": "neural media bias detection using distant supervision with babe - bias annotations by experts",
    "citation_count": 72,
    "authors": [
      "Timo Spinde",
      "Manuel Plank",
      "Jan-David Krieger",
      "Terry Ruas",
      "Bela Gipp",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.102": {
    "title": "Learning and Evaluating a Differentially Private Pre-trained Language Model",
    "volume": "findings",
    "abstract": "Contextual language models have led to significantly better results, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private language model, but this usually comes at the expense of model performance. Also, in the absence of a differentially private vocabulary training, it is not possible to modify the vocabulary to fit the new data, which might further degrade results. In this work we bridge these gaps, and provide guidance to future researchers and practitioners on how to improve privacy while maintaining good model performance. We introduce a novel differentially private word-piece algorithm, which allows training a tailored domain-specific vocabulary while maintaining privacy. We then experiment with entity extraction tasks from clinical notes, and demonstrate how to train a differentially private pre-trained language model (i.e., BERT) with a privacy guarantee of 𝜖=1.1 and with only a small degradation in performance. Finally, as it is hard to tell given a privacy parameter 𝜖 what was the effect on the trained representation, we present experiments showing that the trained model does not memorize private information",
    "checked": true,
    "id": "c3b597011f64e7c5459bbe4502163e463fb13f5a",
    "semantic_title": "learning and evaluating a differentially private pre-trained language model",
    "citation_count": 73,
    "authors": [
      "Shlomo Hoory",
      "Amir Feder",
      "Avichai Tendler",
      "Sofia Erell",
      "Alon Peled-Cohen",
      "Itay Laish",
      "Hootan Nakhost",
      "Uri Stemmer",
      "Ayelet Benjamini",
      "Avinatan Hassidim",
      "Yossi Matias"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.103": {
    "title": "Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions",
    "volume": "findings",
    "abstract": "Popular dialog datasets such as MultiWOZ are created by providing crowd workers an instruction, expressed in natural language, that describes the task to be accomplished. Crowd workers play the role of a user and an agent to generate dialogs to accomplish tasks involving booking restaurant tables, calling a taxi etc. In this paper, we present a data creation strategy that uses the pre-trained language model, GPT2, to simulate the interaction between crowd workers by creating a user bot and an agent bot. We train the simulators using a smaller percentage of actual crowd-generated conversations and their corresponding instructions. We demonstrate that by using the simulated data, we achieve significant improvements in low-resource settings on two publicly available datasets - MultiWOZ dataset and the Persona chat dataset",
    "checked": true,
    "id": "0f3596364943bb03f14cd75d9595a2c465831edb",
    "semantic_title": "simulated chats for building dialog systems: learning to generate conversations from instructions",
    "citation_count": 29,
    "authors": [
      "Biswesh Mohapatra",
      "Gaurav Pandey",
      "Danish Contractor",
      "Sachindra Joshi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.104": {
    "title": "Past, Present, and Future: Conversational Emotion Recognition through Structural Modeling of Psychological Knowledge",
    "volume": "findings",
    "abstract": "Conversational Emotion Recognition (CER) is a task to predict the emotion of an utterance in the context of a conversation. Although modeling the conversational context and interactions between speakers has been studied broadly, it is important to consider the speaker's psychological state, which controls the action and intention of the speaker. The state-of-the-art method introduces CommonSense Knowledge (CSK) to model psychological states in a sequential way (forwards and backwards). However, it ignores the structural psychological interactions between utterances. In this paper, we propose a pSychological-Knowledge-Aware Interaction Graph (SKAIG). In the locally connected graph, the targeted utterance will be enhanced with the information of action inferred from the past context and intention implied by the future context. The utterance is self-connected to consider the present effect from itself. Furthermore, we utilize CSK to enrich edges with knowledge representations and process the SKAIG with a graph transformer. Our method achieves state-of-the-art and competitive performance on four popular CER datasets",
    "checked": true,
    "id": "1c2d2966f0f7bbb1c4abb735a7c8f7ec802645fb",
    "semantic_title": "past, present, and future: conversational emotion recognition through structural modeling of psychological knowledge",
    "citation_count": 93,
    "authors": [
      "Jiangnan Li",
      "Zheng Lin",
      "Peng Fu",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.105": {
    "title": "An unsupervised framework for tracing textual sources of moral change",
    "volume": "findings",
    "abstract": "Morality plays an important role in social well-being, but people's moral perception is not stable and changes over time. Recent advances in natural language processing have shown that text is an effective medium for informing moral change, but no attempt has been made to quantify the origins of these changes. We present a novel unsupervised framework for tracing textual sources of moral change toward entities through time. We characterize moral change with probabilistic topical distributions and infer the source text that exerts prominent influence on the moral time course. We evaluate our framework on a diverse set of data ranging from social media to news articles. We show that our framework not only captures fine-grained human moral judgments, but also identifies coherent source topics of moral change triggered by historical events. We apply our methodology to analyze the news in the COVID-19 pandemic and demonstrate its utility in identifying sources of moral change in high-impact and real-time social events",
    "checked": true,
    "id": "c6935f3f839bf36a406f55aa807df14b771f4c94",
    "semantic_title": "an unsupervised framework for tracing textual sources of moral change",
    "citation_count": 11,
    "authors": [
      "Aida Ramezani",
      "Zining Zhu",
      "Frank Rudzicz",
      "Yang Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.106": {
    "title": "Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization",
    "volume": "findings",
    "abstract": "Unlike well-structured text, such as news reports and encyclopedia articles, dialogue content often comes from two or more interlocutors, exchanging information with each other. In such a scenario, the topic of a conversation can vary upon progression and the key information for a certain topic is often scattered across multiple utterances of different speakers, which poses challenges to abstractly summarize dialogues. To capture the various topic information of a conversation and outline salient facts for the captured topics, this work proposes two topic-aware contrastive learning objectives, namely coherence detection and sub-summary generation objectives, which are expected to implicitly model the topic change and handle information scattering challenges for the dialogue summarization task. The proposed contrastive objectives are framed as auxiliary tasks for the primary dialogue summarization task, united via an alternative parameter updating strategy. Extensive experiments on benchmark datasets demonstrate that the proposed simple method significantly outperforms strong baselines and achieves new state-of-the-art performance. The code and trained models are publicly available via",
    "checked": true,
    "id": "bdcced842278f8286234110670c17d5db74df81c",
    "semantic_title": "topic-aware contrastive learning for abstractive dialogue summarization",
    "citation_count": 66,
    "authors": [
      "Junpeng Liu",
      "Yanyan Zou",
      "Hainan Zhang",
      "Hongshen Chen",
      "Zhuoye Ding",
      "Caixia Yuan",
      "Xiaojie Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.107": {
    "title": "TWT: Table with Written Text for Controlled Data-to-Text Generation",
    "volume": "findings",
    "abstract": "Large pre-trained neural models have recently shown remarkable progress in text generation. In this paper, we propose to generate text conditioned on the structured data (table) and a prefix (the written text) by leveraging the pre-trained models. We present a new data-to-text dataset, Table with Written Text (TWT), by repurposing two existing datasets: ToTTo and TabFact. TWT contains both factual and logical statements that are faithful to the structured data, aiming to serve as a useful benchmark for controlled text generation. Compared with existing data-to-text task settings, TWT is more intuitive, the prefix (usually provided by the user) controls the topic of the generated text. Existing methods usually output hallucinated text that is not faithful on TWT. Therefore, we design a novel approach with table-aware attention visibility and copy mechanism over the table. Experimental results show that our approach outperforms state-of-the-art methods under both automatic and human evaluation metrics",
    "checked": true,
    "id": "1150959eb76cc2438b6996584621958efc9fd08e",
    "semantic_title": "twt: table with written text for controlled data-to-text generation",
    "citation_count": 11,
    "authors": [
      "Tongliang Li",
      "Lei Fang",
      "Jian-Guang Lou",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.108": {
    "title": "ArabicTransformer: Efficient Large Arabic Language Model with Funnel Transformer and ELECTRA Objective",
    "volume": "findings",
    "abstract": "Pre-training Transformer-based models such as BERT and ELECTRA on a collection of Arabic corpora, demonstrated by both AraBERT and AraELECTRA, shows an impressive result on downstream tasks. However, pre-training Transformer-based language models is computationally expensive, especially for large-scale models. Recently, Funnel Transformer has addressed the sequential redundancy inside Transformer architecture by compressing the sequence of hidden states, leading to a significant reduction in the pre-training cost. This paper empirically studies the performance and efficiency of building an Arabic language model with Funnel Transformer and ELECTRA objective. We find that our model achieves state-of-the-art results on several Arabic downstream tasks despite using less computational resources compared to other BERT-based models",
    "checked": true,
    "id": "2245e0c88f8b992d81bb34636813dce562ad4722",
    "semantic_title": "arabictransformer: efficient large arabic language model with funnel transformer and electra objective",
    "citation_count": 5,
    "authors": [
      "Sultan Alrowili",
      "Vijay Shanker"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.109": {
    "title": "Which is Making the Contribution: Modulating Unimodal and Cross-modal Dynamics for Multimodal Sentiment Analysis",
    "volume": "findings",
    "abstract": "Multimodal sentiment analysis (MSA) draws increasing attention with the availability of multimodal data. The boost in performance of MSA models is mainly hindered by two problems. On the one hand, recent MSA works mostly focus on learning cross-modal dynamics, but neglect to explore an optimal solution for unimodal networks, which determines the lower limit of MSA models. On the other hand, noisy information hidden in each modality interferes the learning of correct cross-modal dynamics. To address the above-mentioned problems, we propose a novel MSA framework Modulation Model for Multimodal Sentiment Analysis (M3SA) to identify the contribution of modalities and reduce the impact of noisy information, so as to better learn unimodal and cross-modal dynamics. Specifically, modulation loss is designed to modulate the loss contribution based on the confidence of individual modalities in each utterance, so as to explore an optimal update solution for each unimodal network. Besides, contrary to most existing works which fail to explicitly filter out noisy information, we devise a modality filter module to identify and filter out modality noise for the learning of correct cross-modal embedding. Extensive experiments on publicly datasets demonstrate that our approach achieves state-of-the-art performance",
    "checked": true,
    "id": "b41b4328193440a161d63aa693b2cb03c91b657e",
    "semantic_title": "which is making the contribution: modulating unimodal and cross-modal dynamics for multimodal sentiment analysis",
    "citation_count": 19,
    "authors": [
      "Ying Zeng",
      "Sijie Mai",
      "Haifeng Hu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.110": {
    "title": "CVAE-based Re-anchoring for Implicit Discourse Relation Classification",
    "volume": "findings",
    "abstract": "Training implicit discourse relation classifiers suffers from data sparsity. Variational AutoEncoder (VAE) appears to be the proper solution. It is because ideally VAE is capable of generating inexhaustible varying samples, and this facilitates selective data augmentation. However, our experiments show that coupling VAE with the RoBERTa-based classifier results in severe performance degradation. We ascribe the unusual phenomenon to erroneous sampling that would happen when VAE pursued variations. To overcome the problem, we develop a re-anchoring strategy, where Conditional VAE (CVAE) is used for estimating the risk of erroneous sampling, and meanwhile migrating the anchor to reduce the risk. The test results on PDTB v2.0 illustrate that, compared to the RoBERTa-based baseline, re-anchoring yields substantial improvements. Besides, we observe that re-anchoring can cooperate with other auxiliary strategies (transfer learning and interactive attention mechanism) to further improve the baseline, obtaining the F-scores of about 55%, 63%, 80% and 44% for the four main relation types (Comparison, Contingency, Expansion, Temporality) in the binary classification (Yes/No) scenario",
    "checked": true,
    "id": "01d66a58d42b793ba9ffd2522fcd96ae4c87700b",
    "semantic_title": "cvae-based re-anchoring for implicit discourse relation classification",
    "citation_count": 13,
    "authors": [
      "Zujun Dou",
      "Yu Hong",
      "Yu Sun",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.111": {
    "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation",
    "volume": "findings",
    "abstract": "Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facilitate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield significant performance boost for student models. Hence, in this paper, we introduce a combination of curriculum learning and knowledge distillation for efficient dialogue generation models, where curriculum learning can help knowledge distillation from data and model aspects. To start with, from the data aspect, we cluster the training cases according to their complexity, which is calculated by various types of features such as sentence length and coherence between dialog pairs. Furthermore, we employ an adversarial training strategy to identify the complexity of cases from model level. The intuition is that, if a discriminator can tell the generated response is from the teacher or the student, then the case is difficult that the student model has not adapted to yet. Finally, we use self-paced learning, which is an extension to curriculum learning to assign weights for distillation. In conclusion, we arrange a hierarchical curriculum based on the above two aspects for the student model under the guidance from the teacher model. Experimental results demonstrate that our methods achieve improvements compared with competitive baselines",
    "checked": true,
    "id": "ecd12726118b95117bf339ad069c0a87a102cbcc",
    "semantic_title": "combining curriculum learning and knowledge distillation for dialogue generation",
    "citation_count": 27,
    "authors": [
      "Qingqing Zhu",
      "Xiuying Chen",
      "Pengfei Wu",
      "JunFei Liu",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.112": {
    "title": "Improving End-to-End Task-Oriented Dialog System with A Simple Auxiliary Task",
    "volume": "findings",
    "abstract": "The paradigm of leveraging large pre-trained language models has made significant progress on benchmarks on task-oriented dialogue (TOD) systems. In this paper, we combine this paradigm with multi-task learning framework for end-to-end TOD modeling by adopting span prediction as an auxiliary task. In end-to-end setting, our model achieves new state-of-the-art results with combined scores of 108.3 and 107.5 on MultiWOZ 2.0 and MultiWOZ 2.1, respectively. Furthermore, we demonstrate that multi-task learning improves not only the performance of model but its generalization capability through domain adaptation experiments in the few-shot setting. The code is available at github.com/bepoetree/MTTOD",
    "checked": true,
    "id": "307e3e00c96b3dc62a8258c95e62b3e41d183278",
    "semantic_title": "improving end-to-end task-oriented dialog system with a simple auxiliary task",
    "citation_count": 71,
    "authors": [
      "Yohan Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.113": {
    "title": "EDTC: A Corpus for Discourse-Level Topic Chain Parsing",
    "volume": "findings",
    "abstract": "Discourse analysis has long been known to be fundamental in natural language processing. In this research, we present our insight on discourse-level topic chain (DTC) parsing which aims at discovering new topics and investigating how these topics evolve over time within an article. To address the lack of data, we contribute a new discourse corpus with DTC-style dependency graphs annotated upon news articles. In particular, we ensure the high reliability of the corpus by utilizing a two-step annotation strategy to build the data and filtering out the annotations with low confidence scores. Based on the annotated corpus, we introduce a simple yet robust system for automatic discourse-level topic chain parsing",
    "checked": true,
    "id": "2b92cb35d5eb10279e346216a999526cc041e1ab",
    "semantic_title": "edtc: a corpus for discourse-level topic chain parsing",
    "citation_count": 0,
    "authors": [
      "Longyin Zhang",
      "Xin Tan",
      "Fang Kong",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.114": {
    "title": "Multilingual Neural Machine Translation: Can Linguistic Hierarchies Help?",
    "volume": "findings",
    "abstract": "Multilingual Neural Machine Translation (MNMT) trains a single NMT model that supports translation between multiple languages, rather than training separate models for different languages. Learning a single model can enhance the low-resource translation by leveraging data from multiple languages. However, the performance of an MNMT model is highly dependent on the type of languages used in training, as transferring knowledge from a diverse set of languages degrades the translation performance due to negative transfer. In this paper, we propose a Hierarchical Knowledge Distillation (HKD) approach for MNMT which capitalises on language groups generated according to typological features and phylogeny of languages to overcome the issue of negative transfer. HKD generates a set of multilingual teacher-assistant models via a selective knowledge distillation mechanism based on the language groups, and then distills the ultimate multilingual model from those assistants in an adaptive way. Experimental results derived from the TED dataset with 53 languages demonstrate the effectiveness of our approach in avoiding the negative transfer effect in MNMT, leading to an improved translation performance (about 1 BLEU score in average) compared to strong baselines",
    "checked": true,
    "id": "d4d076eb456840c098d6b52feadba131f02adb47",
    "semantic_title": "multilingual neural machine translation: can linguistic hierarchies help?",
    "citation_count": 6,
    "authors": [
      "Fahimeh Saleh",
      "Wray Buntine",
      "Gholamreza Haffari",
      "Lan Du"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.115": {
    "title": "Self Question-answering: Aspect-based Sentiment Analysis by Role Flipped Machine Reading Comprehension",
    "volume": "findings",
    "abstract": "The pivot for the unified Aspect-based Sentiment Analysis (ABSA) is to couple aspect terms with their corresponding opinion terms, which might further derive easier sentiment predictions. In this paper, we investigate the unified ABSA task from the perspective of Machine Reading Comprehension (MRC) by observing that the aspect and the opinion terms can serve as the query and answer in MRC interchangeably. We propose a new paradigm named Role Flipped Machine Reading Comprehension (RF-MRC) to resolve. At its heart, the predicted results of either the Aspect Term Extraction (ATE) or the Opinion Terms Extraction (OTE) are regarded as the queries, respectively, and the matched opinion or aspect terms are considered as answers. The queries and answers can be flipped for multi-hop detection. Finally, every matched aspect-opinion pair is predicted by the sentiment classifier. RF-MRC can solve the ABSA task without any additional data annotation or transformation. Experiments on three widely used benchmarks and a challenging dataset demonstrate the superiority of the proposed framework",
    "checked": true,
    "id": "04c186f8f3298e3c9f7a3db560e5815cec35769f",
    "semantic_title": "self question-answering: aspect-based sentiment analysis by role flipped machine reading comprehension",
    "citation_count": 14,
    "authors": [
      "Guoxin Yu",
      "Jiwei Li",
      "Ling Luo",
      "Yuxian Meng",
      "Xiang Ao",
      "Qing He"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.116": {
    "title": "Generalization in Text-based Games via Hierarchical Reinforcement Learning",
    "volume": "findings",
    "abstract": "Deep reinforcement learning provides a promising approach for text-based games in studying natural language communication between humans and artificial agents. However, the generalization still remains a big challenge as the agents depend critically on the complexity and variety of training tasks. In this paper, we address this problem by introducing a hierarchical framework built upon the knowledge graph-based RL agent. In the high level, a meta-policy is executed to decompose the whole game into a set of subtasks specified by textual goals, and select one of them based on the KG. Then a sub-policy in the low level is executed to conduct goal-conditioned reinforcement learning. We carry out experiments on games with various difficulty levels and show that the proposed method enjoys favorable generalizability",
    "checked": true,
    "id": "f8717dd893b150977971cadaf893615770e2a251",
    "semantic_title": "generalization in text-based games via hierarchical reinforcement learning",
    "citation_count": 21,
    "authors": [
      "Yunqiu Xu",
      "Meng Fang",
      "Ling Chen",
      "Yali Du",
      "Chengqi Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.117": {
    "title": "A Finer-grain Universal Dialogue Semantic Structures based Model For Abstractive Dialogue Summarization",
    "volume": "findings",
    "abstract": "Although abstractive summarization models have achieved impressive results on document summarization tasks, their performance on dialogue modeling is much less satisfactory due to the crude and straight methods for dialogue encoding. To address this question, we propose a novel end-to-end Transformer-based model FinDS for abstractive dialogue summarization that leverages Finer-grain universal Dialogue semantic Structures to model dialogue and generates better summaries. Experiments on the SAMsum dataset show that FinDS outperforms various dialogue summarization approaches and achieves new state-of-the-art (SOTA) ROUGE results. Finally, we apply FinDS to a more complex scenario, showing the robustness of our model. We also release our source code",
    "checked": true,
    "id": "eb8806241c128decba9ff1965eef7e2e017a98ba",
    "semantic_title": "a finer-grain universal dialogue semantic structures based model for abstractive dialogue summarization",
    "citation_count": 11,
    "authors": [
      "Yuejie Lei",
      "Fujia Zheng",
      "Yuanmeng Yan",
      "Keqing He",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.118": {
    "title": "Constructing contrastive samples via summarization for text classification with limited annotations",
    "volume": "findings",
    "abstract": "Contrastive Learning has emerged as a powerful representation learning method and facilitates various downstream tasks especially when supervised data is limited. How to construct efficient contrastive samples through data augmentation is key to its success. Unlike vision tasks, the data augmentation method for contrastive learning has not been investigated sufficiently in language tasks. In this paper, we propose a novel approach to construct contrastive samples for language tasks using text summarization. We use these samples for supervised contrastive learning to gain better text representations which greatly benefit text classification tasks with limited annotations. To further improve the method, we mix up samples from different classes and add an extra regularization, named Mixsum, in addition to the cross-entropy-loss. Experiments on real-world text classification datasets (Amazon-5, Yelp-5, AG News, and IMDb) demonstrate the effectiveness of the proposed contrastive learning framework with summarization-based data augmentation and Mixsum regularization",
    "checked": true,
    "id": "77583edbeac7fb4f5056243a9fd978b08ccb87ab",
    "semantic_title": "constructing contrastive samples via summarization for text classification with limited annotations",
    "citation_count": 10,
    "authors": [
      "Yangkai Du",
      "Tengfei Ma",
      "Lingfei Wu",
      "Fangli Xu",
      "Xuhong Zhang",
      "Bo Long",
      "Shouling Ji"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.119": {
    "title": "End-to-end Neural Information Status Classification",
    "volume": "findings",
    "abstract": "Most previous studies on information status (IS) classification and bridging anaphora recognition assume that the gold mention or syntactic tree information is given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio, 2020). In this paper, we propose an end-to-end neural approach for information status classification. Our approach consists of a mention extraction component and an information status assignment component. During the inference time, our system takes a raw text as the input and generates mentions together with their information status. On the ISNotes corpus (Markert et al., 2012), we show that our information status assignment component achieves new state-of-the-art results on fine-grained IS classification based on gold mentions. Furthermore, our system performs significantly better than other baselines for both mention extraction and fine-grained IS classification in the end-to-end setting. Finally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger, 2016) to recognize referential bridging anaphora. We find that our end-to-end system trained on ISNotes achieves competitive results on bridging anaphora recognition compared to the previous state-of-the-art system that relies on syntactic information and is trained on the in-domain datasets (Yu and Poesio, 2020)",
    "checked": true,
    "id": "8bbd3a0486a192ebfc1106ae21b377e62f0308be",
    "semantic_title": "end-to-end neural information status classification",
    "citation_count": 6,
    "authors": [
      "Yufang Hou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.120": {
    "title": "EventKE: Event-Enhanced Knowledge Graph Embedding",
    "volume": "findings",
    "abstract": "Relations in most of the traditional knowledge graphs (KGs) only reflect static and factual connections, but fail to represent the dynamic activities and state changes about entities. In this paper, we emphasize the importance of incorporating events in KG representation learning, and propose an event-enhanced KG embedding model EventKE. Specifically, given the original KG, we first incorporate event nodes by building a heterogeneous network, where entity nodes and event nodes are distributed on the two sides of the network inter-connected by event argument links. We then use entity-entity relations from the original KG and event-event temporal links to inner-connect entity and event nodes respectively. We design a novel and effective attention-based message passing method, which is conducted on entity-entity, event-entity, and event-event relations to fuse the event information into KG embeddings. Experimental results on real-world datasets demonstrate that events can greatly improve the quality of the KG embeddings on multiple downstream tasks",
    "checked": true,
    "id": "6f08716480ee5ce761267fbe649cf8c5d436f636",
    "semantic_title": "eventke: event-enhanced knowledge graph embedding",
    "citation_count": 4,
    "authors": [
      "Zixuan Zhang",
      "Hongwei Wang",
      "Han Zhao",
      "Hanghang Tong",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.121": {
    "title": "Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model",
    "volume": "findings",
    "abstract": "Cross-attention is an important component of neural machine translation (NMT), which is always realized by dot-product attention in previous methods. However, dot-product attention only considers the pair-wise correlation between words, resulting in dispersion when dealing with long sentences and neglect of source neighboring relationships. Inspired by linguistics, the above issues are caused by ignoring a type of cross-attention, called concentrated attention, which focuses on several central words and then spreads around them. In this work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention in cross-attention. Experiments and analyses we conducted on three datasets show that the proposed method outperforms the baseline and has significant improvement on alignment quality, N-gram accuracy, and long sentence translation",
    "checked": true,
    "id": "23d11338be48471b3979b13eb172ec67fc22244b",
    "semantic_title": "modeling concentrated cross-attention for neural machine translation with gaussian mixture model",
    "citation_count": 23,
    "authors": [
      "Shaolei Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.122": {
    "title": "Inconsistency Matters: A Knowledge-guided Dual-inconsistency Network for Multi-modal Rumor Detection",
    "volume": "findings",
    "abstract": "Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though a set of rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent relationships among images and texts. Moreover, they also fail to find a powerful way to spot the inconsistency information among the post contents and background knowledge. Motivated by the intuition that rumors are more likely to have inconsistency information in semantics, a novel Knowledge-guided Dual-inconsistency network is proposed to detect rumors with multimedia contents. It can capture the inconsistent semantics at the cross-modal level and the content-knowledge level in one unified framework. Extensive experiments on two public real-world datasets demonstrate that our proposal can outperform the state-of-the-art baselines",
    "checked": true,
    "id": "6c81a4ccfa21342b2a7533c8be1182c14c1774b1",
    "semantic_title": "inconsistency matters: a knowledge-guided dual-inconsistency network for multi-modal rumor detection",
    "citation_count": 21,
    "authors": [
      "Mengzhu Sun",
      "Xi Zhang",
      "Jianqiang Ma",
      "Yazheng Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.123": {
    "title": "EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation",
    "volume": "findings",
    "abstract": "Pre-trained language models have shown remarkable results on various NLP tasks. Nevertheless, due to their bulky size and slow inference speed, it is hard to deploy them on edge devices. In this paper, we have a critical insight that improving the feed-forward network (FFN) in BERT has a higher gain than improving the multi-head attention (MHA) since the computational cost of FFN is 2~3 times larger than MHA. Hence, to compact BERT, we are devoted to designing efficient FFN as opposed to previous works that pay attention to MHA. Since FFN comprises a multilayer perceptron (MLP) that is essential in BERT optimization, we further design a thorough search space towards an advanced MLP and perform a coarse-to-fine mechanism to search for an efficient BERT architecture. Moreover, to accelerate searching and enhance model transferability, we employ a novel warm-up knowledge distillation strategy at each search stage. Extensive experiments show our searched EfficientBERT is 6.9× smaller and 4.4× faster than BERTBASE, and has competitive performances on GLUE and SQuAD Benchmarks. Concretely, EfficientBERT attains a 77.7 average score on GLUE test, 0.7 higher than MobileBERTTINY, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0 dev, 3.2/2.7 higher than TinyBERT4 even without data augmentation. The code is released at https://github.com/cheneydon/efficient-bert",
    "checked": true,
    "id": "e095e7cdff07748f656740c9245b75ab7fd13b9c",
    "semantic_title": "efficientbert: progressively searching multilayer perceptron via warm-up knowledge distillation",
    "citation_count": 28,
    "authors": [
      "Chenhe Dong",
      "Guangrun Wang",
      "Hang Xu",
      "Jiefeng Peng",
      "Xiaozhe Ren",
      "Xiaodan Liang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.124": {
    "title": "Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework for Model Training and Online Serving",
    "volume": "findings",
    "abstract": "News recommendation techniques can help users on news platforms obtain their preferred news information. Most existing news recommendation methods rely on centrally stored user behavior data to train models and serve users. However, user data is usually highly privacy-sensitive, and centrally storing them in the news platform may raise privacy concerns and risks. In this paper, we propose a unified news recommendation framework, which can utilize user data locally stored in user clients to train models and serve users in a privacy-preserving way. Following a widely used paradigm in real-world recommender systems, our framework contains a stage for candidate news generation (i.e., recall) and a stage for candidate news ranking (i.e., ranking). At the recall stage, each client locally learns multiple interest representations from clicked news to comprehensively model user interests. These representations are uploaded to the server to recall candidate news from a large news pool, which are further distributed to the user client at the ranking stage for personalized news display. In addition, we propose an interest decomposer-aggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy",
    "checked": true,
    "id": "28256d719394fd633b8c6ab450a7730780ec76bb",
    "semantic_title": "uni-fedrec: a unified privacy-preserving news recommendation framework for model training and online serving",
    "citation_count": 23,
    "authors": [
      "Tao Qi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Yongfeng Huang",
      "Xing Xie"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.125": {
    "title": "Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning",
    "volume": "findings",
    "abstract": "Mapping natural language instructions to programs that computers can process is a fundamental challenge. Existing approaches focus on likelihood-based training or using reinforcement learning to fine-tune models based on a single reward. In this paper, we pose program generation from language as Inverse Reinforcement Learning. We introduce several interpretable reward components and jointly learn (1) a reward function that linearly combines them, and (2) a policy for program generation. Fine-tuning with our approach achieves significantly better performance than competitive methods using Reinforcement Learning (RL). On the VirtualHome framework, we get improvements of up to 9.0% on the Longest Common Subsequence metric and 14.7% on recall-based metrics over previous work on this framework (Puig et al., 2018). The approach is data-efficient, showing larger gains in performance in the low-data regime. Generated programs are also preferred by human evaluators over an RL-based approach, and rated higher on relevance, completeness, and human-likeness",
    "checked": true,
    "id": "0916d3112978bbe5f123553b5460ac1d05c6a8fd",
    "semantic_title": "mapping language to programs using multiple reward components with inverse reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Sayan Ghosh",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.126": {
    "title": "Topic-Guided Abstractive Multi-Document Summarization",
    "volume": "findings",
    "abstract": "A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that \"summarizes\" texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge scores and human evaluation, meanwhile learns high-quality topics",
    "checked": true,
    "id": "e7216f9e67904157ff5265921956fee5eb32f103",
    "semantic_title": "topic-guided abstractive multi-document summarization",
    "citation_count": 39,
    "authors": [
      "Peng Cui",
      "Le Hu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.127": {
    "title": "An Edge-Enhanced Hierarchical Graph-to-Tree Network for Math Word Problem Solving",
    "volume": "findings",
    "abstract": "Math word problem solving has attracted considerable research interest in recent years. Previous works have shown the effectiveness of utilizing graph neural networks to capture the relationships in the problem. However, these works did not carefully take the edge label information and the long-range word relationship across sentences into consideration. In addition, during generation, they focus on the most relevant areas of the currently generated word, while neglecting the rest of the problem. In this paper, we propose a novel Edge-Enhanced Hierarchical Graph-to-Tree model (EEH-G2T), in which the math word problems are represented as edge-labeled graphs. Specifically, an edge-enhanced hierarchical graph encoder is used to incorporate edge label information. This encoder updates the graph nodes hierarchically in two steps: sentence-level aggregation and problem-level aggregation. Furthermore, a tree-structured decoder with a split attention mechanism is applied to guide the model to pay attention to different parts of the input problem. Experimental results on the MAWPS and Math23K dataset showed that our EEH-G2T can effectively improve performance compared with state-of-the-art methods",
    "checked": true,
    "id": "494a102164b790a63dc99385b0e56d1e80e0a93c",
    "semantic_title": "an edge-enhanced hierarchical graph-to-tree network for math word problem solving",
    "citation_count": 33,
    "authors": [
      "Qinzhuo Wu",
      "Qi Zhang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.128": {
    "title": "SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation",
    "volume": "findings",
    "abstract": "Generating texts in scientific papers requires not only capturing the content contained within the given input but also frequently acquiring the external information called context. We push forward the scientific text generation by proposing a new task, namely context-aware text generation in the scientific domain, aiming at exploiting the contributions of context in generated texts. To this end, we present a novel challenging large-scale Scientific Paper Dataset for ConteXt-Aware Text Generation (SciXGen), consisting of well-annotated 205,304 papers with full references to widely-used objects (e.g., tables, figures, algorithms) in a paper. We comprehensively benchmark, using state-of-the-arts, the efficacy of our newly constructed SciXGen dataset in generating description and paragraph. Our dataset and benchmarks will be made publicly available to hopefully facilitate the scientific text generation research",
    "checked": true,
    "id": "adc13ef61e47628e9efcdca0c27654370e46dae5",
    "semantic_title": "scixgen: a scientific paper dataset for context-aware text generation",
    "citation_count": 19,
    "authors": [
      "Hong Chen",
      "Hiroya Takamura",
      "Hideki Nakayama"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.129": {
    "title": "Don't Miss the Potential Customers! Retrieving Similar Ads to Improve User Targeting",
    "volume": "findings",
    "abstract": "User targeting is an essential task in the modern advertising industry: given a package of ads for a particular category of products (e.g., green tea), identify the online users to whom the ad package should be targeted. A (ad package specific) user targeting model is typically trained using historical clickthrough data: positive instances correspond to users who have clicked on an ad in the package before, whereas negative instances correspond to users who have not clicked on any ads in the package that were displayed to them. Collecting a sufficient amount of positive training data for training an accurate user targeting model, however, is by no means trivial. This paper focuses on the development of a method for automatic augmentation of the set of positive training instances. Experimental results on two datasets, including a real-world company dataset, demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "bbae05adf47bcd194f701efd7b5b37ebc5dc795f",
    "semantic_title": "don't miss the potential customers! retrieving similar ads to improve user targeting",
    "citation_count": 0,
    "authors": [
      "Yi Feng",
      "Ting Wang",
      "Chuanyi Li",
      "Vincent Ng",
      "Jidong Ge",
      "Bin Luo",
      "Yucheng Hu",
      "Xiaopeng Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.130": {
    "title": "Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph",
    "volume": "findings",
    "abstract": "In cross-lingual text classification, it is required that task-specific training data in high-resource source languages are available, where the task is identical to that of a low-resource target language. However, collecting such training data can be infeasible because of the labeling cost, task characteristics, and privacy concerns. This paper proposes an alternative solution that uses only task-independent word embeddings of high-resource languages and bilingual dictionaries. First, we construct a dictionary-based heterogeneous graph (DHG) from bilingual dictionaries. This opens the possibility to use graph neural networks for cross-lingual transfer. The remaining challenge is the heterogeneity of DHG because multiple languages are considered. To address this challenge, we propose dictionary-based heterogeneous graph neural network (DHGNet) that effectively handles the heterogeneity of DHG by two-step aggregations, which are word-level and language-level aggregations. Experimental results demonstrate that our method outperforms pretrained models even though it does not access to large corpora. Furthermore, it can perform well even though dictionaries contain many incorrect translations. Its robustness allows the usage of a wider range of dictionaries such as an automatically constructed dictionary and crowdsourced dictionary, which are convenient for real-world applications",
    "checked": true,
    "id": "094e8c82eac318556b40b31d0121149a1b108269",
    "semantic_title": "cross-lingual transfer for text classification with dictionary-based heterogeneous graph",
    "citation_count": 4,
    "authors": [
      "Nuttapong Chairatanakul",
      "Noppayut Sriwatanasakdi",
      "Nontawat Charoenphakdee",
      "Xin Liu",
      "Tsuyoshi Murata"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.131": {
    "title": "Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning",
    "volume": "findings",
    "abstract": "Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of noise and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutually-beneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods",
    "checked": true,
    "id": "20af1061f27bb44dab8e09aeb80c07efc5e23ac7",
    "semantic_title": "improving distantly-supervised named entity recognition with self-collaborative denoising learning",
    "citation_count": 22,
    "authors": [
      "Xinghua Zhang",
      "Bowen Yu",
      "Tingwen Liu",
      "Zhenyu Zhang",
      "Jiawei Sheng",
      "Xue Mengge",
      "Hongbo Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.132": {
    "title": "Entity-Based Semantic Adequacy for Data-to-Text Generation",
    "volume": "findings",
    "abstract": "While powerful pre-trained language models have improved the fluency of text generation models, semantic adequacy -the ability to generate text that is semantically faithful to the input- remains an unsolved issue. In this paper, we introduce a novel automatic evaluation metric, Entity-Based Semantic Adequacy, which can be used to assess to what extent generation models that verbalise RDF (Resource Description Framework) graphs produce text that contains mentions of the entities occurring in the RDF input. This is important as RDF subject and object entities make up 2/3 of the input. We use our metric to compare 25 models from the WebNLG Shared Tasks and we examine correlation with results from human evaluations of semantic adequacy. We show that while our metric correlates with human evaluation scores, this correlation varies with the specifics of the human evaluation setup. This suggests that in order to measure the entity-based adequacy of generated texts, an automatic metric such as the one proposed here might be more reliable, as less subjective and more focused on correct verbalisation of the input, than human evaluation measures",
    "checked": true,
    "id": "5f5cb6d13a5e78a61178dad4f348b124830d1352",
    "semantic_title": "entity-based semantic adequacy for data-to-text generation",
    "citation_count": 8,
    "authors": [
      "Juliette Faille",
      "Albert Gatt",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.133": {
    "title": "MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization",
    "volume": "findings",
    "abstract": "One of the most challenging aspects of current single-document news summarization is that the summary often contains ‘extrinsic hallucinations', i.e., facts that are not present in the source document, which are often derived via world knowledge. This causes summarisation systems to act more like open-ended language models tending to hallucinate facts that are erroneous. In this paper, we mitigate this problem with the help of multiple supplementary resource documents assisting the task. We present a new dataset MiraNews and benchmark existing summarisation models. In contrast to multi-document summarization, which addresses multiple events from several source documents, we still aim at generating a summary for a single document. We show via data analysis that it's not only the models which are to blame: more than 27% of facts mentioned in the gold summaries of MiraNews are better grounded on assisting documents than in the main source articles. An error analysis of generated summaries from pretrained models fine-tuned on MIRANEWS reveals that this has an even bigger effects on models: assisted summarisation reduces 55% of hallucinations when compared to single-document summarisation models trained on the main article only",
    "checked": true,
    "id": "2e4139b609ef300b68aa52ebcf1dd217d71c2f2f",
    "semantic_title": "miranews: dataset and benchmarks for multi-resource-assisted news summarization",
    "citation_count": 6,
    "authors": [
      "Xinnuo Xu",
      "Ondřej Dušek",
      "Shashi Narayan",
      "Verena Rieser",
      "Ioannis Konstas"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.134": {
    "title": "A Conditional Generative Matching Model for Multi-lingual Reply Suggestion",
    "volume": "findings",
    "abstract": "We study the problem of multilingual automated reply suggestions (RS) model serving many languages simultaneously. Multilingual models are often challenged by model capacity and severe data distribution skew across languages. While prior works largely focus on monolingual models, we propose Conditional Generative Matching models (CGM), optimized within a Variational Autoencoder framework to address challenges arising from multilingual RS. CGM does so with expressive message conditional priors, mixture densities to enhance multilingual data representation, latent alignment for language discrimination, and effective variational optimization techniques for training multilingual RS. The enhancements result in performance that exceed competitive baselines in relevance (ROUGE score) by more than 10% on average, and 16%for low resource languages. CGM also shows remarkable improvements in diversity (80%) illustrating its expressiveness in representation of multi-lingual data",
    "checked": true,
    "id": "305204fc77037c9b8f25eb00b91fbc8b526af2b8",
    "semantic_title": "a conditional generative matching model for multi-lingual reply suggestion",
    "citation_count": 1,
    "authors": [
      "Budhaditya Deb",
      "Guoqing Zheng",
      "Milad Shokouhi",
      "Ahmed Hassan Awadallah"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.135": {
    "title": "Rethinking Sentiment Style Transfer",
    "volume": "findings",
    "abstract": "Though remarkable efforts have been made in non-parallel text style transfer, the evaluation system is unsatisfactory. It always evaluates over samples from only one checkpoint of the model and compares three metrics, i.e., transfer accuracy, BLEU score, and PPL score. In this paper, we argue the inappropriateness of both existing evaluation metrics and the evaluation method. Specifically, for evaluation metrics, we make a detailed analysis and comparison from three aspects: style transfer, content preservation, and naturalness; for the evaluation method, we reiterate the fallacy of picking one checkpoint for model comparison. As a result, we establish a robust evaluation method by examining the trade-off between style transfer and naturalness, and between content preservation and naturalness. Notably, we elaborate the human evaluation and automatically identify the inaccurate measurement of content preservation computed by the BLEU score. To overcome this issue, we propose a graph-based method to extract attribute content and attribute-independent content from input sentences in the YELP dataset and IMDB dataset. With the modified datasets, we design a new evaluation metric called \"attribute hit\" and propose an efficient regularization to leverage the attribute-dependent content and attribute-independent content as guiding signals. Experimental results have demonstrated the effectiveness of the proposed strategy",
    "checked": true,
    "id": "b3e484a80b876c8f62819e7694aaab19704ddaaa",
    "semantic_title": "rethinking sentiment style transfer",
    "citation_count": 6,
    "authors": [
      "Ping Yu",
      "Yang Zhao",
      "Chunyuan Li",
      "Changyou Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.136": {
    "title": "HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge",
    "volume": "findings",
    "abstract": "A hyperbole is an intentional and creative exaggeration not to be taken literally. Despite its ubiquity in daily life, the computational explorations of hyperboles are scarce. In this paper, we tackle the under-explored and challenging task: sentence-level hyperbole generation. We start with a representative syntactic pattern for intensification and systematically study the semantic (commonsense and counterfactual) relationships between each component in such hyperboles. We then leverage commonsense and counterfactual inference to generate hyperbole candidates based on our findings from the pattern, and train neural classifiers to rank and select high-quality hyperboles. Automatic and human evaluations show that our generation method is able to generate hyperboles with high success rate, intensity, funniness, and creativity",
    "checked": true,
    "id": "bb583aef8ebaf53db6d01e39143c7998ec2bfb94",
    "semantic_title": "hypogen: hyperbole generation with commonsense and counterfactual knowledge",
    "citation_count": 27,
    "authors": [
      "Yufei Tian",
      "Arvind krishna Sridhar",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.137": {
    "title": "Profiling News Discourse Structure Using Explicit Subtopic Structures Guided Critics",
    "volume": "findings",
    "abstract": "We present an actor-critic framework to induce subtopical structures in a news article for news discourse profiling. The model uses multiple critics that act according to known subtopic structures while the actor aims to outperform them. The content structures constitute sentences that represent latent subtopic boundaries. Then, we introduce a hierarchical neural network that uses the identified subtopic boundary sentences to model multi-level interaction between sentences, subtopics, and the document. Experimental results and analyses on the NewsDiscourse corpus show that the actor model learns to effectively segment a document into subtopics and improves the performance of the hierarchical model on the news discourse profiling task",
    "checked": true,
    "id": "e55b2dfadc8b758b6a5481da0c5d8e44cd0b156a",
    "semantic_title": "profiling news discourse structure using explicit subtopic structures guided critics",
    "citation_count": 9,
    "authors": [
      "Prafulla Kumar Choubey",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.138": {
    "title": "ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection",
    "volume": "findings",
    "abstract": "The ability to detect Out-of-Domain (OOD) inputs has been a critical requirement in many real-world NLP applications. For example, intent classification in dialogue systems. The reason is that the inclusion of unsupported OOD inputs may lead to catastrophic failure of systems. However, it remains an empirical question whether current methods can tackle such problems reliably in a realistic scenario where zero OOD training data is available. In this study, we propose ProtoInfoMax, a new architecture that extends Prototypical Networks to simultaneously process in-domain and OOD sentences via Mutual Information Maximization (InfoMax) objective. Experimental results show that our proposed method can substantially improve performance up to 20% for OOD detection in low resource settings of text classification. We also show that ProtoInfoMax is less prone to typical overconfidence errors of Neural Networks, leading to more reliable prediction results",
    "checked": true,
    "id": "551e091b9869f26bd330ef9e8ddacb86be9b5d27",
    "semantic_title": "protoinfomax: prototypical networks with mutual information maximization for out-of-domain detection",
    "citation_count": 5,
    "authors": [
      "Iftitahu Nimah",
      "Meng Fang",
      "Vlado Menkovski",
      "Mykola Pechenizkiy"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.139": {
    "title": "Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework",
    "volume": "findings",
    "abstract": "In this work, we study the problem of named entity recognition (NER) in a low resource scenario, focusing on few-shot and zero-shot settings. Built upon large-scale pre-trained language models, we propose a novel NER framework, namely SpanNER, which learns from natural language supervision and enables the identification of never-seen entity classes without using in-domain labeled data. We perform extensive experiments on 5 benchmark datasets and evaluate the proposed method in the few-shot learning, domain transfer and zero-shot learning settings. The experimental results show that the proposed method can bring 10%, 23% and 26% improvements in average over the best baselines in few-shot learning, domain transfer and zero-shot learning settings respectively",
    "checked": true,
    "id": "f8b4cb8d58577115a5f8c30f9653b5971c31b965",
    "semantic_title": "learning from language description: low-shot named entity recognition via decomposed framework",
    "citation_count": 39,
    "authors": [
      "Yaqing Wang",
      "Haoda Chu",
      "Chao Zhang",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.140": {
    "title": "BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks",
    "volume": "findings",
    "abstract": "Biomedical entity linking is the task of linking entity mentions in a biomedical document to referent entities in a knowledge base. Recently, many BERT-based models have been introduced for the task. While these models achieve competitive results on many datasets, they are computationally expensive and contain about 110M parameters. Little is known about the factors contributing to their impressive performance and whether the over-parameterization is needed. In this work, we shed some light on the inner workings of these large BERT-based models. Through a set of probing experiments, we have found that the entity linking performance only changes slightly when the input word order is shuffled or when the attention scope is limited to a fixed window size. From these observations, we propose an efficient convolutional neural network with residual connections for biomedical entity linking. Because of the sparse connectivity and weight sharing properties, our model has a small number of parameters and is highly efficient. On five public datasets, our model achieves comparable or even better linking accuracy than the state-of-the-art BERT-based models while having about 60 times fewer parameters",
    "checked": true,
    "id": "0d0055b72d3fe0a1069ffeef22f13bcbf3ca8ec8",
    "semantic_title": "bert might be overkill: a tiny but effective biomedical entity linker based on residual convolutional neural networks",
    "citation_count": 31,
    "authors": [
      "Tuan Lai",
      "Heng Ji",
      "ChengXiang Zhai"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.141": {
    "title": "Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality",
    "volume": "findings",
    "abstract": "Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword tokenization process of language models as it provides multiple benefits. However, this process is solely based on pre-training data statistics, making it hard for the tokenizer to handle infrequent spellings. On the other hand, though robust to misspellings, pure character-level models often lead to unreasonably long sequences and make it harder for the model to learn meaningful words. To alleviate these challenges, we propose a character-based subword module (char2subword) that learns the subword embedding table in pre-trained models like BERT. Our char2subword module builds representations from characters out of the subword vocabulary, and it can be used as a drop-in replacement of the subword embedding table. The module is robust to character-level alterations such as misspellings, word inflection, casing, and punctuation. We integrate it further with BERT through pre-training while keeping BERT transformer parameters fixed–and thus, providing a practical method. Finally, we show that incorporating our module to mBERT significantly improves the performance on the social media linguistic code-switching evaluation (LinCE) benchmark",
    "checked": true,
    "id": "3fefe4d9ec77cd020463e5828408c9ced650611d",
    "semantic_title": "char2subword: extending the subword embedding space using robust character compositionality",
    "citation_count": 13,
    "authors": [
      "Gustavo Aguilar",
      "Bryan McCann",
      "Tong Niu",
      "Nazneen Rajani",
      "Nitish Shirish Keskar",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.142": {
    "title": "Exploring Multitask Learning for Low-Resource Abstractive Summarization",
    "volume": "findings",
    "abstract": "This paper explores the effect of using multitask learning for abstractive summarization in the context of small training corpora. In particular, we incorporate four different tasks (extractive summarization, language modeling, concept detection, and paraphrase detection) both individually and in combination, with the goal of enhancing the target task of abstractive summarization via multitask learning. We show that for many task combinations, a model trained in a multitask setting outperforms a model trained only for abstractive summarization, with no additional summarization data introduced. Additionally, we do a comprehensive search and find that certain tasks (e.g. paraphrase detection) consistently benefit abstractive summarization, not only when combined with other tasks but also when using different architectures and training corpora",
    "checked": true,
    "id": "dae58784124ddef4e8d2cab3692e02ec8ce4ec66",
    "semantic_title": "exploring multitask learning for low-resource abstractivesummarization",
    "citation_count": 11,
    "authors": [
      "Ahmed Magooda",
      "Diane Litman",
      "Mohamed Elaraby"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.143": {
    "title": "Conical Classification For Efficient One-Class Topic Determination",
    "volume": "findings",
    "abstract": "As the Internet grows in size, so does the amount of text based information that exists. For many application spaces it is paramount to isolate and identify texts that relate to a particular topic. While one-class classification would be ideal for such analysis, there is a relative lack of research regarding efficient approaches with high predictive power. By noting that the range of documents we wish to identify can be represented as positive linear combinations of the Vector Space Model representing our text, we propose Conical classification, an approach that allows us to identify if a document is of a particular topic in a computationally efficient manner. We also propose Normal Exclusion, a modified version of Bi-Normal Separation that makes it more suitable within the one-class classification context. We show in our analysis that our approach not only has higher predictive power on our datasets, but is also faster to compute",
    "checked": true,
    "id": "9f735ae31aa09968d444275a077bbb5477deab58",
    "semantic_title": "conical classification for efficient one-class topic determination",
    "citation_count": 2,
    "authors": [
      "Sameer Khanna"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.144": {
    "title": "Improving Dialogue State Tracking with Turn-based Loss Function and Sequential Data Augmentation",
    "volume": "findings",
    "abstract": "While state-of-the-art Dialogue State Tracking (DST) models show promising results, all of them rely on a traditional cross-entropy loss function during the training process, which may not be optimal for improving the joint goal accuracy. Although several approaches recently propose augmenting the training set by copying user utterances and replacing the real slot values with other possible or even similar values, they are not effective at improving the performance of existing DST models. To address these challenges, we propose a Turn-based Loss Function (TLF) that penalises the model if it inaccurately predicts a slot value at the early turns more so than in later turns in order to improve joint goal accuracy. We also propose a simple but effective Sequential Data Augmentation (SDA) algorithm to generate more complex user utterances and system responses to effectively train existing DST models. Experimental results on two standard DST benchmark collections demonstrate that our proposed TLF and SDA techniques significantly improve the effectiveness of the state-of-the-art DST model by approximately 7-8% relative reduction in error and achieves a new state-of-the-art joint goal accuracy with 59.50 and 54.90 on MultiWOZ2.1 and MultiWOZ2.2, respectively",
    "checked": true,
    "id": "5c65b3130d323e766eb8591838d7a03fb10c456d",
    "semantic_title": "improving dialogue state tracking with turn-based loss function and sequential data augmentation",
    "citation_count": 8,
    "authors": [
      "Jarana Manotumruksa",
      "Jeff Dalton",
      "Edgar Meij",
      "Emine Yilmaz"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.145": {
    "title": "TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling",
    "volume": "findings",
    "abstract": "Human conversations naturally evolve around different topics and fluently move between them. In research on dialog systems, the ability to actively and smoothly transition to new topics is often ignored. In this paper we introduce TIAGE, a new topic-shift aware dialog benchmark constructed utilizing human annotations on topic shifts. Based on TIAGE, we introduce three tasks to investigate different scenarios of topic-shift modeling in dialog settings: topic-shift detection, topic-shift triggered response generation and topic-aware dialog generation. Experiments on these tasks show that the topic-shift signals in TIAGE are useful for topic-shift response generation. On the other hand, dialog systems still struggle to decide when to change topic. This indicates further research is needed in topic-shift aware dialog modeling",
    "checked": true,
    "id": "f53416d2554c7f2444d3b01b06cca20f648b2d37",
    "semantic_title": "tiage: a benchmark for topic-shift aware dialog modeling",
    "citation_count": 27,
    "authors": [
      "Huiyuan Xie",
      "Zhenghao Liu",
      "Chenyan Xiong",
      "Zhiyuan Liu",
      "Ann Copestake"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.146": {
    "title": "Optimal Neural Program Synthesis from Multimodal Specifications",
    "volume": "findings",
    "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user, like natural language, with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using a combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and efficiency, and finds model-optimal programs more frequently",
    "checked": true,
    "id": "2ad6b59ff14d6bec3f14d8b6480025bbebc50e46",
    "semantic_title": "optimal neural program synthesis from multimodal specifications",
    "citation_count": 26,
    "authors": [
      "Xi Ye",
      "Qiaochu Chen",
      "Isil Dillig",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.147": {
    "title": "Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations",
    "volume": "findings",
    "abstract": "The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which require finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening—the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in a human-machine pipeline for turning low-quality, crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews",
    "checked": true,
    "id": "a9b131fefb85936e939c8592d4481699fa72d94c",
    "semantic_title": "sent2span: span detection for pico extraction in the biomedical text without span annotations",
    "citation_count": 14,
    "authors": [
      "Shifeng Liu",
      "Yifang Sun",
      "Bing Li",
      "Wei Wang",
      "Florence T. Bourgeois",
      "Adam G. Dunn"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.148": {
    "title": "When in Doubt: Improving Classification Performance with Alternating Normalization",
    "volume": "findings",
    "abstract": "We introduce Classification with Alternating Normalization (CAN), a non-parametric post-processing step for classification. CAN improves classification accuracy for challenging examples by re-adjusting their predicted class probability distribution using the predicted class distributions of high-confidence validation examples. CAN is easily applicable to any probabilistic classifier, with minimal computation overhead. We analyze the properties of CAN using simulated experiments, and empirically demonstrate its effectiveness across a diverse set of classification tasks",
    "checked": true,
    "id": "c737975c17ca8c058625beda777ba1b06888d8c2",
    "semantic_title": "when in doubt: improving classification performance with alternating normalization",
    "citation_count": 13,
    "authors": [
      "Menglin Jia",
      "Austin Reiter",
      "Ser-Nam Lim",
      "Yoav Artzi",
      "Claire Cardie"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.149": {
    "title": "APGN: Adversarial and Parameter Generation Networks for Multi-Source Cross-Domain Dependency Parsing",
    "volume": "findings",
    "abstract": "Thanks to the strong representation learning capability of deep learning, especially pre-training techniques with language model loss, dependency parsing has achieved great performance boost in the in-domain scenario with abundant labeled training data for target domains. However, the parsing community has to face the more realistic setting where the parsing performance drops drastically when labeled data only exists for several fixed out-domains. In this work, we propose a novel model for multi-source cross-domain dependency parsing. The model consists of two components, i.e., a parameter generation network for distinguishing domain-specific features, and an adversarial network for learning domain-invariant representations. Experiments on a recently released NLPCC-2019 dataset for multi-domain dependency parsing show that our model can consistently improve cross-domain parsing performance by about 2 points in averaged labeled attachment accuracy (LAS) over strong BERT-enhanced baselines. Detailed analysis is conducted to gain more insights on contributions of the two components",
    "checked": true,
    "id": "cd52afe1d90bbbb00bb8539eead2a144a7bfab10",
    "semantic_title": "apgn: adversarial and parameter generation networks for multi-source cross-domain dependency parsing",
    "citation_count": 6,
    "authors": [
      "Ying Li",
      "Meishan Zhang",
      "Zhenghua Li",
      "Min Zhang",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Nicholas Jing Yuan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.150": {
    "title": "Let Your Characters Tell Their Story\": A Dataset for Character-Centric Narrative Understanding",
    "volume": "findings",
    "abstract": "When reading a literary piece, readers often make inferences about various characters' roles, personalities, relationships, intents, actions, etc. While humans can readily draw upon their past experiences to build such a character-centric view of the narrative, understanding characters in narratives can be a challenging task for machines. To encourage research in this field of character-centric narrative understanding, we present LiSCU – a new dataset of literary pieces and their summaries paired with descriptions of characters that appear in them. We also introduce two new tasks on LiSCU: Character Identification and Character Description Generation. Our experiments with several pre-trained language models adapted for these tasks demonstrate that there is a need for better models of narrative comprehension",
    "checked": true,
    "id": "21233223e6f14006106f3acedd60e59af18816e1",
    "semantic_title": "let your characters tell their story\": a dataset for character-centric narrative understanding",
    "citation_count": 54,
    "authors": [
      "Faeze Brahman",
      "Meng Huang",
      "Oyvind Tafjord",
      "Chao Zhao",
      "Mrinmaya Sachan",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.151": {
    "title": "Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation",
    "volume": "findings",
    "abstract": "Pre-trained language-vision models have shown remarkable performance on the visual question answering (VQA) task. However, most pre-trained models are trained by only considering monolingual learning, especially the resource-rich language like English. Training such models for multilingual setups demand high computing resources and multilingual language-vision dataset which hinders their application in practice. To alleviate these challenges, we propose a knowledge distillation approach to extend an English language-vision model (teacher) into an equally effective multilingual and code-mixed model (student). Unlike the existing knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model learns and imitates the teacher from multiple intermediate layers (language and vision encoders) with appropriately designed distillation objectives for incremental knowledge extraction. We also create the large-scale multilingual and code-mixed VQA dataset in eleven different language setups considering the multiple Indian and European languages. Experimental results and in-depth analysis show the effectiveness of the proposed VQA model over the pre-trained language-vision models on eleven diverse language setups",
    "checked": true,
    "id": "e91592b0b68ca0dfb99d1c9859c3a9c552b19d4e",
    "semantic_title": "towards developing a multilingual and code-mixed visual question answering system by knowledge distillation",
    "citation_count": 14,
    "authors": [
      "Humair Raj Khan",
      "Deepak Gupta",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.152": {
    "title": "An Iterative Multi-Knowledge Transfer Network for Aspect-Based Sentiment Analysis",
    "volume": "findings",
    "abstract": "Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect term extraction, opinion term extraction, and aspect-level sentiment classification, which are typically handled in a separate or joint manner. However, previous approaches do not well exploit the interactive relations among three subtasks and do not pertinently leverage the easily available document-level labeled domain/sentiment knowledge, which restricts their performances. To address these issues, we propose a novel Iterative Multi-Knowledge Transfer Network (IMKTN) for end-to-end ABSA. For one thing, through the interactive correlations between the ABSA subtasks, our IMKTN transfers the task-specific knowledge from any two of the three subtasks to another one at the token level by utilizing a well-designed routing algorithm, that is, any two of the three subtasks will help the third one. For another, our IMKTN pertinently transfers the document-level knowledge, i.e., domain-specific and sentiment-related knowledge, to the aspect-level subtasks to further enhance the corresponding performance. Experimental results on three benchmark datasets demonstrate the effectiveness and superiority of our approach",
    "checked": true,
    "id": "74a0041d5fb6d33ea28346d5ea77f85e6d6ddbdc",
    "semantic_title": "an iterative multi-knowledge transfer network for aspect-based sentiment analysis",
    "citation_count": 38,
    "authors": [
      "Yunlong Liang",
      "Fandong Meng",
      "Jinchao Zhang",
      "Yufeng Chen",
      "Jinan Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.153": {
    "title": "Semantic Alignment with Calibrated Similarity for Multilingual Sentence Embedding",
    "volume": "findings",
    "abstract": "Measuring the similarity score between a pair of sentences in different languages is the essential requisite for multilingual sentence embedding methods. Predicting the similarity score consists of two sub-tasks, which are monolingual similarity evaluation and multilingual sentence retrieval. However, conventional methods have mainly tackled only one of the sub-tasks and therefore showed biased performances. In this paper, we suggest a novel and strong method for multilingual sentence embedding, which shows performance improvement on both sub-tasks, consequently resulting in robust predictions of multilingual similarity scores. The suggested method consists of two parts: to learn semantic similarity of sentences in the pivot language and then to extend the learned semantic structure to different languages. To align semantic structures across different languages, we introduce a teacher-student network. The teacher network distills the knowledge of the pivot language to different languages of the student network. During the distillation, the parameters of the teacher network are updated with the slow-moving average. Together with the distillation and the parameter updating, the semantic structure of the student network can be directly aligned across different languages while preserving the ability to measure the semantic similarity. Thus, the multilingual training method drives performance improvement on multilingual similarity evaluation. The suggested model achieves the state-of-the-art performance on extended STS 2017 multilingual similarity evaluation as well as two sub-tasks, which are extended STS 2017 monolingual similarity evaluation and Tatoeba multilingual retrieval in 14 languages",
    "checked": true,
    "id": "c80606ae5e96dc55b9c41ecb63ef1d013011548b",
    "semantic_title": "semantic alignment with calibrated similarity for multilingual sentence embedding",
    "citation_count": 11,
    "authors": [
      "Jiyeon Ham",
      "Eun-Sol Kim"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.154": {
    "title": "fBERT: A Neural Transformer for Identifying Offensive Content",
    "volume": "findings",
    "abstract": "Transformer-based models such as BERT, XLNET, and XLM-R have achieved state-of-the-art performance across various NLP tasks including the identification of offensive language and hate speech, an important problem in social media. In this paper, we present fBERT, a BERT model retrained on SOLID, the largest English offensive language identification corpus available with over 1.4 million offensive instances. We evaluate fBERT's performance on identifying offensive content on multiple English datasets and we test several thresholds for selecting instances from SOLID. The fBERT model will be made freely available to the community",
    "checked": true,
    "id": "bbcea6eb922b0736cc9e9acc649c8d90cbc8264c",
    "semantic_title": "fbert: a neural transformer for identifying offensive content",
    "citation_count": 50,
    "authors": [
      "Diptanu Sarkar",
      "Marcos Zampieri",
      "Tharindu Ranasinghe",
      "Alexander Ororbia"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.155": {
    "title": "WIKIBIAS: Detecting Multi-Span Subjective Biases in Language",
    "volume": "findings",
    "abstract": "Biases continue to be prevalent in modern text and media, especially subjective bias – a special type of bias that introduces improper attitudes or presents a statement with the presupposition of truth. To tackle the problem of detecting and further mitigating subjective bias, we introduce a manually annotated parallel corpus WIKIBIAS with more than 4,000 sentence pairs from Wikipedia edits. This corpus contains annotations towards both sentence-level bias types and token-level biased segments. We present systematic analyses of our dataset and results achieved by a set of state-of-the-art baselines in terms of three tasks: bias classification, tagging biased segments, and neutralizing biased text. We find that current models still struggle with detecting multi-span biases despite their reasonable performances, suggesting that our dataset can serve as a useful research benchmark. We also demonstrate that models trained on our dataset can generalize well to multiple domains such as news and political speeches",
    "checked": true,
    "id": "30b2fcf292cd58d6df5813cc996f9c606276d961",
    "semantic_title": "wikibias: detecting multi-span subjective biases in language",
    "citation_count": 13,
    "authors": [
      "Yang Zhong",
      "Jingfeng Yang",
      "Wei Xu",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.156": {
    "title": "UnClE: Explicitly Leveraging Semantic Similarity to Reduce the Parameters of Word Embeddings",
    "volume": "findings",
    "abstract": "Natural language processing (NLP) models often require a massive number of parameters for word embeddings, which limits their application on mobile devices. Researchers have employed many approaches, e.g. adaptive inputs, to reduce the parameters of word embeddings. However, existing methods rarely pay attention to semantic information. In this paper, we propose a novel method called Unique and Class Embeddings (UnClE), which explicitly leverages semantic similarity with weight sharing to reduce the dimensionality of word embeddings. Inspired by the fact that words with similar semantic can share a part of weights, we divide the embeddings of words into two parts: unique embedding and class embedding. The former is one-to-one mapping like traditional embedding, while the latter is many-to-one mapping and learn the representation of class information. Our method is suitable for both word-level and sub-word level models and can be used to reduce both input and output embeddings. Experimental results on the standard WMT 2014 English-German dataset show that our method is able to reduce the parameters of word embeddings by more than 11x, with about 93% performance retaining in BLEU metrics. For language modeling task, our model can reduce word embeddings by 6x or 11x on PTB/WT2 dataset at the cost of a certain degree of performance degradation",
    "checked": true,
    "id": "cf5fb8ad444f199336c743715fce65e2f4fa27f7",
    "semantic_title": "uncle: explicitly leveraging semantic similarity to reduce the parameters of word embeddings",
    "citation_count": 0,
    "authors": [
      "Zhi Li",
      "Yuchen Zhai",
      "Chengyu Wang",
      "Minghui Qiu",
      "Kailiang Li",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.157": {
    "title": "Grounded Graph Decoding improves Compositional Generalization in Question Answering",
    "volume": "findings",
    "abstract": "Question answering models struggle to generalize to novel compositions of training patterns. Current end-to-end models learn a flat input embedding which can lose input syntax context. Prior approaches improve generalization by learning permutation invariant models, but these methods do not scale to more complex train-test splits. We propose Grounded Graph Decoding, a method to improve compositional generalization of language representations by grounding structured predictions with an attention mechanism. Grounding enables the model to retain syntax information from the input that significantly improves generalization to complex inputs. By predicting a structured graph containing conjunctions of query clauses, we learn a group invariant representation without making assumptions on the target domain. Our model performs competitively on the Compositional Freebase Questions (CFQ) dataset, a challenging benchmark for compositional generalization in question answering. Especially, our model effectively solves the MCD1 split with 98% accuracy. All source is available at https://github.com/gaiyu0/cfq",
    "checked": true,
    "id": "7cc74ffa1215321712d4a830bb9dee19d9f0fb47",
    "semantic_title": "grounded graph decoding improves compositional generalization in question answering",
    "citation_count": 8,
    "authors": [
      "Yu Gai",
      "Paras Jain",
      "Wendi Zhang",
      "Joseph Gonzalez",
      "Dawn Song",
      "Ion Stoica"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.158": {
    "title": "Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser",
    "volume": "findings",
    "abstract": "Considering the importance of building a good Visual Dialog (VD) Questioner, many researchers study the topic under a Q-Bot-A-Bot image-guessing game setting, where the Questioner needs to raise a series of questions to collect information of an undisclosed image. Despite progress has been made in Supervised Learning (SL) and Reinforcement Learning (RL), issues still exist. Firstly, previous methods do not provide explicit and effective guidance for Questioner to generate visually related and informative questions. Secondly, the effect of RL is hampered by an incompetent component, i.e., the Guesser, who makes image predictions based on the generated dialogs and assigns rewards accordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced Questioner (ReeQ) that generates questions under the guidance of related entities and learns entity-based questioning strategy from human dialogs; 2) we propose an Augmented Guesser that is strong and is optimized for VD especially. Experimental results on the VisDial v1.0 dataset show that our approach achieves state-of-the-art performance on both image-guessing task and question diversity. Human study further verifies that our model generates more visually related, informative and coherent questions",
    "checked": true,
    "id": "326d5fff82d6e2a97827ce139eb288e8e0029e43",
    "semantic_title": "enhancing visual dialog questioner with entity-based strategy learning and augmented guesser",
    "citation_count": 12,
    "authors": [
      "Duo Zheng",
      "Zipeng Xu",
      "Fandong Meng",
      "Xiaojie Wang",
      "Jiaan Wang",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.159": {
    "title": "A Pretraining Numerical Reasoning Model for Ordinal Constrained Question Answering on Knowledge Base",
    "volume": "findings",
    "abstract": "Knowledge Base Question Answering (KBQA) is to answer natural language questions posed over knowledge bases (KBs). This paper targets at empowering the IR-based KBQA models with the ability of numerical reasoning for answering ordinal constrained questions. A major challenge is the lack of explicit annotations about numerical properties. To address this challenge, we propose a pretraining numerical reasoning model consisting of NumGNN and NumTransformer, guided by explicit self-supervision signals. The two modules are pretrained to encode the magnitude and ordinal properties of numbers respectively and can serve as model-agnostic plugins for any IR-based KBQA model to enhance its numerical reasoning ability. Extensive experiments on two KBQA benchmarks verify the effectiveness of our method to enhance the numerical reasoning ability for IR-based KBQA models",
    "checked": true,
    "id": "06df1dc2f0d22c9a6f094f5cc50ed1c7a812cf9d",
    "semantic_title": "a pretraining numerical reasoning model for ordinal constrained question answering on knowledge base",
    "citation_count": 10,
    "authors": [
      "Yu Feng",
      "Jing Zhang",
      "Gaole He",
      "Wayne Xin Zhao",
      "Lemao Liu",
      "Quan Liu",
      "Cuiping Li",
      "Hong Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.160": {
    "title": "RoR: Read-over-Read for Long Document Machine Reading Comprehension",
    "volume": "findings",
    "abstract": "Transformer-based pre-trained models, such as BERT, have achieved remarkable results on machine reading comprehension. However, due to the constraint of encoding length (e.g., 512 WordPiece tokens), a long document is usually split into multiple chunks that are independently read. It results in the reading field being limited to individual chunks without information collaboration for long document machine reading comprehension. To address this problem, we propose RoR, a read-over-read method, which expands the reading field from chunk to document. Specifically, RoR includes a chunk reader and a document reader. The former first predicts a set of regional answers for each chunk, which are then compacted into a highly-condensed version of the original document, guaranteeing to be encoded once. The latter further predicts the global answers from this condensed document. Eventually, a voting strategy is utilized to aggregate and rerank the regional and global answers for final prediction. Extensive experiments on two benchmarks QuAC and TriviaQA demonstrate the effectiveness of RoR for long document reading. Notably, RoR ranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of submission (May 17th, 2021)",
    "checked": true,
    "id": "365fbeeefe2b219ed36b3f3a603c8d7bae99d48a",
    "semantic_title": "ror: read-over-read for long document machine reading comprehension",
    "citation_count": 24,
    "authors": [
      "Jing Zhao",
      "Junwei Bao",
      "Yifan Wang",
      "Yongwei Zhou",
      "Youzheng Wu",
      "Xiaodong He",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.161": {
    "title": "Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing",
    "volume": "findings",
    "abstract": "An effective recipe for building seq2seq, non-autoregressive, task-oriented parsers to map utterances to semantic frames proceeds in three steps: encoding an utterance x, predicting a frame's length |y|, and decoding a |y|-sized frame with utterance and ontology tokens. Though empirically strong, these models are typically bottlenecked by length prediction, as even small inaccuracies change the syntactic and semantic characteristics of resulting frames. In our work, we propose span pointer networks, non-autoregressive parsers which shift the decoding task from text generation to span prediction; that is, when imputing utterance spans into frame slots, our model produces endpoints (e.g., [i, j]) as opposed to text (e.g., \"6pm\"). This natural quantization of the output space reduces the variability of gold frames, therefore improving length prediction and, ultimately, exact match. Furthermore, length prediction is now responsible for frame syntax and the decoder is responsible for frame semantics, resulting in a coarse-to-fine model. We evaluate our approach on several task-oriented semantic parsing datasets. Notably, we bridge the quality gap between non-autogressive and autoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020). Furthermore, due to our more consistent gold frames, we show strong improvements in model generalization in both cross-domain and cross-lingual transfer in low-resource settings. Finally, due to our diminished output vocabulary, we observe 70% reduction in latency and 83% reduction in memory at beam size 5 compared to prior non-autoregressive parsers",
    "checked": true,
    "id": "3bdbd9346c6e5a54fcdcb9c42811f4593542bba9",
    "semantic_title": "span pointer networks for non-autoregressive task-oriented semantic parsing",
    "citation_count": 21,
    "authors": [
      "Akshat Shrivastava",
      "Pierce Chuang",
      "Arun Babu",
      "Shrey Desai",
      "Abhinav Arora",
      "Alexander Zotov",
      "Ahmed Aly"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.162": {
    "title": "Language Resource Efficient Learning for Captioning",
    "volume": "findings",
    "abstract": "Due to complex cognitive and inferential efforts involved in the manual generation of one caption per image/video input, the human annotation resources are very limited for captioning tasks. We define language resource efficient as reaching the same performance with fewer annotated captions per input. We first study the performance degradation of caption models in different language resource settings. Our analysis of caption models with SC loss shows that the performance degradation is caused by the increasingly noisy estimation of reward and baseline with fewer language resources. To mitigate this issue, we propose to reduce the variance of noise in the baseline by generalizing the single pairwise comparison in SC loss and using multiple generalized pairwise comparisons. The generalized pairwise comparison (GPC) measures the difference between the evaluation scores of two captions with respect to an input. Empirically, we show that the model trained with the proposed GPC loss is efficient on language resource and achieves similar performance with the state-of-the-art models on MSCOCO by using only half of the language resources. Furthermore, our model significantly outperforms the state-of-the-art models on a video caption dataset that has only one labeled caption per input in the training set",
    "checked": true,
    "id": "c5738ecde7e9c5fdc9e36af84bfb4db5c3ff73a7",
    "semantic_title": "language resource efficient learning for captioning",
    "citation_count": 3,
    "authors": [
      "Jia Chen",
      "Yike Wu",
      "Shiwan Zhao",
      "Qin Jin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.163": {
    "title": "Translation as Cross-Domain Knowledge: Attention Augmentation for Unsupervised Cross-Domain Segmenting and Labeling Tasks",
    "volume": "findings",
    "abstract": "The nature of no word delimiter or inflection that can indicate segment boundaries or word semantics increases the difficulty of Chinese text understanding, and also intensifies the demand for word-level semantic knowledge to accomplish the tagging goal in Chinese segmenting and labeling tasks. However, for unsupervised Chinese cross-domain segmenting and labeling tasks, the model trained on the source domain frequently suffers from the deficient word-level semantic knowledge of the target domain. To address this issue, we propose a novel paradigm based on attention augmentation to introduce crucial cross-domain knowledge via a translation system. The proposed paradigm enables the model attention to draw cross-domain knowledge indicated by the implicit word-level cross-lingual alignment between the input and its corresponding translation. Aside from the model requiring cross-lingual input, we also establish an off-the-shelf model which eludes the dependency on cross-lingual translations. Experiments demonstrate that our proposal significantly advances the state-of-the-art results of cross-domain Chinese segmenting and labeling tasks",
    "checked": true,
    "id": "6764dee446c0428e05c6c1928fa63129b5cd0f6a",
    "semantic_title": "translation as cross-domain knowledge: attention augmentation for unsupervised cross-domain segmenting and labeling tasks",
    "citation_count": 1,
    "authors": [
      "Ruixuan Luo",
      "Yi Zhang",
      "Sishuo Chen",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.164": {
    "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts",
    "volume": "findings",
    "abstract": "Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose \"document-level natural language inference (NLI) for contracts\", a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as \"Some obligations of Agreement may survive termination.\") and a contract, and it is asked to classify whether each hypothesis is \"entailed by\", \"contradicting to\" or \"not mentioned by\" (neutral to) the contract as well as identifying \"evidence\" for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (a) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (b) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement",
    "checked": true,
    "id": "0a1ff1d4102d94a50f8862f60bc2ac21f36ad592",
    "semantic_title": "contractnli: a dataset for document-level natural language inference for contracts",
    "citation_count": 103,
    "authors": [
      "Yuta Koreeda",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.165": {
    "title": "Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning",
    "volume": "findings",
    "abstract": "Parallel texts of Japanese and a non-pro-drop language have the potential of improving the performance of Japanese zero anaphora resolution (ZAR) because pronouns dropped in the former are usually mentioned explicitly in the latter. However, rule-based cross-lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences. In this paper, we propose implicit transfer by injecting machine translation (MT) as an intermediate task between pretraining and ZAR. We employ a pretrained BERT model to initialize the encoder part of the encoder-decoder model for MT, and eject the encoder part for fine-tuning on ZAR. The proposed framework empirically demonstrates that ZAR performance can be improved by transfer learning from MT. In addition, we find that the incorporation of the masked language model training into MT leads to further gains",
    "checked": true,
    "id": "f068c3175c43f07f4b0302c4533f98823a480393",
    "semantic_title": "japanese zero anaphora resolution can benefit from parallel texts through neural transfer learning",
    "citation_count": 9,
    "authors": [
      "Masato Umakoshi",
      "Yugo Murawaki",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.166": {
    "title": "Grouped-Attention for Content-Selection and Content-Plan Generation",
    "volume": "findings",
    "abstract": "Content-planning is an essential part of data-to-text generation to determine the order of data mentioned in generated texts. Recent neural data-to-text generation models employ Pointer Networks to explicitly learn content-plan given a set of attributes as input. They use LSTM to encode the input, which assumes a sequential relationship in the input. This may be sub-optimal to encode a set of attributes, where the attributes have a composite structure: the attributes are disordered while each attribute value is an ordered list of tokens. We handle this problem by proposing a neural content-planner that can capture both local and global contexts of such a structure. Specifically, we propose a novel attention mechanism called GSC-attention. A key component of the GSC-attention is grouped-attention, which is token-level attention constrained within each input attribute that enables our proposed model captures both local and global context. Moreover, our content-planner explicitly learns content-selection, which is integrated into the content-planner to select the most important data to be included in the generated text via an attention masking procedure. Experimental results show that our model outperforms the competitors by 4.92%, 4.70%, and 16.56% in terms of Damerau-Levenshtein Distance scores on three real-world datasets",
    "checked": true,
    "id": "eee4ee3c5291aad3f8051f32f2712fec943fb296",
    "semantic_title": "grouped-attention for content-selection and content-plan generation",
    "citation_count": 0,
    "authors": [
      "Bayu Distiawan Trisedya",
      "Xiaojie Wang",
      "Jianzhong Qi",
      "Rui Zhang",
      "Qingjun Cui"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.167": {
    "title": "An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling",
    "volume": "findings",
    "abstract": "Intent classification (IC) and slot filling (SF) are critical building blocks in task-oriented dialogue systems. These two tasks are closely-related and can flourish each other. Since only a few utterances can be utilized for identifying fast-emerging new intents and slots, data scarcity issue often occurs when implementing IC and SF. However, few IC/SF models perform well when the number of training samples per class is quite small. In this paper, we propose a novel explicit-joint and supervised-contrastive learning framework for few-shot intent classification and slot filling. Its highlights are as follows. (i) The model extracts intent and slot representations via bidirectional interactions, and extends prototypical network to achieve explicit-joint learning, which guarantees that IC and SF tasks can mutually reinforce each other. (ii) The model integrates with supervised contrastive learning, which ensures that samples from same class are pulled together and samples from different classes are pushed apart. In addition, the model follows a not common but practical way to construct the episode, which gets rid of the traditional setting with fixed way and shot, and allows for unbalanced datasets. Extensive experiments on three public datasets show that our model can achieve promising performance",
    "checked": true,
    "id": "eb47ab67117bc686d492081cfd220759334754a2",
    "semantic_title": "an explicit-joint and supervised-contrastive learning framework for few-shot intent classification and slot filling",
    "citation_count": 15,
    "authors": [
      "Han Liu",
      "Feng Zhang",
      "Xiaotong Zhang",
      "Siyang Zhao",
      "Xianchao Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.168": {
    "title": "Retrieve, Discriminate and Rewrite: A Simple and Effective Framework for Obtaining Affective Response in Retrieval-Based Chatbots",
    "volume": "findings",
    "abstract": "Obtaining affective response is a key step in building empathetic dialogue systems. This task has been studied a lot in generation-based chatbots, but the related research in retrieval-based chatbots is still in the early stage. Existing works in retrieval-based chatbots are based on Retrieve-and-Rerank framework, which have a common problem of satisfying affect label at the expense of response quality. To address this problem, we propose a simple and effective Retrieve-Discriminate-Rewrite framework. The framework replaces the reranking mechanism with a new discriminate-and-rewrite mechanism, which predicts the affect label of the retrieved high-quality response via discrimination module and further rewrites the affect unsatisfied response via rewriting module. This can not only guarantee the quality of the response, but also satisfy the given affect label. In addition, another challenge for this line of research is the lack of an off-the-shelf affective response dataset. To address this problem and test our proposed framework, we annotate a Sentimental Douban Conversation Corpus based on the original Douban Conversation Corpus. Experimental results show that our proposed framework is effective and outperforms competitive baselines",
    "checked": true,
    "id": "e33a604bc361c54ecdc1a32d347d88e992cdbecb",
    "semantic_title": "retrieve, discriminate and rewrite: a simple and effective framework for obtaining affective response in retrieval-based chatbots",
    "citation_count": 10,
    "authors": [
      "Xin Lu",
      "Yijian Tian",
      "Yanyan Zhao",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.169": {
    "title": "Span Fine-tuning for Pre-trained Language Models",
    "volume": "findings",
    "abstract": "Pre-trained language models (PrLM) have to carefully manage input units when training on a very large text with a vocabulary consisting of millions of words. Previous works have shown that incorporating span-level information over consecutive words in pre-training could further improve the performance of PrLMs. However, given that span-level clues are introduced and fixed in pre-training, previous methods are time-consuming and lack of flexibility. To alleviate the inconvenience, this paper presents a novel span fine-tuning method for PrLMs, which facilitates the span setting to be adaptively determined by specific downstream tasks during the fine-tuning phase. In detail, any sentences processed by the PrLM will be segmented into multiple spans according to a pre-sampled dictionary. Then the segmentation information will be sent through a hierarchical CNN module together with the representation outputs of the PrLM and ultimately generate a span-enhanced representation. Experiments on GLUE benchmark show that the proposed span fine-tuning method significantly enhances the PrLM, and at the same time, offer more flexibility in an efficient way",
    "checked": true,
    "id": "5722d101859846a6a023b8fa00830742203cd0c1",
    "semantic_title": "span fine-tuning for pre-trained language models",
    "citation_count": 2,
    "authors": [
      "Rongzhou Bao",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.170": {
    "title": "DIRECT: Direct and Indirect Responses in Conversational Text Corpus",
    "volume": "findings",
    "abstract": "We create a large-scale dialogue corpus that provides pragmatic paraphrases to advance technology for understanding the underlying intentions of users. While neural conversation models acquire the ability to generate fluent responses through training on a dialogue corpus, previous corpora have mainly focused on the literal meanings of utterances. However, in reality, people do not always present their intentions directly. For example, if a person said to the operator of a reservation service \"I don't have enough budget.\", they, in fact, mean \"please find a cheaper option for me.\" Our corpus provides a total of 71,498 indirect–direct utterance pairs accompanied by a multi-turn dialogue history extracted from the MultiWoZ dataset. In addition, we propose three tasks to benchmark the ability of models to recognize and generate indirect and direct utterances. We also investigated the performance of state-of-the-art pre-trained models as baselines",
    "checked": true,
    "id": "34f5026190ad5038284cf8854522fd8b36627848",
    "semantic_title": "direct: direct and indirect responses in conversational text corpus",
    "citation_count": 7,
    "authors": [
      "Junya Takayama",
      "Tomoyuki Kajiwara",
      "Yuki Arase"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.171": {
    "title": "Retrieval, Analogy, and Composition: A framework for Compositional Generalization in Image Captioning",
    "volume": "findings",
    "abstract": "Image captioning systems are expected to have the ability to combine individual concepts when describing scenes with concept combinations that are not observed during training. In spite of significant progress in image captioning with the help of the autoregressive generation framework, current approaches fail to generalize well to novel concept combinations. We propose a new framework that revolves around probing several similar image caption training instances (retrieval), performing analogical reasoning over relevant entities in retrieved prototypes (analogy), and enhancing the generation process with reasoning outcomes (composition). Our method augments the generation model by referring to the neighboring instances in the training set to produce novel concept combinations in generated captions. We perform experiments on the widely used image captioning benchmarks. The proposed models achieve substantial improvement over the compared baselines on both composition-related evaluation metrics and conventional image captioning metrics",
    "checked": true,
    "id": "6d466de7180776023a539371fad3d521eb5ff791",
    "semantic_title": "retrieval, analogy, and composition: a framework for compositional generalization in image captioning",
    "citation_count": 7,
    "authors": [
      "Zhan Shi",
      "Hui Liu",
      "Martin Renqiang Min",
      "Christopher Malon",
      "Li Erran Li",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.172": {
    "title": "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation",
    "volume": "findings",
    "abstract": "Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called \"Turing Test\" problem for neural text generation methods. In this work, we present the TURINGBENCH benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels Human, GPT-1, GPT-2_small, GPT-2_medium, GPT-2_large,GPT-2_xl, GPT-2_PyTorch, GPT-3, GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large, FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2, (2) two benchmark tasks–i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TURINGBENCH show that GPT-3 and FAIR_wmt20 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TURINGBENCH is available at: https://turingbench.ist.psu.edu/",
    "checked": true,
    "id": "7845bfb55f5ce573b87d77bb76d4d38829b37620",
    "semantic_title": "turingbench: a benchmark environment for turing test in the age of neural text generation",
    "citation_count": 126,
    "authors": [
      "Adaku Uchendu",
      "Zeyu Ma",
      "Thai Le",
      "Rui Zhang",
      "Dongwon Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.173": {
    "title": "Say ‘YES' to Positivity: Detecting Toxic Language in Workplace Communications",
    "volume": "findings",
    "abstract": "Workplace communication (e.g. email, chat, etc.) is a central part of enterprise productivity. Healthy conversations are crucial for creating an inclusive environment and maintaining harmony in an organization. Toxic communications at the workplace can negatively impact overall job satisfaction and are often subtle, hidden, or demonstrate human biases. The linguistic subtlety of mild yet hurtful conversations has made it difficult for researchers to quantify and extract toxic conversations automatically. While offensive language or hate speech has been extensively studied in social communities, there has been little work studying toxic communication in emails. Specifically, the lack of corpus, sparsity of toxicity in enterprise emails, and well-defined criteria for annotating toxic conversations have prevented researchers from addressing the problem at scale. We take the first step towards studying toxicity in workplace emails by providing (1) a general and computationally viable taxonomy to study toxic language at the workplace (2) a dataset to study toxic language at the workplace based on the taxonomy and (3) analysis on why offensive language and hate-speech datasets are not suitable to detect workplace toxicity",
    "checked": true,
    "id": "295d52878c9147c69f77bb658ca259ebe37c031b",
    "semantic_title": "say 'yes' to positivity: detecting toxic language in workplace communications",
    "citation_count": 20,
    "authors": [
      "Meghana Moorthy Bhat",
      "Saghar Hosseini",
      "Ahmed Hassan Awadallah",
      "Paul Bennett",
      "Weisheng Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.174": {
    "title": "Natural SQL: Making SQL Easier to Infer from Natural Language Specifications",
    "volume": "findings",
    "abstract": "Addressing the mismatch between natural language descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we propose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities of SQL, while it simplifies the queries as follows: (1) dispensing with operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are usually hard to find counterparts in the text descriptions; (2) removing the need of nested subqueries and set operators; and (3) making the schema linking easier by reducing the required number of schema items. On Spider, a challenging text-to-SQL benchmark that contains complex and nested SQL queries, we demonstrate that NatSQL outperforms other IRs, and significantly improves the performance of several previous SOTA models. Furthermore, for existing models that do not support executable SQL generation, NatSQL easily enables them to generate executable SQL queries, and achieves the new state-of-the-art execution accuracy",
    "checked": true,
    "id": "379e9d5bba8a617b3114bd5b562b14aa6abc5282",
    "semantic_title": "natural sql: making sql easier to infer from natural language specifications",
    "citation_count": 93,
    "authors": [
      "Yujian Gan",
      "Xinyun Chen",
      "Jinxia Xie",
      "Matthew Purver",
      "John R. Woodward",
      "John Drake",
      "Qiaofu Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.175": {
    "title": "Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization",
    "volume": "findings",
    "abstract": "This paper explores three simple data manipulation techniques (synthesis, augmentation, curriculum) for improving abstractive summarization models without the need for any additional data. We introduce a method of data synthesis with paraphrasing, a data augmentation technique with sample mixing, and curriculum learning with two new difficulty metrics based on specificity and abstractiveness. We conduct experiments to show that these three techniques can help improve abstractive summarization across two summarization models and two different small datasets. Furthermore, we show that these techniques can improve performance when applied in isolation and when combined",
    "checked": true,
    "id": "19c47a57c4043a7bc04a1639cc1fd72b81c35c8d",
    "semantic_title": "mitigating data scarceness through data synthesis, augmentation and curriculum for abstractive summarization",
    "citation_count": 5,
    "authors": [
      "Ahmed Magooda",
      "Diane Litman"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.176": {
    "title": "Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension",
    "volume": "findings",
    "abstract": "Multi-party dialogue machine reading comprehension (MRC) brings tremendous challenge since it involves multiple speakers at one dialogue, resulting in intricate speaker information flows and noisy dialogue contexts. To alleviate such difficulties, previous models focus on how to incorporate these information using complex graph-based modules and additional manually labeled data, which is usually rare in real scenarios. In this paper, we design two labour-free self- and pseudo-self-supervised prediction tasks on speaker and key-utterance to implicitly model the speaker information flows, and capture salient clues in a long dialogue. Experimental results on two benchmark datasets have justified the effectiveness of our method over competitive baselines and current state-of-the-art models",
    "checked": true,
    "id": "53baebc368d2b7943c7c4fd56a3716ccfc472d51",
    "semantic_title": "self- and pseudo-self-supervised prediction of speaker and key-utterance for multi-party dialogue reading comprehension",
    "citation_count": 24,
    "authors": [
      "Yiyang Li",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.177": {
    "title": "Few-Shot Novel Concept Learning for Semantic Parsing",
    "volume": "findings",
    "abstract": "Humans are capable of learning novel concepts from very few examples; in contrast, state-of-the-art machine learning algorithms typically need thousands of examples to do so. In this paper, we propose an algorithm for learning novel concepts by representing them as programs over existing concepts. This way the concept learning problem is naturally a program synthesis problem and our algorithm learns from a few examples to synthesize a program representing the novel concept. In addition, we perform a theoretical analysis of our approach for the case where the program defining the novel concept over existing ones is context-free. We show that given a learned grammar-based parser and a novel production rule, we can augment the parser with the production rule in a way that provably generalizes. We evaluate our approach by learning concepts in the semantic parsing domain extended to the few-shot novel concept learning setting, showing that our approach significantly outperforms end-to-end neural semantic parsers",
    "checked": true,
    "id": "bd652738efa000e4a74d27f5495577dce8ba34a0",
    "semantic_title": "few-shot novel concept learning for semantic parsing",
    "citation_count": 1,
    "authors": [
      "Soham Dan",
      "Osbert Bastani",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.178": {
    "title": "Compositional Data and Task Augmentation for Instruction Following",
    "volume": "findings",
    "abstract": "Executing natural language instructions in a physically grounded domain requires a model that understands both spatial concepts such as \"left of\" and \"above\", and the compositional language used to identify landmarks and articulate instructions relative to them. In this paper, we study instruction understanding in the blocks world domain. Given an initial arrangement of blocks and a natural language instruction, the system executes the instruction by manipulating selected blocks. The highly compositional instructions are composed of atomic components and understanding these components is a necessary step to executing the instruction. We show that while end-to-end training (supervised only by the correct block location) fails to address the challenges of this task and performs poorly on instructions involving a single atomic component, knowledge-free auxiliary signals can be used to significantly improve performance by providing supervision for the instruction's components. Specifically, we generate signals that aim at helping the model gradually understand components of the compositional instructions, as well as those that help it better understand spatial concepts, and show their benefit to the overall task for two datasets and two state-of-the-art (SOTA) models, especially when the training data is limited—which is usual in such tasks",
    "checked": true,
    "id": "63ba7a91a0c483ac93b670d3784271b2f0dc6ab7",
    "semantic_title": "compositional data and task augmentation for instruction following",
    "citation_count": 2,
    "authors": [
      "Soham Dan",
      "Xinran Han",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.179": {
    "title": "Are Factuality Checkers Reliable? Adversarial Meta-evaluation of Factuality in Summarization",
    "volume": "findings",
    "abstract": "With the continuous upgrading of the summarization systems driven by deep neural networks, researchers have higher requirements on the quality of the generated summaries, which should be not only fluent and informative but also factually correct. As a result, the field of factual evaluation has developed rapidly recently. Despite its initial progress in evaluating generated summaries, the meta-evaluation methodologies of factuality metrics are limited in their opacity, leading to the insufficient understanding of factuality metrics' relative advantages and their applicability. In this paper, we present an adversarial meta-evaluation methodology that allows us to (i) diagnose the fine-grained strengths and weaknesses of 6 existing top-performing metrics over 24 diagnostic test datasets, (ii) search for directions for further improvement by data augmentation. Our observations from this work motivate us to propose several calls for future research. We make all codes, diagnostic test datasets, trained factuality models available: https://github.com/zide05/AdvFact",
    "checked": true,
    "id": "f43add418a4fb95ee8817527190199275476a9b3",
    "semantic_title": "are factuality checkers reliable? adversarial meta-evaluation of factuality in summarization",
    "citation_count": 26,
    "authors": [
      "Yiran Chen",
      "Pengfei Liu",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.180": {
    "title": "On the Effects of Transformer Size on In- and Out-of-Domain Calibration",
    "volume": "findings",
    "abstract": "Large, pre-trained transformer language models, which are pervasive in natural language processing tasks, are notoriously expensive to train. To reduce the cost of training such large models, prior work has developed smaller, more compact models which achieves a significant speedup in training time while maintaining competitive accuracy to the original model on downstream tasks. Though these smaller pre-trained models have been widely adopted by the community, it is not known how well are they calibrated compared to their larger counterparts. In this paper, focusing on a wide range of tasks, we thoroughly investigate the calibration properties of pre-trained transformers, as a function of their size. We demonstrate that when evaluated in-domain, smaller models are able to achieve competitive, and often better, calibration compared to larger models, while achieving significant speedup in training time. Post-hoc calibration techniques further reduce calibration error for all models in-domain. However, when evaluated out-of-domain, larger models tend to be better calibrated, and label-smoothing instead is an effective strategy to calibrate models in this setting",
    "checked": true,
    "id": "89f4ec9907cbdc3c918b6a0cca6821c79b24b092",
    "semantic_title": "on the effects of transformer size on in- and out-of-domain calibration",
    "citation_count": 24,
    "authors": [
      "Soham Dan",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.181": {
    "title": "Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings",
    "volume": "findings",
    "abstract": "Growing polarization of the news media has been blamed for fanning disagreement, controversy and even violence. Early identification of polarized topics is thus an urgent matter that can help mitigate conflict. However, accurate measurement of topic-wise polarization is still an open research challenge. To address this gap, we propose Partisanship-aware Contextualized Topic Embeddings (PaCTE), a method to automatically detect polarized topics from partisan news sources. Specifically, utilizing a language model that has been finetuned on recognizing partisanship of the news articles, we represent the ideology of a news corpus on a topic by corpus-contextualized topic embedding and measure the polarization using cosine distance. We apply our method to a dataset of news articles about the COVID-19 pandemic. Extensive experiments on different news sources and topics demonstrate the efficacy of our method to capture topical polarization, as indicated by its effectiveness of retrieving the most polarized topics",
    "checked": true,
    "id": "be705c5b2572f570a5f1d15bfb808faa7d10f5c4",
    "semantic_title": "detecting polarized topics using partisanship-aware contextualized topic embeddings",
    "citation_count": 5,
    "authors": [
      "Zihao He",
      "Negar Mokhberian",
      "António Câmara",
      "Andres Abeliuk",
      "Kristina Lerman"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.182": {
    "title": "GenerativeRE: Incorporating a Novel Copy Mechanism and Pretrained Model for Joint Entity and Relation Extraction",
    "volume": "findings",
    "abstract": "Previous neural Seq2Seq models have shown the effectiveness for jointly extracting relation triplets. However, most of these models suffer from incompletion and disorder problems when they extract multi-token entities from input sentences. To tackle these problems, we propose a generative, multi-task learning framework, named GenerativeRE. We firstly propose a special entity labelling method on both input and output sequences. During the training stage, GenerativeRE fine-tunes the pre-trained generative model and learns the special entity labels simultaneously. During the inference stage, we propose a novel copy mechanism equipped with three mask strategies, to generate the most probable tokens by diminishing the scope of the model decoder. Experimental results show that our model achieves 4.6% and 0.9% F1 score improvements over the current state-of-the-art methods in the NYT24 and NYT29 benchmark datasets respectively",
    "checked": true,
    "id": "03b292b9bd87df669e318f694609681d992095e7",
    "semantic_title": "generativere: incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction",
    "citation_count": 10,
    "authors": [
      "Jiarun Cao",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.183": {
    "title": "Re-entry Prediction for Online Conversations via Self-Supervised Learning",
    "volume": "findings",
    "abstract": "In recent years, world business in online discussions and opinion sharing on social media is booming. Re-entry prediction task is thus proposed to help people keep track of the discussions which they wish to continue. Nevertheless, existing works only focus on exploiting chatting history and context information, and ignore the potential useful learning signals underlying conversation data, such as conversation thread patterns and repeated engagement of target users, which help better understand the behavior of target users in conversations. In this paper, we propose three interesting and well-founded auxiliary tasks, namely, Spread Pattern, Repeated Target user, and Turn Authorship, as the self-supervised signals for re-entry prediction. These auxiliary tasks are trained together with the main task in a multi-task manner. Experimental results on two datasets newly collected from Twitter and Reddit show that our method outperforms the previous state-of-the-arts with fewer parameters and faster convergence. Extensive experiments and analysis show the effectiveness of our proposed models and also point out some key ideas in designing self-supervised tasks",
    "checked": true,
    "id": "c70641933b04d7e9c9f28ccb5723d08ba53f3bf8",
    "semantic_title": "re-entry prediction for online conversations via self-supervised learning",
    "citation_count": 6,
    "authors": [
      "Lingzhi Wang",
      "Xingshan Zeng",
      "Huang Hu",
      "Kam-Fai Wong",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.184": {
    "title": "proScript: Partially Ordered Scripts Generation",
    "volume": "findings",
    "abstract": "Scripts – prototypical event sequences describing everyday activities – have been shown to help understand narratives by providing expectations, resolving ambiguity, and filling in unstated information. However, to date they have proved hard to author or extract from text. In this work, we demonstrate for the first time that pre-trained neural language models can be finetuned to generate high-quality scripts, at varying levels of granularity, for a wide range of everyday scenarios (e.g., bake a cake). To do this, we collect a large (6.4k) crowdsourced partially ordered scripts (named proScript), that is substantially larger than prior datasets, and develop models that generate scripts by combining language generation and graph structure prediction. We define two complementary tasks: (i) edge prediction: given a scenario and unordered events, organize the events into a valid (possibly partial-order) script, and (ii) script generation: given only a scenario, generate events and organize them into a (possibly partial-order) script. Our experiments show that our models perform well (e.g., F1=75.7 on task (i)), illustrating a new approach to overcoming previous barriers to script collection. We also show that there is still significant room for improvement toward human level performance. Together, our tasks, dataset, and models offer a new research direction for learning script knowledge",
    "checked": true,
    "id": "53e161d4434576355fc5f63fe56afd8e135174b2",
    "semantic_title": "proscript: partially ordered scripts generation via pre-trained language models",
    "citation_count": 25,
    "authors": [
      "Keisuke Sakaguchi",
      "Chandra Bhagavatula",
      "Ronan Le Bras",
      "Niket Tandon",
      "Peter Clark",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.185": {
    "title": "Speaker Turn Modeling for Dialogue Act Classification",
    "volume": "findings",
    "abstract": "Dialogue Act (DA) classification is the task of classifying utterances with respect to the function they serve in a dialogue. Existing approaches to DA classification model utterances without incorporating the turn changes among speakers throughout the dialogue, therefore treating it no different than non-interactive written text. In this paper, we propose to integrate the turn changes in conversations among speakers when modeling DAs. Specifically, we learn conversation-invariant speaker turn embeddings to represent the speaker turns in a conversation; the learned speaker turn embeddings are then merged with the utterance embeddings for the downstream task of DA classification. With this simple yet effective mechanism, our model is able to capture the semantics from the dialogue content while accounting for different speaker turns in a conversation. Validation on three benchmark public datasets demonstrates superior performance of our model",
    "checked": true,
    "id": "935147e13b5c368989b3867b518a544418ca979e",
    "semantic_title": "speaker turn modeling for dialogue act classification",
    "citation_count": 34,
    "authors": [
      "Zihao He",
      "Leili Tavabi",
      "Kristina Lerman",
      "Mohammad Soleymani"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.186": {
    "title": "Unsupervised Domain Adaptation Method with Semantic-Structural Alignment for Dependency Parsing",
    "volume": "findings",
    "abstract": "Unsupervised cross-domain dependency parsing is to accomplish domain adaptation for dependency parsing without using labeled data in target domain. Existing methods are often of the pseudo-annotation type, which generates data through self-annotation of the base model and performing iterative training. However, these methods fail to consider the change of model structure for domain adaptation. In addition, the structural information contained in the text cannot be fully exploited. To remedy these drawbacks, we propose a Semantics-Structure Adaptative Dependency Parser (SSADP), which accomplishes unsupervised cross-domain dependency parsing without relying on pseudo-annotation or data selection. In particular, we design two feature extractors to extract semantic and structural features respectively. For each type of features, a corresponding feature adaptation method is utilized to achieve domain adaptation to align the domain distribution, which effectively enhances the unsupervised cross-domain transfer capability of the model. We validate the effectiveness of our model by conducting experiments on the CODT1 and CTB9 respectively, and the results demonstrate that our model can achieve consistent performance improvement. Besides, we verify the structure transfer ability of the proposed model by introducing Weisfeiler-Lehman Test",
    "checked": true,
    "id": "ca150b46f5baf428d449c86b3c8d3173367303a1",
    "semantic_title": "unsupervised domain adaptation method with semantic-structural alignment for dependency parsing",
    "citation_count": 2,
    "authors": [
      "Boda Lin",
      "Mingzheng Li",
      "Si Li",
      "Yong Luo"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.187": {
    "title": "Devil's Advocate: Novel Boosting Ensemble Method from Psychological Findings for Text Classification",
    "volume": "findings",
    "abstract": "We present a new form of ensemble method–Devil's Advocate, which uses a deliberately dissenting model to force other submodels within the ensemble to better collaborate. Our method consists of two different training settings: one follows the conventional training process (Norm), and the other is trained by artificially generated labels (DevAdv). After training the models, Norm models are fine-tuned through an additional loss function, which uses the DevAdv model as a constraint. In making a final decision, the proposed ensemble model sums the scores of Norm models and then subtracts the score of the DevAdv model. The DevAdv model improves the overall performance of the other models within the ensemble. In addition to our ensemble framework being based on psychological background, it also shows comparable or improved performance on 5 text classification tasks when compared to conventional ensemble methods",
    "checked": true,
    "id": "0c135580370f4c3f9fae9906ab165a4211749cc6",
    "semantic_title": "devil's advocate: novel boosting ensemble method from psychological findings for text classification",
    "citation_count": 1,
    "authors": [
      "Hwiyeol Jo",
      "Jaeseo Lim",
      "Byoung-Tak Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.188": {
    "title": "SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks",
    "volume": "findings",
    "abstract": "Transformer-based pre-trained language models boost the performance of open-domain dialogue systems. Prior works leverage Transformer-based pre-trained language models to generate texts with desired attributes in two general approaches: (1) gradient-based methods: updating all latent representations of pre-trained models with gradients from attribute models; (2) weighted-decoding methods: re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based methods lead to high computation cost and can easily get overfitted on small training sets, while weighted-decoding methods are inherently constrained by the low-variance high-bias pre-trained model. In this work, we propose a novel approach to control the generation of Transformer-based pre-trained language models: the SideControl framework, which leverages a novel control attributes loss to incorporate useful control signals, and is shown to perform well with very limited training samples. We evaluate our proposed method on two benchmark open-domain dialogue datasets, and results show that the SideControl framework has better controllability, higher generation quality and better sample-efficiency than existing gradient-based and weighted-decoding baselines",
    "checked": true,
    "id": "56b7806216e1d88f03e404cd7227f9a7aa5ec02f",
    "semantic_title": "sidecontrol: controlled open-domain dialogue generation via additive side networks",
    "citation_count": 7,
    "authors": [
      "Wanyu Du",
      "Yangfeng Ji"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.189": {
    "title": "Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability",
    "volume": "findings",
    "abstract": "This paper investigates whether the power of the models pre-trained on text data, such as BERT, can be transferred to general token sequence classification applications. To verify pre-trained models' transferability, we test the pre-trained models on text classification tasks with meanings of tokens mismatches, and real-world non-text token sequence classification data, including amino acid, DNA, and music. We find that even on non-text data, the models pre-trained on text converge faster, perform better than the randomly initialized models, and only slightly worse than the models using task-specific knowledge. We also find that the representations of the text and non-text pre-trained models share non-trivial similarities",
    "checked": true,
    "id": "1ea1d73d1192e191a89a3b9cac725f72fbe80c56",
    "semantic_title": "is bert a cross-disciplinary knowledge learner? a surprising finding of pre-trained models' transferability",
    "citation_count": 16,
    "authors": [
      "Wei-Tsung Kao",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.190": {
    "title": "Geo-BERT Pre-training Model for Query Rewriting in POI Search",
    "volume": "findings",
    "abstract": "Query Rewriting (QR) is proposed to solve the problem of the word mismatch between queries and documents in Web search. Existing approaches usually model QR with an end-to-end sequence-to-sequence (seq2seq) model. The state-of-the-art Transformer-based models can effectively learn textual semantics from user session logs, but they often ignore users' geographic location information that is crucial for the Point-of-Interest (POI) search of map services. In this paper, we proposed a pre-training model, called Geo-BERT, to integrate semantics and geographic information in the pre-trained representations of POIs. Firstly, we simulate POI distribution in the real world as a graph, in which nodes represent POIs and multiple geographic granularities. Then we use graph representation learning methods to get geographic representations. Finally, we train a BERT-like pre-training model with text and POIs' graph embeddings to get an integrated representation of both geographic and semantic information, and apply it in the QR of POI search. The proposed model achieves excellent accuracy on a wide range of real-world datasets of map services",
    "checked": true,
    "id": "38e1b11dcd59b7e0a6ffa5bdc11831db0c72bdc3",
    "semantic_title": "geo-bert pre-training model for query rewriting in poi search",
    "citation_count": 21,
    "authors": [
      "Xiao Liu",
      "Juan Hu",
      "Qi Shen",
      "Huan Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.191": {
    "title": "Leveraging Bidding Graphs for Advertiser-Aware Relevance Modeling in Sponsored Search",
    "volume": "findings",
    "abstract": "Recently, sponsored search has become one of the most lucrative channels for marketing. As the fundamental basis of sponsored search, relevance modeling has attracted increasing attention due to the tremendous practical value. Most existing methods solely rely on the query-keyword pairs. However, keywords are usually short texts with scarce semantic information, which may not precisely reflect the underlying advertising intents. In this paper, we investigate the novel problem of advertiser-aware relevance modeling, which leverages the advertisers' information to bridge the gap between the search intents and advertising purposes. Our motivation lies in incorporating the unsupervised bidding behaviors as the complementary graphs to learn desirable advertiser representations. We further propose a Bidding-Graph augmented Triple-based Relevance model BGTR with three towers to deeply fuse the bidding graphs and semantic textual data. Empirically, we evaluate the BGTR model over a large industry dataset, and the experimental results consistently demonstrate its superiority",
    "checked": true,
    "id": "043d047213d6dbfa9cdab1f4ca40d08ca4b78397",
    "semantic_title": "leveraging bidding graphs for advertiser-aware relevance modeling in sponsored search",
    "citation_count": 9,
    "authors": [
      "Shuxian Bi",
      "Chaozhuo Li",
      "Xiao Han",
      "Zheng Liu",
      "Xing Xie",
      "Haizhen Huang",
      "Zengxuan Wen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.192": {
    "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation",
    "volume": "findings",
    "abstract": "Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. We also conduct experiments on our newly proposed benchmark to show that the augmentation effect is not only attributed to memorization. Further ablation studies and a qualitative analysis provide more insights into our approach",
    "checked": true,
    "id": "bbfdcbfee1762d48cae9db8637f21ea3c234ba30",
    "semantic_title": "gpt3mix: leveraging large-scale language models for text augmentation",
    "citation_count": 238,
    "authors": [
      "Kang Min Yoo",
      "Dongju Park",
      "Jaewook Kang",
      "Sang-Woo Lee",
      "Woomyoung Park"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.193": {
    "title": "Context-aware Entity Typing in Knowledge Graphs",
    "volume": "findings",
    "abstract": "Knowledge graph entity typing aims to infer entities' missing types in knowledge graphs which is an important but under-explored issue. This paper proposes a novel method for this task by utilizing entities' contextual information. Specifically, we design two inference mechanisms: i) N2T: independently use each neighbor of an entity to infer its type; ii) Agg2T: aggregate the neighbors of an entity to infer its type. Those mechanisms will produce multiple inference results, and an exponentially weighted pooling method is used to generate the final inference result. Furthermore, we propose a novel loss function to alleviate the false-negative problem during training. Experiments on two real-world KGs demonstrate the effectiveness of our method. The source code and data of this paper can be obtained from https://github.com/CCIIPLab/CET",
    "checked": true,
    "id": "d3f68d15985ae85fb7b2d8ecb2085763f6ef4f00",
    "semantic_title": "context-aware entity typing in knowledge graphs",
    "citation_count": 22,
    "authors": [
      "Weiran Pan",
      "Wei Wei",
      "Xian-Ling Mao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.194": {
    "title": "Attribute Alignment: Controlling Text Generation from Pre-trained Language Models",
    "volume": "findings",
    "abstract": "Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics, remains a challenge. We propose a simple and flexible method for controlling text generation by aligning disentangled attribute representations. In contrast to recent efforts on training a discriminator to perturb the token level distribution for an attribute, we use the same data to learn an alignment function to guide the pre-trained, non-controlled language model to generate texts with the target attribute without changing the original language model parameters. We evaluate our method on sentiment- and topic-controlled generation, and show large performance gains over previous methods while retaining fluency and diversity",
    "checked": true,
    "id": "298dd87e00fe54c683aa9231902d3e4ad5d8f239",
    "semantic_title": "attribute alignment: controlling text generation from pre-trained language models",
    "citation_count": 38,
    "authors": [
      "Dian Yu",
      "Zhou Yu",
      "Kenji Sagae"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.195": {
    "title": "Generate & Rank: A Multi-task Framework for Math Word Problems",
    "volume": "findings",
    "abstract": "Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% to 85.4%) higher than the state-of-the-art. Code could be found at https://github.com/huawei-noah/noah-research",
    "checked": true,
    "id": "4698fc4712f0212c8a3810fd67b41ee8b8896aba",
    "semantic_title": "generate & rank: a multi-task framework for math word problems",
    "citation_count": 129,
    "authors": [
      "Jianhao Shen",
      "Yichun Yin",
      "Lin Li",
      "Lifeng Shang",
      "Xin Jiang",
      "Ming Zhang",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.196": {
    "title": "MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering",
    "volume": "findings",
    "abstract": "In Visual Question Answering (VQA), existing bilinear methods focus on the interaction between images and questions. As a result, the answers are either spliced into the questions or utilized as labels only for classification. On the other hand, trilinear models such as the CTI model efficiently utilize the inter-modality information between answers, questions, and images, while ignoring intra-modality information. Inspired by this observation, we propose a new trilinear interaction framework called MIRTT (Learning Multimodal Interaction Representations from Trilinear Transformers), incorporating the attention mechanisms for capturing inter-modality and intra-modality relationships. Moreover, we design a two-stage workflow where a bilinear model reduces the free-form, open-ended VQA problem into a multiple-choice VQA problem. Furthermore, to obtain accurate and generic multimodal representations, we pre-train MIRTT with masked language prediction. Our method achieves state-of-the-art performance on the Visual7W Telling task and VQA-1.0 Multiple Choice task and outperforms bilinear baselines on the VQA-2.0, TDIUC and GQA datasets",
    "checked": true,
    "id": "dd13620915c44a765a3f7642ececb0ff4ccab839",
    "semantic_title": "mirtt: learning multimodal interaction representations from trilinear transformers for visual question answering",
    "citation_count": 15,
    "authors": [
      "Junjie Wang",
      "Yatai Ji",
      "Jiaqi Sun",
      "Yujiu Yang",
      "Tetsuya Sakai"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.197": {
    "title": "UniteD-SRL: A Unified Dataset for Span- and Dependency-Based Multilingual and Cross-Lingual Semantic Role Labeling",
    "volume": "findings",
    "abstract": "Multilingual and cross-lingual Semantic Role Labeling (SRL) have recently garnered increasing attention as multilingual text representation techniques have become more effective and widely available. While recent work has attained growing success, results on gold multilingual benchmarks are still not easily comparable across languages, making it difficult to grasp where we stand. For example, in CoNLL-2009, the standard benchmark for multilingual SRL, language-to-language comparisons are affected by the fact that each language has its own dataset which differs from the others in size, domains, sets of labels and annotation guidelines. In this paper, we address this issue and propose UniteD-SRL, a new benchmark for multilingual and cross-lingual, span- and dependency-based SRL. UniteD-SRL provides expert-curated parallel annotations using a common predicate-argument structure inventory, allowing direct comparisons across languages and encouraging studies on cross-lingual transfer in SRL. We release UniteD-SRL v1.0 at https://github.com/SapienzaNLP/united-srl",
    "checked": true,
    "id": "7f8aa4c1663f7c0773cde111227d47c149849293",
    "semantic_title": "united-srl: a unified dataset for span- and dependency-based multilingual and cross-lingual semantic role labeling",
    "citation_count": 3,
    "authors": [
      "Rocco Tripodi",
      "Simone Conia",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.198": {
    "title": "Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval",
    "volume": "findings",
    "abstract": "Dual-Encoders is a promising mechanism for answer retrieval in question answering (QA) systems. Currently most conventional Dual-Encoders learn the semantic representations of questions and answers merely through matching score. Researchers proposed to introduce the QA interaction features in scoring function but at the cost of low efficiency in inference stage. To keep independent encoding of questions and answers during inference stage, variational auto-encoder is further introduced to reconstruct answers (questions) from question (answer) embeddings as an auxiliary task to enhance QA interaction in representation learning in training stage. However, the needs of text generation and answer retrieval are different, which leads to hardness in training. In this work, we propose a framework to enhance the Dual-Encoders model with question answer cross-embeddings and a novel Geometry Alignment Mechanism (GAM) to align the geometry of embeddings from Dual-Encoders with that from Cross-Encoders. Extensive experimental results show that our framework significantly improves Dual-Encoders model and outperforms the state-of-the-art method on multiple answer retrieval datasets",
    "checked": true,
    "id": "52526a83cb87ea3ca6f2f793b4918b0351aa70cf",
    "semantic_title": "enhancing dual-encoders with question and answer cross-embeddings for answer retrieval",
    "citation_count": 11,
    "authors": [
      "Yanmeng Wang",
      "Jun Bai",
      "Ye Wang",
      "Jianfei Zhang",
      "Wenge Rong",
      "Zongcheng Ji",
      "Shaojun Wang",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.199": {
    "title": "A Neural Graph-based Local Coherence Model",
    "volume": "findings",
    "abstract": "Entity grids and entity graphs are two frameworks for modeling local coherence. These frameworks represent entity relations between sentences and then extract features from such representations to encode coherence. The benefits of convolutional neural models for extracting informative features from entity grids have been recently studied. In this work, we study the benefits of Relational Graph Convolutional Networks (RGCN) to encode entity graphs for measuring local coherence. We evaluate our neural graph-based model for two benchmark coherence evaluation tasks: sentence ordering (SO) and summary coherence rating (SCR). The results show that our neural graph-based model consistently outperforms the neural grid-based model for both tasks. Our model performs competitively with a strong baseline coherence model, while our model uses 50% fewer parameters. Our work defines a new, efficient, and effective baseline for local coherence modeling",
    "checked": true,
    "id": "b9209b3577a789c79dc9b69eb1112589280e1b71",
    "semantic_title": "a neural graph-based local coherence model",
    "citation_count": 8,
    "authors": [
      "Mohsen Mesgar",
      "Leonardo F. R. Ribeiro",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.200": {
    "title": "GiBERT: Enhancing BERT with Linguistic Information using a Lightweight Gated Injection Method",
    "volume": "findings",
    "abstract": "Large pre-trained language models such as BERT have been the driving force behind recent improvements across many NLP tasks. However, BERT is only trained to predict missing words – either through masking or next sentence prediction – and has no knowledge of lexical, syntactic or semantic information beyond what it picks up through unsupervised pre-training. We propose a novel method to explicitly inject linguistic information in the form of word embeddings into any layer of a pre-trained BERT. When injecting counter-fitted and dependency-based embeddings, the performance improvements on multiple semantic similarity datasets indicate that such information is beneficial and currently missing from the original model. Our qualitative analysis shows that counter-fitted embedding injection is particularly beneficial, with notable improvements on examples that require synonym resolution",
    "checked": true,
    "id": "eaac004e8fcae19d9b34dc9836e1c00e501699d1",
    "semantic_title": "gibert: enhancing bert with linguistic information using a lightweight gated injection method",
    "citation_count": 5,
    "authors": [
      "Nicole Peinelt",
      "Marek Rei",
      "Maria Liakata"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.201": {
    "title": "RollingLDA: An Update Algorithm of Latent Dirichlet Allocation to Construct Consistent Time Series from Textual Data",
    "volume": "findings",
    "abstract": "We propose a rolling version of the Latent Dirichlet Allocation, called RollingLDA. By a sequential approach, it enables the construction of LDA-based time series of topics that are consistent with previous states of LDA models. After an initial modeling, updates can be computed efficiently, allowing for real-time monitoring and detection of events or structural breaks. For this purpose, we propose suitable similarity measures for topics and provide simulation evidence of superiority over other commonly used approaches. The adequacy of the resulting method is illustrated by an application to an example corpus. In particular, we compute the similarity of sequentially obtained topic and word distributions over consecutive time periods. For a representative example corpus consisting of The New York Times articles from 1980 to 2020, we analyze the effect of several tuning parameter choices and we run the RollingLDA method on the full dataset of approximately 4 million articles to demonstrate its feasibility",
    "checked": true,
    "id": "896aaace6570c61537b93b0e8906b07ae75f895b",
    "semantic_title": "rollinglda: an update algorithm of latent dirichlet allocation to construct consistent time series from textual data",
    "citation_count": 17,
    "authors": [
      "Jonas Rieger",
      "Carsten Jentsch",
      "Jörg Rahnenführer"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.202": {
    "title": "What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension",
    "volume": "findings",
    "abstract": "Machine reading comprehension (MRC) is a challenging NLP task for it requires to carefully deal with all linguistic granularities from word, sentence to passage. For extractive MRC, the answer span has been shown mostly determined by key evidence linguistic units, in which it is a sentence in most cases. However, we recently discovered that sentences may not be clearly defined in many languages to different extents, so that this causes so-called location unit ambiguity problem and as a result makes it difficult for the model to determine which sentence exactly contains the answer span when sentence itself has not been clearly defined at all. Taking Chinese language as a case study, we explain and analyze such a linguistic phenomenon and correspondingly propose a reader with Explicit Span-Sentence Predication to alleviate such a problem. Our proposed reader eventually helps achieve a new state-of-the-art on Chinese MRC benchmark and shows great potential in dealing with other languages",
    "checked": true,
    "id": "e9bbe912c894879fdf0c38530137ef70e9680b3f",
    "semantic_title": "what if sentence-hood is hard to define: a case study in chinese reading comprehension",
    "citation_count": 3,
    "authors": [
      "Jiawei Wang",
      "Hai Zhao",
      "Yinggong Zhao",
      "Libin Shen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.203": {
    "title": "Refining BERT Embeddings for Document Hashing via Mutual Information Maximization",
    "volume": "findings",
    "abstract": "Existing unsupervised document hashing methods are mostly established on generative models. Due to the difficulties of capturing long dependency structures, these methods rarely model the raw documents directly, but instead to model the features extracted from them (e.g. bag-of-words (BOG), TFIDF). In this paper, we propose to learn hash codes from BERT embeddings after observing their tremendous successes on downstream tasks. As a first try, we modify existing generative hashing models to accommodate the BERT embeddings. However, little improvement is observed over the codes learned from the old BOG or TFIDF features. We attribute this to the reconstruction requirement in the generative hashing, which will enforce irrelevant information that is abundant in the BERT embeddings also compressed into the codes. To remedy this issue, a new unsupervised hashing paradigm is further proposed based on the mutual information (MI) maximization principle. Specifically, the method first constructs appropriate global and local codes from the documents and then seeks to maximize their mutual information. Experimental results on three benchmark datasets demonstrate that the proposed method is able to generate hash codes that outperform existing ones learned from BOG features by a substantial margin",
    "checked": true,
    "id": "b0e80869c7527f2f1900e046103459e762b3b639",
    "semantic_title": "refining bert embeddings for document hashing via mutual information maximization",
    "citation_count": 6,
    "authors": [
      "Zijing Ou",
      "Qinliang Su",
      "Jianxing Yu",
      "Ruihui Zhao",
      "Yefeng Zheng",
      "Bang Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.204": {
    "title": "REBEL: Relation Extraction By End-to-end Language generation",
    "volume": "findings",
    "abstract": "Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types. We show our model's flexibility by fine-tuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them",
    "checked": true,
    "id": "cbc449ea3d410c127db81e37ca6b6387c6d05cdc",
    "semantic_title": "rebel: relation extraction by end-to-end language generation",
    "citation_count": 269,
    "authors": [
      "Pere-Lluís Huguet Cabot",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.205": {
    "title": "Wine is not v i n. On the Compatibility of Tokenizations across Languages",
    "volume": "findings",
    "abstract": "The size of the vocabulary is a central design choice in large pretrained language models, with respect to both performance and memory requirements. Typically, subword tokenization algorithms such as byte pair encoding and WordPiece are used. In this work, we investigate the compatibility of tokenizations for multilingual static and contextualized embedding spaces and propose a measure that reflects the compatibility of tokenizations across languages. Our goal is to prevent incompatible tokenizations, e.g., \"wine\" (word-level) in English vs. \"v i n\" (character-level) in French, which make it hard to learn good multilingual semantic representations. We show that our compatibility measure allows the system designer to create vocabularies across languages that are compatible – a desideratum that so far has been neglected in multilingual models",
    "checked": true,
    "id": "445f82be81e37b064730285cf8de70edce80d6a5",
    "semantic_title": "wine is not v i n. - on the compatibility of tokenizations across languages",
    "citation_count": 17,
    "authors": [
      "Antonis Maronikolakis",
      "Philipp Dufter",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.206": {
    "title": "Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media",
    "volume": "findings",
    "abstract": "Language use differs between domains and even within a domain, language use changes over time. For pre-trained language models like BERT, domain adaptation through continued pre-training has been shown to improve performance on in-domain downstream tasks. In this article, we investigate whether temporal adaptation can bring additional benefits. For this purpose, we introduce a corpus of social media comments sampled over three years. It contains unlabelled data for adaptation and evaluation on an upstream masked language modelling task as well as labelled data for fine-tuning and evaluation on a downstream document classification task. We find that temporality matters for both tasks: temporal adaptation improves upstream and temporal fine-tuning downstream task performance. Time-specific models generally perform better on past than on future test sets, which matches evidence on the bursty usage of topical words. However, adapting BERT to time and domain does not improve performance on the downstream task over only adapting to domain. Token-level analysis shows that temporal adaptation captures event-driven changes in language use in the downstream task, but not those changes that are actually relevant to task performance. Based on our findings, we discuss when temporal adaptation may be more effective",
    "checked": true,
    "id": "1fa819dd252a8feb5350d3cacac3e019e3b8e5c7",
    "semantic_title": "temporal adaptation of bert and performance on downstream document classification: insights from social media",
    "citation_count": 65,
    "authors": [
      "Paul Röttger",
      "Janet Pierrehumbert"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.207": {
    "title": "Skim-Attention: Learning to Focus via Document Layout",
    "volume": "findings",
    "abstract": "Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention",
    "checked": true,
    "id": "7a79bf9e43df1779f2ef6fdf6df477f0404b6e36",
    "semantic_title": "skim-attention: learning to focus via document layout",
    "citation_count": 9,
    "authors": [
      "Laura Nguyen",
      "Thomas Scialom",
      "Jacopo Staiano",
      "Benjamin Piwowarski"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.208": {
    "title": "Attention-based Contrastive Learning for Winograd Schemas",
    "volume": "findings",
    "abstract": "Self-supervised learning has recently attracted considerable attention in the NLP community for its ability to learn discriminative features using a contrastive objective. This paper investigates whether contrastive learning can be extended to Transfomer attention to tackling the Winograd Schema Challenge. To this end, we propose a novel self-supervised framework, leveraging a contrastive loss directly at the level of self-attention. Experimental analysis of our attention-based models on multiple datasets demonstrates superior commonsense reasoning capabilities. The proposed approach outperforms all comparable unsupervised approaches while occasionally surpassing supervised ones",
    "checked": true,
    "id": "da0af132811c42cf9a154d445f74ba6ca2f1f51f",
    "semantic_title": "attention-based contrastive learning for winograd schemas",
    "citation_count": 3,
    "authors": [
      "Tassilo Klein",
      "Moin Nabi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.209": {
    "title": "Give the Truth: Incorporate Semantic Slot into Abstractive Dialogue Summarization",
    "volume": "findings",
    "abstract": "Abstractive dialogue summarization suffers from a lots of factual errors, which are due to scattered salient elements in the multi-speaker information interaction process. In this work, we design a heterogeneous semantic slot graph with a slot-level mask cross-attention to enhance the slot features for more correct summarization. We also propose a slot-driven beam search algorithm in the decoding process to give priority to generating salient elements in a limited length by \"filling-in-the-blanks\". Besides, an adversarial contrastive learning assisting the training process is introduced to alleviate the exposure bias. Experimental performance on different types of factual errors shows the effectiveness of our methods and human evaluation further verifies the results",
    "checked": true,
    "id": "f1acf5b35888c4dd90f1c949dfd103e3a7a342a7",
    "semantic_title": "give the truth: incorporate semantic slot into abstractive dialogue summarization",
    "citation_count": 10,
    "authors": [
      "Lulu Zhao",
      "Weihao Zeng",
      "Weiran Xu",
      "Jun Guo"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.210": {
    "title": "Challenges in Detoxifying Language Models",
    "volume": "findings",
    "abstract": "Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions—highlighting further the nuances involved in careful evaluation of LM toxicity",
    "checked": true,
    "id": "d64e57b9780f30f5b49bf620fdfb8584651b7f85",
    "semantic_title": "challenges in detoxifying language models",
    "citation_count": 197,
    "authors": [
      "Johannes Welbl",
      "Amelia Glaese",
      "Jonathan Uesato",
      "Sumanth Dathathri",
      "John Mellor",
      "Lisa Anne Hendricks",
      "Kirsty Anderson",
      "Pushmeet Kohli",
      "Ben Coppin",
      "Po-Sen Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.211": {
    "title": "Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation",
    "volume": "findings",
    "abstract": "Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at github.com/SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings",
    "checked": true,
    "id": "d48d1e80b6ea9708fa3a09d1556a7ced3b147da2",
    "semantic_title": "collecting a large-scale gender bias dataset for coreference resolution and machine translation",
    "citation_count": 69,
    "authors": [
      "Shahar Levy",
      "Koren Lazar",
      "Gabriel Stanovsky"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.212": {
    "title": "Competence-based Curriculum Learning for Multilingual Machine Translation",
    "volume": "findings",
    "abstract": "Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation performance of different languages in multilingual translation models are quite different. We argue that this imbalance problem stems from the different learning competencies of different languages. Therefore, we focus on balancing the learning competencies of different languages and propose Competence-based Curriculum Learning for Multilingual Machine Translation, named CCL-M. Specifically, we firstly define two competencies to help schedule the high resource languages (HRLs) and the low resource languages: 1) Self-evaluated Competence, evaluating how well the language itself has been learned; and 2) HRLs-evaluated Competence, evaluating whether an LRL is ready to be learned according to HRLs' Self-evaluated Competence. Based on the above competencies, we utilize the proposed CCL-M algorithm to gradually add new languages into the training set in a curriculum learning manner. Furthermore, we propose a novel competence-aware dynamic balancing sampling strategy for better selecting training samples in multilingual training. Experimental results show that our approach has achieved a steady and significant performance gain compared to the previous state-of-the-art approach on the TED talks dataset",
    "checked": true,
    "id": "e3990c75d70e9ce024a0f17df0903b7291313547",
    "semantic_title": "competence-based curriculum learning for multilingual machine translation",
    "citation_count": 16,
    "authors": [
      "Mingliang Zhang",
      "Fandong Meng",
      "Yunhai Tong",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.213": {
    "title": "Informed Sampling for Diversity in Concept-to-Text NLG",
    "volume": "findings",
    "abstract": "Deep-learning models for language generation tasks tend to produce repetitive output. Various methods have been proposed to encourage lexical diversity during decoding, but this often comes at a cost to the perceived fluency and adequacy of the output. In this work, we propose to ameliorate this cost by using an Imitation Learning approach to explore the level of diversity that a language generation model can reliably produce. Specifically, we augment the decoding process with a meta-classifier trained to distinguish which words at any given timestep will lead to high-quality output. We focus our experiments on concept-to-text generation where models are sensitive to the inclusion of irrelevant words due to the strict relation between input and output. Our analysis shows that previous methods for diversity underperform in this setting, while human evaluation suggests that our proposed method achieves a high level of diversity with minimal effect on the output's fluency and adequacy",
    "checked": true,
    "id": "9a2384b8caeddbc2667786d0b268d956a5d00c41",
    "semantic_title": "informed sampling for diversity in concept-to-text nlg",
    "citation_count": 2,
    "authors": [
      "Giulio Zhou",
      "Gerasimos Lampouras"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.214": {
    "title": "Novel Natural Language Summarization of Program Code via Leveraging Multiple Input Representations",
    "volume": "findings",
    "abstract": "The lack of description of a given program code acts as a big hurdle to those developers new to the code base for its understanding. To tackle this problem, previous work on code summarization, the task of automatically generating code description given a piece of code reported that an auxiliary learning model trained to produce API (Application Programming Interface) embeddings showed promising results when applied to a downstream, code summarization model. However, different codes having different summaries can have the same set of API sequences. If we train a model to generate summaries given an API sequence, the model will not be able to learn effectively. Nevertheless, we note that the API sequence can still be useful and has not been actively utilized. This work proposes a novel multi-task approach that simultaneously trains two similar tasks: 1) summarizing a given code (code to summary), and 2) summarizing a given API sequence (API sequence to summary). We propose a novel code-level encoder based on BERT capable of expressing the semantics of code, and obtain representations for every line of code. Our work is the first code summarization work that utilizes a natural language-based contextual pre-trained language model in its encoder. We evaluate our approach using two common datasets (Java and Python) that have been widely used in previous studies. Our experimental results show that our multi-task approach improves over the baselines and achieves the new state-of-the-art",
    "checked": true,
    "id": "d137c180320e45931bc9a61c294b40b2364237c4",
    "semantic_title": "novel natural language summarization of program code via leveraging multiple input representations",
    "citation_count": 9,
    "authors": [
      "Fuxiang Chen",
      "Mijung Kim",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.215": {
    "title": "WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER",
    "volume": "findings",
    "abstract": "Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. We exploit the texts of Wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for NER. We evaluate our datasets extensively on standard benchmarks for NER, yielding substantial improvements up to 6 span-based F1-score points over previous state-of-the-art systems for data creation",
    "checked": true,
    "id": "f7c14e79d3eb1d24b4184d106244be1672113ce2",
    "semantic_title": "wikineural: combined neural and knowledge-based silver data creation for multilingual ner",
    "citation_count": 78,
    "authors": [
      "Simone Tedeschi",
      "Valentino Maiorca",
      "Niccolò Campolungo",
      "Francesco Cecconi",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.216": {
    "title": "Beyond Grammatical Error Correction: Improving L1-influenced research writing in English using pre-trained encoder-decoder models",
    "volume": "findings",
    "abstract": "In this paper, we present a new method for training a writing improvement model adapted to the writer's first language (L1) that goes beyond grammatical error correction (GEC). Without using annotated training data, we rely solely on pre-trained language models fine-tuned with parallel corpora of reference translation aligned with machine translation. We evaluate our model with corpora of academic papers written in English by L1 Portuguese and L1 Spanish scholars and a reference corpus of expert academic English. We show that our model is able to address specific L1-influenced writing and more complex linguistic phenomena than existing methods, outperforming what a state-of-the-art GEC system can achieve in this regard. Our code and data are open to other researchers",
    "checked": true,
    "id": "4f3d2d28111c38c4755e6c62c27085195c9ec964",
    "semantic_title": "beyond grammatical error correction: improving l1-influenced research writing in english using pre-trained encoder-decoder models",
    "citation_count": 9,
    "authors": [
      "Gustavo Zomer",
      "Ana Frankenberg-Garcia"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.217": {
    "title": "Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews",
    "volume": "findings",
    "abstract": "Online tenant reviews of multifamily residential properties present a unique source of information for commercial real estate investing and research. Real estate professionals frequently read tenant reviews to uncover property-related issues that are otherwise difficult to detect, a process that is both biased and time-consuming. Using this as motivation, we asked whether a text classification-based approach can automate the detection of four carefully defined, major quality-of-life issues: severe crime, noise nuisance, pest burden, and parking difficulties. We aggregate 5.5 million tenant reviews from five sources and use two-stage crowdsourced labeling on 0.1% of the data to produce high-quality labels for subsequent text classification. Following fine-tuning of pretrained language models on millions of reviews, we train a multi-label reviews classifier that achieves a mean AUROC of 0.965 on these labels. We next use the model to reveal temporal and spatial patterns among tens of thousands of multifamily properties. Collectively, these results highlight the feasibility of automated analysis of housing trends and investment opportunities using tenant-perspective data",
    "checked": true,
    "id": "d9eb679c951b20ab8027022830bb3111e9c26d4f",
    "semantic_title": "classification and geotemporal analysis of quality-of-life issues in tenant reviews",
    "citation_count": 0,
    "authors": [
      "Adam Haber",
      "Zeev Waks"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.218": {
    "title": "Probing Pre-trained Language Models for Semantic Attributes and their Values",
    "volume": "findings",
    "abstract": "Pretrained language models (PTLMs) yield state-of-the-art performance on many natural language processing tasks, including syntax, semantics and commonsense. In this paper, we focus on identifying to what extent do PTLMs capture semantic attributes and their values, e.g., the correlation between rich and high net worth. We use PTLMs to predict masked tokens using patterns and lists of items from Wikidata in order to verify how likely PTLMs encode semantic attributes along with their values. Such inferences based on semantics are intuitive for humans as part of our language understanding. Since PTLMs are trained on large amount of Wikipedia data we would assume that they can generate similar predictions, yet our findings reveal that PTLMs are still much worse than humans on this task. We show evidence and analysis explaining how to exploit our methodology to integrate better context and semantics into PTLMs using knowledge bases",
    "checked": true,
    "id": "593284010379e02f6ab57e7208b5511185ce8c0e",
    "semantic_title": "probing pre-trained language models for semantic attributes and their values",
    "citation_count": 11,
    "authors": [
      "Meriem Beloucif",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.219": {
    "title": "Uncovering the Limits of Text-based Emotion Detection",
    "volume": "findings",
    "abstract": "Identifying emotions from text is crucial for a variety of real world tasks. We consider the two largest now-available corpora for emotion classification: GoEmotions, with 58k messages labelled by readers, and Vent, with 33M writer-labelled messages. We design a benchmark and evaluate several feature spaces and learning algorithms, including two simple yet novel models on top of BERT that outperform previous strong baselines on GoEmotions. Through an experiment with human participants, we also analyze the differences between how writers express emotions and how readers perceive them. Our results suggest that emotions expressed by writers are harder to identify than emotions that readers perceive. We share a public web interface for researchers to explore our models",
    "checked": true,
    "id": "4bf8e8d1f4b74e97e5c7f5c67bdb6229c76e5812",
    "semantic_title": "uncovering the limits of text-based emotion detection",
    "citation_count": 26,
    "authors": [
      "Nurudin Alvarez-Gonzalez",
      "Andreas Kaltenbrunner",
      "Vicenç Gómez"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.220": {
    "title": "Named Entity Recognition for Entity Linking: What Works and What's Next",
    "volume": "findings",
    "abstract": "Entity Linking (EL) systems have achieved impressive results on standard benchmarks mainly thanks to the contextualized representations provided by recent pretrained language models. However, such systems still require massive amounts of data – millions of labeled examples – to perform at their best, with training times that often exceed several days, especially when limited computational resources are available. In this paper, we look at how Named Entity Recognition (NER) can be exploited to narrow the gap between EL systems trained on high and low amounts of labeled data. More specifically, we show how and to what extent an EL system can benefit from NER to enhance its entity representations, improve candidate selection, select more effective negative samples and enforce hard and soft constraints on its output entities. We release our software – code and model checkpoints – at https://github.com/Babelscape/ner4el",
    "checked": true,
    "id": "99dd862127f6250a7b832e05de89fb5fb6d2f29b",
    "semantic_title": "named entity recognition for entity linking: what works and what's next",
    "citation_count": 33,
    "authors": [
      "Simone Tedeschi",
      "Simone Conia",
      "Francesco Cecconi",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.221": {
    "title": "Learning Numeracy: A Simple Yet Effective Number Embedding Approach Using Knowledge Graph",
    "volume": "findings",
    "abstract": "Numeracy plays a key role in natural language understanding. However, existing NLP approaches, not only traditional word2vec approach or contextualized transformer-based language models, fail to learn numeracy. As the result, the performance of these models is limited when they are applied to number-intensive applications in clinical and financial domains. In this work, we propose a simple number embedding approach based on knowledge graph. We construct a knowledge graph consisting of number entities and magnitude relations. Knowledge graph embedding method is then applied to obtain number vectors. Our approach is easy to implement, and experiment results on various numeracy-related NLP tasks demonstrate the effectiveness and efficiency of our method",
    "checked": true,
    "id": "f38325f21d7c3a5175893874951540f5bd200b9e",
    "semantic_title": "learning numeracy: a simple yet effective number embedding approach using knowledge graph",
    "citation_count": 11,
    "authors": [
      "Hanyu Duan",
      "Yi Yang",
      "Kar Yan Tam"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.222": {
    "title": "Weakly Supervised Semantic Parsing by Learning from Mistakes",
    "volume": "findings",
    "abstract": "Weakly supervised semantic parsing (WSP) aims at training a parser via utterance-denotation pairs. This task is challenging because it requires (1) searching consistent logical forms in a huge space; and (2) dealing with spurious logical forms. In this work, we propose Learning from Mistakes (LFM), a simple yet effective learning framework for WSP. LFM utilizes the mistakes made by a parser during searching, i.e., generating logical forms that do not execute to correct denotations, for tackling the two challenges. In a nutshell, LFM additionally trains a parser using utterance-logical form pairs created from mistakes, which can quickly bootstrap the parser to search consistent logical forms. Also, it can motivate the parser to learn the correct mapping between utterances and logical forms, thus dealing with the spuriousness of logical forms. We evaluate LFM on WikiTableQuestions, WikiSQL, and TabFact in the WSP setting. The parser trained with LFM outperforms the previous state-of-the-art semantic parsing approaches on the three datasets. Also, we find that LFM can substantially reduce the need for labeled data. Using only 10% of utterance-denotation pairs, the parser achieves 84.2 denotation accuracy on WikiSQL, which is competitive with the previous state-of-the-art approaches using 100% labeled data",
    "checked": true,
    "id": "939b6655a24724bae12589cf4970fca494f654c5",
    "semantic_title": "weakly supervised semantic parsing by learning from mistakes",
    "citation_count": 7,
    "authors": [
      "Jiaqi Guo",
      "Jian-Guang Lou",
      "Ting Liu",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.223": {
    "title": "CodeQA: A Question Answering Dataset for Source Code Comprehension",
    "volume": "findings",
    "abstract": "We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension",
    "checked": true,
    "id": "df6f3c607ae3956db722f76f3f81e672c3dfa803",
    "semantic_title": "codeqa: a question answering dataset for source code comprehension",
    "citation_count": 27,
    "authors": [
      "Chenxiao Liu",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.224": {
    "title": "Subword Mapping and Anchoring across Languages",
    "volume": "findings",
    "abstract": "State-of-the-art multilingual systems rely on shared vocabularies that sufficiently cover all considered languages. To this end, a simple and frequently used approach makes use of subword vocabularies constructed jointly over several languages. We hypothesize that such vocabularies are suboptimal due to false positives (identical subwords with different meanings across languages) and false negatives (different subwords with similar meanings). To address these issues, we propose Subword Mapping and Anchoring across Languages (SMALA), a method to construct bilingual subword vocabularies. SMALA extracts subword alignments using an unsupervised state-of-the-art mapping technique and uses them to create cross-lingual anchors based on subword similarities. We demonstrate the benefits of SMALA for cross-lingual natural language inference (XNLI), where it improves zero-shot transfer to an unseen language without task-specific data, but only by sharing subword embeddings. Moreover, in neural machine translation, we show that joint subword vocabularies obtained with SMALA lead to higher BLEU scores on sentences that contain many false positives and false negatives",
    "checked": true,
    "id": "8dad3a12d2a652122458dd377f62025354b17a2a",
    "semantic_title": "subword mapping and anchoring across languages",
    "citation_count": 12,
    "authors": [
      "Giorgos Vernikos",
      "Andrei Popescu-Belis"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.225": {
    "title": "CDLM: Cross-Document Language Modeling",
    "volume": "findings",
    "abstract": "We introduce a new pretraining approach geared for multi-document language modeling, incorporating two key ideas into the masked language modeling self-supervised objective. First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships. Second, we improve over recent long-range transformers by introducing dynamic global attention that has access to the entire input to predict masked tokens. We release CDLM (Cross-Document Language Model), a new general language model for multi-document setting that can be easily applied to downstream tasks. Our extensive analysis shows that both ideas are essential for the success of CDLM, and work in synergy to set new state-of-the-art results for several multi-text tasks",
    "checked": true,
    "id": "6aaec722a90eee0185d4bbfebbcd4f228ed1577f",
    "semantic_title": "cross-document language modeling",
    "citation_count": 33,
    "authors": [
      "Avi Caciularu",
      "Arman Cohan",
      "Iz Beltagy",
      "Matthew Peters",
      "Arie Cattan",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.226": {
    "title": "Patterns of Polysemy and Homonymy in Contextualised Language Models",
    "volume": "findings",
    "abstract": "One of the central aspects of contextualised language models is that they should be able to distinguish the meaning of lexically ambiguous words by their contexts. In this paper we investigate the extent to which the contextualised embeddings of word forms that display multiplicity of sense reflect traditional distinctions of polysemy and homonymy. To this end, we introduce an extended, human-annotated dataset of graded word sense similarity and co-predication acceptability, and evaluate how well the similarity of embeddings predicts similarity in meaning. Both types of human judgements indicate that the similarity of polysemic interpretations falls in a continuum between identity of meaning and homonymy. However, we also observe significant differences within the similarity ratings of polysemes, forming consistent patterns for different types of polysemic sense alternation. Our dataset thus appears to capture a substantial part of the complexity of lexical ambiguity, and can provide a realistic test bed for contextualised embeddings. Among the tested models, BERT Large shows the strongest correlation with the collected word sense similarity ratings, but struggles to consistently replicate the observed similarity patterns. When clustering ambiguous word forms based on their embeddings, the model displays high confidence in discerning homonyms and some types of polysemic alternations, but consistently fails for others",
    "checked": true,
    "id": "af38f9df6308fd7fa200a83cc3c71c617f217378",
    "semantic_title": "patterns of polysemy and homonymy in contextualised language models",
    "citation_count": 19,
    "authors": [
      "Janosch Haber",
      "Massimo Poesio"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.227": {
    "title": "Cross-Lingual Leveled Reading Based on Language-Invariant Features",
    "volume": "findings",
    "abstract": "Leveled reading (LR) aims to automatically classify texts by the cognitive levels of readers, which is fundamental in providing appropriate reading materials regarding different reading capabilities. However, most state-of-the-art LR methods rely on the availability of copious annotated resources, which prevents their adaptation to low-resource languages like Chinese. In our work, to tackle LR in Chinese, we explore how different language transfer methods perform on English-Chinese LR. Specifically, we focus on adversarial training and cross-lingual pre-training method to transfer the LR knowledge learned from annotated data in the resource-rich English language to Chinese. For evaluation, we first introduce the age-based standard to align datasets with different leveling standards. Then we conduct experiments in both zero-shot and few-shot settings. Comparing these two methods, quantitative and qualitative evaluations show that the cross-lingual pre-training method effectively captures the language-invariant features between English and Chinese. We conduct analysis to propose further improvement in cross-lingual LR",
    "checked": true,
    "id": "c3287f56417f163be22671db36b4900043b6cd24",
    "semantic_title": "cross-lingual leveled reading based on language-invariant features",
    "citation_count": 4,
    "authors": [
      "Simin Rao",
      "Hua Zheng",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.228": {
    "title": "Controlled Neural Sentence-Level Reframing of News Articles",
    "volume": "findings",
    "abstract": "Framing a news article means to portray the reported event from a specific perspective, e.g., from an economic or a health perspective. Reframing means to change this perspective. Depending on the audience or the submessage, reframing can become necessary to achieve the desired effect on the readers. Reframing is related to adapting style and sentiment, which can be tackled with neural text generation techniques. However, it is more challenging since changing a frame requires rewriting entire sentences rather than single phrases. In this paper, we study how to computationally reframe sentences in news articles while maintaining their coherence to the context. We treat reframing as a sentence-level fill-in-the-blank task for which we train neural models on an existing media frame corpus. To guide the training, we propose three strategies: framed-language pretraining, named-entity preservation, and adversarial learning. We evaluate respective models automatically and manually for topic consistency, coherence, and successful reframing. Our results indicate that generating properly-framed text works well but with tradeoffs",
    "checked": true,
    "id": "68d252d5e3b791d01efc04164002f5c949ed5e66",
    "semantic_title": "controlled neural sentence-level reframing of news articles",
    "citation_count": 13,
    "authors": [
      "Wei-Fan Chen",
      "Khalid Al Khatib",
      "Benno Stein",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.229": {
    "title": "DialogueTRM: Exploring Multi-Modal Emotional Dynamics in a Conversation",
    "volume": "findings",
    "abstract": "Emotion dynamics formulates principles explaining the emotional fluctuation during conversations. Recent studies explore the emotion dynamics from the self and inter-personal dependencies, however, ignoring the temporal and spatial dependencies in the situation of multi-modal conversations. To address the issue, we extend the concept of emotion dynamics to multi-modal settings and propose a Dialogue Transformer for simultaneously modeling the intra-modal and inter-modal emotion dynamics. Specifically, the intra-modal emotion dynamics is to not only capture the temporal dependency but also satisfy the context preference in every single modality. The inter-modal emotional dynamics aims at handling multi-grained spatial dependency across all modalities. Our models outperform the state-of-the-art with a margin of 4%-16% for most of the metrics on three benchmark datasets",
    "checked": true,
    "id": "52b3330698131da33073b5b30249651cfc73b709",
    "semantic_title": "dialoguetrm: exploring multi-modal emotional dynamics in a conversation",
    "citation_count": 34,
    "authors": [
      "Yuzhao Mao",
      "Guang Liu",
      "Xiaojie Wang",
      "Weiguo Gao",
      "Xuan Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.230": {
    "title": "Adversarial Examples for Evaluating Math Word Problem Solvers",
    "volume": "findings",
    "abstract": "Standard accuracy metrics have shown that Math Word Problem (MWP) solvers have achieved high performance on benchmark datasets. However, the extent to which existing MWP solvers truly understand language and its relation with numbers is still unclear. In this paper, we generate adversarial attacks to evaluate the robustness of state-of-the-art MWP solvers. We propose two methods, Question Reordering and Sentence Paraphrasing to generate adversarial attacks. We conduct experiments across three neural MWP solvers over two benchmark datasets. On average, our attack method is able to reduce the accuracy of MWP solvers by over 40% on these datasets. Our results demonstrate that existing MWP solvers are sensitive to linguistic variations in the problem text. We verify the validity and quality of generated adversarial examples through human evaluation",
    "checked": true,
    "id": "68900bf4b9cc820f4b47608871eafcac65a33917",
    "semantic_title": "adversarial examples for evaluating math word problem solvers",
    "citation_count": 34,
    "authors": [
      "Vivek Kumar",
      "Rishabh Maheshwary",
      "Vikram Pudi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.231": {
    "title": "Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text",
    "volume": "findings",
    "abstract": "Numerical reasoning skills are essential for complex question answering (CQA) over text. It requires opertaions including counting, comparison, addition and subtraction. A successful approach to CQA on text, Neural Module Networks (NMNs), follows the programmer-interpreter paradigm and leverages specialised modules to perform compositional reasoning. However, the NMNs framework does not consider the relationship between numbers and entities in both questions and paragraphs. We propose effective techniques to improve NMNs' numerical reasoning capabilities by making the interpreter question-aware and capturing the relationship between entities and numbers. On the same subset of the DROP dataset for CQA on text, experimental results show that our additions outperform the original NMNs by 3.0 points for the overall F1 score",
    "checked": true,
    "id": "699fcda71a008b98647f185f0b195b398fdc561d",
    "semantic_title": "improving numerical reasoning skills in the modular approach for complex question answering on text",
    "citation_count": 3,
    "authors": [
      "Xiao-Yu Guo",
      "Yuan-Fang Li",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.232": {
    "title": "Retrieval Augmented Code Generation and Summarization",
    "volume": "findings",
    "abstract": "Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers' code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework",
    "checked": true,
    "id": "c56aced0f0c5cfebefadb530cb08d736c3ac5c05",
    "semantic_title": "retrieval augmented code generation and summarization",
    "citation_count": 186,
    "authors": [
      "Md Rizwan Parvez",
      "Wasi Ahmad",
      "Saikat Chakraborty",
      "Baishakhi Ray",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.233": {
    "title": "Multilingual Translation via Grafting Pre-trained Language Models",
    "volume": "findings",
    "abstract": "Can pre-trained BERT for one language and GPT for another be glued together to translate texts? Self-supervised training using only monolingual data has led to the success of pre-trained (masked) language models in many NLP tasks. However, directly connecting BERT as an encoder and GPT as a decoder can be challenging in machine translation, for GPT-like models lack a cross-attention component that is needed in seq2seq decoders. In this paper, we propose Graformer to graft separately pre-trained (masked) language models for machine translation. With monolingual data for pre-training and parallel data for grafting training, we maximally take advantage of the usage of both types of data. Experiments on 60 directions show that our method achieves average improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with the multilingual Transformer of the same size",
    "checked": true,
    "id": "10b9a1f65d70964f85430d8edf419de736939d41",
    "semantic_title": "multilingual translation via grafting pre-trained language models",
    "citation_count": 22,
    "authors": [
      "Zewei Sun",
      "Mingxuan Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.234": {
    "title": "AEDA: An Easier Data Augmentation Technique for Text Classification",
    "volume": "findings",
    "abstract": "This paper proposes AEDA (An Easier Data Augmentation) technique to help improve the performance on text classification tasks. AEDA includes only random insertion of punctuation marks into the original text. This is an easier technique to implement for data augmentation than EDA method (Wei and Zou, 2019) with which we compare our results. In addition, it keeps the order of the words while changing their positions in the sentence leading to a better generalized performance. Furthermore, the deletion operation in EDA can cause loss of information which, in turn, misleads the network, whereas AEDA preserves all the input information. Following the baseline, we perform experiments on five different datasets for text classification. We show that using the AEDA-augmented data for training, the models show superior performance compared to using the EDA-augmented data in all five datasets. The source code will be made available for further study and reproduction of the results",
    "checked": true,
    "id": "ee7951aa518b348cf1337e3fb109f4c09b9d9990",
    "semantic_title": "aeda: an easier data augmentation technique for text classification",
    "citation_count": 156,
    "authors": [
      "Akbar Karimi",
      "Leonardo Rossi",
      "Andrea Prati"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.235": {
    "title": "A Comprehensive Comparison of Word Embeddings in Event & Entity Coreference Resolution",
    "volume": "findings",
    "abstract": "Coreference Resolution is an important NLP task and most state-of-the-art methods rely on word embeddings for word representation. However, one issue that has been largely overlooked in literature is that of comparing the performance of different embeddings across and within families. Therefore, we frame our study in the context of Event and Entity Coreference Resolution (EvCR & EnCR), and address two questions : 1) Is there a trade-off between performance (predictive and run-time) and embedding size? 2) How do the embeddings' performance compare within and across families? Our experiments reveal several interesting findings. First, we observe diminishing returns in performance with respect to embedding size. E.g. a model using solely a character embedding achieves 86% of the performance of the largest model (Elmo, GloVe, Character) while being 1.2% of its size. Second, the larger models using multiple embeddings learns faster despite being slower per epoch. However, it is still slower at test time. Finally, Elmo performs best on both EvCR and EnCR, while GloVe and FastText perform best in EvCR and EnCR respectively",
    "checked": true,
    "id": "bf5157c0fc13f81262776c0ad6c23f0f942ee362",
    "semantic_title": "a comprehensive comparison of word embeddings in event & entity coreference resolution",
    "citation_count": 2,
    "authors": [
      "Judicael Poumay",
      "Ashwin Ittoo"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.236": {
    "title": "Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition",
    "volume": "findings",
    "abstract": "Unifying acoustic and linguistic representation learning has become increasingly crucial to transfer the knowledge learned on the abundance of high-resource language data for low-resource speech recognition. Existing approaches simply cascade pre-trained acoustic and language models to learn the transfer from speech to text. However, how to solve the representation discrepancy of speech and text is unexplored, which hinders the utilization of acoustic and linguistic information. Moreover, previous works simply replace the embedding layer of the pre-trained language model with the acoustic features, which may cause the catastrophic forgetting problem. In this work, we introduce Wav-BERT, a cooperative acoustic and linguistic representation learning method to fuse and utilize the contextual information of speech and text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a language model (BERT) into an end-to-end trainable framework. A Representation Aggregation Module is designed to aggregate acoustic and linguistic representation, and an Embedding Attention Module is introduced to incorporate acoustic information into BERT, which can effectively facilitate the cooperation of two pre-trained models and thus boost the representation learning. Extensive experiments show that our Wav-BERT significantly outperforms the existing approaches and achieves state-of-the-art performance on low-resource speech recognition",
    "checked": true,
    "id": "d992dddb66ff2400d1967044d1fb148cc27f611f",
    "semantic_title": "wav-bert: cooperative acoustic and linguistic representation learning for low-resource speech recognition",
    "citation_count": 26,
    "authors": [
      "Guolin Zheng",
      "Yubei Xiao",
      "Ke Gong",
      "Pan Zhou",
      "Xiaodan Liang",
      "Liang Lin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.237": {
    "title": "Multilingual AMR Parsing with Noisy Knowledge Distillation",
    "volume": "findings",
    "abstract": "We study multilingual AMR parsing from the perspective of knowledge distillation, where the aim is to learn and improve a multilingual AMR parser by using an existing English parser as its teacher. We constrain our exploration in a strict multilingual setting: there is but one model to parse all different languages including English. We identify that noisy input and precise output are the key to successful distillation. Together with extensive pre-training, we obtain an AMR parser whose performances surpass all previously published results on four different foreign languages, including German, Spanish, Italian, and Chinese, by large margins (up to 18.8 Smatch points on Chinese and on average 11.3 Smatch points). Our parser also achieves comparable performance on English to the latest state-of-the-art English-only parser",
    "checked": true,
    "id": "92b86dda210ddd6f464a78d87350040a95f8a168",
    "semantic_title": "multilingual amr parsing with noisy knowledge distillation",
    "citation_count": 18,
    "authors": [
      "Deng Cai",
      "Xin Li",
      "Jackie Chun-Sing Ho",
      "Lidong Bing",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.238": {
    "title": "Open-Domain Contextual Link Prediction and its Complementarity with Entailment Graphs",
    "volume": "findings",
    "abstract": "An open-domain knowledge graph (KG) has entities as nodes and natural language relations as edges, and is constructed by extracting (subject, relation, object) triples from text. The task of open-domain link prediction is to infer missing relations in the KG. Previous work has used standard link prediction for the task. Since triples are extracted from text, we can ground them in the larger textual context in which they were originally found. However, standard link prediction methods only rely on the KG structure and ignore the textual context that each triple was extracted from. In this paper, we introduce the new task of open-domain contextual link prediction which has access to both the textual context and the KG structure to perform link prediction. We build a dataset for the task and propose a model for it. Our experiments show that context is crucial in predicting missing relations. We also demonstrate the utility of contextual link prediction in discovering context-independent entailments between relations, in the form of entailment graphs (EG), in which the nodes are the relations. The reverse holds too: context-independent EGs assist in predicting relations in context",
    "checked": true,
    "id": "5d9a37481f9d424e702a4d9ac0fc227502bf6da2",
    "semantic_title": "open-domain contextual link prediction and its complementarity with entailment graphs",
    "citation_count": 14,
    "authors": [
      "Mohammad Javad Hosseini",
      "Shay B. Cohen",
      "Mark Johnson",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.239": {
    "title": "Analysis of Language Change in Collaborative Instruction Following",
    "volume": "findings",
    "abstract": "We analyze language change over time in a collaborative, goal-oriented instructional task, where utility-maximizing participants form conventions and increase their expertise. Prior work studied such scenarios mostly in the context of reference games, and consistently found that language complexity is reduced along multiple dimensions, such as utterance length, as conventions are formed. In contrast, we find that, given the ability to increase instruction utility, instructors increase language complexity along these previously studied dimensions to better collaborate with increasingly skilled instruction followers",
    "checked": true,
    "id": "e290fb5faab242b2afc35eee0449c00bdb656cce",
    "semantic_title": "analysis of language change in collaborative instruction following",
    "citation_count": 13,
    "authors": [
      "Anna Effenberger",
      "Rhia Singh",
      "Eva Yan",
      "Alane Suhr",
      "Yoav Artzi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.240": {
    "title": "Counter-Interference Adapter for Multilingual Machine Translation",
    "volume": "findings",
    "abstract": "Developing a unified multilingual model has been a long pursuing goal for machine translation. However, existing approaches suffer from performance degradation - a single multilingual model is inferior to separately trained bilingual ones on rich-resource languages. We conjecture that such a phenomenon is due to interference brought by joint training with multiple languages. To accommodate the issue, we propose CIAT, an adapted Transformer model with a small parameter overhead for multilingual machine translation. We evaluate CIAT on multiple benchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that the CIAT consistently outperforms strong multilingual baselines on 64 of total 66 language directions, 42 of which have above 0.5 BLEU improvement",
    "checked": true,
    "id": "af5d212bbfbeaa76d884de140f5248d666d13312",
    "semantic_title": "counter-interference adapter for multilingual machine translation",
    "citation_count": 59,
    "authors": [
      "Yaoming Zhu",
      "Jiangtao Feng",
      "Chengqi Zhao",
      "Mingxuan Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.241": {
    "title": "Progressive Transformer-Based Generation of Radiology Reports",
    "volume": "findings",
    "abstract": "Inspired by Curriculum Learning, we propose a consecutive (i.e., image-to-text-to-text) generation framework where we divide the problem of radiology report generation into two steps. Contrary to generating the full radiology report from the image at once, the model generates global concepts from the image in the first step and then reforms them into finer and coherent texts using transformer-based architecture. We follow the transformer-based sequence-to-sequence paradigm at each step. We improve upon the state-of-the-art on two benchmark datasets",
    "checked": true,
    "id": "16321f3f712061a090176f5123f62f94001a408f",
    "semantic_title": "progressive transformer-based generation of radiology reports",
    "citation_count": 88,
    "authors": [
      "Farhad Nooralahzadeh",
      "Nicolas Perez Gonzalez",
      "Thomas Frauenfelder",
      "Koji Fujimoto",
      "Michael Krauthammer"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.242": {
    "title": "Be nice to your wife! The restaurants are closed\": Can Gender Stereotype Detection Improve Sexism Classification?",
    "volume": "findings",
    "abstract": "In this paper, we focus on the detection of sexist hate speech against women in tweets studying for the first time the impact of gender stereotype detection on sexism classification. We propose: (1) the first dataset annotated for gender stereotype detection, (2) a new method for data augmentation based on sentence similarity with multilingual external datasets, and (3) a set of deep learning experiments first to detect gender stereotypes and then, to use this auxiliary task for sexism detection. Although the presence of stereotypes does not necessarily entail hateful content, our results show that sexism classification can definitively benefit from gender stereotype detection",
    "checked": true,
    "id": "e5770578122f702e42c90b2700bff2daf907e94c",
    "semantic_title": "be nice to your wife! the restaurants are closed\": can gender stereotype detection improve sexism classification?",
    "citation_count": 28,
    "authors": [
      "Patricia Chiril",
      "Farah Benamara",
      "Véronique Moriceau"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.243": {
    "title": "Automatic Discrimination between Inherited and Borrowed Latin Words in Romance Languages",
    "volume": "findings",
    "abstract": "In this paper, we address the problem of automatically discriminating between inherited and borrowed Latin words. We introduce a new dataset and investigate the case of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), where words directly inherited from Latin coexist with words borrowed from Latin, and explore whether automatic discrimination between them is possible. Having entered the language at a later stage, borrowed words are no longer subject to historical sound shift rules, hence they are presumably less eroded, which is why we expect them to have a different intrinsic structure distinguishable by computational means. We employ several machine learning models to automatically discriminate between inherited and borrowed words and compare their performance with various feature sets. We analyze the models' predictive power on two versions of the datasets, orthographic and phonetic. We also investigate whether prior knowledge of the etymon provides better results, employing n-gram character features extracted from the word-etymon pairs and from their alignment",
    "checked": true,
    "id": "efad7fca30e488a10e20b25b7cebe24bd675f188",
    "semantic_title": "automatic discrimination between inherited and borrowed latin words in romance languages",
    "citation_count": 6,
    "authors": [
      "Alina Maria Cristea",
      "Liviu P. Dinu",
      "Simona Georgescu",
      "Mihnea-Lucian Mihai",
      "Ana Sabina Uban"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.244": {
    "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
    "volume": "findings",
    "abstract": "Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can \"prompt\" the LM with the review and the label description \"Does the user like this movie?\", and ask whether the next word is \"yes\" or \"no\". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better",
    "checked": true,
    "id": "4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca",
    "semantic_title": "adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
    "citation_count": 172,
    "authors": [
      "Ruiqi Zhong",
      "Kristy Lee",
      "Zheng Zhang",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.245": {
    "title": "Knowledge-Interactive Network with Sentiment Polarity Intensity-Aware Multi-Task Learning for Emotion Recognition in Conversations",
    "volume": "findings",
    "abstract": "Emotion Recognition in Conversation (ERC) has gained much attention from the NLP community recently. Some models concentrate on leveraging commonsense knowledge or multi-task learning to help complicated emotional reasoning. However, these models neglect direct utterance-knowledge interaction. In addition, these models utilize emotion-indirect auxiliary tasks, which provide limited affective information for the ERC task. To address the above issues, we propose a Knowledge-Interactive Network with sentiment polarity intensity-aware multi-task learning, namely KI-Net, which leverages both commonsense knowledge and sentiment lexicon to augment semantic information. Specifically, we use a self-matching module for internal utterance-knowledge interaction. Considering correlations with the ERC task, a phrase-level Sentiment Polarity Intensity Prediction (SPIP) task is devised as an auxiliary task. Experiments show that all knowledge integration, self-matching and SPIP modules improve the model performance respectively on three datasets. Moreover, our KI-Net model shows 1.04% performance improvement over the state-of-the-art model on the IEMOCAP dataset",
    "checked": true,
    "id": "994919247045c6cd5eab0a9c129879a2c3f5e69e",
    "semantic_title": "knowledge-interactive network with sentiment polarity intensity-aware multi-task learning for emotion recognition in conversations",
    "citation_count": 39,
    "authors": [
      "Yunhe Xie",
      "Kailai Yang",
      "Chengjie Sun",
      "Bingquan Liu",
      "Zhenzhou Ji"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.246": {
    "title": "Minimizing Annotation Effort via Max-Volume Spectral Sampling",
    "volume": "findings",
    "abstract": "We address the annotation data bottleneck for sequence classification. Specifically we ask the question: if one has a budget of N annotations, which samples should we select for annotation? The solution we propose looks for diversity in the selected sample, by maximizing the amount of information that is useful for the learning algorithm, or equivalently by minimizing the redundancy of samples in the selection. This is formulated in the context of spectral learning of recurrent functions for sequence classification. Our method represents unlabeled data in the form of a Hankel matrix, and uses the notion of spectral max-volume to find a compact sub-block from which annotation samples are drawn. Experiments on sequence classification confirm that our spectral sampling strategy is in fact efficient and yields good models",
    "checked": true,
    "id": "c3cfe631890f175cdeaf228b4c78d055bc4d988f",
    "semantic_title": "minimizing annotation effort via max-volume spectral sampling",
    "citation_count": 1,
    "authors": [
      "Ariadna Quattoni",
      "Xavier Carreras"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.247": {
    "title": "On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at https://github.com/SunbowLiu/PTvsBT",
    "checked": true,
    "id": "beda4bea4c0bee9d942f7db8b4991712cc707f68",
    "semantic_title": "on the complementarity between pre-training and back-translation for neural machine translation",
    "citation_count": 27,
    "authors": [
      "Xuebo Liu",
      "Longyue Wang",
      "Derek F. Wong",
      "Liang Ding",
      "Lidia S. Chao",
      "Shuming Shi",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.248": {
    "title": "Lexicon-Based Graph Convolutional Network for Chinese Word Segmentation",
    "volume": "findings",
    "abstract": "Precise information of word boundary can alleviate the problem of lexical ambiguity to improve the performance of natural language processing (NLP) tasks. Thus, Chinese word segmentation (CWS) is a fundamental task in NLP. Due to the development of pre-trained language models (PLM), pre-trained knowledge can help neural methods solve the main problems of the CWS in significant measure. Existing methods have already achieved high performance on several benchmarks (e.g., Bakeoff-2005). However, recent outstanding studies are limited by the small-scale annotated corpus. To further improve the performance of CWS methods based on fine-tuning the PLMs, we propose a novel neural framework, LBGCN, which incorporates a lexicon-based graph convolutional network into the Transformer encoder. Experimental results on five benchmarks and four cross-domain datasets show the lexicon-based graph convolutional network successfully captures the information of candidate words and helps to improve performance on the benchmarks (Bakeoff-2005 and CTB6) and the cross-domain datasets (SIGHAN-2010). Further experiments and analyses demonstrate that our proposed framework effectively models the lexicon to enhance the ability of basic neural frameworks and strengthens the robustness in the cross-domain scenario",
    "checked": true,
    "id": "f9b945c6ea3d2ba7edcfa2a1f00867f235f0fb6b",
    "semantic_title": "lexicon-based graph convolutional network for chinese word segmentation",
    "citation_count": 9,
    "authors": [
      "Kaiyu Huang",
      "Hao Yu",
      "Junpeng Liu",
      "Wei Liu",
      "Jingxiang Cao",
      "Degen Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.249": {
    "title": "KFCNet: Knowledge Filtering and Contrastive Learning for Generative Commonsense Reasoning",
    "volume": "findings",
    "abstract": "Pre-trained language models have led to substantial gains over a broad range of natural language processing (NLP) tasks, but have been shown to have limitations for natural language generation tasks with high-quality requirements on the output, such as commonsense generation and ad keyword generation. In this work, we present a novel Knowledge Filtering and Contrastive learning Network (KFCNet) which references external knowledge and achieves better generation performance. Specifically, we propose a BERT-based filter model to remove low-quality candidates, and apply contrastive learning separately to each of the encoder and decoder, within a general encoder–decoder architecture. The encoder contrastive module helps to capture global target semantics during encoding, and the decoder contrastive module enhances the utility of retrieved prototypes while learning general features. Extensive experiments on the CommonGen benchmark show that our model outperforms the previous state of the art by a large margin: +6.6 points (42.5 vs. 35.9) for BLEU-4, +3.7 points (33.3 vs. 29.6) for SPICE, and +1.3 points (18.3 vs. 17.0) for CIDEr. We further verify the effectiveness of the proposed contrastive module on ad keyword generation, and show that our model has potential commercial value",
    "checked": true,
    "id": "07078624f62875fe047729a422cf1d1ea4e284bb",
    "semantic_title": "kfcnet: knowledge filtering and contrastive learning for generative commonsense reasoning",
    "citation_count": 17,
    "authors": [
      "Haonan Li",
      "Yeyun Gong",
      "Jian Jiao",
      "Ruofei Zhang",
      "Timothy Baldwin",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.250": {
    "title": "Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus",
    "volume": "findings",
    "abstract": "The development of automated approaches to linguistic acceptability has been greatly fostered by the availability of the English CoLA corpus, which has also been included in the widely used GLUE benchmark. However, this kind of research for languages other than English, as well as the analysis of cross-lingual approaches, has been hindered by the lack of resources with a comparable size in other languages. We have therefore developed the ItaCoLA corpus, containing almost 10,000 sentences with acceptability judgments, which has been created following the same approach and the same steps as the English one. In this paper we describe the corpus creation, we detail its content, and we present the first experiments on this new resource. We compare in-domain and out-of-domain classification, and perform a specific evaluation of nine linguistic phenomena. We also present the first cross-lingual experiments, aimed at assessing whether multilingual transformer-based approaches can benefit from using sentences in two languages during fine-tuning",
    "checked": true,
    "id": "0873047c5913b889f2919c793bf748d28ba98cdb",
    "semantic_title": "monolingual and cross-lingual acceptability judgments with the italian cola corpus",
    "citation_count": 31,
    "authors": [
      "Daniela Trotta",
      "Raffaele Guarasci",
      "Elisa Leonardelli",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.251": {
    "title": "Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction",
    "volume": "findings",
    "abstract": "Knowledge graph embedding (KGE) using low-dimensional representations to predict missing information is widely applied in knowledge completion. Existing embedding methods are mostly built on Euclidean space, which are difficult to handle hierarchical structures. Hyperbolic embedding methods have shown the promise of high fidelity and concise representation for hierarchical data. However, the logical patterns in knowledge graphs are not considered well in these methods. To address this problem, we propose a novel KGE model with extended Poincaré Ball and polar coordinate system to capture hierarchical structures. We use the tangent space and exponential transformation to initialize and map the corresponding vectors to the Poincaré Ball in hyperbolic space. To solve the boundary conditions, the boundary is stretched and zoomed by expanding the modulus length in the Poincaré Ball. We optimize our model using polar coordinate and changing operators in the extended Poincaré Ball. Experiments achieve new state-of-the-art results on part of link prediction tasks, which demonstrates the effectiveness of our method",
    "checked": true,
    "id": "b307e96f59fde63567cd0beb30c9e36d968fad8e",
    "semantic_title": "hyperbolic hierarchy-aware knowledge graph embedding for link prediction",
    "citation_count": 25,
    "authors": [
      "Zhe Pan",
      "Peng Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.252": {
    "title": "A Discourse-Aware Graph Neural Network for Emotion Recognition in Multi-Party Conversation",
    "volume": "findings",
    "abstract": "Emotion recognition in multi-party conversation (ERMC) is becoming increasingly popular as an emerging research topic in natural language processing. Prior research focuses on exploring sequential information but ignores the discourse structures of conversations. In this paper, we investigate the importance of discourse structures in handling informative contextual cues and speaker-specific features for ERMC. To this end, we propose a discourse-aware graph neural network (ERMC-DisGCN) for ERMC. In particular, we design a relational convolution to lever the self-speaker dependency of interlocutors to propagate contextual information. Furthermore, we exploit a gated convolution to select more informative cues for ERMC from dependent utterances. The experimental results show our method outperforms multiple baselines, illustrating that discourse structures are of great value to ERMC",
    "checked": true,
    "id": "db8858e97835efe72afe9025bb0f10a2203cb6e5",
    "semantic_title": "a discourse-aware graph neural network for emotion recognition in multi-party conversation",
    "citation_count": 48,
    "authors": [
      "Yang Sun",
      "Nan Yu",
      "Guohong Fu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.253": {
    "title": "MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection",
    "volume": "findings",
    "abstract": "Much of natural language processing is focused on leveraging large capacity language models, typically trained over single messages with a task of predicting one or more tokens. However, modeling human language at higher-levels of context (i.e., sequences of messages) is under-explored. In stance detection and other social media tasks where the goal is to predict an attribute of a message, we have contextual data that is loosely semantically connected by authorship. Here, we introduce Message-Level Transformer (MeLT) – a hierarchical message-encoder pre-trained over Twitter and applied to the task of stance prediction. We focus on stance prediction as a task benefiting from knowing the context of the message (i.e., the sequence of previous messages). The model is trained using a variant of masked-language modeling; where instead of predicting tokens, it seeks to generate an entire masked (aggregated) message vector via reconstruction loss. We find that applying this pre-trained masked message-level transformer to the downstream task of stance detection achieves F1 performance of 67%",
    "checked": true,
    "id": "0a56bacc4e20e024bcdc6d35e09f1b259d751ab8",
    "semantic_title": "melt: message-level transformer with masked document representations as pre-training for stance detection",
    "citation_count": 18,
    "authors": [
      "Matthew Matero",
      "Nikita Soni",
      "Niranjan Balasubramanian",
      "H. Andrew Schwartz"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.254": {
    "title": "LMSOC: An Approach for Socially Sensitive Pretraining",
    "volume": "findings",
    "abstract": "While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a cloze test \"I enjoyed the _____ game this weekend\": the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speaker's broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive language modeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines",
    "checked": true,
    "id": "1a4be30caf5f6af8c157dcf700165a1b540d3ce7",
    "semantic_title": "lmsoc: an approach for socially sensitive pretraining",
    "citation_count": 13,
    "authors": [
      "Vivek Kulkarni",
      "Shubhanshu Mishra",
      "Aria Haghighi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.255": {
    "title": "Extract, Integrate, Compete: Towards Verification Style Reading Comprehension",
    "volume": "findings",
    "abstract": "In this paper, we present a new verification style reading comprehension dataset named VGaokao from Chinese Language tests of Gaokao. Different from existing efforts, the new dataset is originally designed for native speakers' evaluation, thus requiring more advanced language understanding skills. To address the challenges in VGaokao, we propose a novel Extract-Integrate-Compete approach, which iteratively selects complementary evidence with a novel query updating mechanism and adaptively distills supportive evidence, followed by a pairwise competition to push models to learn the subtle difference among similar text pieces. Experiments show that our methods outperform various baselines on VGaokao with retrieved complementary evidence, while having the merits of efficiency and explainability. Our dataset and code are released for further research",
    "checked": true,
    "id": "1e8672dfcb2be6e371dc5ccbe3845a6ba9716955",
    "semantic_title": "extract, integrate, compete: towards verification style reading comprehension",
    "citation_count": 7,
    "authors": [
      "Chen Zhang",
      "Yuxuan Lai",
      "Yansong Feng",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.256": {
    "title": "Comparing learnability of two dependency schemes: ‘semantic' (UD) and ‘syntactic' (SUD)",
    "volume": "findings",
    "abstract": "This paper contributes to the thread of research on the learnability of different dependency annotation schemes: one (‘semantic') favouring content words as heads of dependency relations and the other (‘syntactic') favouring syntactic heads. Several studies have lent support to the idea that choosing syntactic criteria for assigning heads in dependency trees improves the performance of dependency parsers. This may be explained by postulating that syntactic approaches are generally more learnable. In this study, we test this hypothesis by comparing the performance of five parsing systems (both transition- and graph-based) on a selection of 21 treebanks, each in a ‘semantic' variant, represented by standard UD (Universal Dependencies), and a ‘syntactic' variant, represented by SUD (Surface-syntactic Universal Dependencies): unlike previously reported experiments, which considered learnability of ‘semantic' and ‘syntactic' annotations of particular constructions in vitro, the experiments reported here consider whole annotation schemes in vivo. Additionally, we compare these annotation schemes using a range of quantitative syntactic properties, which may also reflect their learnability. The results of the experiments show that SUD tends to be more learnable than UD, but the advantage of one or the other scheme depends on the parser and the corpus in question",
    "checked": true,
    "id": "c032a0c679ea2eb4e3017b51d4de0d964509f015",
    "semantic_title": "comparing learnability of two dependency schemes: 'semantic' (ud) and 'syntactic' (sud)",
    "citation_count": 0,
    "authors": [
      "Ryszard Tuora",
      "Adam Przepiórkowski",
      "Aleksander Leczkowski"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.257": {
    "title": "Argumentation-Driven Evidence Association in Criminal Cases",
    "volume": "findings",
    "abstract": "Evidence association in criminal cases is dividing a set of judicial evidence into several non-overlapping subsets, improving the interpretability and legality of conviction. Observably, evidence divided into the same subset usually supports the same claim. Therefore, we propose an argumentation-driven supervised learning method to calculate the distance between evidence pairs for the following evidence association step in this paper. Experimental results on a real-world dataset demonstrate the effectiveness of our method",
    "checked": true,
    "id": "454f5ced574663ddb5be8110912580718e98bf2d",
    "semantic_title": "argumentation-driven evidence association in criminal cases",
    "citation_count": 2,
    "authors": [
      "Yefei Teng",
      "WenHan Chao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.258": {
    "title": "Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction",
    "volume": "findings",
    "abstract": "Aspect-level sentiment classification (ALSC) aims at identifying the sentiment polarity of a specified aspect in a sentence. ALSC is a practical setting in aspect-based sentiment analysis due to no opinion term labeling needed, but it fails to interpret why a sentiment polarity is derived for the aspect. To address this problem, recent works fine-tune pre-trained Transformer encoders for ALSC to extract an aspect-centric dependency tree that can locate the opinion words. However, the induced opinion words only provide an intuitive cue far below human-level interpretability. Besides, the pre-trained encoder tends to internalize an aspect's intrinsic sentiment, causing sentiment bias and thus affecting model performance. In this paper, we propose a span-based anti-bias aspect representation learning framework. It first eliminates the sentiment bias in the aspect embedding by adversarial learning against aspects' prior sentiment. Then, it aligns the distilled opinion candidates with the aspect by span-based dependency modeling to highlight the interpretable opinion terms. Our method achieves new state-of-the-art performance on five benchmarks, with the capability of unsupervised opinion extraction",
    "checked": true,
    "id": "4b680821690116671cfef1a80d66244cf71077de",
    "semantic_title": "eliminating sentiment bias for aspect-level sentiment classification with unsupervised opinion extraction",
    "citation_count": 26,
    "authors": [
      "Bo Wang",
      "Tao Shen",
      "Guodong Long",
      "Tianyi Zhou",
      "Yi Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.259": {
    "title": "Data Efficient Masked Language Modeling for Vision and Language",
    "volume": "findings",
    "abstract": "Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings. Further, our pre-training approach substantially outperforms the baseline model on a prompt-based probing task designed to elicit image objects. These results and our analysis indicate that our method allows for better utilization of the training data",
    "checked": true,
    "id": "c59bcb6a98d8b3794106a71734746eb6cd0b8c58",
    "semantic_title": "data efficient masked language modeling for vision and language",
    "citation_count": 20,
    "authors": [
      "Yonatan Bitton",
      "Michael Elhadad",
      "Gabriel Stanovsky",
      "Roy Schwartz"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.260": {
    "title": "Improving Multilingual Neural Machine Translation with Auxiliary Source Languages",
    "volume": "findings",
    "abstract": "Multilingual neural machine translation models typically handle one source language at a time. However, prior work has shown that translating from multiple source languages improves translation quality. Different from existing approaches on multi-source translation that are limited to the test scenario where parallel source sentences from multiple languages are available at inference time, we propose to improve multilingual translation in a more common scenario by exploiting synthetic source sentences from auxiliary languages. We train our model on synthetic multi-source corpora and apply random masking to enable flexible inference with single-source or bi-source inputs. Extensive experiments on Chinese/English-Japanese and a large-scale multilingual translation benchmark show that our model outperforms the multilingual baseline significantly by up to +4.0 BLEU with the largest improvements on low-resource or distant language pairs",
    "checked": true,
    "id": "a225aab16b852dd2cc12cf72749ff9609656389b",
    "semantic_title": "improving multilingual neural machine translation with auxiliary source languages",
    "citation_count": 7,
    "authors": [
      "Weijia Xu",
      "Yuwei Yin",
      "Shuming Ma",
      "Dongdong Zhang",
      "Haoyang Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.261": {
    "title": "How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy",
    "volume": "findings",
    "abstract": "It is widely accepted that fine-tuning pre-trained language models usually brings about performance improvements in downstream tasks. However, there are limited studies on the reasons behind this effectiveness, particularly from the viewpoint of structural changes in the embedding space. Trying to fill this gap, in this paper, we analyze the extent to which the isotropy of the embedding space changes after fine-tuning. We demonstrate that, even though isotropy is a desirable geometrical property, fine-tuning does not necessarily result in isotropy enhancements. Moreover, local structures in pre-trained contextual word representations (CWRs), such as those encoding token types or frequency, undergo a massive change during fine-tuning. Our experiments show dramatic growth in the number of elongated directions in the embedding space, which, in contrast to pre-trained CWRs, carry the essential linguistic knowledge in the fine-tuned embedding space, making existing isotropy enhancement methods ineffective",
    "checked": true,
    "id": "ceef266c59698999c9283a0cda852d8bc1ce27ea",
    "semantic_title": "how does fine-tuning affect the geometry of embedding space: a case study on isotropy",
    "citation_count": 20,
    "authors": [
      "Sara Rajaee",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.262": {
    "title": "Locality Preserving Sentence Encoding",
    "volume": "findings",
    "abstract": "Although researches on word embeddings have made great progress in recent years, many tasks in natural language processing are on the sentence level. Thus, it is essential to learn sentence embeddings. Recently, Sentence BERT (SBERT) is proposed to learn embeddings on the sentence level, and it uses the inner product (or, cosine similarity) to compute semantic similarity between sentences. However, this measurement cannot well describe the semantic structures among sentences. The reason is that sentences may lie on a manifold in the ambient space rather than distribute in an Euclidean space. Thus, cosine similarity cannot approximate distances on the manifold. To tackle the severe problem, we propose a novel sentence embedding method called Sentence BERT with Locality Preserving (SBERT-LP), which discovers the sentence submanifold from a high-dimensional space and yields a compact sentence representation subspace by locally preserving geometric structures of sentences. We compare the SBERT-LP with several existing sentence embedding approaches from three perspectives: sentence similarity, sentence classification and sentence clustering. Experimental results and case studies demonstrate that our method encodes sentences better in the sense of semantic structures",
    "checked": true,
    "id": "5feffec4cf8a7e6f8ac0e5d2764d9c0948ac9415",
    "semantic_title": "locality preserving sentence encoding",
    "citation_count": 4,
    "authors": [
      "Changrong Min",
      "Yonghe Chu",
      "Liang Yang",
      "Bo Xu",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.263": {
    "title": "Knowledge Representation Learning with Contrastive Completion Coding",
    "volume": "findings",
    "abstract": "Knowledge representation learning (KRL) has been used in plenty of knowledge-driven tasks. Despite fruitfully progress, existing methods still suffer from the immaturity on tackling potentially-imperfect knowledge graphs and highly-imbalanced positive-negative instances during training, both of which would hinder the performance of KRL. In this paper, we propose Contrastive Completion Coding (C3), a novel KRL framework that is composed of two functional components: 1. Hierarchical Architecture, which integrates both low-level standalone features and high-level topology-aware features to yield robust embedding for each entity/relation. 2. Normalized Contrasitive Training, which conducts normalized one-to-many contrasitive learning to emphasize different negatives with different weights, delivering better convergence compared to conventional training losses. Extensive experiments on several benchmarks verify the efficacy of the two proposed techniques and combing them together generally achieves superior performance against state-of-the-art approaches",
    "checked": true,
    "id": "61ed7ede416d6341810198f5b8e497b73cc853ca",
    "semantic_title": "knowledge representation learning with contrastive completion coding",
    "citation_count": 8,
    "authors": [
      "Bo Ouyang",
      "Wenbing Huang",
      "Runfa Chen",
      "Zhixing Tan",
      "Yang Liu",
      "Maosong Sun",
      "Jihong Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.264": {
    "title": "Knowledge-Enhanced Evidence Retrieval for Counterargument Generation",
    "volume": "findings",
    "abstract": "Finding counterevidence to statements is key to many tasks, including counterargument generation. We build a system that, given a statement, retrieves counterevidence from diverse sources on the Web. At the core of this system is a natural language inference (NLI) model that determines whether a candidate sentence is valid counterevidence or not. Most NLI models to date, however, lack proper reasoning abilities necessary to find counterevidence that involves complex inference. Thus, we present a knowledge-enhanced NLI model that aims to handle causality- and example-based inference by incorporating knowledge graphs. Our NLI model outperforms baselines for NLI tasks, especially for instances that require the targeted inference. In addition, this NLI model further improves the counterevidence retrieval system, notably finding complex counterevidence better",
    "checked": true,
    "id": "11c41d335dc0d8188e308aa1ab110bb5624e052c",
    "semantic_title": "knowledge-enhanced evidence retrieval for counterargument generation",
    "citation_count": 12,
    "authors": [
      "Yohan Jo",
      "Haneul Yoo",
      "JinYeong Bak",
      "Alice Oh",
      "Chris Reed",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.265": {
    "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model",
    "volume": "findings",
    "abstract": "The transformer-based pre-trained language models have been tremendously successful in most of the conventional NLP tasks. But they often struggle in those tasks where numerical understanding is required. Some possible reasons can be the tokenizers and pre-training objectives which are not specifically designed to learn and preserve numeracy. Here we investigate the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to learn numeracy. We consider four numeracy tasks: numeration, magnitude order prediction, finding minimum and maximum in a series, and sorting. We find that, although T5 models perform reasonably well in the interpolation setting, they struggle considerably in the extrapolation setting across all four tasks",
    "checked": true,
    "id": "37588705a2af7d5b24d901dd33ade1ff293aabdd",
    "semantic_title": "investigating numeracy learning ability of a text-to-text transfer model",
    "citation_count": 19,
    "authors": [
      "Kuntal Kumar Pal",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.266": {
    "title": "Modeling Mathematical Notation Semantics in Academic Papers",
    "volume": "findings",
    "abstract": "Natural language models often fall short when understanding and generating mathematical notation. What is not clear is whether these shortcomings are due to fundamental limitations of the models, or the absence of appropriate tasks. In this paper, we explore the extent to which natural language models can learn semantics between mathematical notation and their surrounding text. We propose two notation prediction tasks, and train a model that selectively masks notation tokens and encodes left and/or right sentences as context. Compared to baseline models trained by masked language modeling, our method achieved significantly better performance at the two tasks, showing that this approach is a good first step towards modeling mathematical texts. However, the current models rarely predict unseen symbols correctly, and token-level predictions are more accurate than symbol-level predictions, indicating more work is needed to represent structural patterns. Based on the results, we suggest future works toward modeling mathematical texts",
    "checked": true,
    "id": "0c969e93c5e3337b8beb96c614d03875fda582f2",
    "semantic_title": "modeling mathematical notation semantics in academic papers",
    "citation_count": 14,
    "authors": [
      "Hwiyeol Jo",
      "Dongyeop Kang",
      "Andrew Head",
      "Marti A. Hearst"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.267": {
    "title": "Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens",
    "volume": "findings",
    "abstract": "Much of the world's population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuating ableist bias against people with disabilities, i.e., prejudice that favors those with typical abilities. We report on various analyses based on word predictions of a large-scale BERT language model. Statistically significant results demonstrate that people with disabilities can be disadvantaged. Findings also explore overlapping forms of discrimination related to interconnected gender and race identities",
    "checked": true,
    "id": "89e5ffd5ce92011e234f0b0ea0d6f2e43647b463",
    "semantic_title": "unpacking the interdependent systems of discrimination: ableist bias in nlp systems through an intersectional lens",
    "citation_count": 38,
    "authors": [
      "Saad Hassan",
      "Matt Huenerfauth",
      "Cecilia Ovesdotter Alm"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.268": {
    "title": "Constructing Emotional Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation",
    "volume": "findings",
    "abstract": "Researches on dialogue empathy aim to endow an agent with the capacity of accurate understanding and proper responding for emotions. Existing models for empathetic dialogue generation focus on the emotion flow in one direction, that is, from the context to response. We argue that conducting an empathetic conversation is a bidirectional process, where empathy occurs when the emotions of two interlocutors could converge on the same point, i.e., reaching an emotional consensus. Besides, we also find that the empathetic dialogue corpus is extremely limited, which further restricts the model performance. To address the above issues, we propose a dual-generative model, Dual-Emp, to simultaneously construct the emotional consensus and utilize some external unpaired data. Specifically, our model integrates a forward dialogue model, a backward dialogue model, and a discrete latent variable representing the emotional consensus into a unified architecture. Then, to alleviate the constraint of paired data, we extract unpaired emotional data from open-domain conversations and employ Dual-Emp to produce pseudo paired empathetic samples, which is more efficient and low-cost than the human annotation. Automatic and human evaluations demonstrate that our method outperforms competitive baselines in producing coherent and empathetic responses",
    "checked": true,
    "id": "04cb438f6d66b91db2676860f3743d8b7a59a5eb",
    "semantic_title": "constructing emotional consensus and utilizing unpaired data for empathetic dialogue generation",
    "citation_count": 12,
    "authors": [
      "Lei Shen",
      "Jinchao Zhang",
      "Jiao Ou",
      "Xiaofang Zhao",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.269": {
    "title": "Automatic rule generation for time expression normalization",
    "volume": "findings",
    "abstract": "The understanding of time expressions includes two sub-tasks: recognition and normalization. In recent years, significant progress has been made in the recognition of time expressions while research on normalization has lagged behind. Existing SOTA normalization methods highly rely on rules or grammars designed by experts, which limits their performance on emerging corpora, such as social media texts. In this paper, we model time expression normalization as a sequence of operations to construct the normalized temporal value, and we present a novel method called ARTime, which can automatically generate normalization rules from training data without expert interventions. Specifically, ARTime automatically captures possible operation sequences from annotated data and generates normalization rules on time expressions with common surface forms. The experimental results show that ARTime can significantly surpass SOTA methods on the Tweets benchmark, and achieves competitive results with existing expert-engineered rule methods on the TempEval-3 benchmark",
    "checked": true,
    "id": "8c97429dc21ae3ba78411bb719e45e8b318e6f49",
    "semantic_title": "automatic rule generation for time expression normalization",
    "citation_count": 8,
    "authors": [
      "Wentao Ding",
      "Jianhao Chen",
      "Jinmao Li",
      "Yuzhong Qu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.270": {
    "title": "RW-KD: Sample-wise Loss Terms Re-Weighting for Knowledge Distillation",
    "volume": "findings",
    "abstract": "Knowledge Distillation (KD) is extensively used in Natural Language Processing to compress the pre-training and task-specific fine-tuning phases of large neural language models. A student model is trained to minimize a convex combination of the prediction loss over the labels and another over the teacher output. However, most existing works either fix the interpolating weight between the two losses apriori or vary the weight using heuristics. In this work, we propose a novel sample-wise loss weighting method, RW-KD. A meta-learner, simultaneously trained with the student, adaptively re-weights the two losses for each sample. We demonstrate, on 7 datasets of the GLUE benchmark, that RW-KD outperforms other loss re-weighting methods for KD",
    "checked": true,
    "id": "da191ab1de049ee2b9fccd2fc3353207b20f0d62",
    "semantic_title": "rw-kd: sample-wise loss terms re-weighting for knowledge distillation",
    "citation_count": 8,
    "authors": [
      "Peng Lu",
      "Abbas Ghaddar",
      "Ahmad Rashid",
      "Mehdi Rezagholizadeh",
      "Ali Ghodsi",
      "Philippe Langlais"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.271": {
    "title": "Visual Cues and Error Correction for Translation Robustness",
    "volume": "findings",
    "abstract": "Neural Machine Translation models are sensitive to noise in the input texts, such as misspelled words and ungrammatical constructions. Existing robustness techniques generally fail when faced with unseen types of noise and their performance degrades on clean texts. In this paper, we focus on three types of realistic noise that are commonly generated by humans and introduce the idea of visual context to improve translation robustness for noisy texts. In addition, we describe a novel error correction training regime that can be used as an auxiliary task to further improve translation robustness. Experiments on English-French and English-German translation show that both multimodal and error correction components improve model robustness to noisy texts, while still retaining translation quality on clean texts",
    "checked": true,
    "id": "61db3cef277f55020d17385c7d8e8fcd9355082b",
    "semantic_title": "visual cues and error correction for translation robustness",
    "citation_count": 3,
    "authors": [
      "Zhenhao Li",
      "Marek Rei",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.272": {
    "title": "Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers",
    "volume": "findings",
    "abstract": "As large-scale, pre-trained language models achieve human-level and superhuman accuracy on existing language understanding tasks, statistical bias in benchmark data and probing studies have recently called into question their true capabilities. For a more informative evaluation than accuracy on text classification tasks can offer, we propose evaluating systems through a novel measure of prediction coherence. We apply our framework to two existing language understanding benchmarks with different properties to demonstrate its versatility. Our experimental results show that this evaluation framework, although simple in ideas and implementation, is a quick, effective, and versatile measure to provide insight into the coherence of machines' predictions",
    "checked": true,
    "id": "b1a71d8fb272016d618917a3fc392551fa941c81",
    "semantic_title": "beyond the tip of the iceberg: assessing coherence of text classifiers",
    "citation_count": 7,
    "authors": [
      "Shane Storks",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.273": {
    "title": "Does Pretraining for Summarization Require Knowledge Transfer?",
    "volume": "findings",
    "abstract": "Pretraining techniques leveraging enormous datasets have driven recent advances in text summarization. While folk explanations suggest that knowledge transfer accounts for pretraining's benefits, little is known about why it works or what makes a pretraining task or dataset suitable. In this paper, we challenge the knowledge transfer story, showing that pretraining on documents consisting of character n-grams selected at random, we can nearly match the performance of models pretrained on real corpora. This work holds the promise of eliminating upstream corpora, which may alleviate some concerns over offensive language, bias, and copyright issues. To see whether the small residual benefit of using real data could be accounted for by the structure of the pretraining task, we design several tasks motivated by a qualitative study of summarization corpora. However, these tasks confer no appreciable benefit, leaving open the possibility of a small role for knowledge transfer",
    "checked": true,
    "id": "3d33c71af053b42c14ad8d476c9df9cf6dfc1e16",
    "semantic_title": "does pretraining for summarization require knowledge transfer?",
    "citation_count": 39,
    "authors": [
      "Kundan Krishna",
      "Jeffrey Bigham",
      "Zachary C. Lipton"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.274": {
    "title": "Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits",
    "volume": "findings",
    "abstract": "Training data for machine translation (MT) is often sourced from a multitude of large corpora that are multi-faceted in nature, e.g. containing contents from multiple domains or different levels of quality or complexity. Naturally, these facets do not occur with equal frequency, nor are they equally important for the test scenario at hand. In this work, we propose to optimize this balance jointly with MT model parameters to relieve system developers from manual schedule design. A multi-armed bandit is trained to dynamically choose between facets in a way that is most beneficial for the MT system. We evaluate it on three different multi-facet applications: balancing translationese and natural training data, or data from multiple domains or multiple language pairs. We find that bandit learning leads to competitive MT systems across tasks, and our analysis provides insights into its learned strategies and the underlying data sets",
    "checked": true,
    "id": "9fbc16debacc5cd9b3188b7ecb73f846c3fd8b97",
    "semantic_title": "bandits don't follow rules: balancing multi-facet machine translation with multi-armed bandits",
    "citation_count": 15,
    "authors": [
      "Julia Kreutzer",
      "David Vilar",
      "Artem Sokolov"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.275": {
    "title": "Sometimes We Want Ungrammatical Translations",
    "volume": "findings",
    "abstract": "Rapid progress in Neural Machine Translation (NMT) systems over the last few years has focused primarily on improving translation quality, and as a secondary focus, improving robustness to perturbations (e.g. spelling). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness or faithfulness, by focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure their effects on the target side. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case",
    "checked": true,
    "id": "22adde9c907b32f34a2cd538beed22eeb8ed03fa",
    "semantic_title": "sometimes we want ungrammatical translations",
    "citation_count": 10,
    "authors": [
      "Prasanna Parthasarathi",
      "Koustuv Sinha",
      "Joelle Pineau",
      "Adina Williams"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.276": {
    "title": "An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog",
    "volume": "findings",
    "abstract": "Online conversations include more than just text. Increasingly, image-based responses such as memes and animated gifs serve as culturally recognized and often humorous responses in conversation. However, while NLP has broadened to multimodal models, conversational dialog systems have largely focused only on generating text replies. Here, we introduce a new dataset of 1.56M text-gif conversation turns and introduce a new multimodal conversational model Pepe the King Prawn for selecting gif-based replies. We demonstrate that our model produces relevant and high-quality gif responses and, in a large randomized control trial of multiple models replying to real users, we show that our model replies with gifs that are significantly better received by the community",
    "checked": true,
    "id": "ca4c772bd828ee30d0a7dd51b708405c97486781",
    "semantic_title": "an animated picture says at least a thousand words: selecting gif-based replies in multimodal dialog",
    "citation_count": 5,
    "authors": [
      "Xingyao Wang",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.277": {
    "title": "SciCap: Generating Captions for Scientific Figures",
    "volume": "findings",
    "abstract": "Researchers use figures to communicate rich, complex information in scientific papers. The captions of these figures are critical to conveying effective messages. However, low-quality figure captions commonly occur in scientific articles and may decrease understanding. In this paper, we propose an end-to-end neural framework to automatically generate informative, high-quality captions for scientific figures. To this end, we introduce SCICAP, a large-scale figure-caption dataset based on computer science arXiv papers published between 2010 and 2020. After pre-processing – including figure-type classification, sub-figure identification, text normalization, and caption text selection – SCICAP contained more than two million figures extracted from over 290,000 papers. We then established baseline models that caption graph plots, the dominant (19.2%) figure type. The experimental results showed both opportunities and steep challenges of generating captions for scientific figures",
    "checked": true,
    "id": "08b3c84da0da5bdc0e49a1c2f440743d829a5e1c",
    "semantic_title": "scicap: generating captions for scientific figures",
    "citation_count": 88,
    "authors": [
      "Ting-Yao Hsu",
      "C Lee Giles",
      "Ting-Hao Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.278": {
    "title": "SentNoB: A Dataset for Analysing Sentiment on Noisy Bangla Texts",
    "volume": "findings",
    "abstract": "In this paper, we propose an annotated sentiment analysis dataset made of informally written Bangla texts. This dataset comprises public comments on news and videos collected from social media covering 13 different domains, including politics, education, and agriculture. These comments are labeled with one of the polarity labels, namely positive, negative, and neutral. One significant characteristic of the dataset is that each of the comments is noisy in terms of the mix of dialects and grammatical incorrectness. Our experiments to develop a benchmark classification system show that hand-crafted lexical features provide superior performance than neural network and pretrained language models. We have made the dataset and accompanying models presented in this paper publicly available at https://git.io/JuuNB",
    "checked": true,
    "id": "013abd0a9ce671a98dc659679b3a1e49988e5c45",
    "semantic_title": "sentnob: a dataset for analysing sentiment on noisy bangla texts",
    "citation_count": 59,
    "authors": [
      "Khondoker Ittehadul Islam",
      "Sudipta Kar",
      "Md Saiful Islam",
      "Mohammad Ruhul Amin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.279": {
    "title": "Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data",
    "volume": "findings",
    "abstract": "While multilingual pretrained language models (LMs) fine-tuned on a single language have shown substantial cross-lingual task transfer capabilities, there is still a wide performance gap in semantic parsing tasks when target language supervision is available. In this paper, we propose a novel Translate-and-Fill (TaF) method to produce silver training data for a multilingual semantic parser. This method simplifies the popular Translate-Align-Project (TAP) pipeline and consists of a sequence-to-sequence filler model that constructs a full parse conditioned on an utterance and a view of the same parse. Our filler is trained on English data only but can accurately complete instances in other languages (i.e., translations of the English training utterances), in a zero-shot fashion. Experimental results on three multilingual semantic parsing datasets show that data augmentation with TaF reaches accuracies competitive with similar systems which rely on traditional alignment techniques",
    "checked": true,
    "id": "3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5",
    "semantic_title": "translate & fill: improving zero-shot multilingual semantic parsing with synthetic data",
    "citation_count": 26,
    "authors": [
      "Massimo Nicosia",
      "Zhongdi Qu",
      "Yasemin Altun"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.280": {
    "title": "NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application",
    "volume": "findings",
    "abstract": "Pre-trained language models (PLMs) like BERT have made great progress in NLP. News articles usually contain rich textual information, and PLMs have the potentials to enhance news text modeling for various intelligent news applications like news recommendation and retrieval. However, most existing PLMs are in huge size with hundreds of millions of parameters. Many online news applications need to serve millions of users with low latency tolerance, which poses great challenges to incorporating PLMs in these scenarios. Knowledge distillation techniques can compress a large PLM into a much smaller one and meanwhile keeps good performance. However, existing language models are pre-trained and distilled on general corpus like Wikipedia, which has gaps with the news domain and may be suboptimal for news intelligence. In this paper, we propose NewsBERT, which can distill PLMs for efficient and effective news intelligence. In our approach, we design a teacher-student joint learning and distillation framework to collaboratively learn both teacher and student models, where the student model can learn from the learning experience of the teacher model. In addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of student model to better transfer the knowledge learned by the teacher model. Thorough experiments on two real-world datasets with three tasks show that NewsBERT can empower various intelligent news applications with much smaller models",
    "checked": true,
    "id": "f6cfffe8f5b6e8474145913d1eede6adad12caa1",
    "semantic_title": "newsbert: distilling pre-trained language model for intelligent news application",
    "citation_count": 45,
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Yang Yu",
      "Tao Qi",
      "Yongfeng Huang",
      "Qi Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.281": {
    "title": "SD-QA: Spoken Dialectal Question Answering for the Real World",
    "volume": "findings",
    "abstract": "Question answering (QA) systems are now available through numerous commercial applications for a wide variety of domains, serving millions of users that interact with them via speech interfaces. However, current benchmarks in QA research do not account for the errors that speech recognition models might introduce, nor do they consider the language variations (dialects) of the users. To address this gap, we augment an existing QA dataset to construct a multi-dialect, spoken QA benchmark on five languages (Arabic, Bengali, English, Kiswahili, Korean) with more than 68k audio prompts in 24 dialects from 255 speakers. We provide baseline results showcasing the real-world performance of QA systems and analyze the effect of language variety and other sensitive speaker attributes on downstream performance. Last, we study the fairness of the ASR and QA models with respect to the underlying user populations",
    "checked": true,
    "id": "e72a2a72c32bd1faff90eed0650fd8a8e22cdeb4",
    "semantic_title": "sd-qa: spoken dialectal question answering for the real world",
    "citation_count": 32,
    "authors": [
      "Fahim Faisal",
      "Sharlina Keshava",
      "Md Mahfuz Ibn Alam",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.282": {
    "title": "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation",
    "volume": "findings",
    "abstract": "A \"bigger is better\" explosion in the number of parameters in deep neural networks has made it increasingly challenging to make state-of-the-art networks accessible in compute-restricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However, evaluation of the trade-offs incurred by popular compression techniques has been centered on high-resource datasets. In this work, we instead consider the impact of compression in a data-limited regime. We introduce the term low-resource double bind to refer to the co-occurrence of data limitations and compute resource constraints. This is a common setting for NLP for low-resource languages, yet the trade-offs in performance are poorly studied. Our work offers surprising insights into the relationship between capacity and generalization in data-limited regimes for the task of machine translation. Our experiments on magnitude pruning for translations from English into Yoruba, Hausa, Igbo and German show that in low-resource regimes, sparsity preserves performance on frequent sentences but has a disparate impact on infrequent ones. However, it improves robustness to out-of-distribution shifts, especially for datasets that are very distinct from the training distribution. Our findings suggest that sparsity can play a beneficial role at curbing memorization of low frequency attributes, and therefore offers a promising solution to the low-resource double bind",
    "checked": true,
    "id": "3db9649f2ae986cac13f3e748375f8802f9b07fc",
    "semantic_title": "the low-resource double bind: an empirical study of pruning for low-resource machine translation",
    "citation_count": 54,
    "authors": [
      "Orevaoghene Ahia",
      "Julia Kreutzer",
      "Sara Hooker"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.283": {
    "title": "Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence",
    "volume": "findings",
    "abstract": "This paper proposes a transformer over transformer framework, called Transformerˆ2, to perform neural text segmentation. It consists of two components: bottom-level sentence encoders using pre-trained transformers, and an upper-level transformer-based segmentation model based on the sentence embeddings. The bottom-level component transfers the pre-trained knowledge learnt from large external corpora under both single and pair-wise supervised NLP tasks to model the sentence embeddings for the documents. Given the sentence embeddings, the upper-level transformer is trained to recover the segmentation boundaries as well as the topic labels of each sentence. Equipped with a multi-task loss and the pre-trained knowledge, Transformerˆ2 can better capture the semantic coherence within the same segments. Our experiments show that (1) Transformerˆ2$manages to surpass state-of-the-art text segmentation models in terms of a commonly-used semantic coherence measure; (2) in most cases, both single and pair-wise pre-trained knowledge contribute to the model performance; (3) bottom-level sentence encoders pre-trained on specific languages yield better performance than those pre-trained on specific domains",
    "checked": true,
    "id": "516fcc326921d1062c2a4718c0237c8d1abd9d57",
    "semantic_title": "transformer over pre-trained transformer for neural text segmentation with enhanced topic coherence",
    "citation_count": 39,
    "authors": [
      "Kelvin Lo",
      "Yuan Jin",
      "Weicong Tan",
      "Ming Liu",
      "Lan Du",
      "Wray Buntine"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.284": {
    "title": "Self-Supervised Neural Topic Modeling",
    "volume": "findings",
    "abstract": "Topic models are useful tools for analyzing and interpreting the main underlying themes of large corpora of text. Most topic models rely on word co-occurrence for computing a topic, i.e., a weighted set of words that together represent a high-level semantic concept. In this paper, we propose a new light-weight Self-Supervised Neural Topic Model (SNTM) that learns a rich context by learning a topic representation jointly from three co-occurring words and a document that the triple originates from. Our experimental results indicate that our proposed neural topic model, SNTM, outperforms previously existing topic models in coherence metrics as well as document clustering accuracy. Moreover, apart from the topic coherence and clustering performance, the proposed neural topic model has a number of advantages, namely, being computationally efficient and easy to train",
    "checked": true,
    "id": "363bcae120d9211e5230aa7800e947c1168f2ce5",
    "semantic_title": "self-supervised neural topic modeling",
    "citation_count": 8,
    "authors": [
      "Seyed Ali Bahrainian",
      "Martin Jaggi",
      "Carsten Eickhoff"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.285": {
    "title": "Coreference-aware Surprisal Predicts Brain Response",
    "volume": "findings",
    "abstract": "Recent evidence supports a role for coreference processing in guiding human expectations about upcoming words during reading, based on covariation between reading times and word surprisal estimated by a coreference-aware semantic processing model (Jaffe et al. 2020).The present study reproduces and elaborates on this finding by (1) enabling the parser to process subword information that might better approximate human morphological knowledge, and (2) extending evaluation of coreference effects from self-paced reading to human brain imaging data. Results show that an expectation-based processing effect of coreference is still evident even in the presence of the stronger psycholinguistic baseline provided by the subword model, and that the coreference effect is observed in both self-paced reading and fMRI data, providing evidence of the effect's robustness",
    "checked": true,
    "id": "6e185c985a70fee3782a41da1e0d96906323ca23",
    "semantic_title": "coreference-aware surprisal predicts brain response",
    "citation_count": 0,
    "authors": [
      "Evan Jaffe",
      "Byung-Doh Oh",
      "William Schuler"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.286": {
    "title": "Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation",
    "volume": "findings",
    "abstract": "Despite the remarkable performance of large-scale generative models in open-domain conversation, they are known to be less practical for building real-time conversation systems due to high latency. On the other hand, retrieval models could return responses with much lower latency but show inferior performance to the large-scale generative models since the conversation quality is bounded by the pre-defined response set. To take advantage of both approaches, we propose a new training method called G2R (Generative-to-Retrieval distillation) that preserves the efficiency of a retrieval model while leveraging the conversational ability of a large-scale generative model by infusing the knowledge of the generative model into the retrieval model. G2R consists of two distinct techniques of distillation: the data-level G2R augments the dialogue dataset with additional responses generated by the large-scale generative model, and the model-level G2R transfers the response quality score assessed by the generative model to the score of the retrieval model by the knowledge distillation loss. Through extensive experiments including human evaluation, we demonstrate that our retrieval-based conversation system trained with G2R shows a substantially improved performance compared to the baseline retrieval model while showing significantly lower inference latency than the large-scale generative models",
    "checked": true,
    "id": "718339cbafd0b4613d3389cc2c22592764b92c62",
    "semantic_title": "distilling the knowledge of large-scale generative models into retrieval models for efficient open-domain conversation",
    "citation_count": 5,
    "authors": [
      "Beomsu Kim",
      "Seokjun Seo",
      "Seungju Han",
      "Enkhbayar Erdenee",
      "Buru Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.287": {
    "title": "Modeling Users and Online Communities for Abuse Detection: A Position on Ethics and Explainability",
    "volume": "findings",
    "abstract": "Abuse on the Internet is an important societal problem of our time. Millions of Internet users face harassment, racism, personal attacks, and other types of abuse across various platforms. The psychological effects of abuse on individuals can be profound and lasting. Consequently, over the past few years, there has been a substantial research effort towards automated abusive language detection in the field of NLP. In this position paper, we discuss the role that modeling of users and online communities plays in abuse detection. Specifically, we review and analyze the state of the art methods that leverage user or community information to enhance the understanding and detection of abusive language. We then explore the ethical challenges of incorporating user and community information, laying out considerations to guide future research. Finally, we address the topic of explainability in abusive language detection, proposing properties that an explainable method should aim to exhibit. We describe how user and community information can facilitate the realization of these properties and discuss the effective operationalization of explainability in view of the properties",
    "checked": true,
    "id": "39d84b48cbc6f57019a01a4644a5377dc19bd852",
    "semantic_title": "modeling users and online communities for abuse detection: a position on ethics and explainability",
    "citation_count": 7,
    "authors": [
      "Pushkar Mishra",
      "Helen Yannakoudakis",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.288": {
    "title": "Detecting Community Sensitive Norm Violations in Online Conversations",
    "volume": "findings",
    "abstract": "Online platforms and communities establish their own norms that govern what behavior is acceptable within the community. Substantial effort in NLP has focused on identifying unacceptable behaviors and, recently, on forecasting them before they occur. However, these efforts have largely focused on toxicity as the sole form of community norm violation. Such focus has overlooked the much larger set of rules that moderators enforce. Here, we introduce a new dataset focusing on a more complete spectrum of community norms and their violations in the local conversational and global community contexts. We introduce a series of models that use this data to develop context- and community-sensitive norm violation detection, showing that these changes give high performance",
    "checked": true,
    "id": "d553a4d7cf26e8971ba0ac63d40e3cbefdfb642b",
    "semantic_title": "detecting community sensitive norm violations in online conversations",
    "citation_count": 23,
    "authors": [
      "Chan Young Park",
      "Julia Mendelsohn",
      "Karthik Radhakrishnan",
      "Kinjal Jain",
      "Tushar Kanakagiri",
      "David Jurgens",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.289": {
    "title": "SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations",
    "volume": "findings",
    "abstract": "While contrastive learning is proven to be an effective training strategy in computer vision, Natural Language Processing (NLP) is only recently adopting it as a self-supervised alternative to Masked Language Modeling (MLM) for improving sequence representations. This paper introduces SupCL-Seq, which extends the supervised contrastive learning from computer vision to the optimization of sequence representations in NLP. By altering the dropout mask probability in standard Transformer architectures (e.g. BERT-base), for every representation (anchor), we generate augmented altered views. A supervised contrastive loss is then utilized to maximize the system's capability of pulling together similar samples (e.g., anchors and their altered views) and pushing apart the samples belonging to the other classes. Despite its simplicity, SupCL-Seq leads to large gains in many sequence classification tasks on the GLUE benchmark compared to a standard BERT-base, including 6% absolute improvement on CoLA, 5.4% on MRPC, 4.7% on RTE and 2.6% on STS-B. We also show consistent gains over self-supervised contrastively learned representations, especially in non-semantic tasks. Finally we show that these gains are not solely due to augmentation, but rather to a downstream optimized sequence representation",
    "checked": true,
    "id": "ba53ea9e88486b8a0eda94e1cd84f6b0c33dffe9",
    "semantic_title": "supcl-seq: supervised contrastive learning for downstream optimized sequence representations",
    "citation_count": 17,
    "authors": [
      "Hooman Sedghamiz",
      "Shivam Raval",
      "Enrico Santus",
      "Tuka Alhanai",
      "Mohammad Ghassemi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.290": {
    "title": "mDAPT: Multilingual Domain Adaptive Pretraining in a Single Model",
    "volume": "findings",
    "abstract": "Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a language model on domain-specific text, improves the modelling of text for downstream tasks within the domain. Numerous real-world applications are based on domain-specific text, e.g. working with financial or biomedical documents, and these applications often need to support multiple languages. However, large-scale domain-specific multilingual pretraining data for such scenarios can be difficult to obtain, due to regulations, legislation, or simply a lack of language- and domain-specific text. One solution is to train a single multilingual model, taking advantage of the data available in as many languages as possible. In this work, we explore the benefits of domain adaptive pretraining with a focus on adapting to multiple languages within a specific domain. We propose different techniques to compose pretraining corpora that enable a language model to both become domain-specific and multilingual. Evaluation on nine domain-specific datasets—for biomedical named entity recognition and financial sentence classification—covering seven different languages show that a single multilingual domain-specific model can outperform the general multilingual model, and performs close to its monolingual counterpart. This finding holds across two different pretraining methods, adapter-based pretraining and full model pretraining",
    "checked": true,
    "id": "633780929ae262e461ea35c20b36a5d7042350e7",
    "semantic_title": "mdapt: multilingual domain adaptive pretraining in a single model",
    "citation_count": 13,
    "authors": [
      "Rasmus Kær Jørgensen",
      "Mareike Hartmann",
      "Xiang Dai",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.291": {
    "title": "COSMic: A Coherence-Aware Generation Metric for Image Descriptions",
    "volume": "findings",
    "abstract": "Developers of text generation models rely on automated evaluation metrics as a stand-in for slow and expensive manual evaluations. However, image captioning metrics have struggled to give accurate learned estimates of the semantic and pragmatic success of output text. We address this weakness by introducing the first discourse-aware learned generation metric for evaluating image descriptions. Our approach is inspired by computational theories of discourse for capturing information goals using coherence. We present a dataset of image–description pairs annotated with coherence relations. We then train a coherence-aware metric on a subset of the Conceptual Captions dataset and measure its effectiveness—its ability to predict human ratings of output captions—on a test set composed of out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient for our proposed metric with the human judgments for the results of a number of state-of-the-art coherence-aware caption generation models when compared to several other metrics including recently proposed learned metrics such as BLEURT and BERTScore",
    "checked": true,
    "id": "dd83137f3bfd14f6bf0f714e4fa08a20c06bbbd0",
    "semantic_title": "cosmic: a coherence-aware generation metric for image descriptions",
    "citation_count": 13,
    "authors": [
      "Mert Inan",
      "Piyush Sharma",
      "Baber Khalid",
      "Radu Soricut",
      "Matthew Stone",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.292": {
    "title": "Relation-Guided Pre-Training for Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "Answering complex open-domain questions requires understanding the latent relations between involving entities. However, we found that the existing QA datasets are extremely imbalanced in some types of relations, which hurts the generalization performance over questions with long-tail relations. To remedy this problem, in this paper, we propose a Relation-Guided Pre-Training (RGPT-QA) framework. We first generate a relational QA dataset covering a wide range of relations from both the Wikidata triplets and Wikipedia hyperlinks. We then pre-train a QA model to infer the latent relations from the question, and then conduct extractive QA to get the target answer entity. We demonstrate that by pre-training with propoed RGPT-QA techique, the popular open-domain QA model, Dense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute improvement in Exact Match accuracy on Natural Questions, TriviaQA, and WebQuestions. Particularly, we show that RGPT-QA improves significantly on questions with long-tail relations",
    "checked": true,
    "id": "a61aebbfe029c4b8eafae4042e6242cdca8f54b7",
    "semantic_title": "relation-guided pre-training for open-domain question answering",
    "citation_count": 6,
    "authors": [
      "Ziniu Hu",
      "Yizhou Sun",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.293": {
    "title": "MURAL: Multimodal, Multitask Representations Across Languages",
    "volume": "findings",
    "abstract": "Both image-caption pairs and translation pairs provide the means to learn deep representations of and connections between languages. We use both types of pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual encoder that solves two tasks: 1) image-text matching and 2) translation pair matching. By incorporating billions of translation pairs, MURAL extends ALIGN (Jia et al.)–a state-of-the-art dual encoder learned from 1.8 billion noisy image-text pairs. When using the same encoders, MURAL's performance matches or exceeds ALIGN's cross-modal retrieval performance on well-resourced languages across several datasets. More importantly, it considerably improves performance on under-resourced languages, showing that text-text learning can overcome a paucity of image-caption examples for these languages. On the Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean recall by 8.1% on average for eight under-resourced languages and by 6.8% on average when fine-tuning. We additionally show that MURAL's text representations cluster not only with respect to genealogical connections but also based on areal linguistics, such as the Balkan Sprachbund",
    "checked": true,
    "id": "621a67092b72ee8cafb509767f87bafb2619ccd1",
    "semantic_title": "mural: multimodal, multitask representations across languages",
    "citation_count": 40,
    "authors": [
      "Aashi Jain",
      "Mandy Guo",
      "Krishna Srinivasan",
      "Ting Chen",
      "Sneha Kudugunta",
      "Chao Jia",
      "Yinfei Yang",
      "Jason Baldridge"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.294": {
    "title": "AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models",
    "volume": "findings",
    "abstract": "Despite their success in a variety of NLP tasks, pre-trained language models, due to their heavy reliance on compositionality, fail in effectively capturing the meanings of multiword expressions (MWEs), especially idioms. Therefore, datasets and methods to improve the representation of MWEs are urgently needed. Existing datasets are limited to providing the degree of idiomaticity of expressions along with the literal and, where applicable, (a single) non-literal interpretation of MWEs. This work presents a novel dataset of naturally occurring sentences containing MWEs manually classified into a fine-grained set of meanings, spanning both English and Portuguese. We use this dataset in two tasks designed to test i) a language model's ability to detect idiom usage, and ii) the effectiveness of a language model in generating representations of sentences containing idioms. Our experiments demonstrate that, on the task of detecting idiomatic usage, these models perform reasonably well in the one-shot and few-shot scenarios, but that there is significant scope for improvement in the zero-shot scenario. On the task of representing idiomaticity, we find that pre-training is not always effective, while fine-tuning could provide a sample efficient method of learning representations of sentences containing MWEs",
    "checked": true,
    "id": "b3df38236d47d6334357c2ecbde4351fef490fe8",
    "semantic_title": "astitchinlanguagemodels: dataset and methods for the exploration of idiomaticity in pre-trained language models",
    "citation_count": 32,
    "authors": [
      "Harish Tayyar Madabushi",
      "Edward Gow-Smith",
      "Carolina Scarton",
      "Aline Villavicencio"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.295": {
    "title": "Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration",
    "volume": "findings",
    "abstract": "Persuasion dialogue system reflects the machine's ability to make strategic moves beyond verbal communication, and therefore differentiates itself from task-oriented or open-domain dialogues and has its own unique values. However, the repetition and inconsistency problems still persist in dialogue response generation and could substantially impact user experience and impede the persuasion outcome. Besides, although reinforcement learning (RL) approaches have achieved big success in strategic tasks such as games, it requires a sophisticated user simulator to provide real-time feedback to the dialogue system, which limits the application of RL on persuasion dialogues. To address these issues towards a better persuasion dialogue system, we apply RL to refine a language model baseline without user simulators, and distill sentence-level information about repetition, inconsistency, and task relevance through rewards. Moreover, to better accomplish the persuasion task, the model learns from human demonstration to imitate human persuasion behavior and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback. We will make the code and model publicly available",
    "checked": true,
    "id": "fac8d660e9c0cef1da5d35aea35c572ed776e887",
    "semantic_title": "refine and imitate: reducing repetition and inconsistency in persuasion dialogues via reinforcement learning and human demonstration",
    "citation_count": 29,
    "authors": [
      "Weiyan Shi",
      "Yu Li",
      "Saurav Sahay",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.296": {
    "title": "A Computational Exploration of Pejorative Language in Social Media",
    "volume": "findings",
    "abstract": "In this paper we study pejorative language, an under-explored topic in computational linguistics. Unlike existing models of offensive language and hate speech, pejorative language manifests itself primarily at the lexical level, and describes a word that is used with a negative connotation, making it different from offensive language or other more studied categories. Pejorativity is also context-dependent: the same word can be used with or without pejorative connotations, thus pejorativity detection is essentially a problem similar to word sense disambiguation. We leverage online dictionaries to build a multilingual lexicon of pejorative terms for English, Spanish, Italian, and Romanian. We additionally release a dataset of tweets annotated for pejorative use. Based on these resources, we present an analysis of the usage and occurrence of pejorative words in social media, and present an attempt to automatically disambiguate pejorative usage in our dataset",
    "checked": true,
    "id": "b1e0c3c630c0e8b75c05e804d36302518f9bb5fc",
    "semantic_title": "a computational exploration of pejorative language in social media",
    "citation_count": 16,
    "authors": [
      "Liviu P. Dinu",
      "Ioan-Bogdan Iordache",
      "Ana Sabina Uban",
      "Marcos Zampieri"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.297": {
    "title": "Evidence-based Fact-Checking of Health-related Claims",
    "volume": "findings",
    "abstract": "The task of verifying the truthfulness of claims in textual documents, or fact-checking, has received significant attention in recent years. Many existing evidence-based factchecking datasets contain synthetic claims and the models trained on these data might not be able to verify real-world claims. Particularly few studies addressed evidence-based fact-checking of health-related claims that require medical expertise or evidence from the scientific literature. In this paper, we introduce HEALTHVER, a new dataset for evidence-based fact-checking of health-related claims that allows to study the validity of real-world claims by evaluating their truthfulness against scientific articles. Using a three-step data creation method, we first retrieved real-world claims from snippets returned by a search engine for questions about COVID-19. Then we automatically retrieved and re-ranked relevant scientific papers using a T5 relevance-based model. Finally, the relations between each evidence statement and the associated claim were manually annotated as SUPPORT, REFUTE and NEUTRAL. To validate the created dataset of 14,330 evidence-claim pairs, we developed baseline models based on pretrained language models. Our experiments showed that training deep learning models on real-world medical claims greatly improves performance compared to models trained on synthetic and open-domain claims. Our results and manual analysis suggest that HEALTHVER provides a realistic and challenging dataset for future efforts on evidence-based fact-checking of health-related claims. The dataset, source code, and a leaderboard are available at https://github.com/sarrouti/healthver",
    "checked": true,
    "id": "5e413c1c7c84e7c7e3b79b6abc461fcb6f595ef2",
    "semantic_title": "evidence-based fact-checking of health-related claims",
    "citation_count": 85,
    "authors": [
      "Mourad Sarrouti",
      "Asma Ben Abacha",
      "Yassine Mrabet",
      "Dina Demner-Fushman"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.298": {
    "title": "Learning and Analyzing Generation Order for Undirected Sequence Models",
    "volume": "findings",
    "abstract": "Undirected neural sequence models have achieved performance competitive with the state-of-the-art directed sequence models that generate monotonically from left to right in machine translation tasks. In this work, we train a policy that learns the generation order for a pre-trained, undirected translation model via reinforcement learning. We show that the translations decoded by our learned orders achieve higher BLEU scores than the outputs decoded from left to right or decoded by the learned order from Mansimov et al. (2019) on the WMT'14 German-English translation task. On examples with a maximum source and target length of 30 from De-En and WMT'16 English-Romanian tasks, our learned order outperforms all heuristic generation orders on three out of four language pairs. We next carefully analyze the learned order patterns via qualitative and quantitative analysis. We show that our policy generally follows an outer-to-inner order, predicting the left-most and right-most positions first, and then moving toward the middle while skipping less important words at the beginning. Furthermore, the policy usually predicts positions for a single syntactic constituent structure in consecutive steps. We believe our findings could provide more insights on the mechanism of undirected generation models and encourage further research in this direction",
    "checked": true,
    "id": "e7f010513bebc23116f03cacf69290878d491f61",
    "semantic_title": "learning and analyzing generation order for undirected sequence models",
    "citation_count": 2,
    "authors": [
      "Yichen Jiang",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.299": {
    "title": "Automatic Bilingual Markup Transfer",
    "volume": "findings",
    "abstract": "We describe the task of bilingual markup transfer, which involves placing markup tags from a source sentence into a fixed target translation. This task arises in practice when a human translator generates the target translation without markup, and then the system infers the placement of markup tags. This task contrasts from previous work in which markup transfer is performed jointly with machine translation. We propose two novel metrics and evaluate several approaches based on unsupervised word alignments as well as a supervised neural sequence-to-sequence model. Our best approach achieves an average accuracy of 94.7% across six language pairs, indicating its potential usefulness for real-world localization tasks",
    "checked": true,
    "id": "09f7f606604f15c0da42d7cc833efa8b42af8e5a",
    "semantic_title": "automatic bilingual markup transfer",
    "citation_count": 7,
    "authors": [
      "Thomas Zenkel",
      "Joern Wuebker",
      "John DeNero"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.300": {
    "title": "Exploring a Unified Sequence-To-Sequence Transformer for Medical Product Safety Monitoring in Social Media",
    "volume": "findings",
    "abstract": "Adverse Events (AE) are harmful events resulting from the use of medical products. Although social media may be crucial for early AE detection, the sheer scale of this data makes it logistically intractable to analyze using human agents, with NLP representing the only low-cost and scalable alternative. In this paper, we frame AE Detection and Extraction as a sequence-to-sequence problem using the T5 model architecture and achieve strong performance improvements over the baselines on several English benchmarks (F1 = 0.71, 12.7% relative improvement for AE Detection; Strict F1 = 0.713, 12.4% relative improvement for AE Extraction). Motivated by the strong commonalities between AE tasks, the class imbalance in AE benchmarks, and the linguistic and structural variety typical of social media texts, we propose a new strategy for multi-task training that accounts, at the same time, for task and dataset characteristics. Our approach increases model robustness, leading to further performance gains. Finally, our framework shows some language transfer capabilities, obtaining higher performance than Multilingual BERT in zero-shot learning on French data",
    "checked": true,
    "id": "f14f34aefe004decf45e041d8a8195a7fafee92c",
    "semantic_title": "exploring a unified sequence-to-sequence transformer for medical product safety monitoring in social media",
    "citation_count": 18,
    "authors": [
      "Shivam Raval",
      "Hooman Sedghamiz",
      "Enrico Santus",
      "Tuka Alhanai",
      "Mohammad Ghassemi",
      "Emmanuele Chersoni"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.301": {
    "title": "Disentangling Generative Factors in Natural Language with Discrete Variational Autoencoders",
    "volume": "findings",
    "abstract": "The ability of learning disentangled representations represents a major step for interpretable NLP systems as it allows latent linguistic features to be controlled. Most approaches to disentanglement rely on continuous variables, both for images and text. We argue that despite being suitable for image datasets, continuous variables may not be ideal to model features of textual data, due to the fact that most generative factors in text are discrete. We propose a Variational Autoencoder based method which models language features as discrete variables and encourages independence between variables for learning disentangled representations. The proposed model outperforms continuous and discrete baselines on several qualitative and quantitative benchmarks for disentanglement as well as on a text style transfer downstream application",
    "checked": true,
    "id": "b19536e8baf580c68f81b62035799c94ca25a58d",
    "semantic_title": "disentangling generative factors in natural language with discrete variational autoencoders",
    "citation_count": 23,
    "authors": [
      "Giangiacomo Mercatali",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.302": {
    "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding",
    "volume": "findings",
    "abstract": "To reduce a model size but retain performance, we often rely on knowledge distillation (KD) which transfers knowledge from a large \"teacher\" model to a smaller \"student\" model. However, KD on multimodal datasets such as vision-language tasks is relatively unexplored, and digesting multimodal information is challenging since different modalities present different types of information. In this paper, we perform a large-scale empirical study to investigate the importance and effects of each modality in knowledge distillation. Furthermore, we introduce a multimodal knowledge distillation framework, modality-specific distillation (MSD), to transfer knowledge from a teacher on multimodal tasks by learning the teacher's behavior within each modality. The idea aims at mimicking a teacher's modality-specific predictions by introducing auxiliary loss terms for each modality. Furthermore, because each modality has different saliency for predictions, we define saliency scores for each modality and investigate saliency-based weighting schemes for the auxiliary losses. We further study a weight learning approach to learn the optimal weights on these loss terms. In our empirical analysis, we examine the saliency of each modality in KD, demonstrate the effectiveness of the weighting scheme in MSD, and show that it achieves better performance than KD on four multimodal datasets",
    "checked": true,
    "id": "d1824e84b3837ec4bc115321e3bb35df22d2ccf2",
    "semantic_title": "msd: saliency-aware knowledge distillation for multimodal understanding",
    "citation_count": 6,
    "authors": [
      "Woojeong Jin",
      "Maziar Sanjabi",
      "Shaoliang Nie",
      "Liang Tan",
      "Xiang Ren",
      "Hamed Firooz"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.303": {
    "title": "Do UD Trees Match Mention Spans in Coreference Annotations?",
    "volume": "findings",
    "abstract": "One can find dozens of data resources for various languages in which coreference - a relation between two or more expressions that refer to the same real-world entity - is manually annotated. One could also assume that such expressions usually constitute syntactically meaningful units; however, mention spans have been annotated simply by delimiting token intervals in most coreference projects, i.e., independently of any syntactic representation. We argue that it could be advantageous to make syntactic and coreference annotations convergent in the long term. We present a pilot empirical study focused on matches and mismatches between hand-annotated linear mention spans and automatically parsed syntactic trees that follow Universal Dependencies conventions. The study covers 9 datasets for 8 different languages",
    "checked": true,
    "id": "cb83eabdec5b20ca7b11ac61754ad9fbc27414b7",
    "semantic_title": "do ud trees match mention spans in coreference annotations?",
    "citation_count": 3,
    "authors": [
      "Martin Popel",
      "Zdeněk Žabokrtský",
      "Anna Nedoluzhko",
      "Michal Novák",
      "Daniel Zeman"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.304": {
    "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference",
    "volume": "findings",
    "abstract": "Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling multilingual translation models to billions of parameters without a proportional increase in training computation. However, MoE models are prohibitively large and practitioners often resort to methods such as distillation for serving. In this work, we investigate routing strategies at different granularity (token, sentence, task) in MoE models to bypass distillation. Experiments on WMT and a web-scale dataset suggest that task-level routing (task-MoE) enables us to extract smaller, ready-to-deploy sub-networks from large sparse models. On WMT, our task-MoE with 32 experts (533M parameters) outperforms the best performing token-level MoE model (token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak inference throughput is also improved by a factor of 1.9x when we route by tasks instead of tokens. While distilling a token-MoE to a smaller dense model preserves only 32% of the BLEU gains, our sub-network task-MoE, by design, preserves all the gains with the same inference cost as the distilled student model. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE (13B parameters) performs competitively with a token-level counterpart, while improving the peak inference throughput by a factor of 2.6x",
    "checked": true,
    "id": "8ae292cbd9144acbf4b42b7ead82b079faf33192",
    "semantic_title": "beyond distillation: task-level mixture-of-experts for efficient inference",
    "citation_count": 108,
    "authors": [
      "Sneha Kudugunta",
      "Yanping Huang",
      "Ankur Bapna",
      "Maxim Krikun",
      "Dmitry Lepikhin",
      "Minh-Thang Luong",
      "Orhan Firat"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.305": {
    "title": "TAG: Gradient Attack on Transformer-based Language Models",
    "volume": "findings",
    "abstract": "Although distributed learning has increasingly gained attention in terms of effectively utilizing local devices for data privacy enhancement, recent studies show that publicly shared gradients in the training process can reveal the private training data (gradient leakage) to a third-party. We have, however, no systematic understanding of the gradient leakage mechanism on the Transformer based language models. In this paper, as the first attempt, we formulate the gradient attack problem on the Transformer-based language models and propose a gradient attack algorithm, TAG, to reconstruct the local training data. Experimental results on Transformer, TinyBERT4, TinyBERT6 BERT_BASE, and BERT_LARGE using GLUE benchmark show that compared with DLG, TAG works well on more weight distributions in reconstructing training data and achieves 1.5x recover rate and 2.5x ROUGE-2 over prior methods without the need of ground truth label. TAG can obtain up to 90% data by attacking gradients in CoLA dataset. In addition, TAG is stronger than previous approaches on larger models, smaller dictionary size, and smaller input length. We hope the proposed TAG will shed some light on the privacy leakage problem in Transformer-based NLP models",
    "checked": true,
    "id": "07033abe45c0e2556e8c3fa1db1879b42dd45915",
    "semantic_title": "tag: gradient attack on transformer-based language models",
    "citation_count": 75,
    "authors": [
      "Jieren Deng",
      "Yijue Wang",
      "Ji Li",
      "Chenghong Wang",
      "Chao Shang",
      "Hang Liu",
      "Sanguthevar Rajasekaran",
      "Caiwen Ding"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.306": {
    "title": "Generating Realistic Natural Language Counterfactuals",
    "volume": "findings",
    "abstract": "Counterfactuals are a valuable means for understanding decisions made by ML systems. However, the counterfactuals generated by the methods currently available for natural language text are either unrealistic or introduce imperceptible changes. We propose CounterfactualGAN: a method that combines a conditional GAN and the embeddings of a pretrained BERT encoder to model-agnostically generate realistic natural language text counterfactuals for explaining regression and classification tasks. Experimental results show that our method produces perceptibly distinguishable counterfactuals, while outperforming four baseline methods on fidelity and human judgments of naturalness, across multiple datasets and multiple predictive models",
    "checked": true,
    "id": "c4a4b4c98b2d746c8215541e2c084d9b7159b333",
    "semantic_title": "generating realistic natural language counterfactuals",
    "citation_count": 40,
    "authors": [
      "Marcel Robeer",
      "Floris Bex",
      "Ad Feelders"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.307": {
    "title": "Unsupervised Chunking as Syntactic Structure Induction with a Knowledge-Transfer Approach",
    "volume": "findings",
    "abstract": "In this paper, we address unsupervised chunking as a new task of syntactic structure induction, which is helpful for understanding the linguistic structures of human languages as well as processing low-resource languages. We propose a knowledge-transfer approach that heuristically induces chunk labels from state-of-the-art unsupervised parsing models; a hierarchical recurrent neural network (HRNN) learns from such induced chunk labels to smooth out the noise of the heuristics. Experiments show that our approach largely bridges the gap between supervised and unsupervised chunking",
    "checked": true,
    "id": "60af7ea858c52df04bdb5d8c874c0549a83e1105",
    "semantic_title": "unsupervised chunking as syntactic structure induction with a knowledge-transfer approach",
    "citation_count": 4,
    "authors": [
      "Anup Anand Deshmukh",
      "Qianqiu Zhang",
      "Ming Li",
      "Jimmy Lin",
      "Lili Mou"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.308": {
    "title": "Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects",
    "volume": "findings",
    "abstract": "A popular approach to decompose the neural bases of language consists in correlating, across individuals, the brain responses to different stimuli (e.g. regular speech versus scrambled words, sentences, or paragraphs). Although successful, this ‘model-free' approach necessitates the acquisition of a large and costly set of neuroimaging data. Here, we show that a model-based approach can reach equivalent results within subjects exposed to natural stimuli. We capitalize on the recently-discovered similarities between deep language models and the human brain to compute the mapping between i) the brain responses to regular speech and ii) the activations of deep language models elicited by modified stimuli (e.g. scrambled words, sentences, or paragraphs). Our model-based approach successfully replicates the seminal study of Lerner et al. (2011), which revealed the hierarchy of language areas by comparing the functional-magnetic resonance imaging (fMRI) of seven subjects listening to 7min of both regular and scrambled narratives. We further extend and precise these results to the brain signals of 305 individuals listening to 4.1 hours of narrated stories. Overall, this study paves the way for efficient and flexible analyses of the brain bases of language",
    "checked": true,
    "id": "a6e259c8813ae419610dab4190a8358adcc7b2b5",
    "semantic_title": "model-based analysis of brain activity reveals the hierarchy of language in 305 subjects",
    "citation_count": 32,
    "authors": [
      "Charlotte Caucheteux",
      "Alexandre Gramfort",
      "Jean-Remi King"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.309": {
    "title": "Gated Transformer for Robust De-noised Sequence-to-Sequence Modelling",
    "volume": "findings",
    "abstract": "Robust sequence-to-sequence modelling is an essential task in the real world where the inputs are often noisy. Both user-generated and machine generated inputs contain various kinds of noises in the form of spelling mistakes, grammatical errors, character recognition errors, all of which impact downstream tasks and affect interpretability of texts. In this work, we devise a novel sequence-to-sequence architecture for detecting and correcting different real world and artificial noises (adversarial attacks) from English texts. Towards that we propose a modified Transformer-based encoder-decoder architecture that uses a gating mechanism to detect types of corrections required and accordingly corrects texts. Experimental results show that our gated architecture with pre-trained language models perform significantly better that the non-gated counterparts and other state-of-the-art error correction models in correcting spelling and grammatical errors. Extrinsic evaluation of our model on Machine Translation (MT) and Summarization tasks show the competitive performance of the model against other generative sequence-to-sequence models under noisy inputs",
    "checked": true,
    "id": "8f61279a58c2033236bb067520819cfa8361d830",
    "semantic_title": "gated transformer for robust de-noised sequence-to-sequence modelling",
    "citation_count": 4,
    "authors": [
      "Ayan Sengupta",
      "Amit Kumar",
      "Sourabh Kumar Bhattacharjee",
      "Suman Roy"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.310": {
    "title": "Token-wise Curriculum Learning for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Existing curriculum learning approaches to Neural Machine Translation (NMT) require sampling sufficient amounts of \"easy\" samples from training data at the early training stage. This is not always achievable for low-resource languages where the amount of training data is limited. To address such a limitation, we propose a novel token-wise curriculum learning approach that creates sufficient amounts of easy samples. Specifically, the model learns to predict a short sub-sequence from the beginning part of each target sentence at the early stage of training. Then the sub-sequence is gradually expanded as the training progresses. Such a new curriculum design is inspired by the cumulative effect of translation errors, which makes the latter tokens more challenging to predict than the beginning ones. Extensive experiments show that our approach can consistently outperform baselines on five language pairs, especially for low-resource languages. Combining our approach with sentence-level methods further improves the performance of high-resource languages",
    "checked": true,
    "id": "59006ac5a85a898f28710cbfbd80d04991c0560e",
    "semantic_title": "token-wise curriculum learning for neural machine translation",
    "citation_count": 4,
    "authors": [
      "Chen Liang",
      "Haoming Jiang",
      "Xiaodong Liu",
      "Pengcheng He",
      "Weizhu Chen",
      "Jianfeng Gao",
      "Tuo Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.311": {
    "title": "RelDiff: Enriching Knowledge Graph Relation Representations for Sensitivity Classification",
    "volume": "findings",
    "abstract": "The relationships that exist between entities can be a reliable indicator for classifying sensitive information, such as commercially sensitive information. For example, the relation person-IsDirectorOf-company can indicate whether an individual's salary should be considered as sensitive personal information. Representations of such relations are often learned using a knowledge graph to produce embeddings for relation types, generalised across different entity-pairs. However, a relation type may or may not correspond to a sensitivity depending on the entities that participate to the relation. Therefore, generalised relation embeddings are typically insufficient for classifying sensitive information. In this work, we propose a novel method for representing entities and relations within a single embedding to better capture the relationship between the entities. Moreover, we show that our proposed entity-relation-entity embedding approach can significantly improve (McNemar's test, p <0.05) the effectiveness of sensitivity classification, compared to classification approaches that leverage relation embedding approaches from the literature. (0.426 F1 vs 0.413 F1)",
    "checked": true,
    "id": "42ddd50c7f954bb5a9fe91887febfc163f278016",
    "semantic_title": "reldiff: enriching knowledge graph relation representations for sensitivity classification",
    "citation_count": 2,
    "authors": [
      "Hitarth Narvala",
      "Graham McDonald",
      "Iadh Ounis"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.312": {
    "title": "Post-Editing Extractive Summaries by Definiteness Prediction",
    "volume": "findings",
    "abstract": "Extractive summarization has been the mainstay of automatic summarization for decades. Despite all the progress, extractive summarizers still suffer from shortcomings including coreference issues arising from extracting sentences away from their original context in the source document. This affects the coherence and readability of extractive summaries. In this work, we propose a lightweight post-editing step for extractive summaries that centers around a single linguistic decision: the definiteness of noun phrases. We conduct human evaluation studies that show that human expert judges substantially prefer the output of our proposed system over the original summaries. Moreover, based on an automatic evaluation study, we provide evidence for our system's ability to generate linguistic decisions that lead to improved extractive summaries. We also draw insights about how the automatic system is exploiting some local cues related to the writing style of the main article texts or summary texts to make the decisions, rather than reasoning about the contexts pragmatically",
    "checked": true,
    "id": "0c3c87e7d106fc1a6a32e146b0c6ffe0fec43c9a",
    "semantic_title": "post-editing extractive summaries by definiteness prediction",
    "citation_count": 2,
    "authors": [
      "Jad Kabbara",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.313": {
    "title": "Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations",
    "volume": "findings",
    "abstract": "Fine-tuning pretrained models for automatically summarizing doctor-patient conversation transcripts presents many challenges: limited training data, significant domain shift, long and noisy transcripts, and high target summary variability. In this paper, we explore the feasibility of using pretrained transformer models for automatically summarizing doctor-patient conversations directly from transcripts. We show that fluent and adequate summaries can be generated with limited training data by fine-tuning BART on a specially constructed dataset. The resulting models greatly surpass the performance of an average human annotator and the quality of previous published work for the task. We evaluate multiple methods for handling long conversations, comparing them to the obvious baseline of truncating the conversation to fit the pretrained model length limit. We introduce a multistage approach that tackles the task by learning two fine-tuned models: one for summarizing conversation chunks into partial summaries, followed by one for rewriting the collection of partial summaries into a complete summary. Using a carefully chosen fine-tuning dataset, this method is shown to be effective at handling longer conversations, improving the quality of generated summaries. We conduct both an automatic evaluation (through ROUGE and two concept-based metrics focusing on medical findings) and a human evaluation (through qualitative examples from literature, assessing hallucination, generalization, fluency, and general quality of the generated summaries)",
    "checked": true,
    "id": "dbae4c89a93597c41ec4373f6da03a93eac2927b",
    "semantic_title": "leveraging pretrained models for automatic summarization of doctor-patient conversations",
    "citation_count": 68,
    "authors": [
      "Longxiang Zhang",
      "Renato Negrinho",
      "Arindam Ghosh",
      "Vasudevan Jagannathan",
      "Hamid Reza Hassanzadeh",
      "Thomas Schaaf",
      "Matthew R. Gormley"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.314": {
    "title": "Distilling Knowledge for Empathy Detection",
    "volume": "findings",
    "abstract": "Empathy is the link between self and others. Detecting and understanding empathy is a key element for improving human-machine interaction. However, annotating data for detecting empathy at a large scale is a challenging task. This paper employs multi-task training with knowledge distillation to incorporate knowledge from available resources (emotion and sentiment) to detect empathy from the natural language in different domains. This approach yields better results on an existing news-related empathy dataset compared to strong baselines. In addition, we build a new dataset for empathy prediction with fine-grained empathy direction, seeking or providing empathy, from Twitter. We release our dataset for research purposes",
    "checked": true,
    "id": "ea26aaeef06df12fa577a54cbe9465d46ce88e3b",
    "semantic_title": "distilling knowledge for empathy detection",
    "citation_count": 26,
    "authors": [
      "Mahshid Hosseini",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.315": {
    "title": "Adapting Entities across Languages and Cultures",
    "volume": "findings",
    "abstract": "How would you explain Bill Gates to a German? He is associated with founding a company in the United States, so perhaps the German founder Carl Benz could stand in for Gates in those contexts. This type of translation is called adaptation in the translation community. Until now, this task has not been done computationally. Automatic adaptation could be used in natural language processing for machine translation and indirectly for generating new question answering datasets and education. We propose two automatic methods and compare them to human results for this novel NLP task. First, a structured knowledge base adapts named entities using their shared properties. Second, vector-arithmetic and orthogonal embedding mappings methods identify better candidates, but at the expense of interpretable features. We evaluate our methods through a new dataset of human adaptations",
    "checked": true,
    "id": "60429842de22f05b2961d4da5f31245ee0e032c5",
    "semantic_title": "adapting entities across languages and cultures",
    "citation_count": 18,
    "authors": [
      "Denis Peskov",
      "Viktor Hangya",
      "Jordan Boyd-Graber",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.316": {
    "title": "ODIST: Open World Classification via Distributionally Shifted Instances",
    "volume": "findings",
    "abstract": "In this work, we address the open-world classification problem with a method called ODIST, open world classification via distributionally shifted instances. This novel and straightforward method can create out-of-domain instances from the in-domain training instances with the help of a pre-trained generative language model. Experimental results show that ODIST performs better than state-of-the-art decision boundary finding method",
    "checked": true,
    "id": "050438e595f7a3b8af6a5f348e7a87898b5e947f",
    "semantic_title": "odist: open world classification via distributionally shifted instances",
    "citation_count": 20,
    "authors": [
      "Lei Shu",
      "Yassine Benajiba",
      "Saab Mansour",
      "Yi Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.317": {
    "title": "LAMAD: A Linguistic Attentional Model for Arabic Text Diacritization",
    "volume": "findings",
    "abstract": "In Arabic Language, diacritics are used to specify meanings as well as pronunciations. However, diacritics are often omitted from written texts, which increases the number of possible meanings and pronunciations. This leads to an ambiguous text and makes the computational process on undiacritized text more difficult. In this paper, we propose a Linguistic Attentional Model for Arabic text Diacritization (LAMAD). In LAMAD, a new linguistic feature representation is presented, which utilizes both word and character contextual features. Then, a linguistic attention mechanism is proposed to capture the important linguistic features. In addition, we explore the impact of the linguistic features extracted from the text on Arabic text diacritization (ATD) by introducing them to the linguistic attention mechanism. The extensive experimental results on three datasets with different sizes illustrate that LAMAD outperforms the existing state-of-the-art models",
    "checked": true,
    "id": "c7f7ac4b2d19268134cf4ba91426f913957b9c96",
    "semantic_title": "lamad: a linguistic attentional model for arabic text diacritization",
    "citation_count": 4,
    "authors": [
      "Raeed Al-Sabri",
      "Jianliang Gao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.318": {
    "title": "Sequence-to-Lattice Models for Fast Translation",
    "volume": "findings",
    "abstract": "Non-autoregressive machine translation (NAT) approaches enable fast generation by utilizing parallelizable generative processes. The remaining bottleneck in these models is their decoder layers; unfortunately unlike in autoregressive models (Kasai et al., 2020), removing decoder layers from NAT models significantly degrades accuracy. This work proposes a sequence-to-lattice model that replaces the decoder with a search lattice. Our approach first constructs a candidate lattice using efficient lookup operations, generates lattice scores from a deep encoder, and finally finds the best path using dynamic programming. Experiments on three machine translation datasets show that our method is faster than past non-autoregressive generation approaches, and more accurate than naively reducing the number of decoder layers",
    "checked": true,
    "id": "103852cac3e52b04eda79e60635b5304717d1fbb",
    "semantic_title": "sequence-to-lattice models for fast translation",
    "citation_count": 0,
    "authors": [
      "Yuntian Deng",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.319": {
    "title": "Towards Realistic Single-Task Continuous Learning Research for NER",
    "volume": "findings",
    "abstract": "There is an increasing interest in continuous learning (CL), as data privacy is becoming a priority for real-world machine learning applications. Meanwhile, there is still a lack of academic NLP benchmarks that are applicable for realistic CL settings, which is a major challenge for the advancement of the field. In this paper we discuss some of the unrealistic data characteristics of public datasets, study the challenges of realistic single-task continuous learning as well as the effectiveness of data rehearsal as a way to mitigate accuracy loss. We construct a CL NER dataset from an existing publicly available dataset and release it along with the code to the research community",
    "checked": true,
    "id": "e7ca4906d0df5c61f6395d779999b82ecb31c93c",
    "semantic_title": "towards realistic single-task continuous learning research for ner",
    "citation_count": 6,
    "authors": [
      "Justin Payan",
      "Yuval Merhav",
      "He Xie",
      "Satyapriya Krishna",
      "Anil Ramakrishna",
      "Mukund Sridhar",
      "Rahul Gupta"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.320": {
    "title": "Retrieval Augmentation Reduces Hallucination in Conversation",
    "volume": "findings",
    "abstract": "Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots",
    "checked": true,
    "id": "a2a7033a5a859e3a6e6f0a83018326400b4c5faa",
    "semantic_title": "retrieval augmentation reduces hallucination in conversation",
    "citation_count": 725,
    "authors": [
      "Kurt Shuster",
      "Spencer Poff",
      "Moya Chen",
      "Douwe Kiela",
      "Jason Weston"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.321": {
    "title": "Towards Automatic Bias Detection in Knowledge Graphs",
    "volume": "findings",
    "abstract": "With the recent surge in social applications relying on knowledge graphs, the need for techniques to ensure fairness in KG based methods is becoming increasingly evident. Previous works have demonstrated that KGs are prone to various social biases, and have proposed multiple methods for debiasing them. However, in such studies, the focus has been on debiasing techniques, while the relations to be debiased are specified manually by the user. As manual specification is itself susceptible to human cognitive bias, there is a need for a system capable of quantifying and exposing biases, that can support more informed decisions on what to debias. To address this gap in the literature, we describe a framework for identifying biases present in knowledge graph embeddings, based on numerical bias metrics. We illustrate the framework with three different bias measures on the task of profession prediction, and it can be flexibly extended to further bias definitions and applications. The relations flagged as biased can then be handed to decision makers for judgement upon subsequent debiasing",
    "checked": true,
    "id": "d105b192d887a84746fb35ceb30e35511495da78",
    "semantic_title": "towards automatic bias detection in knowledge graphs",
    "citation_count": 11,
    "authors": [
      "Daphna Keidar",
      "Mian Zhong",
      "Ce Zhang",
      "Yash Raj Shrestha",
      "Bibek Paudel"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.322": {
    "title": "Searching for More Efficient Dynamic Programs",
    "volume": "findings",
    "abstract": "Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions. Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal asymptotic runtime can be unintuitive, time-consuming, and error-prone. Our work aims to automate this laborious process. Given an initial correct declarative program, we search for a sequence of semantics-preserving transformations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search—like the mental search performed by human programmers—can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system",
    "checked": true,
    "id": "bc5e4b9fb3a40057df4994354a403202218d53a6",
    "semantic_title": "searching for more efficient dynamic programs",
    "citation_count": 3,
    "authors": [
      "Tim Vieira",
      "Ryan Cotterell",
      "Jason Eisner"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.323": {
    "title": "Revisiting Robust Neural Machine Translation: A Transformer Case Study",
    "volume": "findings",
    "abstract": "Transformers have brought a remarkable improvement in the performance of neural machine translation (NMT) systems but they could be surprisingly vulnerable to noise. In this work, we try to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behavior of conventional models for the problem of noise but Transformers are relatively understudied in this context. Motivated by this, we introduce a novel data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two other novel extensions to the original Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that modify the neural architecture as well as the training process to handle noise. One important characteristic of our techniques is that they only impact the training phase and do not impose any overhead at inference time. We evaluated our techniques to translate the English–German pair in both directions and observed that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to 10% of entire test words are infected by noise",
    "checked": true,
    "id": "3f0bc7b3da6bdf1b257d3a2eb6a5436688d43d25",
    "semantic_title": "revisiting robust neural machine translation: a transformer case study",
    "citation_count": 11,
    "authors": [
      "Peyman Passban",
      "Puneeth Saladi",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.324": {
    "title": "Can NLI Models Verify QA Systems' Predictions?",
    "volume": "findings",
    "abstract": "To build robust question answering systems, we need the ability to verify whether answers to questions are truly correct, not just \"good enough\" in the context of imperfect QA datasets. We explore the use of natural language inference (NLI) as a way to achieve this goal, as NLI inherently requires the premise (document context) to contain all necessary information to support the hypothesis (proposed answer to the question). We leverage large pre-trained models and recent prior datasets to construct powerful question conversion and decontextualization modules, which can reformulate QA instances as premise-hypothesis pairs with very high reliability. Then, by combining standard NLI datasets with NLI examples automatically derived from QA training data, we can train NLI models to evaluate QA models' proposed answers. We show that our approach improves the confidence estimation of a QA model across different domains, evaluated in a selective QA setting. Careful manual analysis over the predictions of our NLI model shows that it can further identify cases where the QA model produces the right answer for the wrong reason, i.e., when the answer sentence cannot address all aspects of the question",
    "checked": true,
    "id": "e3bba08dc07c5f1372b78450990ba0ef305a834c",
    "semantic_title": "can nli models verify qa systems' predictions?",
    "citation_count": 54,
    "authors": [
      "Jifan Chen",
      "Eunsol Choi",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.325": {
    "title": "Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models",
    "volume": "findings",
    "abstract": "Domain-specific pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-specific PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-specific knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive areas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-efficient way. More specifically, we propose to encode domain knowledge via adapters, which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-specific adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the benefits of the proposed architecture and the knowledge-specific adapters across multiple PLMs",
    "checked": true,
    "id": "fb3dc58cb17c54b997e6301cbde7773f77427833",
    "semantic_title": "parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models",
    "citation_count": 25,
    "authors": [
      "Qiuhao Lu",
      "Dejing Dou",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.326": {
    "title": "Uncovering Implicit Gender Bias in Narratives through Commonsense Inference",
    "volume": "findings",
    "abstract": "Pre-trained language models learn socially harmful biases from their training corpora, and may repeat these biases when used for generation. We study gender biases associated with the protagonist in model-generated stories. Such biases may be expressed either explicitly (\"women can't park\") or implicitly (e.g. an unsolicited male character guides her into a parking space). We focus on implicit biases, and use a commonsense reasoning engine to uncover them. Specifically, we infer and analyze the protagonist's motivations, attributes, mental states, and implications on others. Our findings regarding implicit biases are in line with prior work that studied explicit biases, for example showing that female characters' portrayal is centered around appearance, while male figures' focus on intellect",
    "checked": true,
    "id": "15124bd493aaa91ec1557e31486b4a0dab212707",
    "semantic_title": "uncovering implicit gender bias in narratives through commonsense inference",
    "citation_count": 34,
    "authors": [
      "Tenghao Huang",
      "Faeze Brahman",
      "Vered Shwartz",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.327": {
    "title": "Contrastive Document Representation Learning with Graph Attention Networks",
    "volume": "findings",
    "abstract": "Recent progress in pretrained Transformer-based language models has shown great success in learning contextual representation of text. However, due to the quadratic self-attention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph attention network allows us to leverage the high-level semantic structure of the document. In addition, based on our graph document model, we design a simple contrastive learning strategy to pretrain our models on a large amount of unlabeled corpus. Empirically, we demonstrate the effectiveness of our approaches in document classification and document retrieval tasks",
    "checked": true,
    "id": "1d21a50118657e94b9a76330ddbe2e807dea68de",
    "semantic_title": "contrastive document representation learning with graph attention networks",
    "citation_count": 9,
    "authors": [
      "Peng Xu",
      "Xinchi Chen",
      "Xiaofei Ma",
      "Zhiheng Huang",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.328": {
    "title": "Convex Aggregation for Opinion Summarization",
    "volume": "findings",
    "abstract": "Recent advances in text autoencoders have significantly improved the quality of the latent space, which enables models to generate grammatical and consistent text from aggregated latent vectors. As a successful application of this property, unsupervised opinion summarization models generate a summary by decoding the aggregated latent vectors of inputs. More specifically, they perform the aggregation via simple average. However, little is known about how the vector aggregation step affects the generation quality. In this study, we revisit the commonly used simple average approach by examining the latent space and generated summaries. We found that text autoencoders tend to generate overly generic summaries from simply averaged latent vectors due to an unexpected L2-norm shrinkage in the aggregated latent vectors, which we refer to as summary vector degeneration. To overcome this issue, we develop a framework Coop, which searches input combinations for the latent vector aggregation using input-output word overlap. Experimental results show that Coop successfully alleviates the summary vector degeneration issue and establishes new state-of-the-art performance on two opinion summarization benchmarks. Code is available at https://github.com/megagonlabs/coop",
    "checked": true,
    "id": "2c331f18fa242585bd0db18bce315fd23fa73672",
    "semantic_title": "convex aggregation for opinion summarization",
    "citation_count": 35,
    "authors": [
      "Hayate Iso",
      "Xiaolan Wang",
      "Yoshihiko Suhara",
      "Stefanos Angelidis",
      "Wang-Chiew Tan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.329": {
    "title": "Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings",
    "volume": "findings",
    "abstract": "Recent studies have proposed different methods to improve multilingual word representations in contextualized settings including techniques that align between source and target embedding spaces. For contextualized embeddings, alignment becomes more complex as we additionally take context into consideration. In this work, we propose using Optimal Transport (OT) as an alignment objective during fine-tuning to further improve multilingual contextualized representations for downstream cross-lingual transfer. This approach does not require word-alignment pairs prior to fine-tuning that may lead to sub-optimal matching and instead learns the word alignments within context in an unsupervised manner. It also allows different types of mappings due to soft matching between source and target sentences. We benchmark our proposed method on two tasks (XNLI and XQuAD) and achieve improvements over baselines as well as competitive results compared to similar recent works",
    "checked": true,
    "id": "2245ab7bb9a4c5a5168059b875f86776b6ae34ef",
    "semantic_title": "using optimal transport as alignment objective for fine-tuning multilingual contextualized embeddings",
    "citation_count": 25,
    "authors": [
      "Sawsan Alqahtani",
      "Garima Lalwani",
      "Yi Zhang",
      "Salvatore Romeo",
      "Saab Mansour"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.330": {
    "title": "Uncertainty-Aware Machine Translation Evaluation",
    "volume": "findings",
    "abstract": "Several neural-based metrics have been recently proposed to evaluate machine translation quality. However, all of them resort to point estimates, which provide limited information at segment level. This is made worse as they are trained on noisy, biased and scarce human judgements, often resulting in unreliable quality predictions. In this paper, we introduce uncertainty-aware MT evaluation and analyze the trustworthiness of the predicted quality. We combine the COMET framework with two uncertainty estimation methods, Monte Carlo dropout and deep ensembles, to obtain quality scores along with confidence intervals. We compare the performance of our uncertainty-aware MT evaluation methods across multiple language pairs from the QT21 dataset and the WMT20 metrics task, augmented with MQM annotations. We experiment with varying numbers of references and further discuss the usefulness of uncertainty-aware quality estimation (without references) to flag possibly critical translation mistakes",
    "checked": true,
    "id": "2a8ec34500e2f90c5070fa34317b50939ac8f6f9",
    "semantic_title": "uncertainty-aware machine translation evaluation",
    "citation_count": 43,
    "authors": [
      "Taisiya Glushkova",
      "Chrysoula Zerva",
      "Ricardo Rei",
      "André F. T. Martins"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.331": {
    "title": "Neural Unification for Logic Reasoning over Natural Language",
    "volume": "findings",
    "abstract": "Automated Theorem Proving (ATP) deals with the development of computer programs being able to show that some conjectures (queries) are a logical consequence of a set of axioms (facts and rules). There exists several successful ATPs where conjectures and axioms are formally provided (e.g. formalised as First Order Logic formulas). Recent approaches, such as Clark et al., have proposed transformer-based architectures for deriving conjectures given axioms expressed in natural language (English). The conjecture is verified through a binary text classifier, where the transformers model is trained to predict the truth value of a conjecture given the axioms. The RuleTaker approach of Clark et al. achieves appealing results both in terms of accuracy and in the ability to generalize, showing that when the model is trained with deep enough queries (at least 3 inference steps), the transformers are able to correctly answer the majority of queries (97.6%) that require up to 5 inference steps. In this work we propose a new architecture, namely the Neural Unifier, and a relative training procedure, which achieves state-of-the-art results in term of generalisation, showing that mimicking a well-known inference procedure, the backward chaining, it is possible to answer deep queries even when the model is trained only on shallow ones. The approach is demonstrated in experiments using a diverse set of benchmark data and the source code is released to the research community for reproducibility",
    "checked": true,
    "id": "396eade7a493413baff2da8e8b2aad70950a65f1",
    "semantic_title": "neural unification for logic reasoning over natural language",
    "citation_count": 13,
    "authors": [
      "Gabriele Picco",
      "Thanh Lam Hoang",
      "Marco Luca Sbodio",
      "Vanessa Lopez"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.332": {
    "title": "From None to Severe: Predicting Severity in Movie Scripts",
    "volume": "findings",
    "abstract": "In this paper, we introduce the task of predicting severity of age-restricted aspects of movie content based solely on the dialogue script. We first investigate categorizing the ordinal severity of movies on 5 aspects: Sex, Violence, Profanity, Substance consumption, and Frightening scenes. The problem is handled using a siamese network-based multitask framework which concurrently improves the interpretability of the predictions. The experimental results show that our method outperforms the previous state-of-the-art model and provides useful information to interpret model predictions. The proposed dataset and source code are publicly available at our GitHub repository",
    "checked": true,
    "id": "e658b89e3cb5bc8317ff3247e57a769c8898937d",
    "semantic_title": "from none to severe: predicting severity in movie scripts",
    "citation_count": 5,
    "authors": [
      "Yigeng Zhang",
      "Mahsa Shafaei",
      "Fabio Gonzalez",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.333": {
    "title": "Benchmarking Meta-embeddings: What Works and What Does Not",
    "volume": "findings",
    "abstract": "In the last few years, several methods have been proposed to build meta-embeddings. The general aim was to obtain new representations integrating complementary knowledge from different source pre-trained embeddings thereby improving their overall quality. However, previous meta-embeddings have been evaluated using a variety of methods and datasets, which makes it difficult to draw meaningful conclusions regarding the merits of each approach. In this paper we propose a unified common framework, including both intrinsic and extrinsic tasks, for a fair and objective meta-embeddings evaluation. Furthermore, we present a new method to generate meta-embeddings, outperforming previous work on a large number of intrinsic evaluation benchmarks. Our evaluation framework also allows us to conclude that previous extrinsic evaluations of meta-embeddings have been overestimated",
    "checked": true,
    "id": "fd53ab030cf82d687823997a8a0b15b9e91f4dac",
    "semantic_title": "benchmarking meta-embeddings: what works and what does not",
    "citation_count": 4,
    "authors": [
      "Iker García-Ferrero",
      "Rodrigo Agerri",
      "German Rigau"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.334": {
    "title": "A Plug-and-Play Method for Controlled Text Generation",
    "volume": "findings",
    "abstract": "Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text",
    "checked": true,
    "id": "ddcd5bed531c13e0da65e30333aaa5c27914f882",
    "semantic_title": "a plug-and-play method for controlled text generation",
    "citation_count": 91,
    "authors": [
      "Damian Pascual",
      "Beni Egressy",
      "Clara Meister",
      "Ryan Cotterell",
      "Roger Wattenhofer"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.335": {
    "title": "A Corpus-based Syntactic Analysis of Two-termed Unlike Coordination",
    "volume": "findings",
    "abstract": "Coordination is a phenomenon of language that conjoins two or more terms or phrases using a coordinating conjunction. Although coordination has been explored extensively in the linguistics literature, the rules and constraints that govern its structure are still largely elusive and widely debated amongst linguists. This paper presents a study of two-termed unlike coordinations in particular, where the two conjuncts of the coordination phrase form valid constituents but have distinct categories. We conducted a syntactic analysis of the phrasal categories that can be conjoined in such unlike coordinations through a computational corpus-based approach, utilizing the Corpus of Contemporary American English (COCA) as the main data source, as well as the Penn Treebank (PTB). The results show that the two conjuncts within unlike coordinations display different properties based on their position, supporting an antisymmetric view of the structure of coordination. This research provides new data and perspectives through the use of statistical techniques that can help shape future theories and models of coordination",
    "checked": true,
    "id": "2d4e2c5063e52b49cd04e328b8150989894cc262",
    "semantic_title": "a corpus-based syntactic analysis of two-termed unlike coordination",
    "citation_count": 3,
    "authors": [
      "Julie Kallini",
      "Christiane Fellbaum"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.336": {
    "title": "Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation",
    "volume": "findings",
    "abstract": "Radiology report generation aims at generating descriptive text from radiology images automatically, which may present an opportunity to improve radiology reporting and interpretation. A typical setting consists of training encoder-decoder models on image-report pairs with a cross entropy loss, which struggles to generate informative sentences for clinical diagnoses since normal findings dominate the datasets. To tackle this challenge and encourage more clinically-accurate text outputs, we propose a novel weakly supervised contrastive loss for medical report generation. Experimental results demonstrate that our method benefits from contrasting target reports with incorrect but semantically-close ones. It outperforms previous work on both clinical correctness and text generation metrics for two public benchmarks",
    "checked": true,
    "id": "0fcd8ca72458a8acd23b7d2ab6d9b35f24a39f7d",
    "semantic_title": "weakly supervised contrastive learning for chest x-ray report generation",
    "citation_count": 64,
    "authors": [
      "An Yan",
      "Zexue He",
      "Xing Lu",
      "Jiang Du",
      "Eric Chang",
      "Amilcare Gentili",
      "Julian McAuley",
      "Chun-Nan Hsu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.337": {
    "title": "NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions",
    "volume": "findings",
    "abstract": "Existing conversational systems are mostly agent-centric, which assumes the user utterances will closely follow the system ontology. However, in real-world scenarios, it is highly desirable that users can speak freely and naturally. In this work, we attempt to build a user-centric dialogue system for conversational recommendation. As there is no clean mapping for a user's free form utterance to an ontology, we first model the user preferences as estimated distributions over the system ontology and map the user's utterances to such distributions. Learning such a mapping poses new challenges on reasoning over various types of knowledge, ranging from factoid knowledge, commonsense knowledge to the users' own situations. To this end, we build a new dataset named NUANCED that focuses on such realistic settings, with 5.1k dialogues, 26k turns of high-quality user responses. We conduct experiments, showing both the usefulness and challenges of our problem setting. We believe NUANCED can serve as a valuable resource to push existing research from the agent-centric system to the user-centric system. The code and data are publicly available",
    "checked": true,
    "id": "3bc8d156f35875073c9f6e0223fc5f8776ce8b0c",
    "semantic_title": "nuanced: natural utterance annotation for nuanced conversation with estimated distributions",
    "citation_count": 1,
    "authors": [
      "Zhiyu Chen",
      "Honglei Liu",
      "Hu Xu",
      "Seungwhan Moon",
      "Hao Zhou",
      "Bing Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.338": {
    "title": "Table-based Fact Verification With Salience-aware Learning",
    "volume": "findings",
    "abstract": "Tables provide valuable knowledge that can be used to verify textual statements. While a number of works have considered table-based fact verification, direct alignments of tabular data with tokens in textual statements are rarely available. Moreover, training a generalized fact verification model requires abundant labeled training data. In this paper, we propose a novel system to address these problems. Inspired by counterfactual causality, our system identifies token-level salience in the statement with probing-based salience estimation. Salience estimation allows enhanced learning of fact verification from two perspectives. From one perspective, our system conducts masked salient token prediction to enhance the model for alignment and reasoning between the table and the statement. From the other perspective, our system applies salience-aware data augmentation to generate a more diverse set of training instances by replacing non-salient terms. Experimental results on TabFact show the effective improvement by the proposed salience-aware learning techniques, leading to the new SOTA performance on the benchmark",
    "checked": true,
    "id": "d664570d91fee4aab29438f41c1794f82ae80afb",
    "semantic_title": "table-based fact verification with salience-aware learning",
    "citation_count": 21,
    "authors": [
      "Fei Wang",
      "Kexuan Sun",
      "Jay Pujara",
      "Pedro Szekely",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.339": {
    "title": "Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence Coverage",
    "volume": "findings",
    "abstract": "News media structure their reporting of events or issues using certain perspectives. When describing an incident involving gun violence, for example, some journalists may focus on mental health or gun regulation, while others may emphasize the discussion of gun rights. Such perspectives are called \"frames\" in communication research. We study, for the first time, the value of combining lead images and their contextual information with text to identify the frame of a given news article. We observe that using multiple modes of information(article- and image-derived features) improves prediction of news frames over any single mode of information when the images are relevant to the frames of the headlines. We also observe that frame image relevance is related to the ease of conveying frames via images, which we call frame concreteness. Additionally, we release the first multimodal news framing dataset related to gun violence in the U.S., curated and annotated by communication researchers. The dataset will allow researchers to further examine the use of multiple information modalities for studying media framing",
    "checked": true,
    "id": "4df8f1b032ca763c9dc73ec45a2d9c57571eb76f",
    "semantic_title": "detecting frames in news headlines and lead images in u.s. gun violence coverage",
    "citation_count": 20,
    "authors": [
      "Isidora Tourni",
      "Lei Guo",
      "Taufiq Husada Daryanto",
      "Fabian Zhafransyah",
      "Edward Edberg Halim",
      "Mona Jalal",
      "Boqi Chen",
      "Sha Lai",
      "Hengchang Hu",
      "Margrit Betke",
      "Prakash Ishwar",
      "Derry Tanti Wijaya"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.340": {
    "title": "Multi-task Learning to Enable Location Mention Identification in the Early Hours of a Crisis Event",
    "volume": "findings",
    "abstract": "Training a robust and reliable deep learning model requires a large amount of data. In the crisis domain, building deep learning models to identify actionable information from the huge influx of data posted by eyewitnesses of crisis events on social media, in a time-critical manner, is central for fast response and relief operations. However, building a large, annotated dataset to train deep learning models is not always feasible in a crisis situation. In this paper, we investigate a multi-task learning approach to concurrently leverage available annotated data for several related tasks from the crisis domain to improve the performance on a main task with limited annotated data. Specifically, we focus on using multi-task learning to improve the performance on the task of identifying location mentions in crisis tweets",
    "checked": true,
    "id": "7f0a0ffa78123c49beb50a405d7461b3a2ccbd61",
    "semantic_title": "multi-task learning to enable location mention identification in the early hours of a crisis event",
    "citation_count": 9,
    "authors": [
      "Sarthak Khanal",
      "Doina Caragea"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.341": {
    "title": "Graph-Based Decoding for Task Oriented Semantic Parsing",
    "volume": "findings",
    "abstract": "The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying graph-based decoding techniques developed for syntactic parsing. We compare various decoding techniques given the same pre-trained Transformer encoder on the TOP dataset, including settings where training data is limited or contains only partially-annotated examples. We find that our graph-based approach is competitive with sequence decoders on the standard setting, and offers significant improvements in data efficiency and settings where partially-annotated data is available",
    "checked": true,
    "id": "739a684580c93d56914361c772b600fbd697f7ae",
    "semantic_title": "graph-based decoding for task oriented semantic parsing",
    "citation_count": 3,
    "authors": [
      "Jeremy Cole",
      "Nanjiang Jiang",
      "Panupong Pasupat",
      "Luheng He",
      "Peter Shaw"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.342": {
    "title": "Expected Validation Performance and Estimation of a Random Variable's Maximum",
    "volume": "findings",
    "abstract": "Research in NLP is often supported by experimental results, and improved reporting of such results can lead to better understanding and more reproducible science. In this paper we analyze three statistical estimators for expected validation performance, a tool used for reporting performance (e.g., accuracy) as a function of computational budget (e.g., number of hyperparameter tuning experiments). Where previous work analyzing such estimators focused on the bias, we also examine the variance and mean squared error (MSE). In both synthetic and realistic scenarios, we evaluate three estimators and find the unbiased estimator has the highest variance, and the estimator with the smallest variance has the largest bias; the estimator with the smallest MSE strikes a balance between bias and variance, displaying a classic bias-variance tradeoff. We use expected validation performance to compare between different models, and analyze how frequently each estimator leads to drawing incorrect conclusions about which of two models performs best. We find that the two biased estimators lead to the fewest incorrect conclusions, which hints at the importance of minimizing variance and MSE",
    "checked": true,
    "id": "caab15c456ba5d8e58e6be14af9c28d03b578331",
    "semantic_title": "expected validation performance and estimation of a random variable's maximum",
    "citation_count": 9,
    "authors": [
      "Jesse Dodge",
      "Suchin Gururangan",
      "Dallas Card",
      "Roy Schwartz",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.343": {
    "title": "How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks",
    "volume": "findings",
    "abstract": "The general goal of text simplification (TS) is to reduce text complexity for human consumption. In this paper, we investigate another potential use of neural TS: assisting machines performing natural language processing (NLP) tasks. We evaluate the use of neural TS in two ways: simplifying input texts at prediction time and augmenting data to provide machines with additional information during training. We demonstrate that the latter scenario provides positive effects on machine performance on two separate datasets. In particular, the latter use of TS improves the performances of LSTM (1.82–1.98%) and SpanBERT (0.7–1.3%) extractors on TACRED, a complex, large-scale, real-world relation extraction task. Further, the same setting yields improvements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT text classifier on MNLI, a practical natural language inference dataset",
    "checked": true,
    "id": "b75131ed4f50fa556ff0ad91c970a2df38b24e5f",
    "semantic_title": "how may i help you? using neural text simplification to improve downstream nlp tasks",
    "citation_count": 9,
    "authors": [
      "Hoang Van",
      "Zheng Tang",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.344": {
    "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers",
    "volume": "findings",
    "abstract": "Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters",
    "checked": true,
    "id": "d1870f667cbd309df45a244c170d1d4ba36bac03",
    "semantic_title": "subformer: exploring weight sharing for parameter efficiency in generative transformers",
    "citation_count": 42,
    "authors": [
      "Machel Reid",
      "Edison Marrese-Taylor",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.345": {
    "title": "Leveraging Information Bottleneck for Scientific Document Summarization",
    "volume": "findings",
    "abstract": "This paper presents an unsupervised extractive approach to summarize scientific long documents based on the Information Bottleneck principle. Inspired by previous work which uses the Information Bottleneck principle for sentence compression, we extend it to document level summarization with two separate steps. In the first step, we use signal(s) as queries to retrieve the key content from the source document. Then, a pre-trained language model conducts further sentence search and edit to return the final extracted summaries. Importantly, our work can be flexibly extended to a multi-view framework by different signals. Automatic evaluation on three scientific document datasets verifies the effectiveness of the proposed framework. The further human evaluation suggests that the extracted summaries cover more content aspects than previous systems",
    "checked": true,
    "id": "882a7756d91e98a498af294c6dd7d54cbe4b490b",
    "semantic_title": "leveraging information bottleneck for scientific document summarization",
    "citation_count": 15,
    "authors": [
      "Jiaxin Ju",
      "Ming Liu",
      "Huan Yee Koh",
      "Yuan Jin",
      "Lan Du",
      "Shirui Pan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.346": {
    "title": "Reconsidering the Past: Optimizing Hidden States in Language Models",
    "volume": "findings",
    "abstract": "We present Hidden-State Optimization (HSO), a gradient-based method for improving the performance of transformer language models at inference time. Similar to dynamic evaluation (Krause et al., 2018), HSO computes the gradient of the log-probability the language model assigns to an evaluation text, but uses it to update the cached hidden states rather than the model parameters. We test HSO with pretrained Transformer-XL and GPT-2 language models, finding improvement on the WikiText-103 and PG-19 datasets in terms of perplexity, especially when evaluating a model outside of its training distribution. We also demonstrate downstream applicability by showing gains in the recently developed prompt-based few-shot evaluation setting, again with no extra parameters or training data",
    "checked": true,
    "id": "b7e94220176d9d329ee064fd4359229bf92ef360",
    "semantic_title": "reconsidering the past: optimizing hidden states in language models",
    "citation_count": 2,
    "authors": [
      "Davis Yoshida",
      "Kevin Gimpel"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.347": {
    "title": "Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots",
    "volume": "findings",
    "abstract": "Few-shot table-to-text generation is a task of composing fluent and faithful sentences to convey table content using limited data. Despite many efforts having been made towards generating impressive fluent sentences by fine-tuning powerful pre-trained language models, the faithfulness of generated content still needs to be improved. To this end, this paper proposes a novel approach Attend, Memorize and Generate (called AMG), inspired by the text generation process of humans. In particular, AMG (1) attends over the multi-granularity of context using a novel strategy based on table slot level and traditional token-by-token level attention to exploit both the table structure and natural linguistic information; (2) dynamically memorizes the table slot allocation states; and (3) generates faithful sentences according to both the context and memory allocation states. Comprehensive experiments with human evaluation on three domains (i.e., humans, songs, and books) of the Wiki dataset show that our model can generate higher qualified texts when compared with several state-of-the-art baselines, in both fluency and faithfulness",
    "checked": true,
    "id": "4145c8939d9e21739aa930e61eaf9ab0c83395da",
    "semantic_title": "attend, memorize and generate: towards faithful table-to-text generation in few shots",
    "citation_count": 9,
    "authors": [
      "Wenting Zhao",
      "Ye Liu",
      "Yao Wan",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.348": {
    "title": "ARCH: Efficient Adversarial Regularized Training with Caching",
    "volume": "findings",
    "abstract": "Adversarial regularization can improve model generalization in many natural language processing tasks. However, conventional approaches are computationally expensive since they need to generate a perturbation for each sample in each epoch. We propose a new adversarial regularization method ARCH (adversarial regularization with caching), where perturbations are generated and cached once every several epochs. As caching all the perturbations imposes memory usage concerns, we adopt a K-nearest neighbors-based strategy to tackle this issue. The strategy only requires caching a small amount of perturbations, without introducing additional training time. We evaluate our proposed method on a set of neural machine translation and natural language understanding tasks. We observe that ARCH significantly eases the computational burden (saves up to 70% of computational time in comparison with conventional approaches). More surprisingly, by reducing the variance of stochastic gradients, ARCH produces a notably better (in most of the tasks) or comparable model generalization. Our code is publicly available",
    "checked": true,
    "id": "d52976a07b1eeb97e8a031605bb02d8cd7833a63",
    "semantic_title": "arch: efficient adversarial regularized training with caching",
    "citation_count": 3,
    "authors": [
      "Simiao Zuo",
      "Chen Liang",
      "Haoming Jiang",
      "Pengcheng He",
      "Xiaodong Liu",
      "Jianfeng Gao",
      "Weizhu Chen",
      "Tuo Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.349": {
    "title": "Probing Commonsense Explanation in Dialogue Response Generation",
    "volume": "findings",
    "abstract": "Humans use commonsense reasoning (CSR) implicitly to produce natural and coherent responses in conversations. Aiming to close the gap between current response generation (RG) models and human communication abilities, we want to understand why RG models respond as they do by probing RG model's understanding of commonsense reasoning that elicits proper responses. We formalize the problem by framing commonsense as a latent variable in the RG task and using explanations for responses as textual form of commonsense. We collect 6k annotated explanations justifying responses from four dialogue datasets and ask humans to verify them and propose two probing settings to evaluate RG models' CSR capabilities. Probing results show that models fail to capture the logical relations between commonsense explanations and responses and fine-tuning on in-domain data and increasing model sizes do not lead to understanding of CSR for RG. We hope our study motivates more research in making RG models emulate the human reasoning process in pursuit of smooth human-AI communication",
    "checked": true,
    "id": "dffedd7dcacb2fab0af708b9a6a6de8424fe2fc2",
    "semantic_title": "probing commonsense explanation in dialogue response generation",
    "citation_count": 17,
    "authors": [
      "Pei Zhou",
      "Pegah Jandaghi",
      "Hyundong Cho",
      "Bill Yuchen Lin",
      "Jay Pujara",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.350": {
    "title": "NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset",
    "volume": "findings",
    "abstract": "While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the reasoning processes to get them. As a result, the state-of-the-art QA research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidence justifying the answers. Second, the QA community has contributed a lot of effort to improve the interpretability of QA models. However, they fail to explicitly show the reasoning process, such as the evidence order for reasoning and the interactions between different pieces of evidence. To address the above shortcoming, we introduce NOAHQA, a conversational and bilingual QA dataset with questions requiring numerical reasoning with compound mathematical expressions. With NOAHQA, we develop an interpretable reasoning graph as well as the appropriate evaluation metric to measure the answer quality. We evaluate the state-of-the-art QA models trained using existing QA datasets on NOAHQA and show that the best among them can only achieve 55.5 exact match scores, while the human performance is 89.7. We also present a new QA model for generating a reasoning graph where the reasoning graph metric still has a large gap compared with that of humans, eg, 28 scores",
    "checked": true,
    "id": "927efd299cffcfca3716efefcc904331b70c153e",
    "semantic_title": "noahqa: numerical reasoning with interpretable graph question answering dataset",
    "citation_count": 17,
    "authors": [
      "Qiyuan Zhang",
      "Lei Wang",
      "Sicheng Yu",
      "Shuohang Wang",
      "Yang Wang",
      "Jing Jiang",
      "Ee-Peng Lim"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.351": {
    "title": "Textual Time Travel: A Temporally Informed Approach to Theory of Mind",
    "volume": "findings",
    "abstract": "Natural language processing systems such as dialogue agents should be able to reason about other people's beliefs, intentions and desires. This capability, called theory of mind (ToM), is crucial, as it allows a model to predict and interpret the needs of users based on their mental states. A recent line of research evaluates the ToM capability of existing memory-augmented neural models through question-answering. These models perform poorly on false belief tasks where beliefs differ from reality, especially when the dataset contains distracting sentences. In this paper, we propose a new temporally informed approach for improving the ToM capability of memory-augmented neural models. Our model incorporates priors about the entities' minds and tracks their mental states as they evolve over time through an extended passage. It then responds to queries through textual time travel–i.e., by accessing the stored memory of an earlier time step. We evaluate our model on ToM datasets and find that this approach improves performance, particularly by correcting the predicted mental states to match the false belief",
    "checked": true,
    "id": "27fd24efc03fc9ec548d9f32ba93542addb7f26b",
    "semantic_title": "textual time travel: a temporally informed approach to theory of mind",
    "citation_count": 8,
    "authors": [
      "Akshatha Arodi",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.352": {
    "title": "Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding",
    "volume": "findings",
    "abstract": "Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to ‘neutralize' sensitive attributes while maintaining the semantic meaning of the original text (e.g. is the candidate qualified?). We propose a gradient-based rewriting framework, Detect and Perturb to Neutralize (DEPEN), that first detects sensitive components and masks them for regeneration, then perturbs the generation model at decoding time under a neutralizing constraint that pushes the (predicted) distribution of sensitive attributes towards a uniform distribution. Our experiments in two different scenarios show that DEPEN can regenerate fluent alternatives that are neutral in the sensitive attribute while maintaining the semantics of other attributes",
    "checked": true,
    "id": "352c0a78f008fcac02a46cf27cbe8261631f084e",
    "semantic_title": "detect and perturb: neutral rewriting of biased and sensitive text via gradient-based decoding",
    "citation_count": 28,
    "authors": [
      "Zexue He",
      "Bodhisattwa Prasad Majumder",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.353": {
    "title": "HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning",
    "volume": "findings",
    "abstract": "Taxonomies are valuable resources for many applications, but the limited coverage due to the expensive manual curation process hinders their general applicability. Prior works attempt to automatically expand existing taxonomies to improve their coverage by learning concept embeddings in Euclidean space, while taxonomies, inherently hierarchical, more naturally align with the geometric properties of a hyperbolic space. In this paper, we present HyperExpan, a taxonomy expansion algorithm that seeks to preserve the structure of a taxonomy in a more expressive hyperbolic embedding space and learn to represent concepts and their relations with a Hyperbolic Graph Neural Network (HGNN). Specifically, HyperExpan leverages position embeddings to exploit the structure of the existing taxonomies, and characterizes the concept profile information to support the inference on new concepts that are unseen during training. Experiments show that our proposed HyperExpan outperforms baseline models with representation learning in a Euclidean feature space and achieves state-of-the-art performance on the taxonomy expansion benchmarks",
    "checked": true,
    "id": "774bb1c98af120c39b89c4396ac79293039a536f",
    "semantic_title": "hyperexpan: taxonomy expansion with hyperbolic representation learning",
    "citation_count": 26,
    "authors": [
      "Mingyu Derek Ma",
      "Muhao Chen",
      "Te-Lin Wu",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.354": {
    "title": "Want To Reduce Labeling Cost? GPT-3 Can Help",
    "volume": "findings",
    "abstract": "Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 170 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance. These results present a cost-effective data labeling methodology that is generalizable to many practical applications",
    "checked": true,
    "id": "4e263b4cd6998bff2501dd143e685f413179b12d",
    "semantic_title": "want to reduce labeling cost? gpt-3 can help",
    "citation_count": 253,
    "authors": [
      "Shuohang Wang",
      "Yang Liu",
      "Yichong Xu",
      "Chenguang Zhu",
      "Michael Zeng"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.355": {
    "title": "Written Justifications are Key to Aggregate Crowdsourced Forecasts",
    "volume": "findings",
    "abstract": "This paper demonstrates that aggregating crowdsourced forecasts benefits from modeling the written justifications provided by forecasters. Our experiments show that the majority and weighted vote baselines are competitive, and that the written justifications are beneficial to call a question throughout its life except in the last quarter. We also conduct an error analysis shedding light into the characteristics that make a justification unreliable",
    "checked": true,
    "id": "0e19cfa9985d76438dd852d94b01df78ee29274a",
    "semantic_title": "written justifications are key to aggregate crowdsourced forecasts",
    "citation_count": 1,
    "authors": [
      "Saketh Kotamraju",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.356": {
    "title": "Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts",
    "volume": "findings",
    "abstract": "Substantial amounts of work are required to clean large collections of digitized books for NLP analysis, both because of the presence of errors in the scanned text and the presence of duplicate volumes in the corpora. In this paper, we consider the issue of deduplication in the presence of optical character recognition (OCR) errors. We present methods to handle these errors, evaluated on a collection of 19,347 texts from the Project Gutenberg dataset and 96,635 texts from the HathiTrust Library. We demonstrate that improvements in language models now enable the detection and correction of OCR errors without consideration of the scanning image itself. The inconsistencies found by aligning pairs of scans of the same underlying work provides training data to build models for detecting and correcting errors. We identify the canonical version for each of 17,136 repeatedly-scanned books from 58,808 scans. Finally, we investigate methods to detect and correct errors in single-copy texts. We show that on average, our method corrects over six times as many errors as it introduces. We also provide interesting analysis on the relation between scanning quality and other factors such as location and publication year",
    "checked": true,
    "id": "136a924365c57b090b35d323d0847574512bf039",
    "semantic_title": "cleaning dirty books: post-ocr processing for previously scanned texts",
    "citation_count": 6,
    "authors": [
      "Allen Kim",
      "Charuta Pethe",
      "Naoya Inoue",
      "Steve Skiena"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.357": {
    "title": "Bag of Tricks for Optimizing Transformer Efficiency",
    "volume": "findings",
    "abstract": "Improving Transformer efficiency has become increasingly attractive recently. A wide range of methods has been proposed, e.g., pruning, quantization, new architectures and etc. But these methods are either sophisticated in implementation or dependent on hardware. In this paper, we show that the efficiency of Transformer can be improved by combining some simple and hardware-agnostic methods, including tuning hyper-parameters, better design choices and training strategies. On the WMT news translation tasks, we improve the inference efficiency of a strong Transformer system by 3.80x on CPU and 2.52x on GPU",
    "checked": true,
    "id": "f852d06c518153643853419ae2e3466dc8a27af1",
    "semantic_title": "bag of tricks for optimizing transformer efficiency",
    "citation_count": 6,
    "authors": [
      "Ye Lin",
      "Yanyang Li",
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.358": {
    "title": "Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Recently, kNN-MT (Khandelwal et al., 2020) has shown the promising capability of directly incorporating the pre-trained neural machine translation (NMT) model with domain-specific token-level k-nearest-neighbor (kNN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, it heavily relies on high-quality in-domain parallel corpora, limiting its capability on unsupervised domain adaptation, where in-domain parallel corpora are scarce or nonexistent. In this paper, we propose a novel framework that directly uses in-domain monolingual sentences in the target language to construct an effective datastore for k-nearest-neighbor retrieval. To this end, we first introduce an autoencoder task based on the target language, and then insert lightweight adapters into the original NMT model to map the token-level representation of this task to the ideal representation of the translation task. Experiments on multi-domain datasets demonstrate that our proposed approach significantly improves the translation accuracy with target-side monolingual data, while achieving comparable performance with back-translation. Our implementation is open-sourced at https://github.com/zhengxxn/UDA-KNN",
    "checked": true,
    "id": "db844aae09f1ab63de6fc07b16ec584bc14cce7a",
    "semantic_title": "non-parametric unsupervised domain adaptation for neural machine translation",
    "citation_count": 25,
    "authors": [
      "Xin Zheng",
      "Zhirui Zhang",
      "Shujian Huang",
      "Boxing Chen",
      "Jun Xie",
      "Weihua Luo",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.359": {
    "title": "The Topic Confusion Task: A Novel Evaluation Scenario for Authorship Attribution",
    "volume": "findings",
    "abstract": "Authorship attribution is the problem of identifying the most plausible author of an anonymous text from a set of candidate authors. Researchers have investigated same-topic and cross-topic scenarios of authorship attribution, which differ according to whether new, unseen topics are used in the testing phase. However, neither scenario allows us to explain whether errors are caused by failure to capture authorship writing style or by the topic shift. Motivated by this, we propose the topic confusion task where we switch the author-topic configuration between the training and testing sets. This setup allows us to investigate two types of errors: one caused by the topic shift and one caused by the features' inability to capture the writing styles. We show that stylometric features with part-of-speech tags are the least susceptible to topic variations. We further show that combining them with other features leads to significantly lower topic confusion and higher attribution accuracy. Finally, we show that pretrained language models such as BERT and RoBERTa perform poorly on this task and are surpassed by simple features such as word-level n-gram",
    "checked": true,
    "id": "ab158a20229999d9d437c23fb194e05deb2801c1",
    "semantic_title": "the topic confusion task: a novel evaluation scenario for authorship attribution",
    "citation_count": 19,
    "authors": [
      "Malik Altakrori",
      "Jackie Chi Kit Cheung",
      "Benjamin C. M. Fung"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.360": {
    "title": "Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health",
    "volume": "findings",
    "abstract": "Many statistical models have high accuracy on test benchmarks, but are not explainable, struggle in low-resource scenarios, cannot be reused for multiple tasks, and cannot easily integrate domain expertise. These factors limit their use, particularly in settings such as mental health, where it is difficult to annotate datasets and model outputs have significant impact. We introduce a micromodel architecture to address these challenges. Our approach allows researchers to build interpretable representations that embed domain knowledge and provide explanations throughout the model's decision process. We demonstrate the idea on multiple mental health tasks: depression classification, PTSD classification, and suicidal risk assessment. Our systems consistently produce strong results, even in low-resource scenarios, and are more interpretable than alternative methods",
    "checked": true,
    "id": "40a37bc3de182713c81c702d2e0b687a87609863",
    "semantic_title": "micromodels for efficient, explainable, and reusable systems: a case study on mental health",
    "citation_count": 24,
    "authors": [
      "Andrew Lee",
      "Jonathan K. Kummerfeld",
      "Larry An",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.361": {
    "title": "Discovering Explanatory Sentences in Legal Case Decisions Using Pre-trained Language Models",
    "volume": "findings",
    "abstract": "Legal texts routinely use concepts that are difficult to understand. Lawyers elaborate on the meaning of such concepts by, among other things, carefully investigating how they have been used in the past. Finding text snippets that mention a particular concept in a useful way is tedious, time-consuming, and hence expensive. We assembled a data set of 26,959 sentences, coming from legal case decisions, and labeled them in terms of their usefulness for explaining selected legal concepts. Using the dataset we study the effectiveness of transformer models pre-trained on large language corpora to detect which of the sentences are useful. In light of models' predictions, we analyze various linguistic properties of the explanatory sentences as well as their relationship to the legal concept that needs to be explained. We show that the transformer-based models are capable of learning surprisingly sophisticated features and outperform the prior approaches to the task",
    "checked": true,
    "id": "7fc21a64ae2a5ed84c1cbeadf7b77a4da53ad364",
    "semantic_title": "discovering explanatory sentences in legal case decisions using pre-trained language models",
    "citation_count": 10,
    "authors": [
      "Jaromir Savelka",
      "Kevin Ashley"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.362": {
    "title": "FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning",
    "volume": "findings",
    "abstract": "Despite the success of neural dialogue systems in achieving high performance on the leader-board, they cannot meet users' requirements in practice, due to their poor reasoning skills. The underlying reason is that most neural dialogue models only capture the syntactic and semantic information, but fail to model the logical consistency between the dialogue history and the generated response. Recently, a new multi-turn dialogue reasoning task has been proposed, to facilitate dialogue reasoning research. However, this task is challenging, because there are only slight differences between the illogical response and the dialogue history. How to effectively solve this challenge is still worth exploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle this problem. Inspired by human's behavior in reading comprehension, a comparison mechanism is proposed to focus on the fine-grained differences in the representation of each response candidate. Specifically, each candidate representation is compared with the whole history to obtain a history consistency representation. Furthermore, the consistency signals between each candidate and the speaker's own history are considered to drive a model prefer a candidate that is logically consistent with the speaker's history logic. Finally, the above consistency representations are employed to output a ranking list of the candidate responses for multi-turn dialogue reasoning. Experimental results on two public dialogue datasets show that our method obtains higher ranking scores than the baseline models",
    "checked": true,
    "id": "cef565dfb89aaa30191ec359c5cf7ca2cbc129fd",
    "semantic_title": "fcm: a fine-grained comparison model for multi-turn dialogue reasoning",
    "citation_count": 7,
    "authors": [
      "Xu Wang",
      "Hainan Zhang",
      "Shuai Zhao",
      "Yanyan Zou",
      "Hongshen Chen",
      "Zhuoye Ding",
      "Bo Cheng",
      "Yanyan Lan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.363": {
    "title": "Reference-based Weak Supervision for Answer Sentence Selection using Web Data",
    "volume": "findings",
    "abstract": "Answer Sentence Selection (AS2) models are core components of efficient retrieval-based Question Answering (QA) systems. We present the Reference-based Weak Supervision (RWS), a fully automatic large-scale data pipeline that harvests high-quality weakly- supervised answer sentences from Web data, only requiring a question-reference pair as input. We evaluated the quality of the RWS-derived data by training TANDA models, which are the state of the art for AS2. Our results show that the data consistently bolsters TANDA on three different datasets. In particular, we set the new state of the art for AS2 to P@1=90.1%, and MAP=92.9%, on WikiQA. We record similar performance gains of RWS on a much larger dataset named Web-based Question Answering (WQA)",
    "checked": true,
    "id": "0f128d9e31b51eccfb073a614668483f58a8665c",
    "semantic_title": "reference-based weak supervision for answer sentence selection using web data",
    "citation_count": 1,
    "authors": [
      "Vivek Krishnamurthy",
      "Thuy Vu",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.364": {
    "title": "A Deep Decomposable Model for Disentangling Syntax and Semantics in Sentence Representation",
    "volume": "findings",
    "abstract": "Recently, disentanglement based on a generative adversarial network or a variational autoencoder has significantly advanced the performance of diverse applications in CV and NLP domains. Nevertheless, those models still work on coarse levels in the disentanglement of closely related properties, such as syntax and semantics in human languages. This paper introduces a deep decomposable model based on VAE to disentangle syntax and semantics by using total correlation penalties on KL divergences. Notably, we decompose the KL divergence term of the original VAE so that the generated latent variables can be separated in a more clear-cut and interpretable way. Experiments on benchmark datasets show that our proposed model can significantly improve the disentanglement quality between syntactic and semantic representations for semantic similarity tasks and syntactic similarity tasks",
    "checked": true,
    "id": "32bf472c8542c8f3951fc8b459ebd45635557ee5",
    "semantic_title": "a deep decomposable model for disentangling syntax and semantics in sentence representation",
    "citation_count": 3,
    "authors": [
      "Dingcheng Li",
      "Hongliang Fei",
      "Shaogang Ren",
      "Ping Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.365": {
    "title": "Improved Word Sense Disambiguation with Enhanced Sense Representations",
    "volume": "findings",
    "abstract": "Current state-of-the-art supervised word sense disambiguation (WSD) systems (such as GlossBERT and bi-encoder model) yield surprisingly good results by purely leveraging pre-trained language models and short dictionary definitions (or glosses) of the different word senses. While concise and intuitive, the sense gloss is just one of many ways to provide information about word senses. In this paper, we focus on enhancing the sense representations via incorporating synonyms, example phrases or sentences showing usage of word senses, and sense gloss of hypernyms. We show that incorporating such additional information boosts the performance on WSD. With the proposed enhancements, our system achieves an F1 score of 82.0% on the standard benchmark test dataset of the English all-words WSD task, surpassing all previous published scores on this benchmark dataset",
    "checked": true,
    "id": "2b9e21f619b158815ac92286b555777d00583960",
    "semantic_title": "improved word sense disambiguation with enhanced sense representations",
    "citation_count": 26,
    "authors": [
      "Yang Song",
      "Xin Cai Ong",
      "Hwee Tou Ng",
      "Qian Lin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.366": {
    "title": "Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables",
    "volume": "findings",
    "abstract": "Zero-shot translation, directly translating between language pairs unseen in training, is a promising capability of multilingual neural machine translation (NMT). However, it usually suffers from capturing spurious correlations between the output language and language invariant semantics due to the maximum likelihood training objective, leading to poor transfer performance on zero-shot translation. In this paper, we introduce a denoising autoencoder objective based on pivot language into traditional training objective to improve the translation accuracy on zero-shot directions. The theoretical analysis from the perspective of latent variables shows that our approach actually implicitly maximizes the probability distributions for zero-shot directions. On two benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively eliminate the spurious correlations and significantly outperforms state-of-the-art methods with a remarkable performance. Our code is available at https://github.com/Victorwz/zs-nmt-dae",
    "checked": true,
    "id": "14a4c0163731f3b492b0e055ac39579dc9c6cbe7",
    "semantic_title": "rethinking zero-shot neural machine translation: from a perspective of latent variables",
    "citation_count": 20,
    "authors": [
      "Weizhi Wang",
      "Zhirui Zhang",
      "Yichao Du",
      "Boxing Chen",
      "Jun Xie",
      "Weihua Luo"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.367": {
    "title": "FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition",
    "volume": "findings",
    "abstract": "Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input for better correction accuracy. FastCorrect 2 adopts non-autoregressive generation for fast inference, which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. However, there are some issues when handling multiple source sentences. First, it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. Thus, we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. Second, the decoder can only take one adjusted source sentence as input, while there are multiple source sentences. Thus, we develop a candidate predictor to detect the most suitable candidate for the decoder. Experiments on our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce the WER over the previous correction model with single candidate by 3.2% and 2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR error correction. FastCorrect 2 achieves better performance than the cascaded re-scoring and correction pipeline and can serve as a unified post-processing module for ASR",
    "checked": true,
    "id": "22d71d98143cbdd1f997c205a15cc393b0e9e840",
    "semantic_title": "fastcorrect 2: fast error correction on multiple candidates for automatic speech recognition",
    "citation_count": 41,
    "authors": [
      "Yichong Leng",
      "Xu Tan",
      "Rui Wang",
      "Linchen Zhu",
      "Jin Xu",
      "Wenjie Liu",
      "Linquan Liu",
      "Xiang-Yang Li",
      "Tao Qin",
      "Edward Lin",
      "Tie-Yan Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.368": {
    "title": "Task-Oriented Clustering for Dialogues",
    "volume": "findings",
    "abstract": "A reliable clustering algorithm for task-oriented dialogues can help developer analysis and define dialogue tasks efficiently. It is challenging to directly apply prior normal text clustering algorithms for task-oriented dialogues, due to the inherent differences between them, such as coreference, omission and diversity expression. In this paper, we propose a Dialogue Task Clustering Network model for task-oriented clustering. The proposed model combines context-aware utterance representations and cross-dialogue utterance cluster representations for task-oriented dialogues clustering. An iterative end-to-end training strategy is utilized for dialogue clustering and representation learning jointly. Experiments on three public datasets show that our model significantly outperform strong baselines in all metrics",
    "checked": true,
    "id": "805500bbb17523cd2853e6a636893961bf90b0f0",
    "semantic_title": "task-oriented clustering for dialogues",
    "citation_count": 3,
    "authors": [
      "Chenxu Lv",
      "Hengtong Lu",
      "Shuyu Lei",
      "Huixing Jiang",
      "Wei Wu",
      "Caixia Yuan",
      "Xiaojie Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.369": {
    "title": "Mitigating Data Poisoning in Text Classification with Differential Privacy",
    "volume": "findings",
    "abstract": "NLP models are vulnerable to data poisoning attacks. One type of attack can plant a backdoor in a model by injecting poisoned examples in training, causing the victim model to misclassify test instances which include a specific pattern. Although defences exist to counter these attacks, they are specific to an attack type or pattern. In this paper, we propose a generic defence mechanism by making the training process robust to poisoning attacks through gradient shaping methods, based on differentially private training. We show that our method is highly effective in mitigating, or even eliminating, poisoning attacks on text classification, with only a small cost in predictive accuracy",
    "checked": true,
    "id": "ae53d93235df119fb9e896e3a172b0803087921e",
    "semantic_title": "mitigating data poisoning in text classification with differential privacy",
    "citation_count": 7,
    "authors": [
      "Chang Xu",
      "Jun Wang",
      "Francisco Guzmán",
      "Benjamin Rubinstein",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.370": {
    "title": "Does Vision-and-Language Pretraining Improve Lexical Grounding?",
    "volume": "findings",
    "abstract": "Linguistic representations derived from text alone have been criticized for their lack of grounding, i.e., connecting words to their meanings in the physical world. Vision-and- Language (VL) models, trained jointly on text and image or video data, have been offered as a response to such criticisms. However, while VL pretraining has shown success on multimodal tasks such as visual question answering, it is not yet known how the internal linguistic representations themselves compare to their text-only counterparts. This paper compares the semantic representations learned via VL vs. text-only pretraining for two recent VL models using a suite of analyses (clustering, probing, and performance on a commonsense question answering task) in a language-only setting. We find that the multimodal models fail to significantly outperform the text-only variants, suggesting that future work is required if multimodal pretraining is to be pursued as a means of improving NLP in general",
    "checked": true,
    "id": "8e88cd6a52fb51a46ca5d80b75cc22ad959f0321",
    "semantic_title": "does vision-and-language pretraining improve lexical grounding?",
    "citation_count": 32,
    "authors": [
      "Tian Yun",
      "Chen Sun",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.371": {
    "title": "Character-based PCFG Induction for Modeling the Syntactic Acquisition of Morphologically Rich Languages",
    "volume": "findings",
    "abstract": "Unsupervised PCFG induction models, which build syntactic structures from raw text, can be used to evaluate the extent to which syntactic knowledge can be acquired from distributional information alone. However, many state-of-the-art PCFG induction models are word-based, meaning that they cannot directly inspect functional affixes, which may provide crucial information for syntactic acquisition in child learners. This work first introduces a neural PCFG induction model that allows a clean ablation of the influence of subword information in grammar induction. Experiments on child-directed speech demonstrate first that the incorporation of subword information results in more accurate grammars with categories that word-based induction models have difficulty finding, and second that this effect is amplified in morphologically richer languages that rely on functional affixes to express grammatical relations. A subsequent evaluation on multilingual treebanks shows that the model with subword information achieves state-of-the-art results on many languages, further supporting a distributional model of syntactic acquisition",
    "checked": true,
    "id": "b3893cc63c2ecf5de8787a45121dd466dc3e2d19",
    "semantic_title": "character-based pcfg induction for modeling the syntactic acquisition of morphologically rich languages",
    "citation_count": 5,
    "authors": [
      "Lifeng Jin",
      "Byung-Doh Oh",
      "William Schuler"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.372": {
    "title": "Block-wise Word Embedding Compression Revisited: Better Weighting and Structuring",
    "volume": "findings",
    "abstract": "Word embedding is essential for neural network models for various natural language processing tasks. Since the word embedding usually has a considerable size, in order to deploy a neural network model having it on edge devices, it should be effectively compressed. There was a study for proposing a block-wise low-rank approximation method for word embedding, called GroupReduce. Even if their structure is effective, the properties behind the concept of the block-wise word embedding compression were not sufficiently explored. Motivated by this, we improve GroupReduce in terms of word weighting and structuring. For word weighting, we propose a simple yet effective method inspired by the term frequency-inverse document frequency method and a novel differentiable method. Based on them, we construct a discriminative word embedding compression algorithm. In the experiments, we demonstrate that the proposed algorithm more effectively finds word weights than competitors in most cases. In addition, we show that the proposed algorithm can act like a framework through successful cooperation with quantization",
    "checked": true,
    "id": "c6d8b71e616f3d0d93b27c17f80b8cb5694a433a",
    "semantic_title": "block-wise word embedding compression revisited: better weighting and structuring",
    "citation_count": 4,
    "authors": [
      "Jong-Ryul Lee",
      "Yong-Ju Lee",
      "Yong-Hyuk Moon"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.373": {
    "title": "Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching",
    "volume": "findings",
    "abstract": "Code-switching (CS), a ubiquitous phenomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks – POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing char-BERT model among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. We plan to release our models and the code for all our experiments",
    "checked": true,
    "id": "11a583c33887e8fc16c0ee8c458fad92d5641dd5",
    "semantic_title": "switch point biased self-training: re-purposing pretrained models for code-switching",
    "citation_count": 6,
    "authors": [
      "Parul Chopra",
      "Sai Krishna Rallabandi",
      "Alan W Black",
      "Khyathi Raghavi Chandu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.374": {
    "title": "Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates",
    "volume": "findings",
    "abstract": "Among the most critical limitations of deep learning NLP models are their lack of interpretability, and their reliance on spurious correlations. Prior work proposed various approaches to interpreting the black-box models to unveil the spurious correlations, but the research was primarily used in human-computer interaction scenarios. It still remains underexplored whether or how such model interpretations can be used to automatically \"unlearn\" confounding features. In this work, we propose influence tuning—a procedure that leverages model interpretations to update the model parameters towards a plausible interpretation (rather than an interpretation that relies on spurious patterns in the data) in addition to learning to predict the task labels. We show that in a controlled setup, influence tuning can help deconfounding the model from spurious patterns in data, significantly outperforming baseline methods that use adversarial training",
    "checked": true,
    "id": "70c4ebdef7577a76f0dac4209690342c6d9c37e9",
    "semantic_title": "influence tuning: demoting spurious correlations via instance attribution and instance-driven updates",
    "citation_count": 30,
    "authors": [
      "Xiaochuang Han",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.375": {
    "title": "Learning Task Sampling Policy for Multitask Learning",
    "volume": "findings",
    "abstract": "It has been shown that training multi-task models with auxiliary tasks can improve the target task quality through cross-task transfer. However, the importance of each auxiliary task to the primary task is likely not known a priori. While the importance weights of auxiliary tasks can be manually tuned, it becomes practically infeasible with the number of tasks scaling up. To address this, we propose a search method that automatically assigns importance weights. We formulate it as a reinforcement learning problem and learn a task sampling schedule based on the evaluation accuracy of the multi-task model. Our empirical evaluation on XNLI and GLUE shows that our method outperforms uniform sampling and the corresponding single-task baseline",
    "checked": true,
    "id": "7c1033d7d4a23234a6b295946247cd81496198b5",
    "semantic_title": "learning task sampling policy for multitask learning",
    "citation_count": 2,
    "authors": [
      "Dhanasekar Sundararaman",
      "Henry Tsai",
      "Kuang-Huei Lee",
      "Iulia Turc",
      "Lawrence Carin"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.376": {
    "title": "Competing Independent Modules for Knowledge Integration and Optimization",
    "volume": "findings",
    "abstract": "This paper presents a neural framework of untied independent modules, used here for integrating off the shelf knowledge sources such as language models, lexica, POS information, and dependency relations. Each knowledge source is implemented as an independent component that can interact and share information with other knowledge sources. We report proof of concept experiments for several standard sentiment analysis tasks and show that the knowledge sources interoperate effectively without interference. As a second use-case, we show that the proposed framework is suitable for optimizing BERT-like language models even without the help of external knowledge sources. We cast each Transformer layer as a separate module and demonstrate performance improvements from this explicit integration of the different information encoded at the different Transformer layers",
    "checked": true,
    "id": "b683fa5b565bb9585dab88a2853761b77cc6eee3",
    "semantic_title": "competing independent modules for knowledge integration and optimization",
    "citation_count": 2,
    "authors": [
      "Parsa Bagherzadeh",
      "Sabine Bergler"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.377": {
    "title": "An Exploratory Study on Long Dialogue Summarization: What Works and What's Next",
    "volume": "findings",
    "abstract": "Dialogue summarization helps readers capture salient information from long conversations in meetings, interviews, and TV series. However, real-world dialogues pose a great challenge to current summarization models, as the dialogue length typically exceeds the input limits imposed by recent transformer-based pre-trained models, and the interactive nature of dialogues makes relevant information more context-dependent and sparsely distributed than news articles. In this work, we perform a comprehensive study on long dialogue summarization by investigating three strategies to deal with the lengthy input problem and locate relevant information: (1) extended transformer models such as Longformer, (2) retrieve-then-summarize pipeline models with several dialogue utterance retrieval methods, and (3) hierarchical dialogue encoding models such as HMNet. Our experimental results on three long dialogue datasets (QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline models yield the best performance. We also demonstrate that the summary quality can be further improved with a stronger retrieval model and pretraining on proper external summarization datasets",
    "checked": true,
    "id": "f8c6d9ed61fdf04cd390dce017a817152cf4d580",
    "semantic_title": "an exploratory study on long dialogue summarization: what works and what's next",
    "citation_count": 57,
    "authors": [
      "Yusen Zhang",
      "Ansong Ni",
      "Tao Yu",
      "Rui Zhang",
      "Chenguang Zhu",
      "Budhaditya Deb",
      "Asli Celikyilmaz",
      "Ahmed Hassan Awadallah",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.378": {
    "title": "Improving Text Auto-Completion with Next Phrase Prediction",
    "volume": "findings",
    "abstract": "Language models such as GPT-2 have performed well on constructing syntactically sound sentences for text auto-completion tasks. However, such models often require considerable training effort to adapt to specific writing domains (e.g., medical). In this paper, we propose an intermediate training strategy to enhance pre-trained language models' performance in the text auto-completion task and fastly adapt them to specific domains. Our strategy includes a novel self-supervised training objective called Next Phrase Prediction (NPP), which encourages a language model to complete the partial query with enriched phrases and eventually improve the model's text auto-completion performance. Preliminary experiments have shown that our approach is able to outperform the baselines in auto-completion for email and academic-writing domains",
    "checked": true,
    "id": "c13b912502ba5480ec0f6eb1b46d611da5eb4aba",
    "semantic_title": "improving text auto-completion with next phrase prediction",
    "citation_count": 4,
    "authors": [
      "Dong-Ho Lee",
      "Zhiqiang Hu",
      "Roy Ka-Wei Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.379": {
    "title": "MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets",
    "volume": "findings",
    "abstract": "Internet memes have become powerful means to transmit political, psychological, and socio-cultural ideas. Although memes are typically humorous, recent days have witnessed an escalation of harmful memes used for trolling, cyberbullying, and abuse. Detecting such memes is challenging as they can be highly satirical and cryptic. Moreover, while previous work has focused on specific aspects of memes such as hate speech and propaganda, there has been little work on harm in general. Here, we aim to bridge this gap. In particular, we focus on two tasks: (i)detecting harmful memes, and (ii) identifying the social entities they target. We further extend the recently released HarMeme dataset, which covered COVID-19, with additional memes and a new topic: US politics. To solve these tasks, we propose MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a novel multimodal deep neural network that uses global and local perspectives to detect harmful memes. MOMENTA systematically analyzes the local and the global perspective of the input meme (in both modalities) and relates it to the background context. MOMENTA is interpretable and generalizable, and our experiments show that it outperforms several strong rivaling approaches",
    "checked": true,
    "id": "60f98a6dffa8f4b4ffc9ae457953d9ce3ef2f906",
    "semantic_title": "momenta: a multimodal framework for detecting harmful memes and their targets",
    "citation_count": 100,
    "authors": [
      "Shraman Pramanick",
      "Shivam Sharma",
      "Dimitar Dimitrov",
      "Md. Shad Akhtar",
      "Preslav Nakov",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.380": {
    "title": "NICE: Neural Image Commenting with Empathy",
    "volume": "findings",
    "abstract": "Emotion and empathy are examples of human qualities lacking in many human-machine interactions. The goal of our work is to generate engaging dialogue grounded in a user-shared image with increased emotion and empathy while minimizing socially inappropriate or offensive outputs. We release the Neural Image Commenting with Empathy (NICE) dataset consisting of almost two million images and the corresponding human-generated comments, a set of human annotations, and baseline performance on a range of models. In-stead of relying on manually labeled emotions, we also use automatically generated linguistic representations as a source of weakly supervised labels. Based on these annotations, we define two different tasks for the NICE dataset. Then, we propose a novel pre-training model - Modeling Affect Generation for Image Comments (MAGIC) - which aims to generate comments for images, conditioned on linguistic representations that capture style and affect, and to help generate more empathetic, emotional, engaging and socially appropriate comments. Using this model we achieve state-of-the-art performance on one of our NICE tasks. The experiments show that the approach can generate more human-like and engaging image comments",
    "checked": true,
    "id": "4c9bff707cd95d284356be0ef534d50ff0df08fa",
    "semantic_title": "nice: neural image commenting with empathy",
    "citation_count": 7,
    "authors": [
      "Kezhen Chen",
      "Qiuyuan Huang",
      "Daniel McDuff",
      "Xiang Gao",
      "Hamid Palangi",
      "Jianfeng Wang",
      "Kenneth Forbus",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.381": {
    "title": "HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks",
    "volume": "findings",
    "abstract": "Jupyter notebook allows data scientists to write machine learning code together with its documentation in cells. In this paper, we propose a new task of code documentation generation (CDG) for computational notebooks. In contrast to the previous CDG tasks which focus on generating documentation for single code snippets, in a computational notebook, one documentation in a markdown cell often corresponds to multiple code cells, and these code cells have an inherent structure. We proposed a new model (HAConvGNN) that uses a hierarchical attention mechanism to consider the relevant code cells and the relevant code tokens information when generating the documentation. Tested on a new corpus constructed from well-documented Kaggle notebooks, we show that our model outperforms other baseline models",
    "checked": true,
    "id": "6e2b1038682cd116b2e38bec19b5721196c41eea",
    "semantic_title": "haconvgnn: hierarchical attention based convolutional graph neural network for code documentation generation in jupyter notebooks",
    "citation_count": 19,
    "authors": [
      "Xuye Liu",
      "Dakuo Wang",
      "April Wang",
      "Yufang Hou",
      "Lingfei Wu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.382": {
    "title": "A multilabel approach to morphosyntactic probing",
    "volume": "findings",
    "abstract": "We propose using a multilabel probing task to assess the morphosyntactic representations of multilingual word embeddings. This tweak on canonical probing makes it easy to explore morphosyntactic representations, both holistically and at the level of individual features (e.g., gender, number, case), and leads more naturally to the study of how language models handle co-occurring features (e.g., agreement phenomena). We demonstrate this task with multilingual BERT (Devlin et al., 2018), training probes for seven typologically diverse languages: Afrikaans, Croatian, Finnish, Hebrew, Korean, Spanish, and Turkish. Through this simple but robust paradigm, we verify that multilingual BERT renders many morphosyntactic features simultaneously extractable. We further evaluate the probes on six held-out languages: Arabic, Chinese, Marathi, Slovenian, Tagalog, and Yoruba. This zero-shot style of probing has the added benefit of revealing which cross-linguistic properties a language model recognizes as being shared by multiple languages",
    "checked": true,
    "id": "b4ab9b198ed140928751143af68c6b45adf396ca",
    "semantic_title": "a multilabel approach to morphosyntactic probing",
    "citation_count": 8,
    "authors": [
      "Naomi Shapiro",
      "Amandalynne Paullada",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.383": {
    "title": "Co-Teaching Student-Model through Submission Results of Shared Task",
    "volume": "findings",
    "abstract": "Shared tasks have a long history and have become the mainstream of NLP research. Most of the shared tasks require participants to submit only system outputs and descriptions. It is uncommon for the shared task to request submission of the system itself because of the license issues and implementation differences. Therefore, many systems are abandoned without being used in real applications or contributing to better systems. In this research, we propose a scheme to utilize all those systems which participated in the shared tasks. We use all participated system outputs as task teachers in this scheme and develop a new model as a student aiming to learn the characteristics of each system. We call this scheme \"Co-Teaching.\" This scheme creates a unified system that performs better than the task's single best system. It only requires the system outputs, and slightly extra effort is needed for the participants and organizers. We apply this scheme to the \"SHINRA2019-JP\" shared task, which has nine participants with various output accuracies, confirming that the unified system outperforms the best system. Moreover, the code used in our experiments has been released",
    "checked": true,
    "id": "bc73e1df5f53e18c8f44c56f20a7f06e03b46d38",
    "semantic_title": "co-teaching student-model through submission results of shared task",
    "citation_count": 1,
    "authors": [
      "Kouta Nakayama",
      "Shuhei Kurita",
      "Akio Kobayashi",
      "Yukino Baba",
      "Satoshi Sekine"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.384": {
    "title": "KLMo: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships",
    "volume": "findings",
    "abstract": "Interactions between entities in knowledge graph (KG) provide rich knowledge for language representation learning. However, existing knowledge-enhanced pretrained language models (PLMs) only focus on entity information and ignore the fine-grained relationships between entities. In this work, we propose to incorporate KG (including both entities and relations) into the language learning process to obtain KG-enhanced pretrained Language Model, namely KLMo. Specifically, a novel knowledge aggregator is designed to explicitly model the interaction between entity spans in text and all entities and relations in a contextual KG. An relation prediction objective is utilized to incorporate relation information by distant supervision. An entity linking objective is further utilized to link entity spans in text to entities in KG. In this way, the structured knowledge can be effectively integrated into language representations. Experimental results demonstrate that KLMo achieves great improvements on several knowledge-driven tasks, such as entity typing and relation classification, comparing with the state-of-the-art knowledge-enhanced PLMs",
    "checked": true,
    "id": "e7d4fd44400391888226b3f5d8acac0ab2106bac",
    "semantic_title": "klmo: knowledge graph enhanced pretrained language model with fine-grained relationships",
    "citation_count": 18,
    "authors": [
      "Lei He",
      "Suncong Zheng",
      "Tao Yang",
      "Feng Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.385": {
    "title": "Do We Know What We Don't Know? Studying Unanswerable Questions beyond SQuAD 2.0",
    "volume": "findings",
    "abstract": "Understanding when a text snippet does not provide a sought after information is an essential part of natural language utnderstanding. Recent work (SQuAD 2.0; Rajpurkar et al., 2018) has attempted to make some progress in this direction by enriching the SQuAD dataset for the Extractive QA task with unanswerable questions. However, as we show, the performance of a top system trained on SQuAD 2.0 drops considerably in out-of-domain scenarios, limiting its use in practical situations. In order to study this we build an out-of-domain corpus, focusing on simple event-based questions and distinguish between two types of IDK questions: competitive questions, where the context includes an entity of the same type as the expected answer, and simpler, non-competitive questions where there is no entity of the same type in the context. We find that SQuAD 2.0-based models fail even in the case of the simpler questions. We then analyze the similarities and differences between the IDK phenomenon in Extractive QA and the Recognizing Textual Entailments task (RTE; Dagan et al., 2013) and investigate the extent to which the latter can be used to improve the performance",
    "checked": true,
    "id": "b61c9799f10e4de9cd222dfd8e423bbd950a7c44",
    "semantic_title": "do we know what we don't know? studying unanswerable questions beyond squad 2.0",
    "citation_count": 17,
    "authors": [
      "Elior Sulem",
      "Jamaal Hay",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.386": {
    "title": "Glyph Enhanced Chinese Character Pre-Training for Lexical Sememe Prediction",
    "volume": "findings",
    "abstract": "Sememes are defined as the atomic units to describe the semantic meaning of concepts. Due to the difficulty of manually annotating sememes and the inconsistency of annotations between experts, the lexical sememe prediction task has been proposed. However, previous methods heavily rely on word or character embeddings, and ignore the fine-grained information. In this paper, we propose a novel pre-training method which is designed to better incorporate the internal information of Chinese character. The Glyph enhanced Chinese Character representation (GCC) is used to assist sememe prediction. We experiment and evaluate our model on HowNet, which is a famous sememe knowledge base. The experimental results show that our method outperforms existing non-external information models",
    "checked": true,
    "id": "3424cf291603b1c9c9507746b788522ef835110d",
    "semantic_title": "glyph enhanced chinese character pre-training for lexical sememe prediction",
    "citation_count": 10,
    "authors": [
      "Boer Lyu",
      "Lu Chen",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.387": {
    "title": "Active Learning for Rumor Identification on Social Media",
    "volume": "findings",
    "abstract": "Social media has emerged as a key channel for seeking information. Online users spend several hours reading, posting, and searching for news on microblogging platforms daily. However, this could act as a double-edged sword especially when not all information online is reliable. Moreover, the inherently unmoderated nature of social media renders identifying unverified information ever more challenging. Most of the existing approaches for rumor tracking are not scalable because of their dependency on a significant amount of labeled data. In this work, we investigate this problem from different angles. We design an Active-Transfer Learning (ATL) strategy to identify rumors with a limited amount of annotated data. We go beyond that and investigate the impact of leveraging various machine learning approaches in addition to different contextual representations. We discuss the impact of multiple classifiers on a limited amount of annotated data followed by an interactive approach to gradually update the models by adding the least certain samples (LCS) from the pool of unlabeled data. Our proposed Active Learning (AL) strategy achieves faster convergence in terms of the F-score while requiring fewer annotated samples (42% of the whole dataset for the best model)",
    "checked": true,
    "id": "284a556ec182eb404cdb8ae06942cc969050ad40",
    "semantic_title": "active learning for rumor identification on social media",
    "citation_count": 16,
    "authors": [
      "Parsa Farinneya",
      "Mohammad Mahdi Abdollah Pour",
      "Sardar Hamidian",
      "Mona Diab"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.388": {
    "title": "Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text",
    "volume": "findings",
    "abstract": "Named entity disambiguation (NED), which involves mapping textual mentions to structured entities, is particularly challenging in the medical domain due to the presence of rare entities. Existing approaches are limited by the presence of coarse-grained structural resources in biomedical knowledge bases as well as the use of training datasets that provide low coverage over uncommon resources. In this work, we address these issues by proposing a cross-domain data integration method that transfers structural knowledge from a general text knowledge base to the medical domain. We utilize our integration scheme to augment structural resources and generate a large biomedical NED dataset for pretraining. Our pretrained model with injected structural knowledge achieves state-of-the-art performance on two benchmark medical NED datasets: MedMentions and BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57 accuracy points",
    "checked": true,
    "id": "fdcffd9d14ed1a1d9ab39eed8250f58f7839919d",
    "semantic_title": "cross-domain data integration for named entity disambiguation in biomedical text",
    "citation_count": 27,
    "authors": [
      "Maya Varma",
      "Laurel Orr",
      "Sen Wu",
      "Megan Leszczynski",
      "Xiao Ling",
      "Christopher Ré"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.389": {
    "title": "Self-Training using Rules of Grammar for Few-Shot NLU",
    "volume": "findings",
    "abstract": "We tackle the problem of self-training networks for NLU in low-resource environment—few labeled data and lots of unlabeled data. The effectiveness of self-training is a result of increasing the amount of training data while training. Yet it becomes less effective in low-resource settings due to unreliable labels predicted by the teacher model on unlabeled data. Rules of grammar, which describe the grammatical structure of data, have been used in NLU for better explainability. We propose to use rules of grammar in self-training as a more reliable pseudo-labeling mechanism, especially when there are few labeled data. We design an effective algorithm that constructs and expands rules of grammar without human involvement. Then we integrate the constructed rules as a pseudo-labeling mechanism into self-training. There are two possible scenarios regarding data distribution: it is unknown or known in prior to training. We empirically demonstrate that our approach substantially outperforms the state-of-the-art methods in three benchmark datasets for both scenarios",
    "checked": true,
    "id": "ab2fbc09b20c835bec76d5d3134277f98a473622",
    "semantic_title": "self-training using rules of grammar for few-shot nlu",
    "citation_count": 3,
    "authors": [
      "Joonghyuk Hahn",
      "Hyunjoon Cheon",
      "Kyuyeol Han",
      "Cheongjae Lee",
      "Junseok Kim",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.390": {
    "title": "Aspect-based Sentiment Analysis in Question Answering Forums",
    "volume": "findings",
    "abstract": "Aspect-based sentiment analysis (ABSA) typically focuses on extracting aspects and predicting their sentiments on individual sentences such as customer reviews. Recently, another kind of opinion sharing platform, namely question answering (QA) forum, has received increasing popularity, which accumulates a large number of user opinions towards various aspects. This motivates us to investigate the task of ABSA on QA forums (ABSA-QA), aiming to jointly detect the discussed aspects and their sentiment polarities for a given QA pair. Unlike review sentences, a QA pair is composed of two parallel sentences, which requires interaction modeling to align the aspect mentioned in the question and the associated opinion clues in the answer. To this end, we propose a model with a specific design of cross-sentence aspect-opinion interaction modeling to address this task. The proposed method is evaluated on three real-world datasets and the results show that our model outperforms several strong baselines adopted from related state-of-the-art models",
    "checked": true,
    "id": "04729df1b5d0d9cf2fec0baf3734f2d302370818",
    "semantic_title": "aspect-based sentiment analysis in question answering forums",
    "citation_count": 18,
    "authors": [
      "Wenxuan Zhang",
      "Yang Deng",
      "Xin Li",
      "Lidong Bing",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.391": {
    "title": "ForumSum: A Multi-Speaker Conversation Summarization Dataset",
    "volume": "findings",
    "abstract": "Abstractive summarization quality had large improvements since recent language pretraining techniques. However, currently there is a lack of datasets for the growing needs of conversation summarization applications. Thus we collected ForumSum, a diverse and high-quality conversation summarization dataset with human written summaries. The conversations in ForumSum dataset are collected from a wide variety of internet forums. To make the dataset easily expandable, we also release the process of dataset creation. Our experiments show that models trained on ForumSum have better zero-shot and few-shot transferability to other datasets than the existing large chat summarization dataset SAMSum. We also show that using a conversational corpus for pre-training improves the quality of the chat summarization model",
    "checked": true,
    "id": "22e7424448e17e3357e03db73ddf7ce2c39b48f6",
    "semantic_title": "forumsum: a multi-speaker conversation summarization dataset",
    "citation_count": 22,
    "authors": [
      "Misha Khalman",
      "Yao Zhao",
      "Mohammad Saleh"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.392": {
    "title": "Question Answering over Electronic Devices: A New Benchmark Dataset and a Multi-Task Learning based QA Framework",
    "volume": "findings",
    "abstract": "Answering questions asked from instructional corpora such as E-manuals, recipe books, etc., has been far less studied than open-domain factoid context-based question answering. This can be primarily attributed to the absence of standard benchmark datasets. In this paper, we meticulously create a large amount of data connected with E-manuals and develop a suitable algorithm to exploit it. We collect E-Manual Corpus, a huge corpus of 307,957 E-manuals, and pretrain RoBERTa on this large corpus. We create various benchmark QA datasets which include question answer pairs curated by experts based upon two E-manuals, real user questions from Community Question Answering Forum pertaining to E-manuals etc. We introduce EMQAP (E-Manual Question Answering Pipeline) that answers questions pertaining to electronics devices. Built upon the pretrained RoBERTa, it harbors a supervised multi-task learning framework which efficiently performs the dual tasks of identifying the section in the E-manual where the answer can be found and the exact answer span within that section. For E-Manual annotated question-answer pairs, we show an improvement of about 40% in ROUGE-L F1 scores over most competitive baseline. We perform a detailed ablation study and establish the versatility of EMQAP across different circumstances. The code and datasets are shared at https://github.com/abhi1nandy2/EMNLP-2021-Findings, and the corresponding project website is https://sites.google.com/view/emanualqa/home",
    "checked": true,
    "id": "404aba86aceced0a4110d624c8b0aa112c666687",
    "semantic_title": "question answering over electronic devices: a new benchmark dataset and a multi-task learning based qa framework",
    "citation_count": 18,
    "authors": [
      "Abhilash Nandy",
      "Soumya Sharma",
      "Shubham Maddhashiya",
      "Kapil Sachdeva",
      "Pawan Goyal",
      "NIloy Ganguly"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.393": {
    "title": "Comprehensive Punctuation Restoration for English and Polish",
    "volume": "findings",
    "abstract": "Punctuation restoration is a fundamental requirement for the readability of text derived from Automatic Speech Recognition (ASR) systems. Most contemporary solutions are limited to predicting only a few of the most frequently occurring marks, such as periods, commas, and question marks - and only one per word. However, in written language, we deal with a much larger number of punctuation characters (such as parentheses, hyphens, etc.), and their combinations (like parenthesis followed by a dot). Such comprehensive punctuation cannot always be unambiguously reduced to a basic set of the most frequently occurring marks. In this work, we evaluate several methods in the comprehensive punctuation reconstruction task. We conduct experiments on parallel corpora of two different languages, English and Polish - languages with a relatively simple and complex morphology, respectively. We also investigate the influence of building a model on comprehensive punctuation on the quality of the basic punctuation restoration task",
    "checked": true,
    "id": "7f856418e2207c87af1b738a1134719a19128533",
    "semantic_title": "comprehensive punctuation restoration for english and polish",
    "citation_count": 2,
    "authors": [
      "Michał Pogoda",
      "Tomasz Walkowiak"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.394": {
    "title": "Syntactically Diverse Adversarial Network for Knowledge-Grounded Conversation Generation",
    "volume": "findings",
    "abstract": "Generative conversation systems tend to produce meaningless and generic responses, which significantly reduce the user experience. In order to generate informative and diverse responses, recent studies proposed to fuse knowledge to improve informativeness and adopt latent variables to enhance the diversity. However, utilizing latent variables will lead to the inaccuracy of knowledge in the responses, and the dissemination of wrong knowledge will mislead the communicators. To address this problem, we propose a Syntactically Diverse Adversarial Network (SDAN) for knowledge-grounded conversation model. SDAN contains an adversarial hierarchical semantic network to keep the semantic coherence, a knowledge-aware network to attend more related knowledge for improving the informativeness and a syntactic latent variable network to generate syntactically diverse responses. Additionally, in order to increase the controllability of syntax, we adopt adversarial learning to decouple semantic and syntactic representations. Experimental results show that our model can not only generate syntactically diverse and knowledge-accurate responses but also significantly achieve the balance between improving the syntactic diversity and maintaining the knowledge accuracy",
    "checked": true,
    "id": "7d0e900a714ca4c82f12be974f9379e88f6324de",
    "semantic_title": "syntactically diverse adversarial network for knowledge-grounded conversation generation",
    "citation_count": 3,
    "authors": [
      "Fuwei Cui",
      "Hui Di",
      "Hongjie Ren",
      "Kazushige Ouchi",
      "Ze Liu",
      "Jinan Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.395": {
    "title": "QACE: Asking Questions to Evaluate an Image Caption",
    "volume": "findings",
    "abstract": "In this paper we propose QACE, a new metric based on Question Answering for Caption Evaluation to evaluate image captioning based on Question Generation(QG) and Question Answering(QA) systems. QACE generates questions on the evaluated caption and check its content by asking the questions on either the reference caption or the source image. We first develop QACE_Ref that compares the answers of the evaluated caption to its reference, and report competitive results with the state-of-the-art metrics. To go further, we propose QACE_Img, that asks the questions directly on the image, instead of reference. A Visual-QA system is necessary for QACE_Img. Unfortunately, the standard VQA models are actually framed a classification among only few thousands categories. Instead, we propose Visual-T5, an abstractive VQA system. The resulting metric, QACE_Img is multi-modal, reference-less and explainable. Our experiments show that QACE_Img compares favorably w.r.t. other reference-less metrics",
    "checked": true,
    "id": "f5a76442659066434b1bdf480cf11f4f549411ab",
    "semantic_title": "qace: asking questions to evaluate an image caption",
    "citation_count": 19,
    "authors": [
      "Hwanhee Lee",
      "Thomas Scialom",
      "Seunghyun Yoon",
      "Franck Dernoncourt",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.396": {
    "title": "Secoco: Self-Correcting Encoding for Neural Machine Translation",
    "volume": "findings",
    "abstract": "This paper presents Self-correcting Encoding (Secoco), a framework that effectively deals with noisy input for robust neural machine translation by introducing self-correcting predictors. Different from previous robust approaches, Secoco enables NMT to explicitly correct noisy inputs and delete specific errors simultaneously with the translation decoding process. Secoco is able to achieve significant improvements over strong baselines on two real-world test sets and a benchmark WMT dataset with good interpretability. We will make our code and dataset publicly available soon",
    "checked": true,
    "id": "0e90847c381b4321123fa65bbfeb363406be04f3",
    "semantic_title": "secoco: self-correcting encoding for neural machine translation",
    "citation_count": 4,
    "authors": [
      "Tao Wang",
      "Chengqi Zhao",
      "Mingxuan Wang",
      "Lei Li",
      "Hang Li",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.397": {
    "title": "Simple or Complex? Complexity-controllable Question Generation with Soft Templates and Deep Mixture of Experts Model",
    "volume": "findings",
    "abstract": "The ability to generate natural-language questions with controlled complexity levels is highly desirable as it further expands the applicability of question generation. In this paper, we propose an end-to-end neural complexity-controllable question generation model, which incorporates a mixture of experts (MoE) as the selector of soft templates to improve the accuracy of complexity control and the quality of generated questions. The soft templates capture question similarity while avoiding the expensive construction of actual templates. Our method introduces a novel, cross-domain complexity estimator to assess the complexity of a question, taking into account the passage, the question, the answer and their interactions. The experimental results on two benchmark QA datasets demonstrate that our QG model is superior to state-of-the-art methods in both automatic and manual evaluation. Moreover, our complexity estimator is significantly more accurate than the baselines in both in-domain and out-domain settings",
    "checked": true,
    "id": "9d5113419702e7c228b9de5f8308278b3313e037",
    "semantic_title": "simple or complex? complexity-controllable question generation with soft templates and deep mixture of experts model",
    "citation_count": 14,
    "authors": [
      "Sheng Bi",
      "Xiya Cheng",
      "Yuan-Fang Li",
      "Lizhen Qu",
      "Shirong Shen",
      "Guilin Qi",
      "Lu Pan",
      "Yinlin Jiang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.398": {
    "title": "Predicting Anti-Asian Hateful Users on Twitter during COVID-19",
    "volume": "findings",
    "abstract": "We investigate predictors of anti-Asian hate among Twitter users throughout COVID-19. With the rise of xenophobia and polarization that has accompanied widespread social media usage in many nations, online hate has become a major social issue, attracting many researchers. Here, we apply natural language processing techniques to characterize social media users who began to post anti-Asian hate messages during COVID-19. We compare two user groups—those who posted anti-Asian slurs and those who did not—with respect to a rich set of features measured with data prior to COVID-19 and show that it is possible to predict who later publicly posted anti-Asian slurs. Our analysis of predictive features underlines the potential impact of news media and information sources that report on online hate and calls for further investigation into the role of polarized communication networks and news media",
    "checked": true,
    "id": "c3b477474a36bd10731a5c5fbe678a1b2dc48cf3",
    "semantic_title": "predicting anti-asian hateful users on twitter during covid-19",
    "citation_count": 28,
    "authors": [
      "Jisun An",
      "Haewoon Kwak",
      "Claire Seungeun Lee",
      "Bogang Jun",
      "Yong-Yeol Ahn"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.399": {
    "title": "Fine-grained Typing of Emerging Entities in Microblogs",
    "volume": "findings",
    "abstract": "Analyzing microblogs where we post what we experience enables us to perform various applications such as social-trend analysis and entity recommendation. To track emerging trends in a variety of areas, we want to categorize information on emerging entities (e.g., Avatar 2) in microblog posts according to their types (e.g., Film). We thus introduce a new entity typing task that assigns a fine-grained type to each emerging entity when a burst of posts containing that entity is first observed in a microblog. The challenge is to perform typing from noisy microblog posts without relying on prior knowledge of the target entity. To tackle this task, we build large-scale Twitter datasets for English and Japanese using time-sensitive distant supervision. We then propose a modular neural typing model that encodes not only the entity and its contexts but also meta information in multiple posts. To type ‘homographic' emerging entities (e.g., ‘Go' means an emerging programming language and a classic board game), which contexts are noisy, we devise a context selector that finds related contexts of the target entity. Experiments on the Twitter datasets confirm the effectiveness of our typing model and the context selector",
    "checked": true,
    "id": "2ea1f7c05dab3562de251c7f86544575ce9ce5e5",
    "semantic_title": "fine-grained typing of emerging entities in microblogs",
    "citation_count": 1,
    "authors": [
      "Satoshi Akasaki",
      "Naoki Yoshinaga",
      "Masashi Toyoda"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.400": {
    "title": "Data-Efficient Language Shaped Few-shot Image Classification",
    "volume": "findings",
    "abstract": "Many existing works have demonstrated that language is a helpful guider for image understanding by neural networks. We focus on a language-shaped learning problem in a few-shot setting, i.e., using language to improve few-shot image classification when language descriptions are only available during training. We propose a data-efficient method that can make the best usage of the few-shot images and the language available only in training. Experimental results on dataset ShapeWorld and Birds show that our method outperforms other state-of-the-art baselines in language-shaped few-shot learning area, especially when training data is more severely limited. Therefore, we call our approach data-efficient language-shaped learning (DF-LSL)",
    "checked": true,
    "id": "c8be1740bd2f7e2635db6c5ab774108183eb4659",
    "semantic_title": "data-efficient language shaped few-shot image classification",
    "citation_count": 0,
    "authors": [
      "Zhenwen Liang",
      "Xiangliang Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.401": {
    "title": "Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Quality Estimation (QE) plays an essential role in applications of Machine Translation (MT). Traditionally, a QE system accepts the original source text and translation from a black-box MT system as input. Recently, a few studies indicate that as a by-product of translation, QE benefits from the model and training data's information of the MT system where the translations come from, and it is called the \"glass-box QE\". In this paper, we extend the definition of \"glass-box QE\" generally to uncertainty quantification with both \"black-box\" and \"glass-box\" approaches and design several features deduced from them to blaze a new trial in improving QE's performance. We propose a framework to fuse the feature engineering of uncertainty quantification into a pre-trained cross-lingual language model to predict the translation quality. Experiment results show that our method achieves state-of-the-art performances on the datasets of WMT 2020 QE shared task",
    "checked": true,
    "id": "014ffd86ffdbfe84df070aa76e33ccca2c80bf48",
    "semantic_title": "beyond glass-box features: uncertainty quantification enhanced quality estimation for neural machine translation",
    "citation_count": 6,
    "authors": [
      "Ke Wang",
      "Yangbin Shi",
      "Jiayi Wang",
      "Yuqi Zhang",
      "Yu Zhao",
      "Xiaolin Zheng"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.402": {
    "title": "Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech",
    "volume": "findings",
    "abstract": "Automatic hate speech detection is hampered by the scarcity of labeled datasetd, leading to poor generalization. We employ pretrained language models (LMs) to alleviate this data bottleneck. We utilize the GPT LM for generating large amounts of synthetic hate speech sequences from available labeled examples, and leverage the generated data in fine-tuning large pretrained LMs on hate detection. An empirical study using the models of BERT, RoBERTa and ALBERT, shows that this approach improves generalization significantly and consistently within and across data distributions. In fact, we find that generating relevant labeled hate speech sequences is preferable to using out-of-domain, and sometimes also within-domain, human-labeled examples",
    "checked": true,
    "id": "14fa6eed1d77cfd8e40c9bf344cd5f0685f394ab",
    "semantic_title": "fight fire with fire: fine-tuning hate detectors using large samples of generated hate speech",
    "citation_count": 41,
    "authors": [
      "Tomer Wullach",
      "Amir Adler",
      "Einat Minkov"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.403": {
    "title": "AutoEQA: Auto-Encoding Questions for Extractive Question Answering",
    "volume": "findings",
    "abstract": "There has been a significant progress in the field of Extractive Question Answering (EQA) in the recent years. However, most of them are reliant on annotations of answer-spans in the corresponding passages. In this work, we address the problem of EQA when no annotations are present for the answer span, i.e., when the dataset contains only questions and corresponding passages. Our method is based on auto-encoding of the question that performs a question answering task during encoding and a question generation task during decoding. We show that our method performs well in a zero-shot setting and can provide an additional loss to boost performance for EQA",
    "checked": true,
    "id": "2f4fd8935077922ec5fde875509eea01b2707a84",
    "semantic_title": "autoeqa: auto-encoding questions for extractive question answering",
    "citation_count": 4,
    "authors": [
      "Stalin Varanasi",
      "Saadullah Amin",
      "Guenter Neumann"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.404": {
    "title": "A Multi-label Multi-hop Relation Detection Model based on Relation-aware Sequence Generation",
    "volume": "findings",
    "abstract": "Multi-hop relation detection in Knowledge Base Question Answering (KBQA) aims at retrieving the relation path starting from the topic entity to the answer node based on a given question, where the relation path may comprise multiple relations. Most of the existing methods treat it as a single-label learning problem while ignoring the fact that for some complex questions, there exist multiple correct relation paths in knowledge bases. Therefore, in this paper, multi-hop relation detection is considered as a multi-label learning problem. However, performing multi-label multi-hop relation detection is challenging since the numbers of both the labels and the hops are unknown. To tackle this challenge, multi-label multi-hop relation detection is formulated as a sequence generation task. A relation-aware sequence relation generation model is proposed to solve the problem in an end-to-end manner. Experimental results show the effectiveness of the proposed method for relation detection and KBQA",
    "checked": true,
    "id": "27800e7ad275d89f227e53d612359da4c1913034",
    "semantic_title": "a multi-label multi-hop relation detection model based on relation-aware sequence generation",
    "citation_count": 4,
    "authors": [
      "Linhai Zhang",
      "Deyu Zhou",
      "Chao Lin",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.405": {
    "title": "Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques",
    "volume": "findings",
    "abstract": "Existing techniques for mitigating dataset bias often leverage a biased model to identify biased instances. The role of these biased instances is then reduced during the training of the main model to enhance its robustness to out-of-distribution data. A common core assumption of these techniques is that the main model handles biased instances similarly to the biased model, in that it will resort to biases whenever available. In this paper, we show that this assumption does not hold in general. We carry out a critical investigation on two well-known datasets in the domain, MNLI and FEVER, along with two biased instance detection methods, partial-input and limited-capacity models. Our experiments show that in around a third to a half of instances, the biased model is unable to predict the main model's behavior, highlighted by the significantly different parts of the input on which they base their decisions. Based on a manual validation, we also show that this estimate is highly in line with human interpretation. Our findings suggest that down-weighting of instances detected by bias detection methods, which is a widely-practiced procedure, is an unnecessary waste of training data. We release our code to facilitate reproducibility and future research",
    "checked": true,
    "id": "a0b7dc285885901f8088d064a8ac717b6a9686ca",
    "semantic_title": "don't discard all the biased instances: investigating a core assumption in dataset bias mitigation techniques",
    "citation_count": 5,
    "authors": [
      "Hossein Amirkhani",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.406": {
    "title": "Stacked AMR Parsing with Silver Data",
    "volume": "findings",
    "abstract": "Lacking sufficient human-annotated data is one main challenge for abstract meaning representation (AMR) parsing. To alleviate this problem, previous works usually make use of silver data or pre-trained language models. In particular, one recent seq-to-seq work directly fine-tunes AMR graph sequences on the encoder-decoder pre-trained language model and achieves new state-of-the-art results, outperforming previous works by a large margin. However, it makes the decoding relatively slower. In this work, we investigate alternative approaches to achieve competitive performance at faster speeds. We propose a simplified AMR parser and a pre-training technique for the effective usage of silver data. We conduct extensive experiments on the widely used AMR2.0 dataset and the results demonstrate that our Transformer-based AMR parser achieves the best performance among the seq2graph-based models. Furthermore, with silver data, our model achieves competitive results with the SOTA model, and the speed is an order of magnitude faster. Detailed analyses are conducted to gain more insights into our proposed model and the effectiveness of the pre-training technique",
    "checked": true,
    "id": "9f9f69545ce9e5bc2de1b237078812001871f954",
    "semantic_title": "stacked amr parsing with silver data",
    "citation_count": 6,
    "authors": [
      "Qingrong Xia",
      "Zhenghua Li",
      "Rui Wang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.407": {
    "title": "Speculative Sampling in Variational Autoencoders for Dialogue Response Generation",
    "volume": "findings",
    "abstract": "Variational autoencoders have been studied as a promising approach to model one-to-many mappings from context to response in chat response generation. However, they often fail to learn proper mappings. One of the reasons for this failure is the discrepancy between a response and a latent variable sampled from an approximated distribution in training. Inappropriately sampled latent variables hinder models from constructing a modulated latent space. As a result, the models stop handling uncertainty in conversations. To resolve that, we propose speculative sampling of latent variables. Our method chooses the most probable one from redundantly sampled latent variables for tying up the variable with a given response. We confirm the efficacy of our method in response generation with massive dialogue data constructed from Twitter posts",
    "checked": true,
    "id": "4d96e5672016af0155b2bbc35b4d798e290fda99",
    "semantic_title": "speculative sampling in variational autoencoders for dialogue response generation",
    "citation_count": 1,
    "authors": [
      "Shoetsu Sato",
      "Naoki Yoshinaga",
      "Masashi Toyoda",
      "Masaru Kitsuregawa"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.408": {
    "title": "Perceived and Intended Sarcasm Detection with Graph Attention Networks",
    "volume": "findings",
    "abstract": "Existing sarcasm detection systems focus on exploiting linguistic markers, context, or user-level priors. However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation. In this work, we propose a framework jointly leveraging (1) a user context from their historical tweets together with (2) the social information from a user's neighborhood in an interaction graph, to contextualize the interpretation of the post. We distinguish between perceived and self-reported sarcasm identification. We use graph attention networks (GAT) over users and tweets in a conversation thread, combined with various dense user history representations. Apart from achieving state-of-the-art results on the recently published dataset of 19k Twitter users with 30K labeled tweets, adding 10M unlabeled tweets as context, our experiments indicate that the graph network contributes to interpreting the sarcastic intentions of the author more than to predicting the sarcasm perception by others",
    "checked": true,
    "id": "0c80358c079dea76ddff1ee7d94a15c918390d68",
    "semantic_title": "perceived and intended sarcasm detection with graph attention networks",
    "citation_count": 23,
    "authors": [
      "Joan Plepi",
      "Lucie Flek"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.409": {
    "title": "Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation",
    "volume": "findings",
    "abstract": "Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target sentence which conforms to the style of the given exemplar while encapsulating the content information of the source sentence. In this paper, we propose a new method with the goal of learning a better representation of the style and the content. This method is mainly motivated by the recent success of contrastive learning which has demonstrated its power in unsupervised feature extraction tasks. The idea is to design two contrastive losses with respect to the content and the style by considering two problem characteristics during training. One characteristic is that the target sentence shares the same content with the source sentence, and the second characteristic is that the target sentence shares the same style with the exemplar. These two contrastive losses are incorporated into the general encoder-decoder paradigm. Experiments on two datasets, namely QQP-Pos and ParaNMT, demonstrate the effectiveness of our proposed constrastive losses",
    "checked": true,
    "id": "2a9e374ab70e7042220c0d0247de1d64009fc4ab",
    "semantic_title": "contrastive representation learning for exemplar-guided paraphrase generation",
    "citation_count": 15,
    "authors": [
      "Haoran Yang",
      "Wai Lam",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.410": {
    "title": "MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer",
    "volume": "findings",
    "abstract": "Adapter modules have emerged as a general parameter-efficient means to specialize a pretrained encoder to new domains. Massively multilingual transformers (MMTs) have particularly benefited from additional training of language-specific adapters. However, this approach is not viable for the vast majority of languages, due to limitations in their corpus size or compute budgets. In this work, we propose MAD-G (Multilingual ADapter Generation), which contextually generates language adapters from language representations based on typological features. In contrast to prior work, our time- and space-efficient MAD-G approach enables (1) sharing of linguistic knowledge across languages and (2) zero-shot inference by generating language adapters for unseen languages. We thoroughly evaluate MAD-G in zero-shot cross-lingual transfer on part-of-speech tagging, dependency parsing, and named entity recognition. While offering (1) improved fine-tuning efficiency (by a factor of around 50 in our experiments), (2) a smaller parameter budget, and (3) increased language coverage, MAD-G remains competitive with more expensive methods for language-specific adapter training across the board. Moreover, it offers substantial benefits for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G's transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available task-specific training data; and (ii) by further fine-tuning generated MAD-G adapters for languages with monolingual data",
    "checked": true,
    "id": "6adc9c231d874ea358554b8680a6aaba4bd6c963",
    "semantic_title": "mad-g: multilingual adapter generation for efficient cross-lingual transfer",
    "citation_count": 91,
    "authors": [
      "Alan Ansell",
      "Edoardo Maria Ponti",
      "Jonas Pfeiffer",
      "Sebastian Ruder",
      "Goran Glavaš",
      "Ivan Vulić",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.411": {
    "title": "Sustainable Modular Debiasing of Language Models",
    "volume": "findings",
    "abstract": "Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which – besides being computationally expensive – comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that – due to its modular nature – ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages",
    "checked": true,
    "id": "130ab5c480860e330b65280a3410f17bb2d50fe1",
    "semantic_title": "sustainable modular debiasing of language models",
    "citation_count": 121,
    "authors": [
      "Anne Lauscher",
      "Tobias Lueken",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.412": {
    "title": "A Divide-And-Conquer Approach for Multi-label Multi-hop Relation Detection in Knowledge Base Question Answering",
    "volume": "findings",
    "abstract": "Relation detection in knowledge base question answering, aims to identify the path(s) of relations starting from the topic entity node that is linked to the answer node in knowledge graph. Such path might consist of multiple relations, which we call multi-hop. Moreover, for a single question, there may exist multiple relation paths to the correct answer, which we call multi-label. However, most of existing approaches only detect one single path to obtain the answer without considering other correct paths, which might affect the final performance. Therefore, in this paper, we propose a novel divide-and-conquer approach for multi-label multi-hop relation detection (DC-MLMH) by decomposing it into head relation detection and conditional relation path generation. In specific, a novel path sampling mechanism is proposed to generate diverse relation paths for the inference stage. A majority-vote policy is employed to detect final KB answer. Comprehensive experiments were conducted on the FreebaseQA benchmark dataset. Experimental results show that the proposed approach not only outperforms other competitive multi-label baselines, but also has superiority over some state-of-art KBQA methods",
    "checked": true,
    "id": "17cac88fb1b4f089f33377a20bc8bf213b29c3ad",
    "semantic_title": "a divide-and-conquer approach for multi-label multi-hop relation detection in knowledge base question answering",
    "citation_count": 3,
    "authors": [
      "Deyu Zhou",
      "Yanzheng Xiang",
      "Linhai Zhang",
      "Chenchen Ye",
      "Qian-Wen Zhang",
      "Yunbo Cao"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.413": {
    "title": "Counterfactual Adversarial Learning with Representation Interpolation",
    "volume": "findings",
    "abstract": "Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce Counterfactual Adversarial Training framework (CAT) to tackle the problem from a causality perspective. Particularly, for a specific sample, CAT first generates a counterfactual representation through latent space interpolation in an adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust sample-wise loss weight dynamically, which encourages the model to explore the true causal effect. Extensive experiments demonstrate that CAT achieves substantial performance improvement over SOTA across different downstream tasks, including sentence classification, natural language inference and question answering",
    "checked": true,
    "id": "520453110522955a3cbdc380252557e1c607a643",
    "semantic_title": "counterfactual adversarial learning with representation interpolation",
    "citation_count": 2,
    "authors": [
      "Wei Wang",
      "Boxin Wang",
      "Ning Shi",
      "Jinfeng Li",
      "Bingyu Zhu",
      "Xiangyu Liu",
      "Rong Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.414": {
    "title": "‘Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP",
    "volume": "findings",
    "abstract": "A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences",
    "checked": true,
    "id": "48709b65320bf979d19c1874aebf6bf8bd85de88",
    "semantic_title": "just what do you think you're doing, dave?' a checklist for responsible data use in nlp",
    "citation_count": 65,
    "authors": [
      "Anna Rogers",
      "Timothy Baldwin",
      "Kobi Leins"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.415": {
    "title": "Counter-Contrastive Learning for Language GANs",
    "volume": "findings",
    "abstract": "Generative Adversarial Networks (GANs) have achieved great success in image synthesis, but have proven to be difficult to generate natural language. Challenges arise from the uninformative learning signals passed from the discriminator. In other words, the poor learning signals limit the learning capacity for generating languages with rich structures and semantics. In this paper, we propose to adopt the counter-contrastive learning (CCL) method to support the generator's training in language GANs. In contrast to standard GANs that adopt a simple binary classifier to discriminate whether a sample is real or fake, we employ a counter-contrastive learning signal that advances the training of language synthesizers by (1) pulling the language representations of generated and real samples together and (2) pushing apart representations of real samples to compete with the discriminator and thus prevent the discriminator from being overtrained. We evaluate our method on both synthetic and real benchmarks and yield competitive performance compared to previous GANs for adversarial sequence generation",
    "checked": true,
    "id": "fd9de49849409bbf852a5f554a8d582f01b242d0",
    "semantic_title": "counter-contrastive learning for language gans",
    "citation_count": 2,
    "authors": [
      "Yekun Chai",
      "Haidong Zhang",
      "Qiyue Yin",
      "Junge Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.416": {
    "title": "Incorporating Circumstances into Narrative Event Prediction",
    "volume": "findings",
    "abstract": "The narrative event prediction aims to predict what happens after a sequence of events, which is essential to modeling sophisticated real-world events. Existing studies focus on mining the inter-events relationships while ignoring how the events happened, which we called circumstances. With our observation, the event circumstances indicate what will happen next. To incorporate event circumstances into the narrative event prediction, we propose the CircEvent, which adopts the two multi-head attention to retrieve circumstances at the local and global levels. We also introduce a regularization of attention weights to leverage the alignment between events and local circumstances. The experimental results demonstrate our CircEvent outperforms existing baselines by 12.2%. The further analysis demonstrates the effectiveness of our multi-head attention modules and regularization",
    "checked": true,
    "id": "e07cc5b6e9505de91cb2c3f49fa934efa737586b",
    "semantic_title": "incorporating circumstances into narrative event prediction",
    "citation_count": 13,
    "authors": [
      "Shichao Wang",
      "Xiangrui Cai",
      "HongBin Wang",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.417": {
    "title": "MultiFix: Learning to Repair Multiple Errors by Optimal Alignment Learning",
    "volume": "findings",
    "abstract": "We consider the problem of learning to repair erroneous C programs by learning optimal alignments with correct programs. Since the previous approaches fix a single error in a line, it is inevitable to iterate the fixing process until no errors remain. In this work, we propose a novel sequence-to-sequence learning framework for fixing multiple program errors at a time. We introduce the edit-distance-based data labeling approach for program error correction. Instead of labeling a program repair example by pairing an erroneous program with a line fix, we label the example by paring an erroneous program with an optimal alignment to the corresponding correct program produced by the edit-distance computation. We evaluate our proposed approach on a publicly available dataset (DeepFix dataset) that consists of erroneous C programs submitted by novice programming students. On a set of 6,975 erroneous C programs from the DeepFix dataset, our approach achieves the state-of-the-art result in terms of full repair rate on the DeepFix dataset (without extra data such as compiler error message or additional source codes for pre-training)",
    "checked": true,
    "id": "132fe110451828394054589ddf060cc2819a3f02",
    "semantic_title": "multifix: learning to repair multiple errors by optimal alignment learning",
    "citation_count": 2,
    "authors": [
      "HyeonTae Seo",
      "Yo-Sub Han",
      "Sang-Ki Ko"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.418": {
    "title": "HOTTER: Hierarchical Optimal Topic Transport with Explanatory Context Representations",
    "volume": "findings",
    "abstract": "Natural language processing (NLP) is often the backbone of today's systems for user interactions, information retrieval and others. Many of such NLP applications rely on specialized learned representations (e.g. neural word embeddings, topic models) that improve the ability to reason about the relationships between documents of a corpus. Paired with the progress in learned representations, the similarity metrics used to compare representations of documents are also evolving, with numerous proposals differing in computation time or interpretability. In this paper we propose an extension to a specific emerging hybrid document distance metric which combines topic models and word embeddings: the Hierarchical Optimal Topic Transport (HOTT). In specific, we extend HOTT by using context-enhanced word representations. We provide a validation of our approach on public datasets, using the language model BERT for a document categorization task. Results indicate competitive performance of the extended HOTT metric. We furthermore apply the HOTT metric and its extension to support educational media research, with a retrieval task of matching topics in German curricula to educational textbooks passages, along with offering an auxiliary explanatory document representing the dominant topic of the retrieved document. In a user study, our explanation method is preferred over regular topic keywords",
    "checked": true,
    "id": "0ec5890013ff1487a72a5343d2a3a4302f189ed1",
    "semantic_title": "hotter: hierarchical optimal topic transport with explanatory context representations",
    "citation_count": 0,
    "authors": [
      "Sabine Wehnert",
      "Christian Scheel",
      "Simona Szakács-Behling",
      "Maret Nieländer",
      "Patrick Mielke",
      "Ernesto William De Luca"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.419": {
    "title": "Grammatical Error Correction with Contrastive Learning in Low Error Density Domains",
    "volume": "findings",
    "abstract": "Although grammatical error correction (GEC) has achieved good performance on texts written by learners of English as a second language, performance on low error density domains where texts are written by English speakers of varying levels of proficiency can still be improved. In this paper, we propose a contrastive learning approach to encourage the GEC model to assign a higher probability to a correct sentence while reducing the probability of incorrect sentences that the model tends to generate, so as to improve the accuracy of the model. Experimental results show that our approach significantly improves the performance of GEC models in low error density domains, when evaluated on the benchmark CWEB dataset",
    "checked": true,
    "id": "1262b9696f1d1e4dab767789de49f459b1ca12a6",
    "semantic_title": "grammatical error correction with contrastive learning in low error density domains",
    "citation_count": 12,
    "authors": [
      "Hannan Cao",
      "Wenmian Yang",
      "Hwee Tou Ng"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.420": {
    "title": "Improving Unsupervised Commonsense Reasoning Using Knowledge-Enabled Natural Language Inference",
    "volume": "findings",
    "abstract": "Recent methods based on pre-trained language models have shown strong supervised performance on commonsense reasoning. However, they rely on expensive data annotation and time-consuming training. Thus, we focus on unsupervised commonsense reasoning. We show the effectiveness of using a common framework, Natural Language Inference (NLI), to solve diverse commonsense reasoning tasks. By leveraging transfer learning from large NLI datasets, and injecting crucial knowledge from commonsense sources such as ATOMIC 2020 and ConceptNet, our method achieved state-of-the-art unsupervised performance on two commonsense reasoning tasks: WinoWhy and CommonsenseQA. Further analysis demonstrated the benefits of multiple categories of knowledge, but problems about quantities and antonyms are still challenging",
    "checked": true,
    "id": "177bc611389fac0e6239355a2d9eaa66f7aac53e",
    "semantic_title": "improving unsupervised commonsense reasoning using knowledge-enabled natural language inference",
    "citation_count": 7,
    "authors": [
      "Canming Huang",
      "Weinan He",
      "Yongmei Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.421": {
    "title": "Does Putting a Linguist in the Loop Improve NLU Data Collection?",
    "volume": "findings",
    "abstract": "Many crowdsourced NLP datasets contain systematic artifacts that are identified only after data collection is complete. Earlier identification of these issues should make it easier to create high-quality training and evaluation data. We attempt this by evaluating protocols in which expert linguists work ‘in the loop' during data collection to identify and address these issues by adjusting task instructions and incentives. Using natural language inference as a test case, we compare three data collection protocols: (i) a baseline protocol with no linguist involvement, (ii) a linguist-in-the-loop intervention with iteratively-updated constraints on the writing task, and (iii) an extension that adds direct interaction between linguists and crowdworkers via a chatroom. We find that linguist involvement does not lead to increased accuracy on out-of-domain test sets compared to baseline, and adding a chatroom has no effect on the data. Linguist involvement does, however, lead to more challenging evaluation data and higher accuracy on some challenge sets, demonstrating the benefits of integrating expert analysis during data collection",
    "checked": true,
    "id": "f571f244c6ec578afce9f9775f8eaae10521eb57",
    "semantic_title": "does putting a linguist in the loop improve nlu data collection?",
    "citation_count": 40,
    "authors": [
      "Alicia Parrish",
      "William Huang",
      "Omar Agha",
      "Soo-Hwan Lee",
      "Nikita Nangia",
      "Alexia Warstadt",
      "Karmanya Aggarwal",
      "Emily Allaway",
      "Tal Linzen",
      "Samuel R. Bowman"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.422": {
    "title": "Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding",
    "volume": "findings",
    "abstract": "Large-scale, pre-trained language models (LMs) have achieved human-level performance on a breadth of language understanding tasks. However, evaluations only based on end task performance shed little light on machines' true ability in language understanding and reasoning. In this paper, we highlight the importance of evaluating the underlying reasoning process in addition to end performance. Toward this goal, we introduce Tiered Reasoning for Intuitive Physics (TRIP), a novel commonsense reasoning dataset with dense annotations that enable multi-tiered evaluation of machines' reasoning process. Our empirical results show that while large LMs can achieve high end performance, they struggle to support their predictions with valid supporting evidence. The TRIP dataset and our baseline results will motivate verifiable evaluation of commonsense reasoning and facilitate future research toward developing better language understanding and reasoning models",
    "checked": true,
    "id": "5f6a790722f18f0aa7419f8e0c6404c46acd99ba",
    "semantic_title": "tiered reasoning for intuitive physics: toward verifiable commonsense language understanding",
    "citation_count": 23,
    "authors": [
      "Shane Storks",
      "Qiaozi Gao",
      "Yichi Zhang",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.423": {
    "title": "Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets",
    "volume": "findings",
    "abstract": "For interpreting the behavior of a probabilistic model, it is useful to measure a model's calibration—the extent to which it produces reliable confidence scores. We address the open problem of calibration for tagging models with sparse tagsets, and recommend strategies to measure and reduce calibration error (CE) in such models. We show that several post-hoc recalibration techniques all reduce calibration error across the marginal distribution for two existing sequence taggers. Moreover, we propose tag frequency grouping (TFG) as a way to measure calibration error in different frequency bands. Further, recalibrating each group separately promotes a more equitable reduction of calibration error across the tag frequency spectrum",
    "checked": true,
    "id": "90a70fc5658fbafd364cb798dd55dc7b074506a4",
    "semantic_title": "making heads and tails of models with marginal calibration for sparse tagsets",
    "citation_count": 2,
    "authors": [
      "Michael Kranzlein",
      "Nelson F. Liu",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2021.findings-emnlp.424": {
    "title": "GeDi: Generative Discriminator Guided Sequence Generation",
    "volume": "findings",
    "abstract": "",
    "checked": true,
    "id": "07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
    "semantic_title": "gedi: generative discriminator guided sequence generation",
    "citation_count": 402,
    "authors": [
      "Ben Krause",
      "Akhilesh Deepak Gotmare",
      "Bryan McCann",
      "Nitish Shirish Keskar",
      "Shafiq Joty",
      "Richard Socher",
      "Nazneen Fatema Rajani"
    ]
  }
}