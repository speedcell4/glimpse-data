{
  "https://openreview.net/forum?id=BkbY4psgg": {
    "title": "Making Neural Programming Architectures Generalize via Recursion",
    "volume": "oral",
    "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system's behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion",
    "checked": true,
    "id": "6b024162f81e8ff7aa34c3a43d601a912d012c78",
    "semantic_title": "making neural programming architectures generalize via recursion",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4_qw5xe": {
    "title": "Towards Principled Methods for Training Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them",
    "checked": true,
    "id": "9a700c7a7e7468e436f00c34551fbe3e0f70e42f",
    "semantic_title": "towards principled methods for training generative adversarial networks",
    "citation_count": 2125,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Bb3D5gg": {
    "title": "Learning End-to-End Goal-Oriented Dialog",
    "volume": "oral",
    "abstract": "Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End- to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service",
    "checked": true,
    "id": "f81be44000814e7bcb12ae04b4e2d9c01b6515b3",
    "semantic_title": "learning end-to-end goal-oriented dialog",
    "citation_count": 781,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Ue8Hcxg": {
    "title": "Neural Architecture Search with Reinforcement Learning",
    "volume": "oral",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214",
    "checked": true,
    "id": "67d968c7450878190e45ac7886746de867bf673d",
    "semantic_title": "neural architecture search with reinforcement learning",
    "citation_count": 5442,
    "authors": []
  },
  "https://openreview.net/forum?id=S1RP6GLle": {
    "title": "Amortised MAP Inference for Image Super-resolution",
    "volume": "oral",
    "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders",
    "checked": true,
    "id": "3c7092347a5b7804d9534b6f3f52032c1502cbf8",
    "semantic_title": "amortised map inference for image super-resolution",
    "citation_count": 435,
    "authors": []
  },
  "https://openreview.net/forum?id=H1oyRlYgg": {
    "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
    "volume": "oral",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap",
    "checked": true,
    "id": "8ec5896b4490c6e127d1718ffc36a3439d84cb81",
    "semantic_title": "on large-batch training for deep learning: generalization gap and sharp minima",
    "citation_count": 2982,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwoSDPgg": {
    "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data",
    "volume": "oral",
    "abstract": "Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ''teachers'' for a ''student'' model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning",
    "checked": true,
    "id": "e70b9a38fcf8373865dd6e7b45e45cca7ff2eaa9",
    "semantic_title": "semi-supervised knowledge transfer for deep learning from private training data",
    "citation_count": 1029,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ0NvFzxl": {
    "title": "Learning Graphical State Transitions",
    "volume": "oral",
    "abstract": "Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that produces sequences from graph-structured input. In this work I introduce the Gated Graph Transformer Neural Network (GGT-NN), an extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines",
    "checked": true,
    "id": "edbc873a248768a626ef2bc57b3d1eff30de0e11",
    "semantic_title": "learning graphical state transitions",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk8N3Sclg": {
    "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language",
    "volume": "oral",
    "abstract": "The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are in- terested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communi- cation. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message to the receiver, while the receiver must rely on it to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore whether the \"word meanings\" induced in the game reflect intuitive semantic properties of the objects depicted in the image, and we present a simple strategy for grounding the agents' code into natural language, a necessary step in developing machines that should eventually be able to communicate with humans",
    "checked": true,
    "id": "f91d1db0fa2ac12e0262a8f5e6a371da29a3c100",
    "semantic_title": "multi-agent cooperation and the emergence of (natural) language",
    "citation_count": 439,
    "authors": []
  },
  "https://openreview.net/forum?id=rJY0-Kcll": {
    "title": "Optimization as a Model for Few-Shot Learning",
    "volume": "oral",
    "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning",
    "checked": true,
    "id": "29c887794eed2ca9462638ff853e6fe1ab91d5d8",
    "semantic_title": "optimization as a model for few-shot learning",
    "citation_count": 3433,
    "authors": []
  },
  "https://openreview.net/forum?id=rJLS7qKel": {
    "title": "Learning to Act by Predicting the Future",
    "volume": "oral",
    "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments",
    "checked": true,
    "id": "4c25f50c7451fa72c562e21e3b11e416b11f74c8",
    "semantic_title": "learning to act by predicting the future",
    "citation_count": 282,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxdQ3jeg": {
    "title": "End-to-end Optimized Image Compression",
    "volume": "oral",
    "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM",
    "checked": true,
    "id": "232148b97bd0543613ffd98fb4edcff79434ce1a",
    "semantic_title": "end-to-end optimized image compression",
    "citation_count": 1753,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ3rcZcxl": {
    "title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic",
    "volume": "oral",
    "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments",
    "checked": true,
    "id": "524513b6f4ddca331c33bcc70a9f677fa240cfa3",
    "semantic_title": "q-prop: sample-efficient policy gradient with an off-policy critic",
    "citation_count": 346,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy8gdB9xx": {
    "title": "Understanding deep learning requires rethinking generalization",
    "volume": "oral",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models",
    "checked": true,
    "id": "54ddb00fa691728944fd8becea90a373d21597cf",
    "semantic_title": "understanding deep learning requires rethinking generalization",
    "citation_count": 4667,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ6yPD5xg": {
    "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
    "volume": "oral",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth",
    "checked": true,
    "id": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
    "semantic_title": "reinforcement learning with unsupervised auxiliary tasks",
    "citation_count": 1238,
    "authors": []
  },
  "https://openreview.net/forum?id=S1VaB4cex": {
    "title": "FractalNet: Ultra-Deep Neural Networks without Residuals",
    "volume": "poster",
    "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer",
    "checked": true,
    "id": "d0156126edbfc524c8d108bdc0cf811cfe3129aa",
    "semantic_title": "fractalnet: ultra-deep neural networks without residuals",
    "citation_count": 949,
    "authors": []
  },
  "https://openreview.net/forum?id=H1W1UN9gg": {
    "title": "Deep Information Propagation",
    "volume": "poster",
    "abstract": "We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively",
    "checked": true,
    "id": "4fdc7df2c737141a1bf5aec27a438b77d01f8af0",
    "semantic_title": "deep information propagation",
    "citation_count": 372,
    "authors": []
  },
  "https://openreview.net/forum?id=SJGCiw5gl": {
    "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference",
    "volume": "poster",
    "abstract": "We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation-a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach",
    "checked": true,
    "id": "3db8730c203f88d7f08a6a99e8c02a077dc9b011",
    "semantic_title": "pruning convolutional neural networks for resource efficient inference",
    "citation_count": 1993,
    "authors": []
  },
  "https://openreview.net/forum?id=r1VdcHcxx": {
    "title": "Recurrent Batch Normalization",
    "volume": "poster",
    "abstract": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization",
    "checked": true,
    "id": "952454718139dba3aafc6b3b67c4f514ac3964af",
    "semantic_title": "recurrent batch normalization",
    "citation_count": 411,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy2fzU9gl": {
    "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
    "volume": "poster",
    "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data",
    "checked": true,
    "id": "6f7af4709e399e89ba898efc3459cb844fa0e981",
    "semantic_title": "beta-vae: learning basic visual concepts with a constrained variational framework",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hdzd5lg": {
    "title": "Words or Characters? Fine-grained Gating for Reading Comprehension",
    "volume": "poster",
    "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test and Who Did What datasets. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task",
    "checked": true,
    "id": "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239",
    "semantic_title": "words or characters? fine-grained gating for reading comprehension",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=Bks8cPcxe": {
    "title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning",
    "volume": "poster",
    "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications. In this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides (1) intuitive constructs to support compact encoding of deep networks; (2) symbolic gradient derivation of the networks; (3) static analysis for memory consumption and error detection; and (4) DSL-level optimization to improve memory and runtime efficiency. DeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries",
    "checked": true,
    "id": "8902ce81d33571d62445e6152aac1d03a426049a",
    "semantic_title": "deepdsl: a compilation-based domain-specific language for deep learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=rkpACe1lx": {
    "title": "HyperNetworks",
    "volume": "poster",
    "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network. We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks",
    "checked": true,
    "id": "563783de03452683a9206e85fe6d661714436686",
    "semantic_title": "hypernetworks",
    "citation_count": 1656,
    "authors": []
  },
  "https://openreview.net/forum?id=BydARw9ex": {
    "title": "Capacity and Trainability in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures",
    "checked": true,
    "id": "401d68e1a930b0f7e02030cab4c185fb1839cb11",
    "semantic_title": "capacity and trainability in recurrent neural networks",
    "citation_count": 205,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGODLqgx": {
    "title": "Recurrent Hidden Semi-Markov Model",
    "volume": "poster",
    "abstract": "Segmentation and labeling of high dimensional time series data has wide applications in behavior understanding and medical diagnosis. Due to the difficulty in obtaining the label information for high dimensional data, realizing this objective in an unsupervised way is highly desirable. Hidden Semi-Markov Model (HSMM) is a classical tool for this problem. However, existing HSMM and its variants has simple conditional assumptions of observations, thus the ability to capture the nonlinear and complex dynamics within segments is limited. To tackle this limitation, we propose to incorporate the Recurrent Neural Network (RNN) to model the generative process in HSMM, resulting the Recurrent HSMM (R-HSMM). To accelerate the inference while preserving accuracy, we designed a structure encoding function to mimic the exact inference. By generalizing the penalty method to distribution space, we are able to train the model and the encoding function simultaneously. Empirical results show that the proposed R-HSMM achieves the state-of-the-art performances on both synthetic and real-world datasets",
    "checked": true,
    "id": "e20d94e43a20315f76067a00d67e4f7523ca7311",
    "semantic_title": "recurrent hidden semi-markov model",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=S11KBYclx": {
    "title": "Learning Curve Prediction with Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time. Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. Exploiting the same information in automatic Bayesian hyperparameter optimization requires a probabilistic model of learning curves across hyperparameter settings. Here, we study the use of Bayesian neural networks for this purpose and improve their performance by a specialized learning curve layer",
    "checked": true,
    "id": "925cab60b1795c94ae6f488fda7ad71be71b5822",
    "semantic_title": "learning curve prediction with bayesian neural networks",
    "citation_count": 260,
    "authors": []
  },
  "https://openreview.net/forum?id=SyK00v5xx": {
    "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
    "volume": "poster",
    "abstract": "The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new \"smoothing\" terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts",
    "checked": true,
    "id": "3f1802d3f4f5f6d66875dac09112f978f12e1e1e",
    "semantic_title": "a simple but tough-to-beat baseline for sentence embeddings",
    "citation_count": 1323,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GOWV5eg": {
    "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space and implicitly enabling planning through sequences of repetitive macro-actions. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain",
    "checked": true,
    "id": "2d6075ffa9ef3fbfe338bfeb469a50883242bdcd",
    "semantic_title": "learning to repeat: fine grained action repetition for deep reinforcement learning",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=B184E5qee": {
    "title": "Improving Neural Language Models with a Continuous Cache",
    "volume": "poster",
    "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks",
    "checked": true,
    "id": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
    "semantic_title": "improving neural language models with a continuous cache",
    "citation_count": 302,
    "authors": []
  },
  "https://openreview.net/forum?id=BJYwwY9ll": {
    "title": "Snapshot Ensembles: Train 1, Get M for Free",
    "volume": "poster",
    "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively",
    "checked": true,
    "id": "b134d0911e2e13ac169ffa5f478a39e6ef77869a",
    "semantic_title": "snapshot ensembles: train 1, get m for free",
    "citation_count": 967,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGwcKclx": {
    "title": "Soft Weight-Sharing for Neural Network Compression",
    "volume": "poster",
    "abstract": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of \"soft weight-sharing\" (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle",
    "checked": true,
    "id": "ac37a459c46a464ac1326c821ba7d5595b50f1af",
    "semantic_title": "soft weight-sharing for neural network compression",
    "citation_count": 422,
    "authors": []
  },
  "https://openreview.net/forum?id=r1nTpv9eg": {
    "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations. We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions",
    "checked": true,
    "id": "079021d93a88c6a4721af75397d14c2125af1f26",
    "semantic_title": "learning to perform physics experiments via deep reinforcement learning",
    "citation_count": 347,
    "authors": []
  },
  "https://openreview.net/forum?id=B1MRcPclx": {
    "title": "Query-Reduction Networks for Question Answering",
    "volume": "poster",
    "abstract": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference",
    "checked": true,
    "id": "4bf7edee5a4c4cfdbdd43a607c402420129fa277",
    "semantic_title": "query-reduction networks for question answering",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=BJm4T4Kgx": {
    "title": "Adversarial Machine Learning at Scale",
    "volume": "poster",
    "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ``label leaking'' effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process",
    "checked": true,
    "id": "e2a85a6766b982ff7c8980e57ca6342d22493827",
    "semantic_title": "adversarial machine learning at scale",
    "citation_count": 3178,
    "authors": []
  },
  "https://openreview.net/forum?id=rk9eAFcxg": {
    "title": "Variational Recurrent Adversarial Deep Domain Adaptation",
    "volume": "poster",
    "abstract": "We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model's ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches",
    "checked": true,
    "id": "9c4f30fe94ce07a89eb9b1789b338064d5c44811",
    "semantic_title": "variational recurrent adversarial deep domain adaptation",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=ryMxXPFex": {
    "title": "Discrete Variational Autoencoders",
    "volume": "poster",
    "abstract": "Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets",
    "checked": true,
    "id": "711683de61ee8d2aa929ec4d5d97d616b8e281d3",
    "semantic_title": "discrete variational autoencoders",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=r1fYuytex": {
    "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks",
    "volume": "poster",
    "abstract": "Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks",
    "checked": true,
    "id": "138ea97004fe7586977b4eb72835df615319c2df",
    "semantic_title": "sparsely-connected neural networks: towards efficient vlsi implementation of deep neural networks",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=SJRpRfKxx": {
    "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention",
    "volume": "poster",
    "abstract": "In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets",
    "checked": true,
    "id": "f3158ce4e1c9e908e7d06533d711d84205c973b9",
    "semantic_title": "recurrent mixture density network for spatiotemporal visual attention",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ3filKll": {
    "title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks",
    "volume": "poster",
    "abstract": "We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data. Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper",
    "checked": true,
    "id": "ed9acc61319c2aad04ce25c3bc348b955296bed3",
    "semantic_title": "efficient representation of low-dimensional manifolds using deep networks",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=r1VGvBcxl": {
    "title": "Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU",
    "volume": "poster",
    "abstract": "We introduce a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. We analyze its computational traits and concentrate on aspects critical to leveraging the GPU's computational power. We introduce a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. Our hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant speed up compared to a CPU implementation; we make it publicly available to other researchers at https://github.com/NVlabs/GA3C",
    "checked": true,
    "id": "48830e2e4272fa88dc256f1ac9cf81be14112bdb",
    "semantic_title": "reinforcement learning through asynchronous advantage actor-critic on a gpu",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk0FWVcgx": {
    "title": "Topology and Geometry of Half-Rectified Network Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0f94591cc05e6f75c21749d507ef58d204f63b7d",
    "semantic_title": "topology and geometry of half-rectified network optimization",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=HyAbMKwxe": {
    "title": "Tighter bounds lead to improved classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0020a29cd42db351fbfb3c8bb0fd6fa597d67b76",
    "semantic_title": "tighter bounds lead to improved classifiers",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=rJqBEPcxe": {
    "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105",
    "semantic_title": "zoneout: regularizing rnns by randomly preserving hidden activations",
    "citation_count": 318,
    "authors": []
  },
  "https://openreview.net/forum?id=SkYbF1slg": {
    "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4a592e21d52558498b8cd788badee01aac9525d",
    "semantic_title": "an information-theoretic framework for fast and robust unsupervised learning via neural population infomax",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=B16dGcqlx": {
    "title": "Third Person Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2e1a1b9c2e8feeb31c6855292859bf94101e8382",
    "semantic_title": "third-person imitation learning",
    "citation_count": 237,
    "authors": []
  },
  "https://openreview.net/forum?id=HyTqHL5xg": {
    "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6768ed8d51d1eb0d06f9726a42cef2f17a401f65",
    "semantic_title": "deep variational bayes filters: unsupervised learning of state space models from raw data",
    "citation_count": 378,
    "authors": []
  },
  "https://openreview.net/forum?id=BymIbLKgl": {
    "title": "Learning Invariant Representations Of Planar Curves",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc828a99cd936bd4aea2071169504a971a3659da",
    "semantic_title": "learning invariant representations of planar curves",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=HkpbnH9lx": {
    "title": "Density estimation using Real NVP",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09879f7956dddc2a9328f5c1472feeb8402bcbcf",
    "semantic_title": "density estimation using real nvp",
    "citation_count": 3786,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk8BvDqex": {
    "title": "Metacontrol for Adaptive Imagination-Based Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "099cdb087f240352a02286bf9a3e7810c7ebb02b",
    "semantic_title": "metacontrol for adaptive imagination-based optimization",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ25-B5eg": {
    "title": "The Neural Noisy Channel",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d19b712f90cde698cc96ebd5fe291b410e3f0f9c",
    "semantic_title": "the neural noisy channel",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=HJTzHtqee": {
    "title": "A Compare-Aggregate Model for Matching Text Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3f567d3d4975359fadfa9d750e1e4c4722d666c8",
    "semantic_title": "a compare-aggregate model for matching text sequences",
    "citation_count": 277,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxeqhP9ll": {
    "title": "Calibrating Energy-based Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0496691c6d54fd746c111d1004349acf7654145e",
    "semantic_title": "calibrating energy-based generative adversarial networks",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=ry2YOrcge": {
    "title": "Learning a Natural Language Interface with Neural Programmer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8fdf01235a0d4bb8bfbd98327b5c86c199b35e48",
    "semantic_title": "learning a natural language interface with neural programmer",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=r1rz6U5lg": {
    "title": "Learning to superoptimize programs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8567ca5729d9f7af210ae426e8e0ebcb1d8607d3",
    "semantic_title": "learning to superoptimize programs",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=ryelgY5eg": {
    "title": "Optimal Binary Autoencoding with Pairwise Correlations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6a65cd0e80a7b96fd861186f73fc04fc975213e0",
    "semantic_title": "optimal binary autoencoding with pairwise correlations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HJjiFK5gx": {
    "title": "Neural Program Lattices",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15064b7e47b43a68b6661bc7c3eaaad207493191",
    "semantic_title": "neural program lattices",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ElR4cgg": {
    "title": "Adversarially Learned Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
    "semantic_title": "adversarially learned inference",
    "citation_count": 1316,
    "authors": []
  },
  "https://openreview.net/forum?id=ByIAPUcee": {
    "title": "Frustratingly Short Attention Spans in Neural Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58eb3a2f0a67acf2f5c7c2cb4a22852b65314eb5",
    "semantic_title": "frustratingly short attention spans in neural language modeling",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=rJQKYt5ll": {
    "title": "Steerable CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "db1207440d20104854464d51da4cf5f79c5de240",
    "semantic_title": "steerable cnns",
    "citation_count": 505,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxDkvqee": {
    "title": "Multi-view Recurrent Neural Acoustic Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e018afcc15257db6ed2cfd3beebd11d11d0e93f",
    "semantic_title": "multi-view recurrent neural acoustic word embeddings",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxpMd9lx": {
    "title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "189e6bb7523733c4e524214b9e6ae92d4ed50dac",
    "semantic_title": "transfer learning for sequence tagging with hierarchical recurrent networks",
    "citation_count": 352,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk3mPK5gg": {
    "title": "Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "add7b8b65355d5408a1ffb93a94b0ae688806bc4",
    "semantic_title": "training agent for first-person shooter game with actor-critic curriculum learning",
    "citation_count": 187,
    "authors": []
  },
  "https://openreview.net/forum?id=rJiNwv9gg": {
    "title": "Lossy Image Compression with Compressive Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "977560251c2bd4c28a6c7c707c29f4091c5e6247",
    "semantic_title": "lossy image compression with compressive autoencoders",
    "citation_count": 1062,
    "authors": []
  },
  "https://openreview.net/forum?id=H1fl8S9ee": {
    "title": "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2af9eaae9191ee522cb97dc57615a071e5403d26",
    "semantic_title": "learning and policy search in stochastic dynamical systems with bayesian neural networks",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNKFiGex": {
    "title": "Neural Photo Editing with Introspective Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1dcda6ac3fd33bf938574aca9542aec5e5cace26",
    "semantic_title": "neural photo editing with introspective adversarial networks",
    "citation_count": 461,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfMusFll": {
    "title": "Batch Policy Gradient Methods for Improving Neural Conversation Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11732140ada8e05c84e9024927c297565ebd6ad8",
    "semantic_title": "batch policy gradient methods for improving neural conversation models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zJ-v5xl": {
    "title": "Quasi-Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2d876ed1dd2c58058d7197b734a8e4d349b8f231",
    "semantic_title": "quasi-recurrent neural networks",
    "citation_count": 450,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNRsU5ge": {
    "title": "Sigma Delta Quantized Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6fd4b90193efbd0922554672abb52fa1886f607f",
    "semantic_title": "sigma delta quantized networks",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=S1jE5L5gl": {
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "515a21e90117941150923e559729c59f5fdade1c",
    "semantic_title": "the concrete distribution: a continuous relaxation of discrete random variables",
    "citation_count": 2572,
    "authors": []
  },
  "https://openreview.net/forum?id=Skn9Shcxe": {
    "title": "Highway and Residual Networks learn Unrolled Iterative Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "535347e07b2ce5ac229349156db1e7bed0486b91",
    "semantic_title": "highway and residual networks learn unrolled iterative estimation",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=HJKkY35le": {
    "title": "Mode Regularized Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "024d30897e0a2b036bc122163a954b7f1a1d0679",
    "semantic_title": "mode regularized generative adversarial networks",
    "citation_count": 560,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg8bDqee": {
    "title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9bc3eaa4f816d186b9deac9ba980f278f85a1cd8",
    "semantic_title": "introspection: accelerating neural network training by learning weight evolution",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=S1X7nhsxl": {
    "title": "Improving Generative Adversarial Networks with Denoising Feature Matching",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ee38873273aead30c8e561ba807a3032204f870",
    "semantic_title": "improving generative adversarial networks with denoising feature matching",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ckMDqlg": {
    "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "510e26733aaff585d65701b9f1be7ca9d5afc586",
    "semantic_title": "outrageously large neural networks: the sparsely-gated mixture-of-experts layer",
    "citation_count": 2840,
    "authors": []
  },
  "https://openreview.net/forum?id=rk5upnsxe": {
    "title": "Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2a7564039c2eb69073aafe4b2363490de5b8e3c3",
    "semantic_title": "normalizing the normalizers: comparing and extending network normalization schemes",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=ryhqQFKgl": {
    "title": "Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ff1dbf5c5851eed853b85c34c52331759613581",
    "semantic_title": "towards deep interpretability (mus-rover ii): learning hierarchical representations of tonal music",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGTuv9eg": {
    "title": "Incorporating long-range consistency in CNN-based texture generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60520d06b5540c15effa403d0c63b98a2a49159d",
    "semantic_title": "incorporating long-range consistency in cnn-based texture generation",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=HkljfjFee": {
    "title": "Support Regularized Sparse Coding and Its Fast Encoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d580cf2cf9493b67d08fa8b2300f3bbc9ed864b7",
    "semantic_title": "support regularized sparse coding and its fast encoder",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gtu5ilg": {
    "title": "Transfer of View-manifold Learning to Similarity Perception of Novel Objects",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01fce96b99aedfb5d041ef4411ad8e9699b2c4df",
    "semantic_title": "transfer of view-manifold learning to similarity perception of novel objects",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=S1LVSrcge": {
    "title": "Variable Computation in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6746a18b2820f757334f75bc95428b3ea58d6603",
    "semantic_title": "variable computation in recurrent neural networks",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=r1YNw6sxg": {
    "title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a82201136fd3e835fbe240368047feeed62a084e",
    "semantic_title": "learning visual servoing with deep features and fitted q-iteration",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ1kmv9xx": {
    "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c789f8e3f40f57335aef2565f4d2bac69d2941d8",
    "semantic_title": "lr-gan: layered recursive generative adversarial networks for image generation",
    "citation_count": 241,
    "authors": []
  },
  "https://openreview.net/forum?id=BkLhzHtlg": {
    "title": "Learning Recurrent Representations for Hierarchical Behavior Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b530755b7d4e1ce03d96812f5e065c30eab8538c",
    "semantic_title": "learning recurrent representations for hierarchical behavior modeling",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=r1LXit5ee": {
    "title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "454d0168492917fdb87f2ed034686c8edd4cbe90",
    "semantic_title": "episodic exploration for deep deterministic policies for starcraft micromanagement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJMGPrcle": {
    "title": "Learning to Navigate in Complex Environments",
    "volume": "poster",
    "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks to bootstrap learning. In particular we consider jointly learning the goal-driven reinforcement learning problem with an unsupervised depth prediction task and a self-supervised loop closure classification task. Using this approach we can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, that show that the agent implicitly learns key navigation abilities, with only sparse rewards and without direct supervision",
    "checked": true,
    "id": "d35b05f440b5ba00d9429139edef7182bf9f7ce7",
    "semantic_title": "learning to navigate in complex environments",
    "citation_count": 883,
    "authors": []
  },
  "https://openreview.net/forum?id=S1dIzvclg": {
    "title": "A recurrent neural network without chaos",
    "volume": "poster",
    "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior",
    "checked": true,
    "id": "f451d0212e65ab9970a58b584b3363a853c9811c",
    "semantic_title": "a recurrent neural network without chaos",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=ryh9pmcee": {
    "title": "Energy-based Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images",
    "checked": true,
    "id": "0496691c6d54fd746c111d1004349acf7654145e",
    "semantic_title": "calibrating energy-based generative adversarial networks",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=S1c2cvqee": {
    "title": "Designing Neural Network Architectures using Reinforcement Learning",
    "volume": "poster",
    "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks",
    "checked": true,
    "id": "6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
    "semantic_title": "designing neural network architectures using reinforcement learning",
    "citation_count": 1481,
    "authors": []
  },
  "https://openreview.net/forum?id=HkE0Nvqlg": {
    "title": "Structured Attention Networks",
    "volume": "poster",
    "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention",
    "checked": true,
    "id": "13d9323a8716131911bfda048a40e2cde1a76a46",
    "semantic_title": "structured attention networks",
    "citation_count": 465,
    "authors": []
  },
  "https://openreview.net/forum?id=BJC_jUqxe": {
    "title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING",
    "volume": "poster",
    "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks",
    "checked": true,
    "id": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
    "semantic_title": "a structured self-attentive sentence embedding",
    "citation_count": 2153,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ8Je4clg": {
    "title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening",
    "volume": "poster",
    "abstract": "We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy",
    "checked": true,
    "id": "85d8b1b3483c7f4db999e7cf6b3e6231954c43dc",
    "semantic_title": "learning to play in a day: faster deep reinforcement learning by optimality tightening",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=ryrGawqex": {
    "title": "Deep Learning with Dynamic Computation Graphs",
    "volume": "poster",
    "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature",
    "checked": true,
    "id": "7176fdb1c3618c26d331bc49e0d8d0874d2555ed",
    "semantic_title": "deep learning with dynamic computation graphs",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxQzBceg": {
    "title": "Deep Variational Information Bottleneck",
    "volume": "poster",
    "abstract": "We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack",
    "checked": true,
    "id": "a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c",
    "semantic_title": "deep variational information bottleneck",
    "citation_count": 1765,
    "authors": []
  },
  "https://openreview.net/forum?id=ryHlUtqge": {
    "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning",
    "volume": "poster",
    "abstract": "Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while it is deployed and performing useful tasks. However, this learning requires access to a reward function, to tell the agent whether it is succeeding or failing at its task. Such reward functions are often hard to measure in the real world, especially in domains such as robotics and dialog systems, where the reward could depend on the unknown positions of objects or the emotional state of the user. On the other hand, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present, or in a controlled laboratory setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect in the unstructured real world? In this paper, we formalize this problem setting as semi-supervised reinforcement learning (SSRL), where the reward function can only be evaluated in a set of \"labeled\" MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of \"unlabeled\" MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging, continuous control tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward",
    "checked": true,
    "id": "4603f26093d64fc4107d3ec49667003f60210654",
    "semantic_title": "generalizing skills with semi-supervised reinforcement learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=S1oWlN9ll": {
    "title": "Loss-aware Binarization of Deep Networks",
    "volume": "poster",
    "abstract": "Deep neural network models, though very powerful and highly successful, are computationally expensive in terms of space and time. Recently, there have been a number of attempts on binarizing the network weights and activations. This greatly reduces the network size, and replaces the underlying multiplications to additions or even XNOR bit operations. However, existing binarization schemes are based on simple matrix approximations and ignore the effect of binarization on the loss. In this paper, we propose a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss w.r.t. the binarized weights. The underlying proximal step has an efficient closed-form solution, and the second-order information can be efficiently obtained from the second moments already computed by the Adam optimizer. Experiments on both feedforward and recurrent networks show that the proposed loss-aware binarization algorithm outperforms existing binarization schemes, and is also more robust for wide and deep networks",
    "checked": true,
    "id": "43e2519d094bff2fc10c3b9ea944a2457e3884be",
    "semantic_title": "loss-aware binarization of deep networks",
    "citation_count": 220,
    "authors": []
  },
  "https://openreview.net/forum?id=B1-q5Pqxl": {
    "title": "Machine Comprehension Using Match-LSTM and Answer Pointer",
    "volume": "poster",
    "abstract": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our tasks. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features. Besides, our boundary model also achieves the best performance on the MSMARCO dataset (Nguyen et al. 2016)",
    "checked": true,
    "id": "ff1861b71eaedba46cb679bbe2c585dbe18f9b19",
    "semantic_title": "machine comprehension using match-lstm and answer pointer",
    "citation_count": 595,
    "authors": []
  },
  "https://openreview.net/forum?id=B1s6xvqlx": {
    "title": "Recurrent Environment Simulators",
    "volume": "poster",
    "abstract": "Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes",
    "checked": true,
    "id": "bb430ec2f25e4a1513073a2a4098cbb942c2e3e0",
    "semantic_title": "recurrent environment simulators",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=ry18Ww5ee": {
    "title": "Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization",
    "volume": "poster",
    "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation. We present Hyperband, a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound. Hyperband is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations. We compare Hyperband with state-of-the-art Bayesian Optimization methods on several hyperparameter optimization problems. We observe that Hyperband can provide over an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems",
    "checked": true,
    "id": "04fe6b11280c79b91c060934be66856877e532c6",
    "semantic_title": "hyperband: bandit-based configuration evaluation for hyperparameter optimization",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=S1di0sfgl": {
    "title": "Hierarchical Multiscale Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Learning both hierarchical and temporal representation has been among the long- standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation",
    "checked": true,
    "id": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
    "semantic_title": "hierarchical multiscale recurrent neural networks",
    "citation_count": 538,
    "authors": []
  },
  "https://openreview.net/forum?id=HkYhZDqxg": {
    "title": "Tree-structured decoding with doubly-recurrent neural networks",
    "volume": "poster",
    "abstract": "We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly-recurrent neural network that models separately the width and depth recurrences across the tree, and combines them inside each cell to generate an output. The topology of the tree is explicitly modeled, allowing the network to predict both content and topology of the tree when decoding. That is, given only an encoded vector representation, the network is able to simultaneously generate a tree from it and predict labels for the nodes. We test this architecture in an encoder-decoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs",
    "checked": true,
    "id": "9c69926bdb72912725d20a55af7147f86bed01ae",
    "semantic_title": "tree-structured decoding with doubly-recurrent neural networks",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=Hku9NK5lx": {
    "title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty",
    "volume": "poster",
    "abstract": "Deep models have achieved great success on a variety of challenging tasks. How- ever, the models that achieve great performance often have an enormous number of parameters, leading to correspondingly great demands on both computational and memory resources, especially for fully-connected layers. In this work, we propose a new \"density-diversity penalty\" regularizer that can be applied to fully-connected layers of neural networks during training. We show that using this regularizer results in significantly fewer parameters (i.e., high sparsity), and also significantly fewer distinct values (i.e., low diversity), so that the trained weight matrices can be highly compressed without any appreciable loss in performance. The resulting trained models can hence reside on computational platforms (e.g., portables, Internet-of-Things devices) where it otherwise would be prohibitive",
    "checked": true,
    "id": "9f6422dbb45f1bb183da514517f224b7a6e79f87",
    "semantic_title": "training compressed fully-connected networks with a density-diversity penalty",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk-oDY9ge": {
    "title": "Diet Networks: Thin Parameters for Fat Genomics",
    "volume": "poster",
    "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier",
    "checked": true,
    "id": "a36d5dee672e66b5f4db9e9df7498ef89b4b9cf5",
    "semantic_title": "diet networks: thin parameters for fat genomic",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=BJhZeLsxx": {
    "title": "What does it take to generate natural textures?",
    "volume": "poster",
    "abstract": "Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, e.g. for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis and show that none of the above aspects are indispensable. Instead, we demonstrate that natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters",
    "checked": true,
    "id": "abdd228205f0878ee0a426d8ef94ab15db4a07e8",
    "semantic_title": "what does it take to generate natural textures?",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=rkFBJv9gg": {
    "title": "Learning Features of Music From Scratch",
    "volume": "poster",
    "abstract": "This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio",
    "checked": true,
    "id": "f6154535699c65633243c482d2b97d4b66036633",
    "semantic_title": "learning features of music from scratch",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=SJvYgH9xe": {
    "title": "Automatic Rule Extraction from Long Short Term Memory Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57b9ec1b2c4ff17baed4ecca932880fd13eb5d19",
    "semantic_title": "automatic rule extraction from long short term memory networks",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk8TGSKlg": {
    "title": "Reasoning with Memory Augmented Neural Networks for Language Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "24021461b49c726606bb9acdedd05cea5277c491",
    "semantic_title": "reasoning with memory augmented neural networks for language comprehension",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=HJpfMIFll": {
    "title": "Geometry of Polysemy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57a94c641412a39d4caa6b1a080ed3dae7ea75c5",
    "semantic_title": "geometry of polysemy",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk2Im59ex": {
    "title": "Unsupervised Cross-Domain Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4ee64022cc3ccd14c7f9d4935c59b16456067d3",
    "semantic_title": "unsupervised cross-domain image generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyq4yhile": {
    "title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f710932a0aa99f2c2ddbec6e7765f10c48d3fc2",
    "semantic_title": "learning invariant feature spaces to transfer skills with reinforcement learning",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=B1YfAfcgl": {
    "title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6583fe9c9dc52bb129aff4cefc60519349f3b4c",
    "semantic_title": "entropy-sgd: biasing gradient descent into wide valleys",
    "citation_count": 784,
    "authors": []
  },
  "https://openreview.net/forum?id=Sks9_ajex": {
    "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f7b032a4df721d4ed2bab97f6acd33d62477b7a5",
    "semantic_title": "paying more attention to attention: improving the performance of convolutional neural networks via attention transfer",
    "citation_count": 2627,
    "authors": []
  },
  "https://openreview.net/forum?id=S1TER2oll": {
    "title": "FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ef8ba551c025747333382bb627f305b540543dc",
    "semantic_title": "filter shaping for convolutional neural networks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=BydrOIcle": {
    "title": "Unrolled Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "488bb25e0b1777847f04c943e6dbc4f84415b712",
    "semantic_title": "unrolled generative adversarial networks",
    "citation_count": 1016,
    "authors": []
  },
  "https://openreview.net/forum?id=SJDaqqveg": {
    "title": "An Actor-Critic Algorithm for Sequence Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d24a0695c9fc669e643bad51d4e14f056329dec",
    "semantic_title": "an actor-critic algorithm for sequence prediction",
    "citation_count": 643,
    "authors": []
  },
  "https://openreview.net/forum?id=BJAFbaolg": {
    "title": "Learning to Generate Samples from Noise through Infusion Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d2860bb05f747e4628e95e4d84018263831bab0d",
    "semantic_title": "learning to generate samples from noise through infusion training",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy6b4Pqee": {
    "title": "Deep Probabilistic Programming",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8e9532262bb862c62d2d011a93da014da2d97ee9",
    "semantic_title": "deep probabilistic programming",
    "citation_count": 194,
    "authors": []
  },
  "https://openreview.net/forum?id=r1G4z8cge": {
    "title": "Mollifying Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b9728a8279c12f93fff089b6fac96afd2d3bab04",
    "semantic_title": "mollifying networks",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=ry_sjFqgx": {
    "title": "Program Synthesis for Character Level Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "de7e3537d974c2ca28e3ca130d531e5eb23eb79f",
    "semantic_title": "program synthesis for character level language modeling",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxB0Rtxx": {
    "title": "Identity Matters in Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8fbb115c578e8bfbcc1615bd7af990396abf6776",
    "semantic_title": "identity matters in deep learning",
    "citation_count": 401,
    "authors": []
  },
  "https://openreview.net/forum?id=r1rhWnZkg": {
    "title": "Hadamard Product for Low-rank Bilinear Pooling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "f05cea9192e2e584c2efdefcfd8fe2208560bfb0",
    "semantic_title": "mett vii  7 th workshop on matrix equations and tensor techniques pisa , 13  14 feb 2017",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkVsEMYel": {
    "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "989e261e232c51cc48aa07064c573316f9f818c3",
    "semantic_title": "inductive bias of deep convolutional networks through pooling geometry",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=rJqFGTslg": {
    "title": "Pruning Filters for Efficient ConvNets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
    "semantic_title": "pruning filters for efficient convnets",
    "citation_count": 3750,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxKPDv5xl": {
    "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e221e2c2ca8bd74a7b818406c8a2a342760e7d65",
    "semantic_title": "samplernn: an unconditional end-to-end neural audio generation model",
    "citation_count": 599,
    "authors": []
  },
  "https://openreview.net/forum?id=SkhU2fcll": {
    "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "468a80bcd4ff9b3f47beb9145ff81140777bb3f3",
    "semantic_title": "deep multi-task representation learning: a tensor factorisation approach",
    "citation_count": 255,
    "authors": []
  },
  "https://openreview.net/forum?id=SyWvgP5el": {
    "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9228fa3b363229780da4cb1d258942e0c13c2947",
    "semantic_title": "epopt: learning robust neural network policies using model ensembles",
    "citation_count": 357,
    "authors": []
  },
  "https://openreview.net/forum?id=ryuxYmvel": {
    "title": "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1318d15d7d8ab626b92d0c70dbdc5b5d37e223f",
    "semantic_title": "holstep: a machine learning dataset for higher-order logic theorem proving",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=HJDBUF5le": {
    "title": "Towards a Neural Statistician",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "405c31c85a324942811f3c9dc53ce3528f9284df",
    "semantic_title": "towards a neural statistician",
    "citation_count": 428,
    "authors": []
  },
  "https://openreview.net/forum?id=BylSPv9gx": {
    "title": "Exploring Sparsity in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1cc2f313bcb3b106af081f7031b924c9ad2662bd",
    "semantic_title": "exploring sparsity in recurrent neural networks",
    "citation_count": 313,
    "authors": []
  },
  "https://openreview.net/forum?id=Skq89Scxx": {
    "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b022f2a277a4bf5f42382e86e4380b96340b9e86",
    "semantic_title": "sgdr: stochastic gradient descent with warm restarts",
    "citation_count": 8405,
    "authors": []
  },
  "https://openreview.net/forum?id=rJEgeXFex": {
    "title": "Predicting Medications from Diagnostic Codes with Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e6c752c47d96ff83f44109a4cabd85c8294a37e",
    "semantic_title": "predicting medications from diagnostic codes with recurrent neural networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ByqiJIqxg": {
    "title": "Online Bayesian Transfer Learning for Sequential Data Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20be936439c626c11611ce62771e8b8ba3c797f9",
    "semantic_title": "online bayesian transfer learning for sequential data modeling",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=BJh6Ztuxl": {
    "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e44da7d8c71edcc6e575fa7faadd5e75785a7901",
    "semantic_title": "fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
    "citation_count": 549,
    "authors": []
  },
  "https://openreview.net/forum?id=rJTKKKqeg": {
    "title": "Tracking the World State with Recurrent Entity Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "033dd6cf61a6017e9aa9b46068d3c89082849cf3",
    "semantic_title": "tracking the world state with recurrent entity networks",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=Byj72udxe": {
    "title": "Pointer Sentinel Mixture Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "efbd381493bb9636f489b965a2034d529cd56bcd",
    "semantic_title": "pointer sentinel mixture models",
    "citation_count": 2997,
    "authors": []
  },
  "https://openreview.net/forum?id=H12GRgcxg": {
    "title": "Training deep neural-networks using a noise adaptation layer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc550ee45f4194f86c52152c10d302965c3563ca",
    "semantic_title": "training deep neural-networks using a noise adaptation layer",
    "citation_count": 738,
    "authors": []
  },
  "https://openreview.net/forum?id=HyM25Mqel": {
    "title": "Sample Efficient Actor-Critic with Experience Replay",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6a43d91c8d883e3463b358571125fa0ec7298b3a",
    "semantic_title": "sample efficient actor-critic with experience replay",
    "citation_count": 765,
    "authors": []
  },
  "https://openreview.net/forum?id=SJkXfE5xx": {
    "title": "Revisiting Classifier Two-Sample Tests",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cfec1e2e670bc5de1a0621f66eaed7abb56387f3",
    "semantic_title": "revisiting classifier two-sample tests",
    "citation_count": 417,
    "authors": []
  },
  "https://openreview.net/forum?id=BJrFC6ceg": {
    "title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2e77b99e8bd10b9e4551a780c0bde9dd10fdbe9b",
    "semantic_title": "pixelcnn++: improving the pixelcnn with discretized logistic mixture likelihood and other modifications",
    "citation_count": 953,
    "authors": []
  },
  "https://openreview.net/forum?id=rkE3y85ee": {
    "title": "Categorical Reparameterization with Gumbel-Softmax",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29e944711a354c396fad71936f536e83025b6ce0",
    "semantic_title": "categorical reparameterization with gumbel-softmax",
    "citation_count": 5494,
    "authors": []
  },
  "https://openreview.net/forum?id=SkB-_mcel": {
    "title": "Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01dc0a157e355ddc34a426f121fc871601fda567",
    "semantic_title": "central moment discrepancy (cmd) for domain-invariant representation learning",
    "citation_count": 590,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOvsIqeg": {
    "title": "Regularizing CNNs with Locally Constrained Decorrelations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2ed2415c39d4023f9af410947b96970803d9f192",
    "semantic_title": "regularizing cnns with locally constrained decorrelations",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ewdt9xe": {
    "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad367b44f3434b9ba6b46b41ab083210f6827a9f",
    "semantic_title": "deep predictive coding networks for video prediction and unsupervised learning",
    "citation_count": 937,
    "authors": []
  },
  "https://openreview.net/forum?id=S1jmAotxg": {
    "title": "Stick-Breaking Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cdc9b1759ee485e34045dbe8de947967d259d191",
    "semantic_title": "stick-breaking variational autoencoders",
    "citation_count": 164,
    "authors": []
  },
  "https://openreview.net/forum?id=SkpSlKIel": {
    "title": "Why Deep Neural Networks for Function Approximation?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "87524c2bb725f967b7fac73442d6412b43d422d1",
    "semantic_title": "why deep neural networks for function approximation?",
    "citation_count": 386,
    "authors": []
  },
  "https://openreview.net/forum?id=H1acq85gx": {
    "title": "Maximum Entropy Flow Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b88e7d3ecbc62baaf3959fb62702e68790e2bfa6",
    "semantic_title": "maximum entropy flow networks",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=SyQq185lg": {
    "title": "Latent Sequence Decompositions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "105788dd22393d5a4333c167814ec3d38c7d6612",
    "semantic_title": "latent sequence decompositions",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=HJWHIKqgl": {
    "title": "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d772a21f2f687985b41bec2dc47bc2760c57ed2f",
    "semantic_title": "generative models and model criticism via optimized maximum mean discrepancy",
    "citation_count": 263,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Igu2ogg": {
    "title": "Efficient Vector Representation for Documents through Corruption",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1da8eabfd02436dda5f3131fe404dc840a04e6cc",
    "semantic_title": "efficient vector representation for documents through corruption",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=ByldLrqlx": {
    "title": "DeepCoder: Learning to Write Programs",
    "volume": "poster",
    "abstract": "We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites",
    "checked": true,
    "id": "8a25c9403d8a0e2fb8ca362a1b26262afd57417f",
    "semantic_title": "deepcoder: learning to write programs",
    "citation_count": 577,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzCSf9xg": {
    "title": "On Detecting Adversarial Perturbations",
    "volume": "poster",
    "abstract": "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small ``detector'' subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack",
    "checked": true,
    "id": "061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a",
    "semantic_title": "on detecting adversarial perturbations",
    "citation_count": 955,
    "authors": []
  },
  "https://openreview.net/forum?id=SJTQLdqlg": {
    "title": "Learning to Remember Rare Events",
    "volume": "poster",
    "abstract": "Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task",
    "checked": true,
    "id": "29092f0deaac3898e43b3f094bf15d82b6a99afd",
    "semantic_title": "learning to remember rare events",
    "citation_count": 366,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VyHY9gg": {
    "title": "Data Noising as Smoothing in Neural Network Language Models",
    "volume": "poster",
    "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing",
    "checked": true,
    "id": "2d5069a99bfa0b47c095bbb5cefd6dba974f72a7",
    "semantic_title": "data noising as smoothing in neural network language models",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=S1_pAu9xl": {
    "title": "Trained Ternary Quantization",
    "volume": "poster",
    "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it's as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%",
    "checked": true,
    "id": "d418295cd3027c43eccc5592ae5b8303ba8192be",
    "semantic_title": "trained ternary quantization",
    "citation_count": 1037,
    "authors": []
  },
  "https://openreview.net/forum?id=r10FA8Kxg": {
    "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "def52f9e3650ec4e9cc18b3e316f48e9bf046adf",
    "semantic_title": "do deep convolutional nets really need to be deep and convolutional?",
    "citation_count": 246,
    "authors": []
  },
  "https://openreview.net/forum?id=BysvGP5ee": {
    "title": "Variational Lossy Autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "590c9a1ff422b03477f7830b20609f212c85aa13",
    "semantic_title": "variational lossy autoencoder",
    "citation_count": 679,
    "authors": []
  },
  "https://openreview.net/forum?id=Byk-VI9eg": {
    "title": "Generative Multi-Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "423fab20c1c39cb6b4d6cf6098f61c092e6d1e3f",
    "semantic_title": "generative multi-adversarial networks",
    "citation_count": 346,
    "authors": []
  },
  "https://openreview.net/forum?id=HyQJ-mclg": {
    "title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "407ead18083e68626e82e07db1a9289ff0b7e862",
    "semantic_title": "incremental network quantization: towards lossless cnns with low-precision weights",
    "citation_count": 1061,
    "authors": []
  },
  "https://openreview.net/forum?id=HkEI22jeg": {
    "title": "Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f8bed08cef7b777da9273f483461ebf4f2e0c63",
    "semantic_title": "multilayer recurrent network models of primate retinal ganglion cell responses",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkab5dqxe": {
    "title": "A Compositional Object-Based Approach to Learning Physical Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a1786540a4e15f0757e1b84a02f98ed436a969e0",
    "semantic_title": "a compositional object-based approach to learning physical dynamics",
    "citation_count": 441,
    "authors": []
  },
  "https://openreview.net/forum?id=rkEFLFqee": {
    "title": "Decomposing Motion and Content for Natural Video Sequence Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8375ff50b8a6f1a10dd809129a18df96888ac8b",
    "semantic_title": "decomposing motion and content for natural video sequence prediction",
    "citation_count": 601,
    "authors": []
  },
  "https://openreview.net/forum?id=BybtVK9lg": {
    "title": "Autoencoding Variational Inference For Topic Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0c25e5f544167840756950e0ff3edd238a19cfd1",
    "semantic_title": "autoencoding variational inference for topic models",
    "citation_count": 568,
    "authors": []
  },
  "https://openreview.net/forum?id=Sys6GJqxl": {
    "title": "Delving into Transferable Adversarial Examples and Black-box Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99e5a8c10cf92749d4a7c2949691c3a6046e499a",
    "semantic_title": "delving into transferable adversarial examples and black-box attacks",
    "citation_count": 1761,
    "authors": []
  },
  "https://openreview.net/forum?id=ry4Vrt5gl": {
    "title": "Learning to Optimize",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ead9a671428631e44f6fe49324efe69da628bc47",
    "semantic_title": "learning to optimize",
    "citation_count": 258,
    "authors": []
  },
  "https://openreview.net/forum?id=SJGPL9Dex": {
    "title": "Understanding Trainable Sparse Coding with Matrix Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9459d92b99220d2bfe8fc530410275d65b71dcd2",
    "semantic_title": "understanding trainable sparse coding via matrix factorization",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJKxrsgl": {
    "title": "Emergence of foveal image sampling from learning to attend in visual scenes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d49a7ad76e779df812bda5c4b037a0b9d6847be",
    "semantic_title": "emergence of foveal image sampling from learning to attend in visual scenes",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=ryT4pvqll": {
    "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9caf76734a611286513b478ce10557bda477e8ce",
    "semantic_title": "improving policy gradient by exploring under-appreciated rewards",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ0-tY5xe": {
    "title": "Learning to Query, Reason, and Answer Questions On Ambiguous Texts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf3db68c31e9850d6039bc2436314faef93973d3",
    "semantic_title": "learning to query, reason, and answer questions on ambiguous texts",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ0JwFcex": {
    "title": "Neuro-Symbolic Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "644ca74f80463415613847ab01cff067fb58f0ad",
    "semantic_title": "neuro-symbolic program synthesis",
    "citation_count": 322,
    "authors": []
  },
  "https://openreview.net/forum?id=HyoST_9xl": {
    "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "950619635df80e87c6f25b486cc5eaad4d71d0b0",
    "semantic_title": "dsd: dense-sparse-dense training for deep neural networks",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=BJKYvt5lg": {
    "title": "PixelVAE: A Latent Variable Model for Natural Images",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "42e9055ec712ec9c7f0a79d963ea034a72dc7fa8",
    "semantic_title": "pixelvae: a latent variable model for natural images",
    "citation_count": 341,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk95PK9le": {
    "title": "Deep Biaffine Attention for Neural Dependency Parsing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2",
    "semantic_title": "deep biaffine attention for neural dependency parsing",
    "citation_count": 1226,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ8uNptgl": {
    "title": "Towards the Limit of Network Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d749712ef4f02a87f4e362058469a1da46c8bcc",
    "semantic_title": "towards the limit of network quantization",
    "citation_count": 195,
    "authors": []
  },
  "https://openreview.net/forum?id=B1oK8aoxe": {
    "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3deecaee4ec1a37de3cb10420eaabff067669e17",
    "semantic_title": "stochastic neural networks for hierarchical reinforcement learning",
    "citation_count": 361,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeKjwvclx": {
    "title": "Dynamic Coattention Networks For Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e978d832a4d86571e1b52aa1685dc32ccb250f50",
    "semantic_title": "dynamic coattention networks for question answering",
    "citation_count": 685,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg4TI9xl": {
    "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ff2a434578ff2746b9283e45abf296887f48a2d",
    "semantic_title": "a baseline for detecting misclassified and out-of-distribution examples in neural networks",
    "citation_count": 3537,
    "authors": []
  },
  "https://openreview.net/forum?id=BJtNZAFgg": {
    "title": "Adversarial Feature Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1db6e3078597386ac4222ba6c3f4f61b61f53539",
    "semantic_title": "adversarial feature learning",
    "citation_count": 1838,
    "authors": []
  },
  "https://openreview.net/forum?id=rkGabzZgl": {
    "title": "Dropout with Expectation-linear Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "437da3d1024f7312fde8a5287c99c36b559ed8f3",
    "semantic_title": "dropout with expectation-linear regularization",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=rJPcZ3txx": {
    "title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fe9bc9944984cbc6f9f12e99c1065a47d662e24c",
    "semantic_title": "faster cnns with direct sparse convolutions and guided pruning",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=SkkTMpjex": {
    "title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0c7a93498360ce6c0ff1a7c37fd9b6aa0c054f85",
    "semantic_title": "distributed second-order optimization using kronecker-factored approximations",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=rJbbOLcex": {
    "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7ab2166f6cdb1737e000df66d29c6538afc6811d",
    "semantic_title": "topicrnn: a recurrent neural network with long-range semantic dependency",
    "citation_count": 245,
    "authors": []
  },
  "https://openreview.net/forum?id=B1M8JF9xx": {
    "title": "On the Quantitative Analysis of Decoder-Based Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "858dc7408c27702ec42778599fc8d11f73ef3f76",
    "semantic_title": "on the quantitative analysis of decoder-based generative models",
    "citation_count": 224,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Aab85gg": {
    "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "02e45b3472fa4c879f8e9ca0dbb283c774a338cb",
    "semantic_title": "offline bilingual word vectors, orthogonal transformations and the inverted softmax",
    "citation_count": 539,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy6iJDqlx": {
    "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8bada64d0b04a66165b464a51c29f61e1f474f2a",
    "semantic_title": "attend, adapt and transfer: attentive deep architecture for adaptive transfer from multiple sources in the same domain",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=rkE8pVcle": {
    "title": "Learning through Dialogue Interactions by Asking Questions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2685cab753e4c1e9ac6e68b28ad8e2e1f3673f73",
    "semantic_title": "learning through dialogue interactions",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVVJ85lg": {
    "title": "Paleo: A Performance Model for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "36348af997c8ff309fabe0561487ae97cf317ef4",
    "semantic_title": "paleo: a performance model for deep neural networks",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=BJO-BuT1g": {
    "title": "A Learned Representation For Artistic Style",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99542f614d7e4146cad17196e76c997e57a69e4d",
    "semantic_title": "a learned representation for artistic style",
    "citation_count": 1172,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ5UeU9xx": {
    "title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29069976eb7f828de91ed243cd12fd99fef56d94",
    "semantic_title": "visualizing deep neural network decisions: prediction difference analysis",
    "citation_count": 713,
    "authors": []
  },
  "https://openreview.net/forum?id=r1aPbsFle": {
    "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "424aef7340ee618132cc3314669400e23ad910ba",
    "semantic_title": "tying word vectors and word classifiers: a loss framework for language modeling",
    "citation_count": 386,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgXCV9xx": {
    "title": "Dialogue Learning With Human-in-the-Loop",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3d3b4ec7789f634e0752d50484dad7d2ea2460d5",
    "semantic_title": "dialogue learning with human-in-the-loop",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ0UKP9ge": {
    "title": "Bidirectional Attention Flow for Machine Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
    "semantic_title": "bidirectional attention flow for machine comprehension",
    "citation_count": 2095,
    "authors": []
  },
  "https://openreview.net/forum?id=Skvgqgqxe": {
    "title": "Learning to Compose Words into Sentences with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "599f7863721d542dcef2da49b41d82b21e4f80b3",
    "semantic_title": "learning to compose words into sentences with reinforcement learning",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=BJK3Xasel": {
    "title": "Nonparametric Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46357ea4ad096e0674569efa29d3e34006786d04",
    "semantic_title": "nonparametric neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ6oOfqge": {
    "title": "Temporal Ensembling for Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d2e4587744a89bad95fea69e08842cad6c8ff0dd",
    "semantic_title": "temporal ensembling for semi-supervised learning",
    "citation_count": 2587,
    "authors": []
  },
  "https://openreview.net/forum?id=SJU4ayYgl": {
    "title": "Semi-Supervised Classification with Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "36eff562f65125511b5dfab68ce7f7a943c27478",
    "semantic_title": "semi-supervised classification with graph convolutional networks",
    "citation_count": 29682,
    "authors": []
  },
  "https://openreview.net/forum?id=By5e2L9gl": {
    "title": "Trusting SVM for Piecewise Linear CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cd4cd2ca053d8b7d5becbf4abf90a62a9ba80c1c",
    "semantic_title": "trusting svm for piecewise linear cnns",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Byiy-Pqlx": {
    "title": "Lie-Access Neural Turing Machines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15778857f5b4de361dffa3fd156a269184dd2f9e",
    "semantic_title": "lie-access neural turing machines",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=r1X3g2_xl": {
    "title": "Adversarial Training Methods for Semi-Supervised Text Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2cd55ded95d5d13430edfa223ba591b514ebe8a5",
    "semantic_title": "adversarial training methods for semi-supervised text classification",
    "citation_count": 1066,
    "authors": []
  },
  "https://openreview.net/forum?id=B1kJ6H9ex": {
    "title": "Combining policy gradient and Q-learning",
    "volume": "poster",
    "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning",
    "checked": true,
    "id": "c40dd8f235aabe6efbb93c59c0536adf491f9ead",
    "semantic_title": "pgq: combining policy gradient and q-learning",
    "citation_count": 140,
    "authors": []
  }
}