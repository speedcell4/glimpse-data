{
  "https://openreview.net/forum?id=BkbY4psgg": {
    "title": "Making Neural Programming Architectures Generalize via Recursion",
    "volume": "oral",
    "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system's behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion",
    "checked": true,
    "id": "6b024162f81e8ff7aa34c3a43d601a912d012c78",
    "semantic_title": "making neural programming architectures generalize via recursion",
    "citation_count": 147,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4_qw5xe": {
    "title": "Towards Principled Methods for Training Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them",
    "checked": true,
    "id": "9a700c7a7e7468e436f00c34551fbe3e0f70e42f",
    "semantic_title": "towards principled methods for training generative adversarial networks",
    "citation_count": 2125,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Bb3D5gg": {
    "title": "Learning End-to-End Goal-Oriented Dialog",
    "volume": "oral",
    "abstract": "Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End- to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service",
    "checked": true,
    "id": "f81be44000814e7bcb12ae04b4e2d9c01b6515b3",
    "semantic_title": "learning end-to-end goal-oriented dialog",
    "citation_count": 781,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Ue8Hcxg": {
    "title": "Neural Architecture Search with Reinforcement Learning",
    "volume": "oral",
    "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214",
    "checked": true,
    "id": "67d968c7450878190e45ac7886746de867bf673d",
    "semantic_title": "neural architecture search with reinforcement learning",
    "citation_count": 5442,
    "authors": []
  },
  "https://openreview.net/forum?id=S1RP6GLle": {
    "title": "Amortised MAP Inference for Image Super-resolution",
    "volume": "oral",
    "abstract": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders",
    "checked": true,
    "id": "3c7092347a5b7804d9534b6f3f52032c1502cbf8",
    "semantic_title": "amortised map inference for image super-resolution",
    "citation_count": 435,
    "authors": []
  },
  "https://openreview.net/forum?id=H1oyRlYgg": {
    "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
    "volume": "oral",
    "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions---and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap",
    "checked": true,
    "id": "8ec5896b4490c6e127d1718ffc36a3439d84cb81",
    "semantic_title": "on large-batch training for deep learning: generalization gap and sharp minima",
    "citation_count": 2982,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwoSDPgg": {
    "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data",
    "volume": "oral",
    "abstract": "Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ''teachers'' for a ''student'' model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning",
    "checked": true,
    "id": "e70b9a38fcf8373865dd6e7b45e45cca7ff2eaa9",
    "semantic_title": "semi-supervised knowledge transfer for deep learning from private training data",
    "citation_count": 1029,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ0NvFzxl": {
    "title": "Learning Graphical State Transitions",
    "volume": "oral",
    "abstract": "Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that produces sequences from graph-structured input. In this work I introduce the Gated Graph Transformer Neural Network (GGT-NN), an extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines",
    "checked": true,
    "id": "edbc873a248768a626ef2bc57b3d1eff30de0e11",
    "semantic_title": "learning graphical state transitions",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk8N3Sclg": {
    "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language",
    "volume": "oral",
    "abstract": "The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are in- terested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communi- cation. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message to the receiver, while the receiver must rely on it to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore whether the \"word meanings\" induced in the game reflect intuitive semantic properties of the objects depicted in the image, and we present a simple strategy for grounding the agents' code into natural language, a necessary step in developing machines that should eventually be able to communicate with humans",
    "checked": true,
    "id": "f91d1db0fa2ac12e0262a8f5e6a371da29a3c100",
    "semantic_title": "multi-agent cooperation and the emergence of (natural) language",
    "citation_count": 439,
    "authors": []
  },
  "https://openreview.net/forum?id=rJY0-Kcll": {
    "title": "Optimization as a Model for Few-Shot Learning",
    "volume": "oral",
    "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning",
    "checked": true,
    "id": "29c887794eed2ca9462638ff853e6fe1ab91d5d8",
    "semantic_title": "optimization as a model for few-shot learning",
    "citation_count": 3433,
    "authors": []
  },
  "https://openreview.net/forum?id=rJLS7qKel": {
    "title": "Learning to Act by Predicting the Future",
    "volume": "oral",
    "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments",
    "checked": true,
    "id": "4c25f50c7451fa72c562e21e3b11e416b11f74c8",
    "semantic_title": "learning to act by predicting the future",
    "citation_count": 282,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxdQ3jeg": {
    "title": "End-to-end Optimized Image Compression",
    "volume": "oral",
    "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM",
    "checked": true,
    "id": "232148b97bd0543613ffd98fb4edcff79434ce1a",
    "semantic_title": "end-to-end optimized image compression",
    "citation_count": 1753,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ3rcZcxl": {
    "title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic",
    "volume": "oral",
    "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments",
    "checked": true,
    "id": "524513b6f4ddca331c33bcc70a9f677fa240cfa3",
    "semantic_title": "q-prop: sample-efficient policy gradient with an off-policy critic",
    "citation_count": 346,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy8gdB9xx": {
    "title": "Understanding deep learning requires rethinking generalization",
    "volume": "oral",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models",
    "checked": true,
    "id": "54ddb00fa691728944fd8becea90a373d21597cf",
    "semantic_title": "understanding deep learning requires rethinking generalization",
    "citation_count": 4667,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ6yPD5xg": {
    "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
    "volume": "oral",
    "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth",
    "checked": true,
    "id": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
    "semantic_title": "reinforcement learning with unsupervised auxiliary tasks",
    "citation_count": 1238,
    "authors": []
  },
  "https://openreview.net/forum?id=S1VaB4cex": {
    "title": "FractalNet: Ultra-Deep Neural Networks without Residuals",
    "volume": "poster",
    "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer",
    "checked": true,
    "id": "d0156126edbfc524c8d108bdc0cf811cfe3129aa",
    "semantic_title": "fractalnet: ultra-deep neural networks without residuals",
    "citation_count": 949,
    "authors": []
  },
  "https://openreview.net/forum?id=H1W1UN9gg": {
    "title": "Deep Information Propagation",
    "volume": "poster",
    "abstract": "We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively",
    "checked": true,
    "id": "4fdc7df2c737141a1bf5aec27a438b77d01f8af0",
    "semantic_title": "deep information propagation",
    "citation_count": 372,
    "authors": []
  },
  "https://openreview.net/forum?id=SJGCiw5gl": {
    "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference",
    "volume": "poster",
    "abstract": "We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation-a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach",
    "checked": true,
    "id": "3db8730c203f88d7f08a6a99e8c02a077dc9b011",
    "semantic_title": "pruning convolutional neural networks for resource efficient inference",
    "citation_count": 1993,
    "authors": []
  },
  "https://openreview.net/forum?id=r1VdcHcxx": {
    "title": "Recurrent Batch Normalization",
    "volume": "poster",
    "abstract": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization",
    "checked": true,
    "id": "952454718139dba3aafc6b3b67c4f514ac3964af",
    "semantic_title": "recurrent batch normalization",
    "citation_count": 411,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy2fzU9gl": {
    "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
    "volume": "poster",
    "abstract": "Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data",
    "checked": true,
    "id": "6f7af4709e399e89ba898efc3459cb844fa0e981",
    "semantic_title": "beta-vae: learning basic visual concepts with a constrained variational framework",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hdzd5lg": {
    "title": "Words or Characters? Fine-grained Gating for Reading Comprehension",
    "volume": "poster",
    "abstract": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test and Who Did What datasets. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task",
    "checked": true,
    "id": "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239",
    "semantic_title": "words or characters? fine-grained gating for reading comprehension",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=Bks8cPcxe": {
    "title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning",
    "volume": "poster",
    "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications. In this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides (1) intuitive constructs to support compact encoding of deep networks; (2) symbolic gradient derivation of the networks; (3) static analysis for memory consumption and error detection; and (4) DSL-level optimization to improve memory and runtime efficiency. DeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries",
    "checked": true,
    "id": "8902ce81d33571d62445e6152aac1d03a426049a",
    "semantic_title": "deepdsl: a compilation-based domain-specific language for deep learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=rkpACe1lx": {
    "title": "HyperNetworks",
    "volume": "poster",
    "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network. We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks",
    "checked": true,
    "id": "563783de03452683a9206e85fe6d661714436686",
    "semantic_title": "hypernetworks",
    "citation_count": 1653,
    "authors": []
  },
  "https://openreview.net/forum?id=BydARw9ex": {
    "title": "Capacity and Trainability in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures",
    "checked": true,
    "id": "401d68e1a930b0f7e02030cab4c185fb1839cb11",
    "semantic_title": "capacity and trainability in recurrent neural networks",
    "citation_count": 205,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGODLqgx": {
    "title": "Recurrent Hidden Semi-Markov Model",
    "volume": "poster",
    "abstract": "Segmentation and labeling of high dimensional time series data has wide applications in behavior understanding and medical diagnosis. Due to the difficulty in obtaining the label information for high dimensional data, realizing this objective in an unsupervised way is highly desirable. Hidden Semi-Markov Model (HSMM) is a classical tool for this problem. However, existing HSMM and its variants has simple conditional assumptions of observations, thus the ability to capture the nonlinear and complex dynamics within segments is limited. To tackle this limitation, we propose to incorporate the Recurrent Neural Network (RNN) to model the generative process in HSMM, resulting the Recurrent HSMM (R-HSMM). To accelerate the inference while preserving accuracy, we designed a structure encoding function to mimic the exact inference. By generalizing the penalty method to distribution space, we are able to train the model and the encoding function simultaneously. Empirical results show that the proposed R-HSMM achieves the state-of-the-art performances on both synthetic and real-world datasets",
    "checked": true,
    "id": "e20d94e43a20315f76067a00d67e4f7523ca7311",
    "semantic_title": "recurrent hidden semi-markov model",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=S11KBYclx": {
    "title": "Learning Curve Prediction with Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "925cab60b1795c94ae6f488fda7ad71be71b5822",
    "semantic_title": "learning curve prediction with bayesian neural networks",
    "citation_count": 260,
    "authors": []
  },
  "https://openreview.net/forum?id=SyK00v5xx": {
    "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3f1802d3f4f5f6d66875dac09112f978f12e1e1e",
    "semantic_title": "a simple but tough-to-beat baseline for sentence embeddings",
    "citation_count": 1322,
    "authors": []
  },
  "https://openreview.net/forum?id=B1GOWV5eg": {
    "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2d6075ffa9ef3fbfe338bfeb469a50883242bdcd",
    "semantic_title": "learning to repeat: fine grained action repetition for deep reinforcement learning",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=B184E5qee": {
    "title": "Improving Neural Language Models with a Continuous Cache",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2d7782c225e0fc123d6e227f2cb253e58279ac73",
    "semantic_title": "improving neural language models with a continuous cache",
    "citation_count": 302,
    "authors": []
  },
  "https://openreview.net/forum?id=BJYwwY9ll": {
    "title": "Snapshot Ensembles: Train 1, Get M for Free",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b134d0911e2e13ac169ffa5f478a39e6ef77869a",
    "semantic_title": "snapshot ensembles: train 1, get m for free",
    "citation_count": 964,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGwcKclx": {
    "title": "Soft Weight-Sharing for Neural Network Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac37a459c46a464ac1326c821ba7d5595b50f1af",
    "semantic_title": "soft weight-sharing for neural network compression",
    "citation_count": 422,
    "authors": []
  },
  "https://openreview.net/forum?id=r1nTpv9eg": {
    "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "079021d93a88c6a4721af75397d14c2125af1f26",
    "semantic_title": "learning to perform physics experiments via deep reinforcement learning",
    "citation_count": 348,
    "authors": []
  },
  "https://openreview.net/forum?id=B1MRcPclx": {
    "title": "Query-Reduction Networks for Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4bf7edee5a4c4cfdbdd43a607c402420129fa277",
    "semantic_title": "query-reduction networks for question answering",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=BJm4T4Kgx": {
    "title": "Adversarial Machine Learning at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2a85a6766b982ff7c8980e57ca6342d22493827",
    "semantic_title": "adversarial machine learning at scale",
    "citation_count": 3170,
    "authors": []
  },
  "https://openreview.net/forum?id=rk9eAFcxg": {
    "title": "Variational Recurrent Adversarial Deep Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9c4f30fe94ce07a89eb9b1789b338064d5c44811",
    "semantic_title": "variational recurrent adversarial deep domain adaptation",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=ryMxXPFex": {
    "title": "Discrete Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "711683de61ee8d2aa929ec4d5d97d616b8e281d3",
    "semantic_title": "discrete variational autoencoders",
    "citation_count": 263,
    "authors": []
  },
  "https://openreview.net/forum?id=r1fYuytex": {
    "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "138ea97004fe7586977b4eb72835df615319c2df",
    "semantic_title": "sparsely-connected neural networks: towards efficient vlsi implementation of deep neural networks",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=SJRpRfKxx": {
    "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f3158ce4e1c9e908e7d06533d711d84205c973b9",
    "semantic_title": "recurrent mixture density network for spatiotemporal visual attention",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ3filKll": {
    "title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed9acc61319c2aad04ce25c3bc348b955296bed3",
    "semantic_title": "efficient representation of low-dimensional manifolds using deep networks",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=r1VGvBcxl": {
    "title": "Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "48830e2e4272fa88dc256f1ac9cf81be14112bdb",
    "semantic_title": "reinforcement learning through asynchronous advantage actor-critic on a gpu",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk0FWVcgx": {
    "title": "Topology and Geometry of Half-Rectified Network Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0f94591cc05e6f75c21749d507ef58d204f63b7d",
    "semantic_title": "topology and geometry of half-rectified network optimization",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=HyAbMKwxe": {
    "title": "Tighter bounds lead to improved classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0020a29cd42db351fbfb3c8bb0fd6fa597d67b76",
    "semantic_title": "tighter bounds lead to improved classifiers",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=rJqBEPcxe": {
    "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105",
    "semantic_title": "zoneout: regularizing rnns by randomly preserving hidden activations",
    "citation_count": 318,
    "authors": []
  },
  "https://openreview.net/forum?id=SkYbF1slg": {
    "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4a592e21d52558498b8cd788badee01aac9525d",
    "semantic_title": "an information-theoretic framework for fast and robust unsupervised learning via neural population infomax",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=B16dGcqlx": {
    "title": "Third Person Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2e1a1b9c2e8feeb31c6855292859bf94101e8382",
    "semantic_title": "third-person imitation learning",
    "citation_count": 237,
    "authors": []
  },
  "https://openreview.net/forum?id=HyTqHL5xg": {
    "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6768ed8d51d1eb0d06f9726a42cef2f17a401f65",
    "semantic_title": "deep variational bayes filters: unsupervised learning of state space models from raw data",
    "citation_count": 378,
    "authors": []
  },
  "https://openreview.net/forum?id=BymIbLKgl": {
    "title": "Learning Invariant Representations Of Planar Curves",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc828a99cd936bd4aea2071169504a971a3659da",
    "semantic_title": "learning invariant representations of planar curves",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=HkpbnH9lx": {
    "title": "Density estimation using Real NVP",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09879f7956dddc2a9328f5c1472feeb8402bcbcf",
    "semantic_title": "density estimation using real nvp",
    "citation_count": 3770,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk8BvDqex": {
    "title": "Metacontrol for Adaptive Imagination-Based Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "099cdb087f240352a02286bf9a3e7810c7ebb02b",
    "semantic_title": "metacontrol for adaptive imagination-based optimization",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ25-B5eg": {
    "title": "The Neural Noisy Channel",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d19b712f90cde698cc96ebd5fe291b410e3f0f9c",
    "semantic_title": "the neural noisy channel",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=HJTzHtqee": {
    "title": "A Compare-Aggregate Model for Matching Text Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3f567d3d4975359fadfa9d750e1e4c4722d666c8",
    "semantic_title": "a compare-aggregate model for matching text sequences",
    "citation_count": 277,
    "authors": []
  },
  "https://openreview.net/forum?id=SyxeqhP9ll": {
    "title": "Calibrating Energy-based Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0496691c6d54fd746c111d1004349acf7654145e",
    "semantic_title": "calibrating energy-based generative adversarial networks",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=ry2YOrcge": {
    "title": "Learning a Natural Language Interface with Neural Programmer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8fdf01235a0d4bb8bfbd98327b5c86c199b35e48",
    "semantic_title": "learning a natural language interface with neural programmer",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=r1rz6U5lg": {
    "title": "Learning to superoptimize programs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8567ca5729d9f7af210ae426e8e0ebcb1d8607d3",
    "semantic_title": "learning to superoptimize programs",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=ryelgY5eg": {
    "title": "Optimal Binary Autoencoding with Pairwise Correlations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6a65cd0e80a7b96fd861186f73fc04fc975213e0",
    "semantic_title": "optimal binary autoencoding with pairwise correlations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HJjiFK5gx": {
    "title": "Neural Program Lattices",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15064b7e47b43a68b6661bc7c3eaaad207493191",
    "semantic_title": "neural program lattices",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ElR4cgg": {
    "title": "Adversarially Learned Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
    "semantic_title": "adversarially learned inference",
    "citation_count": 1316,
    "authors": []
  },
  "https://openreview.net/forum?id=ByIAPUcee": {
    "title": "Frustratingly Short Attention Spans in Neural Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58eb3a2f0a67acf2f5c7c2cb4a22852b65314eb5",
    "semantic_title": "frustratingly short attention spans in neural language modeling",
    "citation_count": 112,
    "authors": []
  },
  "https://openreview.net/forum?id=rJQKYt5ll": {
    "title": "Steerable CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "db1207440d20104854464d51da4cf5f79c5de240",
    "semantic_title": "steerable cnns",
    "citation_count": 505,
    "authors": []
  },
  "https://openreview.net/forum?id=rJxDkvqee": {
    "title": "Multi-view Recurrent Neural Acoustic Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e018afcc15257db6ed2cfd3beebd11d11d0e93f",
    "semantic_title": "multi-view recurrent neural acoustic word embeddings",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=ByxpMd9lx": {
    "title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "189e6bb7523733c4e524214b9e6ae92d4ed50dac",
    "semantic_title": "transfer learning for sequence tagging with hierarchical recurrent networks",
    "citation_count": 352,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk3mPK5gg": {
    "title": "Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "add7b8b65355d5408a1ffb93a94b0ae688806bc4",
    "semantic_title": "training agent for first-person shooter game with actor-critic curriculum learning",
    "citation_count": 188,
    "authors": []
  },
  "https://openreview.net/forum?id=rJiNwv9gg": {
    "title": "Lossy Image Compression with Compressive Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "977560251c2bd4c28a6c7c707c29f4091c5e6247",
    "semantic_title": "lossy image compression with compressive autoencoders",
    "citation_count": 1058,
    "authors": []
  },
  "https://openreview.net/forum?id=H1fl8S9ee": {
    "title": "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2af9eaae9191ee522cb97dc57615a071e5403d26",
    "semantic_title": "learning and policy search in stochastic dynamical systems with bayesian neural networks",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNKFiGex": {
    "title": "Neural Photo Editing with Introspective Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1dcda6ac3fd33bf938574aca9542aec5e5cace26",
    "semantic_title": "neural photo editing with introspective adversarial networks",
    "citation_count": 461,
    "authors": []
  },
  "https://openreview.net/forum?id=rJfMusFll": {
    "title": "Batch Policy Gradient Methods for Improving Neural Conversation Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "11732140ada8e05c84e9024927c297565ebd6ad8",
    "semantic_title": "batch policy gradient methods for improving neural conversation models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zJ-v5xl": {
    "title": "Quasi-Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2d876ed1dd2c58058d7197b734a8e4d349b8f231",
    "semantic_title": "quasi-recurrent neural networks",
    "citation_count": 445,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNRsU5ge": {
    "title": "Sigma Delta Quantized Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6fd4b90193efbd0922554672abb52fa1886f607f",
    "semantic_title": "sigma delta quantized networks",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=S1jE5L5gl": {
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "515a21e90117941150923e559729c59f5fdade1c",
    "semantic_title": "the concrete distribution: a continuous relaxation of discrete random variables",
    "citation_count": 2569,
    "authors": []
  },
  "https://openreview.net/forum?id=Skn9Shcxe": {
    "title": "Highway and Residual Networks learn Unrolled Iterative Estimation",
    "volume": "poster",
    "abstract": "The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer. In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of highway and residual networks. Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures",
    "checked": true,
    "id": "535347e07b2ce5ac229349156db1e7bed0486b91",
    "semantic_title": "highway and residual networks learn unrolled iterative estimation",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=HJKkY35le": {
    "title": "Mode Regularized Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution during the early phases of training, thus providing a unified solution to the missing modes problem",
    "checked": true,
    "id": "024d30897e0a2b036bc122163a954b7f1a1d0679",
    "semantic_title": "mode regularized generative adversarial networks",
    "citation_count": 560,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg8bDqee": {
    "title": "Introspection:Accelerating Neural Network Training By Learning Weight Evolution",
    "volume": "poster",
    "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks",
    "checked": true,
    "id": "9bc3eaa4f816d186b9deac9ba980f278f85a1cd8",
    "semantic_title": "introspection: accelerating neural network training by learning weight evolution",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=S1X7nhsxl": {
    "title": "Improving Generative Adversarial Networks with Denoising Feature Matching",
    "volume": "poster",
    "abstract": "We propose an augmented training procedure for generative adversarial networks designed to address shortcomings of the original by directing the generator towards probable configurations of abstract discriminator features. We estimate and track the distribution of these features, as computed from data, with a denoising auto-encoder, and use it to propose high-level targets for the generator. We combine this new loss with the original and evaluate the hybrid criterion on the task of unsupervised image synthesis from datasets comprising a diverse set of visual categories, noting a qualitative and quantitative improvement in the ``objectness'' of the resulting samples",
    "checked": true,
    "id": "6ee38873273aead30c8e561ba807a3032204f870",
    "semantic_title": "improving generative adversarial networks with denoising feature matching",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ckMDqlg": {
    "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
    "volume": "poster",
    "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost",
    "checked": true,
    "id": "510e26733aaff585d65701b9f1be7ca9d5afc586",
    "semantic_title": "outrageously large neural networks: the sparsely-gated mixture-of-experts layer",
    "citation_count": 2809,
    "authors": []
  },
  "https://openreview.net/forum?id=rk5upnsxe": {
    "title": "Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes",
    "volume": "poster",
    "abstract": "Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work well in the recurrent setting. In this paper we propose a unified view of normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework in the context of convolutional neural nets and recurrent neural networks, showing improvements over baselines in image classification, language modeling as well as super-resolution",
    "checked": true,
    "id": "2a7564039c2eb69073aafe4b2363490de5b8e3c3",
    "semantic_title": "normalizing the normalizers: comparing and extending network normalization schemes",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=ryhqQFKgl": {
    "title": "Towards Deep Interpretability (MUS-ROVER II): Learning Hierarchical Representations of Tonal Music",
    "volume": "poster",
    "abstract": "Music theory studies the regularity of patterns in music to capture concepts underlying music styles and composers' decisions. This paper continues the study of building \\emph{automatic theorists} (rovers) to learn and represent music concepts that lead to human interpretable knowledge and further lead to materials for educating people. Our previous work took a first step in algorithmic concept learning of tonal music, studying high-level representations (concepts) of symbolic music (scores) and extracting interpretable rules for composition. This paper further studies the representation \\emph{hierarchy} through the learning process, and supports \\emph{adaptive} 2D memory selection in the resulting language model. This leads to a deeper-level interpretability that expands from individual rules to a dynamic system of rules, making the entire rule learning process more cognitive. The outcome is a new rover, MUS-ROVER \\RN{2}, trained on Bach's chorales, which outputs customizable syllabi for learning compositional rules. We demonstrate comparable results to our music pedagogy, while also presenting the differences and variations. In addition, we point out the rover's potential usages in style recognition and synthesis, as well as applications beyond music",
    "checked": true,
    "id": "0ff1dbf5c5851eed853b85c34c52331759613581",
    "semantic_title": "towards deep interpretability (mus-rover ii): learning hierarchical representations of tonal music",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=HyGTuv9eg": {
    "title": "Incorporating long-range consistency in CNN-based texture generation",
    "volume": "poster",
    "abstract": "Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer",
    "checked": true,
    "id": "60520d06b5540c15effa403d0c63b98a2a49159d",
    "semantic_title": "incorporating long-range consistency in cnn-based texture generation",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=HkljfjFee": {
    "title": "Support Regularized Sparse Coding and Its Fast Encoder",
    "volume": "poster",
    "abstract": "Sparse coding represents a signal by a linear combination of only a few atoms of a learned over-complete dictionary. While sparse coding exhibits compelling performance for various machine learning tasks, the process of obtaining sparse code with fixed dictionary is independent for each data point without considering the geometric information and manifold structure of the entire data. We propose Support Regularized Sparse Coding (SRSC) which produces sparse codes that account for the manifold structure of the data by encouraging nearby data in the manifold to choose similar dictionary atoms. In this way, the obtained support regularized sparse codes capture the locally linear structure of the data manifold and enjoy robustness to data noise. We present the optimization algorithm of SRSC with theoretical guarantee for the optimization over the sparse codes. We also propose a feed-forward neural network termed Deep Support Regularized Sparse Coding (Deep-SRSC) as a fast encoder to approximate the sparse codes generated by SRSC. Extensive experimental results demonstrate the effectiveness of SRSC and Deep-SRSC",
    "checked": true,
    "id": "d580cf2cf9493b67d08fa8b2300f3bbc9ed864b7",
    "semantic_title": "support regularized sparse coding and its fast encoder",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gtu5ilg": {
    "title": "Transfer of View-manifold Learning to Similarity Perception of Novel Objects",
    "volume": "poster",
    "abstract": "We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks",
    "checked": true,
    "id": "01fce96b99aedfb5d041ef4411ad8e9699b2c4df",
    "semantic_title": "transfer of view-manifold learning to similarity perception of novel objects",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=S1LVSrcge": {
    "title": "Variable Computation in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks",
    "checked": true,
    "id": "6746a18b2820f757334f75bc95428b3ea58d6603",
    "semantic_title": "variable computation in recurrent neural networks",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=r1YNw6sxg": {
    "title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration",
    "volume": "poster",
    "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing",
    "checked": true,
    "id": "a82201136fd3e835fbe240368047feeed62a084e",
    "semantic_title": "learning visual servoing with deep features and fitted q-iteration",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ1kmv9xx": {
    "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation",
    "volume": "poster",
    "abstract": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with conventional gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than baseline GANs",
    "checked": true,
    "id": "c789f8e3f40f57335aef2565f4d2bac69d2941d8",
    "semantic_title": "lr-gan: layered recursive generative adversarial networks for image generation",
    "citation_count": 241,
    "authors": []
  },
  "https://openreview.net/forum?id=BkLhzHtlg": {
    "title": "Learning Recurrent Representations for Hierarchical Behavior Modeling",
    "volume": "poster",
    "abstract": "We propose a framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals, using a generative recurrent neural network. The network has a discriminative part (classifying actions) and a generative part (predicting motion), whose recurrent cells are laterally connected, allowing higher levels of the network to represent high level behavioral phenomena. We test our framework on two types of tracking data, fruit fly behavior and online handwriting. Our results show that 1) taking advantage of unlabeled sequences, by predicting future motion, significantly improves action detection performance when training labels are scarce, 2) the network learns to represent high level phenomena such as writer identity and fly gender, without supervision, and 3) simulated motion trajectories, generated by treating motion prediction as input to the network, look realistic and may be used to qualitatively evaluate whether the model has learnt generative control rules",
    "checked": true,
    "id": "b530755b7d4e1ce03d96812f5e065c30eab8538c",
    "semantic_title": "learning recurrent representations for hierarchical behavior modeling",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=r1LXit5ee": {
    "title": "Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement",
    "volume": "poster",
    "abstract": "We consider scenarios from the real-time strategy game StarCraft as benchmarks for reinforcement learning algorithms. We focus on micromanagement, that is, the short-term, low-level control of team members during a battle. We propose several scenarios that are challenging for reinforcement learning algorithms because the state- action space is very large, and there is no obvious feature representation for the value functions. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. We also present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm collects traces for learning using deterministic policies, which appears much more efficient than, e.g., -greedy exploration. Experiments show that this algorithm allows to successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle",
    "checked": true,
    "id": "454d0168492917fdb87f2ed034686c8edd4cbe90",
    "semantic_title": "episodic exploration for deep deterministic policies for starcraft micromanagement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJMGPrcle": {
    "title": "Learning to Navigate in Complex Environments",
    "volume": "poster",
    "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks to bootstrap learning. In particular we consider jointly learning the goal-driven reinforcement learning problem with an unsupervised depth prediction task and a self-supervised loop closure classification task. Using this approach we can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, that show that the agent implicitly learns key navigation abilities, with only sparse rewards and without direct supervision",
    "checked": true,
    "id": "d35b05f440b5ba00d9429139edef7182bf9f7ce7",
    "semantic_title": "learning to navigate in complex environments",
    "citation_count": 883,
    "authors": []
  },
  "https://openreview.net/forum?id=S1dIzvclg": {
    "title": "A recurrent neural network without chaos",
    "volume": "poster",
    "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior",
    "checked": true,
    "id": "f451d0212e65ab9970a58b584b3363a853c9811c",
    "semantic_title": "a recurrent neural network without chaos",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=ryh9pmcee": {
    "title": "Energy-based Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images",
    "checked": true,
    "id": "0496691c6d54fd746c111d1004349acf7654145e",
    "semantic_title": "calibrating energy-based generative adversarial networks",
    "citation_count": 111,
    "authors": []
  },
  "https://openreview.net/forum?id=S1c2cvqee": {
    "title": "Designing Neural Network Architectures using Reinforcement Learning",
    "volume": "poster",
    "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks",
    "checked": true,
    "id": "6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
    "semantic_title": "designing neural network architectures using reinforcement learning",
    "citation_count": 1476,
    "authors": []
  },
  "https://openreview.net/forum?id=HkE0Nvqlg": {
    "title": "Structured Attention Networks",
    "volume": "poster",
    "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention",
    "checked": true,
    "id": "13d9323a8716131911bfda048a40e2cde1a76a46",
    "semantic_title": "structured attention networks",
    "citation_count": 465,
    "authors": []
  },
  "https://openreview.net/forum?id=BJC_jUqxe": {
    "title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
    "semantic_title": "a structured self-attentive sentence embedding",
    "citation_count": 2151,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ8Je4clg": {
    "title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "85d8b1b3483c7f4db999e7cf6b3e6231954c43dc",
    "semantic_title": "learning to play in a day: faster deep reinforcement learning by optimality tightening",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=ryrGawqex": {
    "title": "Deep Learning with Dynamic Computation Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7176fdb1c3618c26d331bc49e0d8d0874d2555ed",
    "semantic_title": "deep learning with dynamic computation graphs",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=HyxQzBceg": {
    "title": "Deep Variational Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c",
    "semantic_title": "deep variational information bottleneck",
    "citation_count": 1756,
    "authors": []
  },
  "https://openreview.net/forum?id=ryHlUtqge": {
    "title": "Generalizing Skills with Semi-Supervised Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4603f26093d64fc4107d3ec49667003f60210654",
    "semantic_title": "generalizing skills with semi-supervised reinforcement learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=S1oWlN9ll": {
    "title": "Loss-aware Binarization of Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43e2519d094bff2fc10c3b9ea944a2457e3884be",
    "semantic_title": "loss-aware binarization of deep networks",
    "citation_count": 220,
    "authors": []
  },
  "https://openreview.net/forum?id=B1-q5Pqxl": {
    "title": "Machine Comprehension Using Match-LSTM and Answer Pointer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff1861b71eaedba46cb679bbe2c585dbe18f9b19",
    "semantic_title": "machine comprehension using match-lstm and answer pointer",
    "citation_count": 595,
    "authors": []
  },
  "https://openreview.net/forum?id=B1s6xvqlx": {
    "title": "Recurrent Environment Simulators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bb430ec2f25e4a1513073a2a4098cbb942c2e3e0",
    "semantic_title": "recurrent environment simulators",
    "citation_count": 214,
    "authors": []
  },
  "https://openreview.net/forum?id=ry18Ww5ee": {
    "title": "Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "04fe6b11280c79b91c060934be66856877e532c6",
    "semantic_title": "hyperband: bandit-based configuration evaluation for hyperparameter optimization",
    "citation_count": 176,
    "authors": []
  },
  "https://openreview.net/forum?id=S1di0sfgl": {
    "title": "Hierarchical Multiscale Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
    "semantic_title": "hierarchical multiscale recurrent neural networks",
    "citation_count": 538,
    "authors": []
  },
  "https://openreview.net/forum?id=HkYhZDqxg": {
    "title": "Tree-structured decoding with doubly-recurrent neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9c69926bdb72912725d20a55af7147f86bed01ae",
    "semantic_title": "tree-structured decoding with doubly-recurrent neural networks",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=Hku9NK5lx": {
    "title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f6422dbb45f1bb183da514517f224b7a6e79f87",
    "semantic_title": "training compressed fully-connected networks with a density-diversity penalty",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk-oDY9ge": {
    "title": "Diet Networks: Thin Parameters for Fat Genomics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a36d5dee672e66b5f4db9e9df7498ef89b4b9cf5",
    "semantic_title": "diet networks: thin parameters for fat genomic",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=BJhZeLsxx": {
    "title": "What does it take to generate natural textures?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "abdd228205f0878ee0a426d8ef94ab15db4a07e8",
    "semantic_title": "what does it take to generate natural textures?",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=rkFBJv9gg": {
    "title": "Learning Features of Music From Scratch",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6154535699c65633243c482d2b97d4b66036633",
    "semantic_title": "learning features of music from scratch",
    "citation_count": 208,
    "authors": []
  },
  "https://openreview.net/forum?id=SJvYgH9xe": {
    "title": "Automatic Rule Extraction from Long Short Term Memory Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57b9ec1b2c4ff17baed4ecca932880fd13eb5d19",
    "semantic_title": "automatic rule extraction from long short term memory networks",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk8TGSKlg": {
    "title": "Reasoning with Memory Augmented Neural Networks for Language Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "24021461b49c726606bb9acdedd05cea5277c491",
    "semantic_title": "reasoning with memory augmented neural networks for language comprehension",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=HJpfMIFll": {
    "title": "Geometry of Polysemy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57a94c641412a39d4caa6b1a080ed3dae7ea75c5",
    "semantic_title": "geometry of polysemy",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk2Im59ex": {
    "title": "Unsupervised Cross-Domain Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b4ee64022cc3ccd14c7f9d4935c59b16456067d3",
    "semantic_title": "unsupervised cross-domain image generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyq4yhile": {
    "title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f710932a0aa99f2c2ddbec6e7765f10c48d3fc2",
    "semantic_title": "learning invariant feature spaces to transfer skills with reinforcement learning",
    "citation_count": 271,
    "authors": []
  },
  "https://openreview.net/forum?id=B1YfAfcgl": {
    "title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6583fe9c9dc52bb129aff4cefc60519349f3b4c",
    "semantic_title": "entropy-sgd: biasing gradient descent into wide valleys",
    "citation_count": 783,
    "authors": []
  },
  "https://openreview.net/forum?id=Sks9_ajex": {
    "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f7b032a4df721d4ed2bab97f6acd33d62477b7a5",
    "semantic_title": "paying more attention to attention: improving the performance of convolutional neural networks via attention transfer",
    "citation_count": 2621,
    "authors": []
  },
  "https://openreview.net/forum?id=S1TER2oll": {
    "title": "FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0ef8ba551c025747333382bb627f305b540543dc",
    "semantic_title": "filter shaping for convolutional neural networks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=BydrOIcle": {
    "title": "Unrolled Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "488bb25e0b1777847f04c943e6dbc4f84415b712",
    "semantic_title": "unrolled generative adversarial networks",
    "citation_count": 1012,
    "authors": []
  },
  "https://openreview.net/forum?id=SJDaqqveg": {
    "title": "An Actor-Critic Algorithm for Sequence Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d24a0695c9fc669e643bad51d4e14f056329dec",
    "semantic_title": "an actor-critic algorithm for sequence prediction",
    "citation_count": 643,
    "authors": []
  },
  "https://openreview.net/forum?id=BJAFbaolg": {
    "title": "Learning to Generate Samples from Noise through Infusion Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d2860bb05f747e4628e95e4d84018263831bab0d",
    "semantic_title": "learning to generate samples from noise through infusion training",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy6b4Pqee": {
    "title": "Deep Probabilistic Programming",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8e9532262bb862c62d2d011a93da014da2d97ee9",
    "semantic_title": "deep probabilistic programming",
    "citation_count": 194,
    "authors": []
  },
  "https://openreview.net/forum?id=r1G4z8cge": {
    "title": "Mollifying Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b9728a8279c12f93fff089b6fac96afd2d3bab04",
    "semantic_title": "mollifying networks",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=ry_sjFqgx": {
    "title": "Program Synthesis for Character Level Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "de7e3537d974c2ca28e3ca130d531e5eb23eb79f",
    "semantic_title": "program synthesis for character level language modeling",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=ryxB0Rtxx": {
    "title": "Identity Matters in Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8fbb115c578e8bfbcc1615bd7af990396abf6776",
    "semantic_title": "identity matters in deep learning",
    "citation_count": 401,
    "authors": []
  },
  "https://openreview.net/forum?id=r1rhWnZkg": {
    "title": "Hadamard Product for Low-rank Bilinear Pooling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "f05cea9192e2e584c2efdefcfd8fe2208560bfb0",
    "semantic_title": "mett vii  7 th workshop on matrix equations and tensor techniques pisa , 13  14 feb 2017",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkVsEMYel": {
    "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "989e261e232c51cc48aa07064c573316f9f818c3",
    "semantic_title": "inductive bias of deep convolutional networks through pooling geometry",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=rJqFGTslg": {
    "title": "Pruning Filters for Efficient ConvNets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
    "semantic_title": "pruning filters for efficient convnets",
    "citation_count": 3741,
    "authors": []
  },
  "https://openreview.net/forum?id=SkxKPDv5xl": {
    "title": "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e221e2c2ca8bd74a7b818406c8a2a342760e7d65",
    "semantic_title": "samplernn: an unconditional end-to-end neural audio generation model",
    "citation_count": 599,
    "authors": []
  },
  "https://openreview.net/forum?id=SkhU2fcll": {
    "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "468a80bcd4ff9b3f47beb9145ff81140777bb3f3",
    "semantic_title": "deep multi-task representation learning: a tensor factorisation approach",
    "citation_count": 255,
    "authors": []
  },
  "https://openreview.net/forum?id=SyWvgP5el": {
    "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9228fa3b363229780da4cb1d258942e0c13c2947",
    "semantic_title": "epopt: learning robust neural network policies using model ensembles",
    "citation_count": 355,
    "authors": []
  },
  "https://openreview.net/forum?id=ryuxYmvel": {
    "title": "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1318d15d7d8ab626b92d0c70dbdc5b5d37e223f",
    "semantic_title": "holstep: a machine learning dataset for higher-order logic theorem proving",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=HJDBUF5le": {
    "title": "Towards a Neural Statistician",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "405c31c85a324942811f3c9dc53ce3528f9284df",
    "semantic_title": "towards a neural statistician",
    "citation_count": 427,
    "authors": []
  },
  "https://openreview.net/forum?id=BylSPv9gx": {
    "title": "Exploring Sparsity in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1cc2f313bcb3b106af081f7031b924c9ad2662bd",
    "semantic_title": "exploring sparsity in recurrent neural networks",
    "citation_count": 313,
    "authors": []
  },
  "https://openreview.net/forum?id=Skq89Scxx": {
    "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b022f2a277a4bf5f42382e86e4380b96340b9e86",
    "semantic_title": "sgdr: stochastic gradient descent with warm restarts",
    "citation_count": 8366,
    "authors": []
  },
  "https://openreview.net/forum?id=rJEgeXFex": {
    "title": "Predicting Medications from Diagnostic Codes with Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e6c752c47d96ff83f44109a4cabd85c8294a37e",
    "semantic_title": "predicting medications from diagnostic codes with recurrent neural networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=ByqiJIqxg": {
    "title": "Online Bayesian Transfer Learning for Sequential Data Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20be936439c626c11611ce62771e8b8ba3c797f9",
    "semantic_title": "online bayesian transfer learning for sequential data modeling",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=BJh6Ztuxl": {
    "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e44da7d8c71edcc6e575fa7faadd5e75785a7901",
    "semantic_title": "fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
    "citation_count": 549,
    "authors": []
  },
  "https://openreview.net/forum?id=rJTKKKqeg": {
    "title": "Tracking the World State with Recurrent Entity Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "033dd6cf61a6017e9aa9b46068d3c89082849cf3",
    "semantic_title": "tracking the world state with recurrent entity networks",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=Byj72udxe": {
    "title": "Pointer Sentinel Mixture Models",
    "volume": "poster",
    "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and corpora we also introduce the freely available WikiText corpus",
    "checked": true,
    "id": "efbd381493bb9636f489b965a2034d529cd56bcd",
    "semantic_title": "pointer sentinel mixture models",
    "citation_count": 2981,
    "authors": []
  },
  "https://openreview.net/forum?id=H12GRgcxg": {
    "title": "Training deep neural-networks using a noise adaptation layer",
    "volume": "poster",
    "abstract": "The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise and to estimate the correct label. In this study we present a neural-network approach that optimizes the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended to the case where the noisy labels are dependent on the features in addition to the correct labels. Experimental results demonstrate that this approach outperforms previous methods",
    "checked": true,
    "id": "bc550ee45f4194f86c52152c10d302965c3563ca",
    "semantic_title": "training deep neural-networks using a noise adaptation layer",
    "citation_count": 737,
    "authors": []
  },
  "https://openreview.net/forum?id=HyM25Mqel": {
    "title": "Sample Efficient Actor-Critic with Experience Replay",
    "volume": "poster",
    "abstract": "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method",
    "checked": true,
    "id": "6a43d91c8d883e3463b358571125fa0ec7298b3a",
    "semantic_title": "sample efficient actor-critic with experience replay",
    "citation_count": 765,
    "authors": []
  },
  "https://openreview.net/forum?id=SJkXfE5xx": {
    "title": "Revisiting Classifier Two-Sample Tests",
    "volume": "poster",
    "abstract": "The goal of two-sample tests is to assess whether two samples, $S_P \\sim P^n$ and $S_Q \\sim Q^m$, are drawn from the same distribution. Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the $n$ examples in $S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a negative label. If the null hypothesis ``$P = Q$'' is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. As we will show, such \\emph{Classifier Two-Sample Tests} (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where $P$ and $Q$ differ. The goal of this paper is to establish the properties, performance, and uses of C2ST. First, we analyze their main theoretical properties. Second, we compare their performance against a variety of state-of-the-art alternatives. Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs). Fourth, we showcase the novel application of GANs together with C2ST for causal discovery",
    "checked": true,
    "id": "cfec1e2e670bc5de1a0621f66eaed7abb56387f3",
    "semantic_title": "revisiting classifier two-sample tests",
    "citation_count": 417,
    "authors": []
  },
  "https://openreview.net/forum?id=BJrFC6ceg": {
    "title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications",
    "volume": "poster",
    "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications",
    "checked": true,
    "id": "2e77b99e8bd10b9e4551a780c0bde9dd10fdbe9b",
    "semantic_title": "pixelcnn++: improving the pixelcnn with discretized logistic mixture likelihood and other modifications",
    "citation_count": 953,
    "authors": []
  },
  "https://openreview.net/forum?id=rkE3y85ee": {
    "title": "Categorical Reparameterization with Gumbel-Softmax",
    "volume": "poster",
    "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification",
    "checked": true,
    "id": "29e944711a354c396fad71936f536e83025b6ce0",
    "semantic_title": "categorical reparameterization with gumbel-softmax",
    "citation_count": 5478,
    "authors": []
  },
  "https://openreview.net/forum?id=SkB-_mcel": {
    "title": "Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning",
    "volume": "poster",
    "abstract": "The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been considered before. We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w. r. t. parameter changes in a certain interval. The source code of the experiments is publicly available",
    "checked": true,
    "id": "01dc0a157e355ddc34a426f121fc871601fda567",
    "semantic_title": "central moment discrepancy (cmd) for domain-invariant representation learning",
    "citation_count": 588,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOvsIqeg": {
    "title": "Regularizing CNNs with Locally Constrained Decorrelations",
    "volume": "poster",
    "abstract": "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN",
    "checked": true,
    "id": "2ed2415c39d4023f9af410947b96970803d9f192",
    "semantic_title": "regularizing cnns with locally constrained decorrelations",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ewdt9xe": {
    "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
    "volume": "poster",
    "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. These results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure",
    "checked": true,
    "id": "ad367b44f3434b9ba6b46b41ab083210f6827a9f",
    "semantic_title": "deep predictive coding networks for video prediction and unsupervised learning",
    "citation_count": 937,
    "authors": []
  },
  "https://openreview.net/forum?id=S1jmAotxg": {
    "title": "Stick-Breaking Variational Autoencoders",
    "volume": "poster",
    "abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semi-supervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's",
    "checked": true,
    "id": "cdc9b1759ee485e34045dbe8de947967d259d191",
    "semantic_title": "stick-breaking variational autoencoders",
    "citation_count": 163,
    "authors": []
  },
  "https://openreview.net/forum?id=SkpSlKIel": {
    "title": "Why Deep Neural Networks for Function Approximation?",
    "volume": "poster",
    "abstract": "Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\\varepsilon$) require $\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\\varepsilon$) require $\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU",
    "checked": true,
    "id": "87524c2bb725f967b7fac73442d6412b43d422d1",
    "semantic_title": "why deep neural networks for function approximation?",
    "citation_count": 386,
    "authors": []
  },
  "https://openreview.net/forum?id=H1acq85gx": {
    "title": "Maximum Entropy Flow Networks",
    "volume": "poster",
    "abstract": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, rather than the traditional method of optimizing over the continuous density directly, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks",
    "checked": true,
    "id": "b88e7d3ecbc62baaf3959fb62702e68790e2bfa6",
    "semantic_title": "maximum entropy flow networks",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=SyQq185lg": {
    "title": "Latent Sequence Decompositions",
    "volume": "poster",
    "abstract": "Sequence-to-sequence models rely on a fixed decomposition of the target sequences into a sequence of tokens that may be words, word-pieces or characters. The choice of these tokens and the decomposition of the target sequences into a sequence of tokens is often static, and independent of the input, output data domains. This can potentially lead to a sub-optimal choice of token dictionaries, as the decomposition is not informed by the particular problem being solved. In this paper we present Latent Sequence Decompositions (LSD), a framework in which the decomposition of sequences into constituent tokens is learnt during the training of the model. The decomposition depends both on the input sequence and on the output sequence. In LSD, during training, the model samples decompositions incrementally, from left to right by locally sampling between valid extensions. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve a WER of 9.6%",
    "checked": true,
    "id": "105788dd22393d5a4333c167814ec3d38c7d6612",
    "semantic_title": "latent sequence decompositions",
    "citation_count": 62,
    "authors": []
  },
  "https://openreview.net/forum?id=HJWHIKqgl": {
    "title": "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy",
    "volume": "poster",
    "abstract": "We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier",
    "checked": true,
    "id": "d772a21f2f687985b41bec2dc47bc2760c57ed2f",
    "semantic_title": "generative models and model criticism via optimized maximum mean discrepancy",
    "citation_count": 263,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Igu2ogg": {
    "title": "Efficient Vector Representation for Documents through Corruption",
    "volume": "poster",
    "abstract": "We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time",
    "checked": true,
    "id": "1da8eabfd02436dda5f3131fe404dc840a04e6cc",
    "semantic_title": "efficient vector representation for documents through corruption",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=ByldLrqlx": {
    "title": "DeepCoder: Learning to Write Programs",
    "volume": "poster",
    "abstract": "We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites",
    "checked": true,
    "id": "8a25c9403d8a0e2fb8ca362a1b26262afd57417f",
    "semantic_title": "deepcoder: learning to write programs",
    "citation_count": 577,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzCSf9xg": {
    "title": "On Detecting Adversarial Perturbations",
    "volume": "poster",
    "abstract": "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small ``detector'' subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack",
    "checked": true,
    "id": "061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a",
    "semantic_title": "on detecting adversarial perturbations",
    "citation_count": 955,
    "authors": []
  },
  "https://openreview.net/forum?id=SJTQLdqlg": {
    "title": "Learning to Remember Rare Events",
    "volume": "poster",
    "abstract": "Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task",
    "checked": true,
    "id": "29092f0deaac3898e43b3f094bf15d82b6a99afd",
    "semantic_title": "learning to remember rare events",
    "citation_count": 366,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VyHY9gg": {
    "title": "Data Noising as Smoothing in Neural Network Language Models",
    "volume": "poster",
    "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing",
    "checked": true,
    "id": "2d5069a99bfa0b47c095bbb5cefd6dba974f72a7",
    "semantic_title": "data noising as smoothing in neural network language models",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=S1_pAu9xl": {
    "title": "Trained Ternary Quantization",
    "volume": "poster",
    "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it's as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%",
    "checked": true,
    "id": "d418295cd3027c43eccc5592ae5b8303ba8192be",
    "semantic_title": "trained ternary quantization",
    "citation_count": 1037,
    "authors": []
  },
  "https://openreview.net/forum?id=r10FA8Kxg": {
    "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "def52f9e3650ec4e9cc18b3e316f48e9bf046adf",
    "semantic_title": "do deep convolutional nets really need to be deep and convolutional?",
    "citation_count": 246,
    "authors": []
  },
  "https://openreview.net/forum?id=BysvGP5ee": {
    "title": "Variational Lossy Autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "590c9a1ff422b03477f7830b20609f212c85aa13",
    "semantic_title": "variational lossy autoencoder",
    "citation_count": 679,
    "authors": []
  },
  "https://openreview.net/forum?id=Byk-VI9eg": {
    "title": "Generative Multi-Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "423fab20c1c39cb6b4d6cf6098f61c092e6d1e3f",
    "semantic_title": "generative multi-adversarial networks",
    "citation_count": 346,
    "authors": []
  },
  "https://openreview.net/forum?id=HyQJ-mclg": {
    "title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "407ead18083e68626e82e07db1a9289ff0b7e862",
    "semantic_title": "incremental network quantization: towards lossless cnns with low-precision weights",
    "citation_count": 1061,
    "authors": []
  },
  "https://openreview.net/forum?id=HkEI22jeg": {
    "title": "Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f8bed08cef7b777da9273f483461ebf4f2e0c63",
    "semantic_title": "multilayer recurrent network models of primate retinal ganglion cell responses",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkab5dqxe": {
    "title": "A Compositional Object-Based Approach to Learning Physical Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a1786540a4e15f0757e1b84a02f98ed436a969e0",
    "semantic_title": "a compositional object-based approach to learning physical dynamics",
    "citation_count": 441,
    "authors": []
  },
  "https://openreview.net/forum?id=rkEFLFqee": {
    "title": "Decomposing Motion and Content for Natural Video Sequence Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8375ff50b8a6f1a10dd809129a18df96888ac8b",
    "semantic_title": "decomposing motion and content for natural video sequence prediction",
    "citation_count": 601,
    "authors": []
  },
  "https://openreview.net/forum?id=BybtVK9lg": {
    "title": "Autoencoding Variational Inference For Topic Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0c25e5f544167840756950e0ff3edd238a19cfd1",
    "semantic_title": "autoencoding variational inference for topic models",
    "citation_count": 568,
    "authors": []
  },
  "https://openreview.net/forum?id=Sys6GJqxl": {
    "title": "Delving into Transferable Adversarial Examples and Black-box Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99e5a8c10cf92749d4a7c2949691c3a6046e499a",
    "semantic_title": "delving into transferable adversarial examples and black-box attacks",
    "citation_count": 1761,
    "authors": []
  },
  "https://openreview.net/forum?id=ry4Vrt5gl": {
    "title": "Learning to Optimize",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ead9a671428631e44f6fe49324efe69da628bc47",
    "semantic_title": "learning to optimize",
    "citation_count": 258,
    "authors": []
  },
  "https://openreview.net/forum?id=SJGPL9Dex": {
    "title": "Understanding Trainable Sparse Coding with Matrix Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9459d92b99220d2bfe8fc530410275d65b71dcd2",
    "semantic_title": "understanding trainable sparse coding via matrix factorization",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJKxrsgl": {
    "title": "Emergence of foveal image sampling from learning to attend in visual scenes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d49a7ad76e779df812bda5c4b037a0b9d6847be",
    "semantic_title": "emergence of foveal image sampling from learning to attend in visual scenes",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=ryT4pvqll": {
    "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9caf76734a611286513b478ce10557bda477e8ce",
    "semantic_title": "improving policy gradient by exploring under-appreciated rewards",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ0-tY5xe": {
    "title": "Learning to Query, Reason, and Answer Questions On Ambiguous Texts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf3db68c31e9850d6039bc2436314faef93973d3",
    "semantic_title": "learning to query, reason, and answer questions on ambiguous texts",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ0JwFcex": {
    "title": "Neuro-Symbolic Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "644ca74f80463415613847ab01cff067fb58f0ad",
    "semantic_title": "neuro-symbolic program synthesis",
    "citation_count": 322,
    "authors": []
  },
  "https://openreview.net/forum?id=HyoST_9xl": {
    "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "950619635df80e87c6f25b486cc5eaad4d71d0b0",
    "semantic_title": "dsd: dense-sparse-dense training for deep neural networks",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=BJKYvt5lg": {
    "title": "PixelVAE: A Latent Variable Model for Natural Images",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "42e9055ec712ec9c7f0a79d963ea034a72dc7fa8",
    "semantic_title": "pixelvae: a latent variable model for natural images",
    "citation_count": 341,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk95PK9le": {
    "title": "Deep Biaffine Attention for Neural Dependency Parsing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2",
    "semantic_title": "deep biaffine attention for neural dependency parsing",
    "citation_count": 1226,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ8uNptgl": {
    "title": "Towards the Limit of Network Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d749712ef4f02a87f4e362058469a1da46c8bcc",
    "semantic_title": "towards the limit of network quantization",
    "citation_count": 195,
    "authors": []
  },
  "https://openreview.net/forum?id=B1oK8aoxe": {
    "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3deecaee4ec1a37de3cb10420eaabff067669e17",
    "semantic_title": "stochastic neural networks for hierarchical reinforcement learning",
    "citation_count": 361,
    "authors": []
  },
  "https://openreview.net/forum?id=rJeKjwvclx": {
    "title": "Dynamic Coattention Networks For Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e978d832a4d86571e1b52aa1685dc32ccb250f50",
    "semantic_title": "dynamic coattention networks for question answering",
    "citation_count": 685,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkg4TI9xl": {
    "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ff2a434578ff2746b9283e45abf296887f48a2d",
    "semantic_title": "a baseline for detecting misclassified and out-of-distribution examples in neural networks",
    "citation_count": 3537,
    "authors": []
  },
  "https://openreview.net/forum?id=BJtNZAFgg": {
    "title": "Adversarial Feature Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1db6e3078597386ac4222ba6c3f4f61b61f53539",
    "semantic_title": "adversarial feature learning",
    "citation_count": 1838,
    "authors": []
  },
  "https://openreview.net/forum?id=rkGabzZgl": {
    "title": "Dropout with Expectation-linear Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "437da3d1024f7312fde8a5287c99c36b559ed8f3",
    "semantic_title": "dropout with expectation-linear regularization",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=rJPcZ3txx": {
    "title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fe9bc9944984cbc6f9f12e99c1065a47d662e24c",
    "semantic_title": "faster cnns with direct sparse convolutions and guided pruning",
    "citation_count": 184,
    "authors": []
  },
  "https://openreview.net/forum?id=SkkTMpjex": {
    "title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0c7a93498360ce6c0ff1a7c37fd9b6aa0c054f85",
    "semantic_title": "distributed second-order optimization using kronecker-factored approximations",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=rJbbOLcex": {
    "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7ab2166f6cdb1737e000df66d29c6538afc6811d",
    "semantic_title": "topicrnn: a recurrent neural network with long-range semantic dependency",
    "citation_count": 245,
    "authors": []
  },
  "https://openreview.net/forum?id=B1M8JF9xx": {
    "title": "On the Quantitative Analysis of Decoder-Based Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "858dc7408c27702ec42778599fc8d11f73ef3f76",
    "semantic_title": "on the quantitative analysis of decoder-based generative models",
    "citation_count": 224,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Aab85gg": {
    "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "02e45b3472fa4c879f8e9ca0dbb283c774a338cb",
    "semantic_title": "offline bilingual word vectors, orthogonal transformations and the inverted softmax",
    "citation_count": 539,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy6iJDqlx": {
    "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8bada64d0b04a66165b464a51c29f61e1f474f2a",
    "semantic_title": "attend, adapt and transfer: attentive deep architecture for adaptive transfer from multiple sources in the same domain",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=rkE8pVcle": {
    "title": "Learning through Dialogue Interactions by Asking Questions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2685cab753e4c1e9ac6e68b28ad8e2e1f3673f73",
    "semantic_title": "learning through dialogue interactions",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=SyVVJ85lg": {
    "title": "Paleo: A Performance Model for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "36348af997c8ff309fabe0561487ae97cf317ef4",
    "semantic_title": "paleo: a performance model for deep neural networks",
    "citation_count": 228,
    "authors": []
  },
  "https://openreview.net/forum?id=BJO-BuT1g": {
    "title": "A Learned Representation For Artistic Style",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99542f614d7e4146cad17196e76c997e57a69e4d",
    "semantic_title": "a learned representation for artistic style",
    "citation_count": 1172,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ5UeU9xx": {
    "title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "29069976eb7f828de91ed243cd12fd99fef56d94",
    "semantic_title": "visualizing deep neural network decisions: prediction difference analysis",
    "citation_count": 713,
    "authors": []
  },
  "https://openreview.net/forum?id=r1aPbsFle": {
    "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "424aef7340ee618132cc3314669400e23ad910ba",
    "semantic_title": "tying word vectors and word classifiers: a loss framework for language modeling",
    "citation_count": 386,
    "authors": []
  },
  "https://openreview.net/forum?id=HJgXCV9xx": {
    "title": "Dialogue Learning With Human-in-the-Loop",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3d3b4ec7789f634e0752d50484dad7d2ea2460d5",
    "semantic_title": "dialogue learning with human-in-the-loop",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ0UKP9ge": {
    "title": "Bidirectional Attention Flow for Machine Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
    "semantic_title": "bidirectional attention flow for machine comprehension",
    "citation_count": 2095,
    "authors": []
  },
  "https://openreview.net/forum?id=Skvgqgqxe": {
    "title": "Learning to Compose Words into Sentences with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "599f7863721d542dcef2da49b41d82b21e4f80b3",
    "semantic_title": "learning to compose words into sentences with reinforcement learning",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=BJK3Xasel": {
    "title": "Nonparametric Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46357ea4ad096e0674569efa29d3e34006786d04",
    "semantic_title": "nonparametric neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ6oOfqge": {
    "title": "Temporal Ensembling for Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d2e4587744a89bad95fea69e08842cad6c8ff0dd",
    "semantic_title": "temporal ensembling for semi-supervised learning",
    "citation_count": 2587,
    "authors": []
  },
  "https://openreview.net/forum?id=SJU4ayYgl": {
    "title": "Semi-Supervised Classification with Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "36eff562f65125511b5dfab68ce7f7a943c27478",
    "semantic_title": "semi-supervised classification with graph convolutional networks",
    "citation_count": 29682,
    "authors": []
  },
  "https://openreview.net/forum?id=By5e2L9gl": {
    "title": "Trusting SVM for Piecewise Linear CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cd4cd2ca053d8b7d5becbf4abf90a62a9ba80c1c",
    "semantic_title": "trusting svm for piecewise linear cnns",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Byiy-Pqlx": {
    "title": "Lie-Access Neural Turing Machines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "15778857f5b4de361dffa3fd156a269184dd2f9e",
    "semantic_title": "lie-access neural turing machines",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=r1X3g2_xl": {
    "title": "Adversarial Training Methods for Semi-Supervised Text Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2cd55ded95d5d13430edfa223ba591b514ebe8a5",
    "semantic_title": "adversarial training methods for semi-supervised text classification",
    "citation_count": 1066,
    "authors": []
  },
  "https://openreview.net/forum?id=B1kJ6H9ex": {
    "title": "Combining policy gradient and Q-learning",
    "volume": "poster",
    "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning",
    "checked": true,
    "id": "c40dd8f235aabe6efbb93c59c0536adf491f9ead",
    "semantic_title": "pgq: combining policy gradient and q-learning",
    "citation_count": 140,
    "authors": []
  }
}